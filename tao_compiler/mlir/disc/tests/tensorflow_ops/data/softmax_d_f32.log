exec ${PAGER:-/usr/bin/less} "$0" || exit 1
Executing tests from //tensorflow/compiler/mlir/disc/tests/tensorflow_ops:softmax.cpp.test
-----------------------------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from TFSoftmaxOpTest
[ RUN      ] TFSoftmaxOpTest.FullyDynamicShape3DF32
2022-08-24 06:21:49.982450: I tensorflow/compiler/mlir/disc/tests/mlir_feature_test.cc:306] Testing for CUDA backend
2022-08-24 06:21:49.982542: I tensorflow/compiler/mlir/disc/tests/mlir_feature_test.cc:145] Original TF code: func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?x?xf32> attributes {tf.entry_function = {inputs = "{{INPUTS}}", outputs = "{{OUTPUTS}}", input_placements="{{INPUT_PLACEMENTS}}", output_placements="{{OUTPUT_PLACEMENTS}}"}} {
  %graph = tf_executor.graph {
    %1:2 = tf_executor.island wraps "tf.Softmax"(%arg0) : (tensor<?x?x?xf32>) -> tensor<?x?x?xf32>
    tf_executor.fetch %1 : tensor <?x?x?xf32>
  }
  return %graph : tensor<?x?x?xf32>
}
2022-08-24 06:21:49.982556: I tensorflow/compiler/mlir/disc/tests/mlir_feature_test.cc:149] New TF code: func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?x?xf32> attributes {tf.entry_function = {inputs = "input0", outputs = "output0", input_placements="gpu", output_placements="gpu"}} {
  %graph = tf_executor.graph {
    %1:2 = tf_executor.island wraps "tf.Softmax"(%arg0) : (tensor<?x?x?xf32>) -> tensor<?x?x?xf32>
    tf_executor.fetch %1 : tensor <?x?x?xf32>
  }
  return %graph : tensor<?x?x?xf32>
}
2022-08-24 06:21:49.982631: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:232] mlir_file_path: /root/.cache/bazel/_bazel_root/4cf787dd368afbf575ff6140b2d24691/execroot/org_tensorflow/_tmp/e877672218cbaf0733fb74fb1644a598/tempfile-330bb2f99ddd-844a7b9d-16839-5e6f6b239176e
2022-08-24 06:21:49.982634: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:233] tmp_dir: /root/.cache/bazel/_bazel_root/4cf787dd368afbf575ff6140b2d24691/execroot/org_tensorflow/_tmp/e877672218cbaf0733fb74fb1644a598
2022-08-24 06:21:49.982636: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:234] test_name: FullyDynamicShape3DF32_0
2022-08-24 06:21:50.057734: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:264] Executed: tensorflow/compiler/mlir/tf-opt --tf-standard-pipeline /root/.cache/bazel/_bazel_root/4cf787dd368afbf575ff6140b2d24691/execroot/org_tensorflow/_tmp/e877672218cbaf0733fb74fb1644a598/tempfile-330bb2f99ddd-844a7b9d-16839-5e6f6b239176e -o /root/.cache/bazel/_bazel_root/4cf787dd368afbf575ff6140b2d24691/execroot/org_tensorflow/_tmp/e877672218cbaf0733fb74fb1644a598FullyDynamicShape3DF32_0_tf_dialect.mlir 
2022-08-24 06:21:50.057748: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:265] tensorflow/compiler/mlir/tf-opt: 0
2022-08-24 06:21:50.057751: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:266] -- stdout:

============ END ============

2022-08-24 06:21:50.057753: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:268] -- stderr:
2022-08-24 06:21:50.025512: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.

============ END ============

2022-08-24 06:21:50.943059: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:264] Executed: tensorflow/compiler/mlir/disc/disc_compiler_main --mlir-print-elementsattrs-with-hex-if-larger -1 --mlir-elide-elementsattrs-if-larger 8 /root/.cache/bazel/_bazel_root/4cf787dd368afbf575ff6140b2d24691/execroot/org_tensorflow/_tmp/e877672218cbaf0733fb74fb1644a598FullyDynamicShape3DF32_0_tf_dialect.mlir /root/.cache/bazel/_bazel_root/4cf787dd368afbf575ff6140b2d24691/execroot/org_tensorflow/_tmp/e877672218cbaf0733fb74fb1644a598FullyDynamicShape3DF32_0.so 
2022-08-24 06:21:50.943080: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:265] tensorflow/compiler/mlir/disc/disc_compiler_main: 0
2022-08-24 06:21:50.943087: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:266] -- stdout:

============ END ============

2022-08-24 06:21:50.945602: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:268] -- stderr:
======== BEGIN Original Module =========
module {
  func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = "tf.Softmax"(%arg0) : (tensor<?x?x?xf32>) -> tensor<?x?x?xf32>
    return %0 : tensor<?x?x?xf32>
  }
}

======= END Original Module ==========
[[ INFO ]] Running TF2XLA
2022-08-24 06:21:50.101657: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
// -----// IR Dump After LegalizeTF (xla-legalize-tf) //----- //
func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %0 = mhlo.constant dense<-1> : tensor<1xi64>
  %1 = mhlo.convert %arg0 : tensor<?x?x?xf32>
  %2 = mhlo.constant dense<0xFF800000> : tensor<f32>
  %3 = mhlo.reduce(%1 init: %2) applies mhlo.maximum across dimensions = [2] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
  %4 = mhlo.convert %3 : tensor<?x?xf32>
  %cst = arith.constant dense<2> : tensor<1xi32>
  %5 = shape.shape_of %4 : tensor<?x?xf32> -> tensor<2xindex>
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %6 = tensor.extract %5[%c0] : tensor<2xindex>
  %c1_0 = arith.constant 1 : index
  %7 = tensor.extract %5[%c1_0] : tensor<2xindex>
  %8 = tensor.from_elements %6, %7, %c1 : tensor<3xindex>
  %9 = mhlo.dynamic_reshape %4, %8 : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x1xf32>
  %10 = shape.shape_of %arg0 : tensor<?x?x?xf32> -> tensor<3xindex>
  %11 = shape.shape_of %9 : tensor<?x?x1xf32> -> tensor<3xindex>
  %12 = shape.cstr_broadcastable %10, %11 : tensor<3xindex>, tensor<3xindex>
  %13 = shape.assuming %12 -> (tensor<?x?x?xf32>) {
    %28 = shape.shape_of %arg0 : tensor<?x?x?xf32> -> tensor<3xindex>
    %29 = shape.shape_of %9 : tensor<?x?x1xf32> -> tensor<3xindex>
    %30 = shape.broadcast %28, %29 : tensor<3xindex>, tensor<3xindex> -> tensor<3xindex>
    %31 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %30) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %32 = "mhlo.dynamic_broadcast_in_dim"(%9, %30) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x1xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %33 = mhlo.subtract %31, %32 : tensor<?x?x?xf32>
    shape.assuming_yield %33 : tensor<?x?x?xf32>
  }
  %14 = mhlo.exponential %13 : tensor<?x?x?xf32>
  %15 = mhlo.convert %14 : tensor<?x?x?xf32>
  %16 = mhlo.constant dense<-0.000000e+00> : tensor<f32>
  %17 = mhlo.reduce(%15 init: %16) applies mhlo.add across dimensions = [2] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
  %18 = mhlo.convert %17 : tensor<?x?xf32>
  %cst_1 = arith.constant dense<2> : tensor<1xi32>
  %19 = shape.shape_of %18 : tensor<?x?xf32> -> tensor<2xindex>
  %c1_2 = arith.constant 1 : index
  %c0_3 = arith.constant 0 : index
  %20 = tensor.extract %19[%c0_3] : tensor<2xindex>
  %c1_4 = arith.constant 1 : index
  %21 = tensor.extract %19[%c1_4] : tensor<2xindex>
  %22 = tensor.from_elements %20, %21, %c1_2 : tensor<3xindex>
  %23 = mhlo.dynamic_reshape %18, %22 : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x1xf32>
  %24 = shape.shape_of %14 : tensor<?x?x?xf32> -> tensor<3xindex>
  %25 = shape.shape_of %23 : tensor<?x?x1xf32> -> tensor<3xindex>
  %26 = shape.cstr_broadcastable %24, %25 : tensor<3xindex>, tensor<3xindex>
  %27 = shape.assuming %26 -> (tensor<?x?x?xf32>) {
    %28 = shape.shape_of %14 : tensor<?x?x?xf32> -> tensor<3xindex>
    %29 = shape.shape_of %23 : tensor<?x?x1xf32> -> tensor<3xindex>
    %30 = shape.broadcast %28, %29 : tensor<3xindex>, tensor<3xindex> -> tensor<3xindex>
    %31 = "mhlo.dynamic_broadcast_in_dim"(%14, %30) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %32 = "mhlo.dynamic_broadcast_in_dim"(%23, %30) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x1xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %33 = mhlo.divide %31, %32 : tensor<?x?x?xf32>
    shape.assuming_yield %33 : tensor<?x?x?xf32>
  }
  return %27 : tensor<?x?x?xf32>
}

// -----// IR Dump After DiscLowerTfPass (disc-lower-tf) //----- //
func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %0 = mhlo.constant dense<-0.000000e+00> : tensor<f32>
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %1 = mhlo.constant dense<0xFF800000> : tensor<f32>
  %2 = mhlo.reduce(%arg0 init: %1) applies mhlo.maximum across dimensions = [2] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
  %3 = shape.shape_of %2 : tensor<?x?xf32> -> tensor<2xindex>
  %4 = tensor.extract %3[%c0] : tensor<2xindex>
  %5 = tensor.extract %3[%c1] : tensor<2xindex>
  %6 = tensor.from_elements %4, %5, %c1 : tensor<3xindex>
  %7 = mhlo.dynamic_reshape %2, %6 : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x1xf32>
  %8 = shape.shape_of %arg0 : tensor<?x?x?xf32> -> tensor<3xindex>
  %9 = shape.shape_of %7 : tensor<?x?x1xf32> -> tensor<3xindex>
  %10 = shape.cstr_broadcastable %8, %9 : tensor<3xindex>, tensor<3xindex>
  %11 = shape.assuming %10 -> (tensor<?x?x?xf32>) {
    %23 = shape.shape_of %arg0 : tensor<?x?x?xf32> -> tensor<3xindex>
    %24 = shape.shape_of %7 : tensor<?x?x1xf32> -> tensor<3xindex>
    %25 = shape.broadcast %23, %24 : tensor<3xindex>, tensor<3xindex> -> tensor<3xindex>
    %26 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %25) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %27 = "mhlo.dynamic_broadcast_in_dim"(%7, %25) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x1xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %28 = mhlo.subtract %26, %27 : tensor<?x?x?xf32>
    shape.assuming_yield %28 : tensor<?x?x?xf32>
  }
  %12 = mhlo.exponential %11 : tensor<?x?x?xf32>
  %13 = mhlo.reduce(%12 init: %0) applies mhlo.add across dimensions = [2] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
  %14 = shape.shape_of %13 : tensor<?x?xf32> -> tensor<2xindex>
  %15 = tensor.extract %14[%c0] : tensor<2xindex>
  %16 = tensor.extract %14[%c1] : tensor<2xindex>
  %17 = tensor.from_elements %15, %16, %c1 : tensor<3xindex>
  %18 = mhlo.dynamic_reshape %13, %17 : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x1xf32>
  %19 = shape.shape_of %12 : tensor<?x?x?xf32> -> tensor<3xindex>
  %20 = shape.shape_of %18 : tensor<?x?x1xf32> -> tensor<3xindex>
  %21 = shape.cstr_broadcastable %19, %20 : tensor<3xindex>, tensor<3xindex>
  %22 = shape.assuming %21 -> (tensor<?x?x?xf32>) {
    %23 = shape.shape_of %12 : tensor<?x?x?xf32> -> tensor<3xindex>
    %24 = shape.shape_of %18 : tensor<?x?x1xf32> -> tensor<3xindex>
    %25 = shape.broadcast %23, %24 : tensor<3xindex>, tensor<3xindex> -> tensor<3xindex>
    %26 = "mhlo.dynamic_broadcast_in_dim"(%12, %25) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %27 = "mhlo.dynamic_broadcast_in_dim"(%18, %25) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x1xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %28 = mhlo.divide %26, %27 : tensor<?x?x?xf32>
    shape.assuming_yield %28 : tensor<?x?x?xf32>
  }
  return %22 : tensor<?x?x?xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %0 = mhlo.constant dense<-0.000000e+00> : tensor<f32>
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %1 = mhlo.constant dense<0xFF800000> : tensor<f32>
  %2 = mhlo.reduce(%arg0 init: %1) applies mhlo.maximum across dimensions = [2] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
  %3 = tensor.dim %2, %c0 : tensor<?x?xf32>
  %4 = tensor.dim %2, %c1 : tensor<?x?xf32>
  %5 = tensor.from_elements %3, %4, %c1 : tensor<3xindex>
  %6 = mhlo.dynamic_reshape %2, %5 : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x1xf32>
  %7 = shape.shape_of %arg0 : tensor<?x?x?xf32> -> tensor<3xindex>
  %8 = shape.cstr_broadcastable %7, %5 : tensor<3xindex>, tensor<3xindex>
  %9 = shape.assuming %8 -> (tensor<?x?x?xf32>) {
    %19 = shape.shape_of %arg0 : tensor<?x?x?xf32> -> tensor<3xindex>
    %20 = shape.broadcast %19, %5 : tensor<3xindex>, tensor<3xindex> -> tensor<3xindex>
    %21 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %20) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %22 = "mhlo.dynamic_broadcast_in_dim"(%6, %20) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x1xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %23 = mhlo.subtract %21, %22 : tensor<?x?x?xf32>
    shape.assuming_yield %23 : tensor<?x?x?xf32>
  }
  %10 = mhlo.exponential %9 : tensor<?x?x?xf32>
  %11 = mhlo.reduce(%10 init: %0) applies mhlo.add across dimensions = [2] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
  %12 = tensor.dim %11, %c0 : tensor<?x?xf32>
  %13 = tensor.dim %11, %c1 : tensor<?x?xf32>
  %14 = tensor.from_elements %12, %13, %c1 : tensor<3xindex>
  %15 = mhlo.dynamic_reshape %11, %14 : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x1xf32>
  %16 = shape.shape_of %10 : tensor<?x?x?xf32> -> tensor<3xindex>
  %17 = shape.cstr_broadcastable %16, %14 : tensor<3xindex>, tensor<3xindex>
  %18 = shape.assuming %17 -> (tensor<?x?x?xf32>) {
    %19 = shape.shape_of %10 : tensor<?x?x?xf32> -> tensor<3xindex>
    %20 = shape.broadcast %19, %14 : tensor<3xindex>, tensor<3xindex> -> tensor<3xindex>
    %21 = "mhlo.dynamic_broadcast_in_dim"(%10, %20) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %22 = "mhlo.dynamic_broadcast_in_dim"(%15, %20) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x1xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %23 = mhlo.divide %21, %22 : tensor<?x?x?xf32>
    shape.assuming_yield %23 : tensor<?x?x?xf32>
  }
  return %18 : tensor<?x?x?xf32>
}

======== BEGIN After TF2HLO =========
module {
  func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = mhlo.constant dense<-0.000000e+00> : tensor<f32>
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %1 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %2 = mhlo.reduce(%arg0 init: %1) applies mhlo.maximum across dimensions = [2] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
    %3 = tensor.dim %2, %c0 : tensor<?x?xf32>
    %4 = tensor.dim %2, %c1 : tensor<?x?xf32>
    %5 = tensor.from_elements %3, %4, %c1 : tensor<3xindex>
    %6 = mhlo.dynamic_reshape %2, %5 : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x1xf32>
    %7 = shape.shape_of %arg0 : tensor<?x?x?xf32> -> tensor<3xindex>
    %8 = shape.cstr_broadcastable %7, %5 : tensor<3xindex>, tensor<3xindex>
    %9 = shape.assuming %8 -> (tensor<?x?x?xf32>) {
      %19 = shape.shape_of %arg0 : tensor<?x?x?xf32> -> tensor<3xindex>
      %20 = shape.broadcast %19, %5 : tensor<3xindex>, tensor<3xindex> -> tensor<3xindex>
      %21 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %20) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
      %22 = "mhlo.dynamic_broadcast_in_dim"(%6, %20) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x1xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
      %23 = mhlo.subtract %21, %22 : tensor<?x?x?xf32>
      shape.assuming_yield %23 : tensor<?x?x?xf32>
    }
    %10 = mhlo.exponential %9 : tensor<?x?x?xf32>
    %11 = mhlo.reduce(%10 init: %0) applies mhlo.add across dimensions = [2] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
    %12 = tensor.dim %11, %c0 : tensor<?x?xf32>
    %13 = tensor.dim %11, %c1 : tensor<?x?xf32>
    %14 = tensor.from_elements %12, %13, %c1 : tensor<3xindex>
    %15 = mhlo.dynamic_reshape %11, %14 : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x1xf32>
    %16 = shape.shape_of %10 : tensor<?x?x?xf32> -> tensor<3xindex>
    %17 = shape.cstr_broadcastable %16, %14 : tensor<3xindex>, tensor<3xindex>
    %18 = shape.assuming %17 -> (tensor<?x?x?xf32>) {
      %19 = shape.shape_of %10 : tensor<?x?x?xf32> -> tensor<3xindex>
      %20 = shape.broadcast %19, %14 : tensor<3xindex>, tensor<3xindex> -> tensor<3xindex>
      %21 = "mhlo.dynamic_broadcast_in_dim"(%10, %20) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
      %22 = "mhlo.dynamic_broadcast_in_dim"(%15, %20) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x1xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
      %23 = mhlo.divide %21, %22 : tensor<?x?x?xf32>
      shape.assuming_yield %23 : tensor<?x?x?xf32>
    }
    return %18 : tensor<?x?x?xf32>
  }
}

======= END After TF2HLO ==========
// -----// IR Dump After RemoveShapeConstraintsPass (disc-remove-shape-constraint) //----- //
func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %0 = shape.const_witness true
  %1 = mhlo.constant dense<-0.000000e+00> : tensor<f32>
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %2 = mhlo.constant dense<0xFF800000> : tensor<f32>
  %3 = mhlo.reduce(%arg0 init: %2) applies mhlo.maximum across dimensions = [2] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
  %4 = tensor.dim %3, %c0 : tensor<?x?xf32>
  %5 = tensor.dim %3, %c1 : tensor<?x?xf32>
  %6 = tensor.from_elements %4, %5, %c1 : tensor<3xindex>
  %7 = mhlo.dynamic_reshape %3, %6 : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x1xf32>
  %8 = shape.assuming %0 -> (tensor<?x?x?xf32>) {
    %16 = shape.shape_of %arg0 : tensor<?x?x?xf32> -> tensor<3xindex>
    %17 = shape.broadcast %16, %6 : tensor<3xindex>, tensor<3xindex> -> tensor<3xindex>
    %18 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %17) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %19 = "mhlo.dynamic_broadcast_in_dim"(%7, %17) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x1xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %20 = mhlo.subtract %18, %19 : tensor<?x?x?xf32>
    shape.assuming_yield %20 : tensor<?x?x?xf32>
  }
  %9 = mhlo.exponential %8 : tensor<?x?x?xf32>
  %10 = mhlo.reduce(%9 init: %1) applies mhlo.add across dimensions = [2] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
  %11 = tensor.dim %10, %c0 : tensor<?x?xf32>
  %12 = tensor.dim %10, %c1 : tensor<?x?xf32>
  %13 = tensor.from_elements %11, %12, %c1 : tensor<3xindex>
  %14 = mhlo.dynamic_reshape %10, %13 : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x1xf32>
  %15 = shape.assuming %0 -> (tensor<?x?x?xf32>) {
    %16 = shape.shape_of %9 : tensor<?x?x?xf32> -> tensor<3xindex>
    %17 = shape.broadcast %16, %13 : tensor<3xindex>, tensor<3xindex> -> tensor<3xindex>
    %18 = "mhlo.dynamic_broadcast_in_dim"(%9, %17) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %19 = "mhlo.dynamic_broadcast_in_dim"(%14, %17) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x1xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %20 = mhlo.divide %18, %19 : tensor<?x?x?xf32>
    shape.assuming_yield %20 : tensor<?x?x?xf32>
  }
  return %15 : tensor<?x?x?xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %0 = mhlo.constant dense<-0.000000e+00> : tensor<f32>
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %1 = mhlo.constant dense<0xFF800000> : tensor<f32>
  %2 = mhlo.reduce(%arg0 init: %1) applies mhlo.maximum across dimensions = [2] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
  %3 = tensor.dim %2, %c0 : tensor<?x?xf32>
  %4 = tensor.dim %2, %c1 : tensor<?x?xf32>
  %5 = tensor.from_elements %3, %4, %c1 : tensor<3xindex>
  %6 = mhlo.dynamic_reshape %2, %5 : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x1xf32>
  %7 = shape.shape_of %arg0 : tensor<?x?x?xf32> -> tensor<3xindex>
  %8 = shape.broadcast %7, %5 : tensor<3xindex>, tensor<3xindex> -> tensor<3xindex>
  %9 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %8) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
  %10 = "mhlo.dynamic_broadcast_in_dim"(%6, %8) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x1xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
  %11 = mhlo.subtract %9, %10 : tensor<?x?x?xf32>
  %12 = mhlo.exponential %11 : tensor<?x?x?xf32>
  %13 = mhlo.reduce(%12 init: %0) applies mhlo.add across dimensions = [2] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
  %14 = tensor.dim %13, %c0 : tensor<?x?xf32>
  %15 = tensor.dim %13, %c1 : tensor<?x?xf32>
  %16 = tensor.from_elements %14, %15, %c1 : tensor<3xindex>
  %17 = mhlo.dynamic_reshape %13, %16 : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x1xf32>
  %18 = shape.shape_of %12 : tensor<?x?x?xf32> -> tensor<3xindex>
  %19 = shape.broadcast %18, %16 : tensor<3xindex>, tensor<3xindex> -> tensor<3xindex>
  %20 = "mhlo.dynamic_broadcast_in_dim"(%12, %19) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
  %21 = "mhlo.dynamic_broadcast_in_dim"(%17, %19) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x1xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
  %22 = mhlo.divide %20, %21 : tensor<?x?x?xf32>
  return %22 : tensor<?x?x?xf32>
}

// -----// IR Dump After ConvertShapeToStandardPass (disc-convert-shape-to-std) //----- //
func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %0 = mhlo.constant dense<-0.000000e+00> : tensor<f32>
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %1 = mhlo.constant dense<0xFF800000> : tensor<f32>
  %2 = mhlo.reduce(%arg0 init: %1) applies mhlo.maximum across dimensions = [2] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
  %3 = tensor.dim %2, %c0 : tensor<?x?xf32>
  %4 = tensor.dim %2, %c1 : tensor<?x?xf32>
  %5 = tensor.from_elements %3, %4, %c1 : tensor<3xindex>
  %6 = mhlo.dynamic_reshape %2, %5 : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x1xf32>
  %c0_0 = arith.constant 0 : index
  %7 = tensor.dim %arg0, %c0_0 : tensor<?x?x?xf32>
  %c1_1 = arith.constant 1 : index
  %8 = tensor.dim %arg0, %c1_1 : tensor<?x?x?xf32>
  %c2 = arith.constant 2 : index
  %9 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32>
  %10 = tensor.from_elements %7, %8, %9 : tensor<3xindex>
  %c0_2 = arith.constant 0 : index
  %c1_3 = arith.constant 1 : index
  %c0_4 = arith.constant 0 : index
  %11 = tensor.extract %10[%c0_4] : tensor<3xindex>
  %12 = arith.cmpi eq, %11, %c1_3 : index
  %13 = arith.select %12, %c1_3, %11 : index
  %c0_5 = arith.constant 0 : index
  %14 = tensor.extract %5[%c0_5] : tensor<3xindex>
  %15 = arith.cmpi eq, %14, %c1_3 : index
  %16 = arith.select %15, %13, %14 : index
  %c1_6 = arith.constant 1 : index
  %c1_7 = arith.constant 1 : index
  %17 = tensor.extract %10[%c1_7] : tensor<3xindex>
  %18 = arith.cmpi eq, %17, %c1_6 : index
  %19 = arith.select %18, %c1_6, %17 : index
  %c1_8 = arith.constant 1 : index
  %20 = tensor.extract %5[%c1_8] : tensor<3xindex>
  %21 = arith.cmpi eq, %20, %c1_6 : index
  %22 = arith.select %21, %19, %20 : index
  %c1_9 = arith.constant 1 : index
  %c2_10 = arith.constant 2 : index
  %23 = tensor.extract %10[%c2_10] : tensor<3xindex>
  %24 = arith.cmpi eq, %23, %c1_9 : index
  %25 = arith.select %24, %c1_9, %23 : index
  %c2_11 = arith.constant 2 : index
  %26 = tensor.extract %5[%c2_11] : tensor<3xindex>
  %27 = arith.cmpi eq, %26, %c1_9 : index
  %28 = arith.select %27, %25, %26 : index
  %29 = tensor.from_elements %16, %22, %28 : tensor<3xindex>
  %30 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %29) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
  %31 = "mhlo.dynamic_broadcast_in_dim"(%6, %29) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x1xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
  %32 = mhlo.subtract %30, %31 : tensor<?x?x?xf32>
  %33 = mhlo.exponential %32 : tensor<?x?x?xf32>
  %34 = mhlo.reduce(%33 init: %0) applies mhlo.add across dimensions = [2] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
  %35 = tensor.dim %34, %c0 : tensor<?x?xf32>
  %36 = tensor.dim %34, %c1 : tensor<?x?xf32>
  %37 = tensor.from_elements %35, %36, %c1 : tensor<3xindex>
  %38 = mhlo.dynamic_reshape %34, %37 : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x1xf32>
  %c0_12 = arith.constant 0 : index
  %39 = tensor.dim %33, %c0_12 : tensor<?x?x?xf32>
  %c1_13 = arith.constant 1 : index
  %40 = tensor.dim %33, %c1_13 : tensor<?x?x?xf32>
  %c2_14 = arith.constant 2 : index
  %41 = tensor.dim %33, %c2_14 : tensor<?x?x?xf32>
  %42 = tensor.from_elements %39, %40, %41 : tensor<3xindex>
  %c0_15 = arith.constant 0 : index
  %c1_16 = arith.constant 1 : index
  %c0_17 = arith.constant 0 : index
  %43 = tensor.extract %42[%c0_17] : tensor<3xindex>
  %44 = arith.cmpi eq, %43, %c1_16 : index
  %45 = arith.select %44, %c1_16, %43 : index
  %c0_18 = arith.constant 0 : index
  %46 = tensor.extract %37[%c0_18] : tensor<3xindex>
  %47 = arith.cmpi eq, %46, %c1_16 : index
  %48 = arith.select %47, %45, %46 : index
  %c1_19 = arith.constant 1 : index
  %c1_20 = arith.constant 1 : index
  %49 = tensor.extract %42[%c1_20] : tensor<3xindex>
  %50 = arith.cmpi eq, %49, %c1_19 : index
  %51 = arith.select %50, %c1_19, %49 : index
  %c1_21 = arith.constant 1 : index
  %52 = tensor.extract %37[%c1_21] : tensor<3xindex>
  %53 = arith.cmpi eq, %52, %c1_19 : index
  %54 = arith.select %53, %51, %52 : index
  %c1_22 = arith.constant 1 : index
  %c2_23 = arith.constant 2 : index
  %55 = tensor.extract %42[%c2_23] : tensor<3xindex>
  %56 = arith.cmpi eq, %55, %c1_22 : index
  %57 = arith.select %56, %c1_22, %55 : index
  %c2_24 = arith.constant 2 : index
  %58 = tensor.extract %37[%c2_24] : tensor<3xindex>
  %59 = arith.cmpi eq, %58, %c1_22 : index
  %60 = arith.select %59, %57, %58 : index
  %61 = tensor.from_elements %48, %54, %60 : tensor<3xindex>
  %62 = "mhlo.dynamic_broadcast_in_dim"(%33, %61) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
  %63 = "mhlo.dynamic_broadcast_in_dim"(%38, %61) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x1xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
  %64 = mhlo.divide %62, %63 : tensor<?x?x?xf32>
  return %64 : tensor<?x?x?xf32>
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %1 = mhlo.constant dense<-0.000000e+00> : tensor<f32>
    %2 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32>
    %3 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32>
    %4 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32>
    %5 = "disc_shape.tie_shape"(%arg0, %2, %3, %4) : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %6 = mhlo.reduce(%5 init: %0) applies mhlo.maximum across dimensions = [2] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
    %7 = "disc_shape.tie_shape"(%6, %2, %3) : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %8 = tensor.from_elements %2, %3, %c1 : tensor<3xindex>
    %9 = "disc_shape.tie_shape"(%8, %c3) : (tensor<3xindex>, index) -> tensor<3xindex>
    %10 = mhlo.dynamic_reshape %7, %9 : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x1xf32>
    %11 = "disc_shape.tie_shape"(%10, %2, %3, %c1) : (tensor<?x?x1xf32>, index, index, index) -> tensor<?x?x1xf32>
    %12 = tensor.from_elements %2, %3, %4 : tensor<3xindex>
    %13 = "disc_shape.tie_shape"(%12, %c3) : (tensor<3xindex>, index) -> tensor<3xindex>
    %14 = "mhlo.dynamic_broadcast_in_dim"(%5, %13) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %15 = "disc_shape.tie_shape"(%14, %2, %3, %4) : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %16 = "mhlo.dynamic_broadcast_in_dim"(%11, %13) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x1xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %17 = "disc_shape.tie_shape"(%16, %2, %3, %4) : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %18 = mhlo.subtract %15, %17 : tensor<?x?x?xf32>
    %19 = "disc_shape.tie_shape"(%18, %2, %3, %4) : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %20 = mhlo.exponential %19 : tensor<?x?x?xf32>
    %21 = "disc_shape.tie_shape"(%20, %2, %3, %4) : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %22 = mhlo.reduce(%21 init: %1) applies mhlo.add across dimensions = [2] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
    %23 = "disc_shape.tie_shape"(%22, %2, %3) : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %24 = mhlo.dynamic_reshape %23, %9 : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x1xf32>
    %25 = "disc_shape.tie_shape"(%24, %2, %3, %c1) : (tensor<?x?x1xf32>, index, index, index) -> tensor<?x?x1xf32>
    %26 = "mhlo.dynamic_broadcast_in_dim"(%21, %13) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %27 = "disc_shape.tie_shape"(%26, %2, %3, %4) : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %28 = "mhlo.dynamic_broadcast_in_dim"(%25, %13) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x1xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %29 = "disc_shape.tie_shape"(%28, %2, %3, %4) : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %30 = mhlo.divide %27, %29 : tensor<?x?x?xf32>
    %31 = "disc_shape.tie_shape"(%30, %2, %3, %4) : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    return %31 : tensor<?x?x?xf32>
  }
}


// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %1 = mhlo.constant dense<-0.000000e+00> : tensor<f32>
    %2 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %3 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %4 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %5 = mhlo.reduce(%arg0 init: %0) applies mhlo.maximum across dimensions = [2] : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<f32>) -> tensor<?x?xf32, [@S0, @S1]>
    %6 = tensor.from_elements %2, %3, %c1 : tensor<3xindex>
    %7 = mhlo.dynamic_reshape %5, %6 : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x1xf32, [@S0, @S1, @C1]>
    %8 = tensor.from_elements %2, %3, %4 : tensor<3xindex>
    %9 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %8) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %10 = "mhlo.dynamic_broadcast_in_dim"(%7, %8) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x1xf32, [@S0, @S1, @C1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %11 = mhlo.subtract %9, %10 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %12 = mhlo.exponential %11 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %13 = mhlo.reduce(%12 init: %1) applies mhlo.add across dimensions = [2] : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<f32>) -> tensor<?x?xf32, [@S0, @S1]>
    %14 = mhlo.dynamic_reshape %13, %6 : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x1xf32, [@S0, @S1, @C1]>
    %15 = "mhlo.dynamic_broadcast_in_dim"(%12, %8) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %16 = "mhlo.dynamic_broadcast_in_dim"(%14, %8) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x?x1xf32, [@S0, @S1, @C1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %17 = mhlo.divide %15, %16 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    return %17 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    return
  }
}


// -----// IR Dump After DiscAlgebraSimplifierPass (disc-algebra-simplifier) //----- //
func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
  %1 = mhlo.constant dense<-0.000000e+00> : tensor<f32>
  %2 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %3 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %4 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %5 = mhlo.reduce(%arg0 init: %0) applies mhlo.maximum across dimensions = [2] : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<f32>) -> tensor<?x?xf32, [@S0, @S1]>
  %6 = tensor.from_elements %2, %3, %4 : tensor<3xindex>
  %7 = "mhlo.dynamic_broadcast_in_dim"(%5, %6) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %8 = mhlo.subtract %arg0, %7 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %9 = mhlo.exponential %8 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %10 = mhlo.reduce(%9 init: %1) applies mhlo.add across dimensions = [2] : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<f32>) -> tensor<?x?xf32, [@S0, @S1]>
  %11 = "mhlo.dynamic_broadcast_in_dim"(%10, %6) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %12 = mhlo.divide %9, %11 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  return %12 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
}

// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant dense<-0.000000e+00> : tensor<f32>
    %1 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %2 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %3 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %4 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %5 = mhlo.reduce(%arg0 init: %1) applies mhlo.maximum across dimensions = [2] : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<f32>) -> tensor<?x?xf32, [@S0, @S1]>
    %6 = tensor.from_elements %2, %3, %4 : tensor<3xindex>
    %7 = "mhlo.dynamic_broadcast_in_dim"(%5, %6) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %8 = mhlo.subtract %arg0, %7 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %9 = mhlo.exponential %8 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %10 = mhlo.reduce(%9 init: %0) applies mhlo.add across dimensions = [2] : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<f32>) -> tensor<?x?xf32, [@S0, @S1]>
    %11 = "mhlo.dynamic_broadcast_in_dim"(%10, %6) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %12 = mhlo.divide %9, %11 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    return %12 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    return
  }
}


// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %1 = mhlo.constant dense<-0.000000e+00> : tensor<f32>
    %2 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %3 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %4 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %5 = mhlo.reduce(%arg0 init: %0) applies mhlo.maximum across dimensions = [2] : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<f32>) -> tensor<?x?xf32, [@S0, @S1]>
    %6 = tensor.from_elements %2, %3, %4 : tensor<3xindex>
    %7 = "mhlo.dynamic_broadcast_in_dim"(%5, %6) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %8 = mhlo.subtract %arg0, %7 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %9 = mhlo.exponential %8 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %10 = mhlo.reduce(%9 init: %1) applies mhlo.add across dimensions = [2] : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<f32>) -> tensor<?x?xf32, [@S0, @S1]>
    %11 = "mhlo.dynamic_broadcast_in_dim"(%10, %6) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %12 = mhlo.divide %9, %11 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    return %12 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    return
  }
}


// -----// IR Dump After HloCanonicalizeReductionPass (hlo-canonicalize-reduction) //----- //
func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
  %1 = mhlo.constant dense<-0.000000e+00> : tensor<f32>
  %2 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %3 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %4 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %c1_i32 = arith.constant 1 : i32
  %c2_0 = arith.constant 2 : index
  %5 = tensor.dim %arg0, %c2_0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %6 = arith.index_cast %5 : index to i32
  %7 = arith.muli %c1_i32, %6 : i32
  %c0_1 = arith.constant 0 : index
  %8 = tensor.dim %arg0, %c0_1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %9 = arith.index_cast %8 : index to i32
  %10 = arith.muli %c1_i32, %9 : i32
  %c1_2 = arith.constant 1 : index
  %11 = tensor.dim %arg0, %c1_2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %12 = arith.index_cast %11 : index to i32
  %13 = arith.muli %10, %12 : i32
  %14 = tensor.from_elements %13, %7 : tensor<2xi32>
  %15 = mhlo.dynamic_reshape %arg0, %14 : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32>
  %16 = mhlo.reduce(%15 init: %0) applies mhlo.maximum across dimensions = [1] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
  %c0_3 = arith.constant 0 : index
  %17 = tensor.dim %arg0, %c0_3 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %18 = arith.index_cast %17 : index to i32
  %c1_4 = arith.constant 1 : index
  %19 = tensor.dim %arg0, %c1_4 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %20 = arith.index_cast %19 : index to i32
  %21 = tensor.from_elements %18, %20 : tensor<2xi32>
  %22 = mhlo.dynamic_reshape %16, %21 : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
  %23 = tensor.from_elements %2, %3, %4 : tensor<3xindex>
  %24 = "mhlo.dynamic_broadcast_in_dim"(%22, %23) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %25 = mhlo.subtract %arg0, %24 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %26 = mhlo.exponential %25 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %c1_i32_5 = arith.constant 1 : i32
  %c2_6 = arith.constant 2 : index
  %27 = tensor.dim %26, %c2_6 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %28 = arith.index_cast %27 : index to i32
  %29 = arith.muli %c1_i32_5, %28 : i32
  %c0_7 = arith.constant 0 : index
  %30 = tensor.dim %26, %c0_7 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %31 = arith.index_cast %30 : index to i32
  %32 = arith.muli %c1_i32_5, %31 : i32
  %c1_8 = arith.constant 1 : index
  %33 = tensor.dim %26, %c1_8 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %34 = arith.index_cast %33 : index to i32
  %35 = arith.muli %32, %34 : i32
  %36 = tensor.from_elements %35, %29 : tensor<2xi32>
  %37 = mhlo.dynamic_reshape %26, %36 : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32>
  %38 = mhlo.reduce(%37 init: %1) applies mhlo.add across dimensions = [1] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
  %c0_9 = arith.constant 0 : index
  %39 = tensor.dim %26, %c0_9 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %40 = arith.index_cast %39 : index to i32
  %c1_10 = arith.constant 1 : index
  %41 = tensor.dim %26, %c1_10 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %42 = arith.index_cast %41 : index to i32
  %43 = tensor.from_elements %40, %42 : tensor<2xi32>
  %44 = mhlo.dynamic_reshape %38, %43 : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
  %45 = "mhlo.dynamic_broadcast_in_dim"(%44, %23) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %46 = mhlo.divide %26, %45 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  return %46 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c3 = arith.constant 3 : index
    %0 = mhlo.constant dense<-0.000000e+00> : tensor<f32>
    %1 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %2 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %3 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %4 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %5 = "disc_shape.tie_shape"(%arg0, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %6 = arith.index_cast %4 : index to i32
    %7 = arith.index_cast %2 : index to i32
    %8 = arith.index_cast %3 : index to i32
    %9 = arith.muli %7, %8 : i32
    %10 = tensor.from_elements %9, %6 : tensor<2xi32>
    %11 = "disc_shape.tie_shape"(%10, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %12 = mhlo.dynamic_reshape %5, %11 : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32>
    %13 = arith.index_cast %9 : i32 to index
    %14 = "disc_shape.tie_shape"(%12, %13, %4) : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %15 = mhlo.reduce(%14 init: %1) applies mhlo.maximum across dimensions = [1] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
    %16 = "disc_shape.tie_shape"(%15, %13) : (tensor<?xf32>, index) -> tensor<?xf32>
    %17 = tensor.from_elements %7, %8 : tensor<2xi32>
    %18 = "disc_shape.tie_shape"(%17, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %19 = mhlo.dynamic_reshape %16, %18 : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %20 = "disc_shape.tie_shape"(%19, %2, %3) : (tensor<?x?xf32, [@S0, @S1]>, index, index) -> tensor<?x?xf32, [@S0, @S1]>
    %21 = tensor.from_elements %2, %3, %4 : tensor<3xindex>
    %22 = "disc_shape.tie_shape"(%21, %c3) : (tensor<3xindex>, index) -> tensor<3xindex>
    %23 = "mhlo.dynamic_broadcast_in_dim"(%20, %22) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %24 = "disc_shape.tie_shape"(%23, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %25 = mhlo.subtract %5, %24 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %26 = "disc_shape.tie_shape"(%25, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %27 = mhlo.exponential %26 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %28 = "disc_shape.tie_shape"(%27, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %29 = mhlo.dynamic_reshape %28, %11 : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32>
    %30 = "disc_shape.tie_shape"(%29, %13, %4) : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %31 = mhlo.reduce(%30 init: %0) applies mhlo.add across dimensions = [1] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
    %32 = "disc_shape.tie_shape"(%31, %13) : (tensor<?xf32>, index) -> tensor<?xf32>
    %33 = mhlo.dynamic_reshape %32, %18 : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %34 = "disc_shape.tie_shape"(%33, %2, %3) : (tensor<?x?xf32, [@S0, @S1]>, index, index) -> tensor<?x?xf32, [@S0, @S1]>
    %35 = "mhlo.dynamic_broadcast_in_dim"(%34, %22) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %36 = "disc_shape.tie_shape"(%35, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %37 = mhlo.divide %28, %36 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %38 = "disc_shape.tie_shape"(%37, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    return %38 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    return
  }
}


// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = mhlo.constant dense<-0.000000e+00> : tensor<f32>
    %1 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %2 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %3 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %4 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %5 = arith.index_cast %4 : index to i32
    %6 = arith.index_cast %2 : index to i32
    %7 = arith.index_cast %3 : index to i32
    %8 = arith.muli %6, %7 : i32
    %9 = tensor.from_elements %8, %5 : tensor<2xi32>
    %10 = mhlo.dynamic_reshape %arg0, %9 : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %11 = mhlo.reduce(%10 init: %1) applies mhlo.maximum across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %12 = tensor.from_elements %6, %7 : tensor<2xi32>
    %13 = mhlo.dynamic_reshape %11, %12 : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %14 = tensor.from_elements %2, %3, %4 : tensor<3xindex>
    %15 = "mhlo.dynamic_broadcast_in_dim"(%13, %14) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %16 = mhlo.subtract %arg0, %15 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %17 = mhlo.exponential %16 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %18 = mhlo.dynamic_reshape %17, %9 : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %19 = mhlo.reduce(%18 init: %0) applies mhlo.add across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %20 = mhlo.dynamic_reshape %19, %12 : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %21 = "mhlo.dynamic_broadcast_in_dim"(%20, %14) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %22 = mhlo.divide %17, %21 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    return %22 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscMarkShapeCalculationPass (disc-mhlo-mark-shape-calc) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = mhlo.constant dense<-0.000000e+00> : tensor<f32>
    %1 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %2 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %3 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %4 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %5 = arith.index_cast %4 : index to i32
    %6 = arith.index_cast %2 : index to i32
    %7 = arith.index_cast %3 : index to i32
    %8 = arith.muli %6, %7 : i32
    %9 = tensor.from_elements %8, %5 {disc.shape_op = true} : tensor<2xi32>
    %10 = mhlo.dynamic_reshape %arg0, %9 : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %11 = mhlo.reduce(%10 init: %1) applies mhlo.maximum across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %12 = tensor.from_elements %6, %7 {disc.shape_op = true} : tensor<2xi32>
    %13 = mhlo.dynamic_reshape %11, %12 : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %14 = tensor.from_elements %2, %3, %4 {disc.shape_op = true} : tensor<3xindex>
    %15 = "mhlo.dynamic_broadcast_in_dim"(%13, %14) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %16 = mhlo.subtract %arg0, %15 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %17 = mhlo.exponential %16 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %18 = mhlo.dynamic_reshape %17, %9 : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %19 = mhlo.reduce(%18 init: %0) applies mhlo.add across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %20 = mhlo.dynamic_reshape %19, %12 : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %21 = "mhlo.dynamic_broadcast_in_dim"(%20, %14) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %22 = mhlo.divide %17, %21 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    return %22 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After PlaceOpsPass (mhlo-place-ops) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<-0.000000e+00> : tensor<f32>
    %1 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %2 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %3 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %4 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %5 = arith.index_cast %4 : index to i32
    %6 = arith.index_cast %2 : index to i32
    %7 = arith.index_cast %3 : index to i32
    %8 = arith.muli %6, %7 : i32
    %9 = tensor.from_elements %8, %5 {disc.shape_op = true} : tensor<2xi32>
    %10 = mhlo.dynamic_reshape %arg0, %9 {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %11 = mhlo.reduce(%10 init: %1) applies mhlo.maximum across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %12 = tensor.from_elements %6, %7 {disc.shape_op = true} : tensor<2xi32>
    %13 = mhlo.dynamic_reshape %11, %12 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %14 = tensor.from_elements %2, %3, %4 {disc.shape_op = true} : tensor<3xindex>
    %15 = "mhlo.dynamic_broadcast_in_dim"(%13, %14) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %16 = mhlo.subtract %arg0, %15 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %17 = mhlo.exponential %16 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %18 = mhlo.dynamic_reshape %17, %9 {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %19 = mhlo.reduce(%18 init: %0) applies mhlo.add across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %20 = mhlo.dynamic_reshape %19, %12 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %21 = "mhlo.dynamic_broadcast_in_dim"(%20, %14) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %22 = mhlo.divide %17, %21 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    return %22 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c3 = arith.constant 3 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %1 = mhlo.constant {disc.device = "gpu"} dense<-0.000000e+00> : tensor<f32>
    %2 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %3 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %4 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %5 = "disc_shape.tie_shape"(%arg0, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %6 = arith.index_cast %4 : index to i32
    %7 = arith.index_cast %2 : index to i32
    %8 = arith.index_cast %3 : index to i32
    %9 = arith.muli %7, %8 : i32
    %10 = tensor.from_elements %9, %6 {disc.shape_op = true} : tensor<2xi32>
    %11 = "disc_shape.tie_shape"(%10, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %12 = mhlo.dynamic_reshape %5, %11 {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %13 = arith.index_cast %9 : i32 to index
    %14 = "disc_shape.tie_shape"(%12, %13, %4) : (tensor<?x?xf32, [@S3, @S2]>, index, index) -> tensor<?x?xf32, [@S3, @S2]>
    %15 = mhlo.reduce(%14 init: %0) applies mhlo.maximum across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %16 = "disc_shape.tie_shape"(%15, %13) : (tensor<?xf32, [@S3]>, index) -> tensor<?xf32, [@S3]>
    %17 = tensor.from_elements %7, %8 {disc.shape_op = true} : tensor<2xi32>
    %18 = "disc_shape.tie_shape"(%17, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %19 = mhlo.dynamic_reshape %16, %18 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %20 = "disc_shape.tie_shape"(%19, %2, %3) : (tensor<?x?xf32, [@S0, @S1]>, index, index) -> tensor<?x?xf32, [@S0, @S1]>
    %21 = tensor.from_elements %2, %3, %4 {disc.shape_op = true} : tensor<3xindex>
    %22 = "disc_shape.tie_shape"(%21, %c3) : (tensor<3xindex>, index) -> tensor<3xindex>
    %23 = "mhlo.dynamic_broadcast_in_dim"(%20, %22) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %24 = "disc_shape.tie_shape"(%23, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %25 = mhlo.subtract %5, %24 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %26 = "disc_shape.tie_shape"(%25, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %27 = mhlo.exponential %26 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %28 = "disc_shape.tie_shape"(%27, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %29 = mhlo.dynamic_reshape %28, %11 {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %30 = "disc_shape.tie_shape"(%29, %13, %4) : (tensor<?x?xf32, [@S3, @S2]>, index, index) -> tensor<?x?xf32, [@S3, @S2]>
    %31 = mhlo.reduce(%30 init: %1) applies mhlo.add across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %32 = "disc_shape.tie_shape"(%31, %13) : (tensor<?xf32, [@S3]>, index) -> tensor<?xf32, [@S3]>
    %33 = mhlo.dynamic_reshape %32, %18 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %34 = "disc_shape.tie_shape"(%33, %2, %3) : (tensor<?x?xf32, [@S0, @S1]>, index, index) -> tensor<?x?xf32, [@S0, @S1]>
    %35 = "mhlo.dynamic_broadcast_in_dim"(%34, %22) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %36 = "disc_shape.tie_shape"(%35, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %37 = mhlo.divide %28, %36 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %38 = "disc_shape.tie_shape"(%37, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    return %38 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %1 = mhlo.constant {disc.device = "gpu"} dense<-0.000000e+00> : tensor<f32>
    %2 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %3 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %4 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %5 = arith.index_cast %4 : index to i32
    %6 = arith.index_cast %2 : index to i32
    %7 = arith.index_cast %3 : index to i32
    %8 = arith.muli %6, %7 : i32
    %9 = tensor.from_elements %8, %5 {disc.shape_op = true} : tensor<2xi32>
    %10 = mhlo.dynamic_reshape %arg0, %9 {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %11 = mhlo.reduce(%10 init: %0) applies mhlo.maximum across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %12 = tensor.from_elements %6, %7 {disc.shape_op = true} : tensor<2xi32>
    %13 = mhlo.dynamic_reshape %11, %12 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %14 = tensor.from_elements %2, %3, %4 {disc.shape_op = true} : tensor<3xindex>
    %15 = "mhlo.dynamic_broadcast_in_dim"(%13, %14) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %16 = mhlo.subtract %arg0, %15 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %17 = mhlo.exponential %16 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %18 = mhlo.dynamic_reshape %17, %9 {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %19 = mhlo.reduce(%18 init: %1) applies mhlo.add across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %20 = mhlo.dynamic_reshape %19, %12 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %21 = "mhlo.dynamic_broadcast_in_dim"(%20, %14) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %22 = mhlo.divide %17, %21 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    return %22 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c3 = arith.constant 3 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<-0.000000e+00> : tensor<f32>
    %1 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %2 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %3 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %4 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %5 = "disc_shape.tie_shape"(%arg0, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %6 = arith.index_cast %4 : index to i32
    %7 = arith.index_cast %2 : index to i32
    %8 = arith.index_cast %3 : index to i32
    %9 = arith.muli %7, %8 : i32
    %10 = tensor.from_elements %9, %6 {disc.shape_op = true} : tensor<2xi32>
    %11 = "disc_shape.tie_shape"(%10, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %12 = mhlo.dynamic_reshape %5, %11 {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %13 = arith.index_cast %9 : i32 to index
    %14 = "disc_shape.tie_shape"(%12, %13, %4) : (tensor<?x?xf32, [@S3, @S2]>, index, index) -> tensor<?x?xf32, [@S3, @S2]>
    %15 = mhlo.reduce(%14 init: %1) applies mhlo.maximum across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %16 = "disc_shape.tie_shape"(%15, %13) : (tensor<?xf32, [@S3]>, index) -> tensor<?xf32, [@S3]>
    %17 = tensor.from_elements %7, %8 {disc.shape_op = true} : tensor<2xi32>
    %18 = "disc_shape.tie_shape"(%17, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %19 = mhlo.dynamic_reshape %16, %18 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %20 = "disc_shape.tie_shape"(%19, %2, %3) : (tensor<?x?xf32, [@S0, @S1]>, index, index) -> tensor<?x?xf32, [@S0, @S1]>
    %21 = tensor.from_elements %2, %3, %4 {disc.shape_op = true} : tensor<3xindex>
    %22 = "disc_shape.tie_shape"(%21, %c3) : (tensor<3xindex>, index) -> tensor<3xindex>
    %23 = "mhlo.dynamic_broadcast_in_dim"(%20, %22) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %24 = "disc_shape.tie_shape"(%23, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %25 = mhlo.subtract %5, %24 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %26 = "disc_shape.tie_shape"(%25, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %27 = mhlo.exponential %26 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %28 = "disc_shape.tie_shape"(%27, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %29 = mhlo.dynamic_reshape %28, %11 {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %30 = "disc_shape.tie_shape"(%29, %13, %4) : (tensor<?x?xf32, [@S3, @S2]>, index, index) -> tensor<?x?xf32, [@S3, @S2]>
    %31 = mhlo.reduce(%30 init: %0) applies mhlo.add across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %32 = "disc_shape.tie_shape"(%31, %13) : (tensor<?xf32, [@S3]>, index) -> tensor<?xf32, [@S3]>
    %33 = mhlo.dynamic_reshape %32, %18 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %34 = "disc_shape.tie_shape"(%33, %2, %3) : (tensor<?x?xf32, [@S0, @S1]>, index, index) -> tensor<?x?xf32, [@S0, @S1]>
    %35 = "mhlo.dynamic_broadcast_in_dim"(%34, %22) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %36 = "disc_shape.tie_shape"(%35, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %37 = mhlo.divide %28, %36 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %38 = "disc_shape.tie_shape"(%37, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    return %38 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<-0.000000e+00> : tensor<f32>
    %1 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %2 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %3 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %4 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %5 = arith.index_cast %4 : index to i32
    %6 = arith.index_cast %2 : index to i32
    %7 = arith.index_cast %3 : index to i32
    %8 = arith.muli %6, %7 : i32
    %9 = tensor.from_elements %8, %5 {disc.shape_op = true} : tensor<2xi32>
    %10 = mhlo.dynamic_reshape %arg0, %9 {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %11 = mhlo.reduce(%10 init: %1) applies mhlo.maximum across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %12 = tensor.from_elements %6, %7 {disc.shape_op = true} : tensor<2xi32>
    %13 = mhlo.dynamic_reshape %11, %12 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %14 = tensor.from_elements %2, %3, %4 {disc.shape_op = true} : tensor<3xindex>
    %15 = "mhlo.dynamic_broadcast_in_dim"(%13, %14) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %16 = mhlo.subtract %arg0, %15 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %17 = mhlo.exponential %16 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %18 = mhlo.dynamic_reshape %17, %9 {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %19 = mhlo.reduce(%18 init: %0) applies mhlo.add across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %20 = mhlo.dynamic_reshape %19, %12 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %21 = "mhlo.dynamic_broadcast_in_dim"(%20, %14) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %22 = mhlo.divide %17, %21 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    return %22 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c3 = arith.constant 3 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %1 = mhlo.constant {disc.device = "gpu"} dense<-0.000000e+00> : tensor<f32>
    %2 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %3 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %4 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %5 = "disc_shape.tie_shape"(%arg0, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %6 = arith.index_cast %4 : index to i32
    %7 = arith.index_cast %2 : index to i32
    %8 = arith.index_cast %3 : index to i32
    %9 = arith.muli %7, %8 : i32
    %10 = tensor.from_elements %9, %6 {disc.shape_op = true} : tensor<2xi32>
    %11 = "disc_shape.tie_shape"(%10, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %12 = mhlo.dynamic_reshape %5, %11 {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %13 = arith.index_cast %9 : i32 to index
    %14 = "disc_shape.tie_shape"(%12, %13, %4) : (tensor<?x?xf32, [@S3, @S2]>, index, index) -> tensor<?x?xf32, [@S3, @S2]>
    %15 = mhlo.reduce(%14 init: %0) applies mhlo.maximum across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %16 = "disc_shape.tie_shape"(%15, %13) : (tensor<?xf32, [@S3]>, index) -> tensor<?xf32, [@S3]>
    %17 = tensor.from_elements %7, %8 {disc.shape_op = true} : tensor<2xi32>
    %18 = "disc_shape.tie_shape"(%17, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %19 = mhlo.dynamic_reshape %16, %18 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %20 = "disc_shape.tie_shape"(%19, %2, %3) : (tensor<?x?xf32, [@S0, @S1]>, index, index) -> tensor<?x?xf32, [@S0, @S1]>
    %21 = tensor.from_elements %2, %3, %4 {disc.shape_op = true} : tensor<3xindex>
    %22 = "disc_shape.tie_shape"(%21, %c3) : (tensor<3xindex>, index) -> tensor<3xindex>
    %23 = "mhlo.dynamic_broadcast_in_dim"(%20, %22) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %24 = "disc_shape.tie_shape"(%23, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %25 = mhlo.subtract %5, %24 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %26 = "disc_shape.tie_shape"(%25, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %27 = mhlo.exponential %26 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %28 = "disc_shape.tie_shape"(%27, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %29 = mhlo.dynamic_reshape %28, %11 {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %30 = "disc_shape.tie_shape"(%29, %13, %4) : (tensor<?x?xf32, [@S3, @S2]>, index, index) -> tensor<?x?xf32, [@S3, @S2]>
    %31 = mhlo.reduce(%30 init: %1) applies mhlo.add across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %32 = "disc_shape.tie_shape"(%31, %13) : (tensor<?xf32, [@S3]>, index) -> tensor<?xf32, [@S3]>
    %33 = mhlo.dynamic_reshape %32, %18 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %34 = "disc_shape.tie_shape"(%33, %2, %3) : (tensor<?x?xf32, [@S0, @S1]>, index, index) -> tensor<?x?xf32, [@S0, @S1]>
    %35 = "mhlo.dynamic_broadcast_in_dim"(%34, %22) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %36 = "disc_shape.tie_shape"(%35, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %37 = mhlo.divide %28, %36 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %38 = "disc_shape.tie_shape"(%37, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    return %38 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %1 = mhlo.constant {disc.device = "gpu"} dense<-0.000000e+00> : tensor<f32>
    %2 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %3 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %4 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %5 = arith.index_cast %4 : index to i32
    %6 = arith.index_cast %2 : index to i32
    %7 = arith.index_cast %3 : index to i32
    %8 = arith.muli %6, %7 : i32
    %9 = tensor.from_elements %8, %5 {disc.shape_op = true} : tensor<2xi32>
    %10 = mhlo.dynamic_reshape %arg0, %9 {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %11 = mhlo.reduce(%10 init: %0) applies mhlo.maximum across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %12 = tensor.from_elements %6, %7 {disc.shape_op = true} : tensor<2xi32>
    %13 = mhlo.dynamic_reshape %11, %12 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %14 = tensor.from_elements %2, %3, %4 {disc.shape_op = true} : tensor<3xindex>
    %15 = "mhlo.dynamic_broadcast_in_dim"(%13, %14) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %16 = mhlo.subtract %arg0, %15 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %17 = mhlo.exponential %16 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %18 = mhlo.dynamic_reshape %17, %9 {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %19 = mhlo.reduce(%18 init: %1) applies mhlo.add across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %20 = mhlo.dynamic_reshape %19, %12 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %21 = "mhlo.dynamic_broadcast_in_dim"(%20, %14) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %22 = mhlo.divide %17, %21 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    return %22 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c3 = arith.constant 3 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<-0.000000e+00> : tensor<f32>
    %1 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %2 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %3 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %4 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %5 = "disc_shape.tie_shape"(%arg0, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %6 = arith.index_cast %4 : index to i32
    %7 = arith.index_cast %2 : index to i32
    %8 = arith.index_cast %3 : index to i32
    %9 = arith.muli %7, %8 : i32
    %10 = tensor.from_elements %9, %6 {disc.shape_op = true} : tensor<2xi32>
    %11 = "disc_shape.tie_shape"(%10, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %12 = mhlo.dynamic_reshape %5, %11 {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %13 = arith.index_cast %9 : i32 to index
    %14 = "disc_shape.tie_shape"(%12, %13, %4) : (tensor<?x?xf32, [@S3, @S2]>, index, index) -> tensor<?x?xf32, [@S3, @S2]>
    %15 = mhlo.reduce(%14 init: %1) applies mhlo.maximum across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %16 = "disc_shape.tie_shape"(%15, %13) : (tensor<?xf32, [@S3]>, index) -> tensor<?xf32, [@S3]>
    %17 = tensor.from_elements %7, %8 {disc.shape_op = true} : tensor<2xi32>
    %18 = "disc_shape.tie_shape"(%17, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %19 = mhlo.dynamic_reshape %16, %18 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %20 = "disc_shape.tie_shape"(%19, %2, %3) : (tensor<?x?xf32, [@S0, @S1]>, index, index) -> tensor<?x?xf32, [@S0, @S1]>
    %21 = tensor.from_elements %2, %3, %4 {disc.shape_op = true} : tensor<3xindex>
    %22 = "disc_shape.tie_shape"(%21, %c3) : (tensor<3xindex>, index) -> tensor<3xindex>
    %23 = "mhlo.dynamic_broadcast_in_dim"(%20, %22) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %24 = "disc_shape.tie_shape"(%23, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %25 = mhlo.subtract %5, %24 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %26 = "disc_shape.tie_shape"(%25, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %27 = mhlo.exponential %26 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %28 = "disc_shape.tie_shape"(%27, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %29 = mhlo.dynamic_reshape %28, %11 {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S3, @S2]>
    %30 = "disc_shape.tie_shape"(%29, %13, %4) : (tensor<?x?xf32, [@S3, @S2]>, index, index) -> tensor<?x?xf32, [@S3, @S2]>
    %31 = mhlo.reduce(%30 init: %0) applies mhlo.add across dimensions = [1] : (tensor<?x?xf32, [@S3, @S2]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %32 = "disc_shape.tie_shape"(%31, %13) : (tensor<?xf32, [@S3]>, index) -> tensor<?xf32, [@S3]>
    %33 = mhlo.dynamic_reshape %32, %18 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S1]>
    %34 = "disc_shape.tie_shape"(%33, %2, %3) : (tensor<?x?xf32, [@S0, @S1]>, index, index) -> tensor<?x?xf32, [@S0, @S1]>
    %35 = "mhlo.dynamic_broadcast_in_dim"(%34, %22) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32, [@S0, @S1]>, tensor<3xindex>) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %36 = "disc_shape.tie_shape"(%35, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %37 = mhlo.divide %28, %36 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %38 = "disc_shape.tie_shape"(%37, %2, %3, %4) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    return %38 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c3 = arith.constant 3 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<-0.000000e+00> : tensor<f32>
    %1 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %2 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32>
    %3 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32>
    %4 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32>
    %5 = "disc_shape.tie_shape"(%arg0, %2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %6 = arith.index_cast %4 : index to i32
    %7 = arith.index_cast %2 : index to i32
    %8 = arith.index_cast %3 : index to i32
    %9 = arith.muli %7, %8 : i32
    %10 = tensor.from_elements %9, %6 {disc.shape_op = true} : tensor<2xi32>
    %11 = "disc_shape.tie_shape"(%10, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %12 = arith.index_cast %9 : i32 to index
    %13 = mhlo.dynamic_reshape %5, %11 {disc.device = "gpu"} : (tensor<?x?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %14 = "disc_shape.tie_shape"(%13, %12, %4) {kDiscSymbolicDimAttr = [@S3, @S2]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %15 = mhlo.reduce(%14 init: %1) applies mhlo.maximum across dimensions = [1] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
    %16 = "disc_shape.tie_shape"(%15, %12) {kDiscSymbolicDimAttr = [@S3]} : (tensor<?xf32>, index) -> tensor<?xf32>
    %17 = tensor.from_elements %7, %8 {disc.shape_op = true} : tensor<2xi32>
    %18 = "disc_shape.tie_shape"(%17, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %19 = mhlo.dynamic_reshape %16, %18 {disc.device = "gpu"} : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %20 = "disc_shape.tie_shape"(%19, %2, %3) {kDiscSymbolicDimAttr = [@S0, @S1]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %21 = tensor.from_elements %2, %3, %4 {disc.shape_op = true} : tensor<3xindex>
    %22 = "disc_shape.tie_shape"(%21, %c3) : (tensor<3xindex>, index) -> tensor<3xindex>
    %23 = "mhlo.dynamic_broadcast_in_dim"(%20, %22) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %24 = "disc_shape.tie_shape"(%23, %2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %25 = mhlo.subtract %5, %24 {disc.device = "gpu"} : tensor<?x?x?xf32>
    %26 = "disc_shape.tie_shape"(%25, %2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %27 = mhlo.exponential %26 {disc.device = "gpu"} : tensor<?x?x?xf32>
    %28 = "disc_shape.tie_shape"(%27, %2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %29 = mhlo.dynamic_reshape %28, %11 {disc.device = "gpu"} : (tensor<?x?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %30 = "disc_shape.tie_shape"(%29, %12, %4) {kDiscSymbolicDimAttr = [@S3, @S2]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %31 = mhlo.reduce(%30 init: %0) applies mhlo.add across dimensions = [1] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
    %32 = "disc_shape.tie_shape"(%31, %12) {kDiscSymbolicDimAttr = [@S3]} : (tensor<?xf32>, index) -> tensor<?xf32>
    %33 = mhlo.dynamic_reshape %32, %18 {disc.device = "gpu"} : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %34 = "disc_shape.tie_shape"(%33, %2, %3) {kDiscSymbolicDimAttr = [@S0, @S1]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %35 = "mhlo.dynamic_broadcast_in_dim"(%34, %22) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %36 = "disc_shape.tie_shape"(%35, %2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %37 = mhlo.divide %28, %36 {disc.device = "gpu"} : tensor<?x?x?xf32>
    %38 = "disc_shape.tie_shape"(%37, %2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    return %38 : tensor<?x?x?xf32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = mhlo.constant {disc.device = "gpu"} dense<-0.000000e+00> : tensor<f32>
  %1 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
  %2 = tensor.dim %arg0, %c0 : tensor<?x?x?xf32>
  %3 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32>
  %4 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32>
  %5 = "disc_shape.tie_shape"(%arg0, %2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
  %6 = arith.index_cast %4 : index to i32
  %7 = arith.index_cast %2 : index to i32
  %8 = arith.index_cast %3 : index to i32
  %9 = arith.muli %7, %8 : i32
  %10 = tensor.from_elements %9, %6 {disc.shape_op = true} : tensor<2xi32>
  %11 = arith.index_cast %9 : i32 to index
  %12 = mhlo.dynamic_reshape %5, %10 {disc.device = "gpu"} : (tensor<?x?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
  %13 = "disc_shape.tie_shape"(%12, %11, %4) {kDiscSymbolicDimAttr = [@S3, @S2]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
  %14 = mhlo.reduce(%13 init: %1) applies mhlo.maximum across dimensions = [1] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
  %15 = "disc_shape.tie_shape"(%14, %11) {kDiscSymbolicDimAttr = [@S3]} : (tensor<?xf32>, index) -> tensor<?xf32>
  %16 = tensor.from_elements %7, %8 {disc.shape_op = true} : tensor<2xi32>
  %17 = mhlo.dynamic_reshape %15, %16 {disc.device = "gpu"} : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
  %18 = "disc_shape.tie_shape"(%17, %2, %3) {kDiscSymbolicDimAttr = [@S0, @S1]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
  %19 = tensor.from_elements %2, %3, %4 {disc.shape_op = true} : tensor<3xindex>
  %20 = "mhlo.dynamic_broadcast_in_dim"(%18, %19) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
  %21 = "disc_shape.tie_shape"(%20, %2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
  %22 = mhlo.subtract %5, %21 {disc.device = "gpu"} : tensor<?x?x?xf32>
  %23 = "disc_shape.tie_shape"(%22, %2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
  %24 = mhlo.exponential %23 {disc.device = "gpu"} : tensor<?x?x?xf32>
  %25 = "disc_shape.tie_shape"(%24, %2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
  %26 = mhlo.dynamic_reshape %25, %10 {disc.device = "gpu"} : (tensor<?x?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
  %27 = "disc_shape.tie_shape"(%26, %11, %4) {kDiscSymbolicDimAttr = [@S3, @S2]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
  %28 = mhlo.reduce(%27 init: %0) applies mhlo.add across dimensions = [1] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
  %29 = "disc_shape.tie_shape"(%28, %11) {kDiscSymbolicDimAttr = [@S3]} : (tensor<?xf32>, index) -> tensor<?xf32>
  %30 = mhlo.dynamic_reshape %29, %16 {disc.device = "gpu"} : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
  %31 = "disc_shape.tie_shape"(%30, %2, %3) {kDiscSymbolicDimAttr = [@S0, @S1]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
  %32 = "mhlo.dynamic_broadcast_in_dim"(%31, %19) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
  %33 = "disc_shape.tie_shape"(%32, %2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
  %34 = mhlo.divide %25, %33 {disc.device = "gpu"} : tensor<?x?x?xf32>
  %35 = "disc_shape.tie_shape"(%34, %2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
  return %35 : tensor<?x?x?xf32>
}

// -----// IR Dump After FuncBufferize (func-bufferize) //----- //
module {
  func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = bufferization.to_tensor %arg0 : memref<?x?x?xf32>
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %1 = mhlo.constant {disc.device = "gpu"} dense<-0.000000e+00> : tensor<f32>
    %2 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %3 = tensor.dim %0, %c0 : tensor<?x?x?xf32>
    %4 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
    %5 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
    %6 = "disc_shape.tie_shape"(%0, %3, %4, %5) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %7 = arith.index_cast %5 : index to i32
    %8 = arith.index_cast %3 : index to i32
    %9 = arith.index_cast %4 : index to i32
    %10 = arith.muli %8, %9 : i32
    %11 = tensor.from_elements %10, %7 {disc.shape_op = true} : tensor<2xi32>
    %12 = arith.index_cast %10 : i32 to index
    %13 = mhlo.dynamic_reshape %6, %11 {disc.device = "gpu"} : (tensor<?x?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %14 = "disc_shape.tie_shape"(%13, %12, %5) {kDiscSymbolicDimAttr = [@S3, @S2]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %15 = mhlo.reduce(%14 init: %2) applies mhlo.maximum across dimensions = [1] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
    %16 = "disc_shape.tie_shape"(%15, %12) {kDiscSymbolicDimAttr = [@S3]} : (tensor<?xf32>, index) -> tensor<?xf32>
    %17 = tensor.from_elements %8, %9 {disc.shape_op = true} : tensor<2xi32>
    %18 = mhlo.dynamic_reshape %16, %17 {disc.device = "gpu"} : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %19 = "disc_shape.tie_shape"(%18, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %20 = tensor.from_elements %3, %4, %5 {disc.shape_op = true} : tensor<3xindex>
    %21 = "mhlo.dynamic_broadcast_in_dim"(%19, %20) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %22 = "disc_shape.tie_shape"(%21, %3, %4, %5) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %23 = mhlo.subtract %6, %22 {disc.device = "gpu"} : tensor<?x?x?xf32>
    %24 = "disc_shape.tie_shape"(%23, %3, %4, %5) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %25 = mhlo.exponential %24 {disc.device = "gpu"} : tensor<?x?x?xf32>
    %26 = "disc_shape.tie_shape"(%25, %3, %4, %5) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %27 = mhlo.dynamic_reshape %26, %11 {disc.device = "gpu"} : (tensor<?x?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %28 = "disc_shape.tie_shape"(%27, %12, %5) {kDiscSymbolicDimAttr = [@S3, @S2]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %29 = mhlo.reduce(%28 init: %1) applies mhlo.add across dimensions = [1] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
    %30 = "disc_shape.tie_shape"(%29, %12) {kDiscSymbolicDimAttr = [@S3]} : (tensor<?xf32>, index) -> tensor<?xf32>
    %31 = mhlo.dynamic_reshape %30, %17 {disc.device = "gpu"} : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %32 = "disc_shape.tie_shape"(%31, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %33 = "mhlo.dynamic_broadcast_in_dim"(%32, %20) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %34 = "disc_shape.tie_shape"(%33, %3, %4, %5) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %35 = mhlo.divide %26, %34 {disc.device = "gpu"} : tensor<?x?x?xf32>
    %36 = "disc_shape.tie_shape"(%35, %3, %4, %5) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %37 = bufferization.to_memref %36 : memref<?x?x?xf32>
    return %37 : memref<?x?x?xf32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscHloLegalizeToLhloPass (disc-hlo-legalize-to-lhlo) //----- //
module {
  func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = bufferization.to_tensor %arg0 : memref<?x?x?xf32>
    %1 = bufferization.to_memref %0 : memref<?x?x?xf32>
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %2 = mhlo.constant {disc.device = "gpu"} dense<-0.000000e+00> : tensor<f32>
    %3 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %4 = tensor.dim %0, %c0 : tensor<?x?x?xf32>
    %5 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
    %6 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
    %c1_0 = arith.constant 1 : index
    %7 = arith.muli %c1_0, %6 : index
    %8 = arith.muli %7, %5 : index
    %9 = arith.muli %8, %4 : index
    %10 = memref.reinterpret_cast %1 to offset: [0], sizes: [%4, %5, %6], strides: [%8, %7, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %11 = bufferization.to_tensor %10 : memref<?x?x?xf32>
    %12 = arith.index_cast %6 : index to i32
    %13 = arith.index_cast %4 : index to i32
    %14 = arith.index_cast %5 : index to i32
    %15 = arith.muli %13, %14 : i32
    %16 = tensor.from_elements %15, %12 {disc.shape_op = true} : tensor<2xi32>
    %17 = arith.index_cast %15 : i32 to index
    %18 = mhlo.dynamic_reshape %11, %16 {disc.device = "gpu"} : (tensor<?x?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %19 = bufferization.to_memref %18 : memref<?x?xf32>
    %c1_1 = arith.constant 1 : index
    %20 = arith.muli %c1_1, %6 : index
    %21 = arith.muli %20, %17 : index
    %22 = memref.reinterpret_cast %19 to offset: [0], sizes: [%17, %6], strides: [%20, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
    %23 = bufferization.to_tensor %22 : memref<?x?xf32>
    %24 = mhlo.reduce(%23 init: %3) applies mhlo.maximum across dimensions = [1] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
    %25 = bufferization.to_memref %24 : memref<?xf32>
    %c1_2 = arith.constant 1 : index
    %26 = arith.muli %c1_2, %17 : index
    %27 = memref.reinterpret_cast %25 to offset: [0], sizes: [%17], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
    %28 = bufferization.to_tensor %27 : memref<?xf32>
    %29 = tensor.from_elements %13, %14 {disc.shape_op = true} : tensor<2xi32>
    %30 = mhlo.dynamic_reshape %28, %29 {disc.device = "gpu"} : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %31 = bufferization.to_memref %30 : memref<?x?xf32>
    %c1_3 = arith.constant 1 : index
    %32 = arith.muli %c1_3, %5 : index
    %33 = arith.muli %32, %4 : index
    %34 = memref.reinterpret_cast %31 to offset: [0], sizes: [%4, %5], strides: [%32, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
    %35 = bufferization.to_tensor %34 : memref<?x?xf32>
    %36 = tensor.from_elements %4, %5, %6 {disc.shape_op = true} : tensor<3xindex>
    %37 = "mhlo.dynamic_broadcast_in_dim"(%35, %36) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %38 = bufferization.to_memref %37 : memref<?x?x?xf32>
    %c1_4 = arith.constant 1 : index
    %39 = arith.muli %c1_4, %6 : index
    %40 = arith.muli %39, %5 : index
    %41 = arith.muli %40, %4 : index
    %42 = memref.reinterpret_cast %38 to offset: [0], sizes: [%4, %5, %6], strides: [%40, %39, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %43 = bufferization.to_tensor %42 : memref<?x?x?xf32>
    %44 = mhlo.subtract %11, %43 {disc.device = "gpu"} : tensor<?x?x?xf32>
    %45 = bufferization.to_memref %44 : memref<?x?x?xf32>
    %c1_5 = arith.constant 1 : index
    %46 = arith.muli %c1_5, %6 : index
    %47 = arith.muli %46, %5 : index
    %48 = arith.muli %47, %4 : index
    %49 = memref.reinterpret_cast %45 to offset: [0], sizes: [%4, %5, %6], strides: [%47, %46, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %50 = bufferization.to_tensor %49 : memref<?x?x?xf32>
    %51 = mhlo.exponential %50 {disc.device = "gpu"} : tensor<?x?x?xf32>
    %52 = bufferization.to_memref %51 : memref<?x?x?xf32>
    %c1_6 = arith.constant 1 : index
    %53 = arith.muli %c1_6, %6 : index
    %54 = arith.muli %53, %5 : index
    %55 = arith.muli %54, %4 : index
    %56 = memref.reinterpret_cast %52 to offset: [0], sizes: [%4, %5, %6], strides: [%54, %53, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %57 = bufferization.to_tensor %56 : memref<?x?x?xf32>
    %58 = mhlo.dynamic_reshape %57, %16 {disc.device = "gpu"} : (tensor<?x?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %59 = bufferization.to_memref %58 : memref<?x?xf32>
    %c1_7 = arith.constant 1 : index
    %60 = arith.muli %c1_7, %6 : index
    %61 = arith.muli %60, %17 : index
    %62 = memref.reinterpret_cast %59 to offset: [0], sizes: [%17, %6], strides: [%60, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
    %63 = bufferization.to_tensor %62 : memref<?x?xf32>
    %64 = mhlo.reduce(%63 init: %2) applies mhlo.add across dimensions = [1] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
    %65 = bufferization.to_memref %64 : memref<?xf32>
    %c1_8 = arith.constant 1 : index
    %66 = arith.muli %c1_8, %17 : index
    %67 = memref.reinterpret_cast %65 to offset: [0], sizes: [%17], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
    %68 = bufferization.to_tensor %67 : memref<?xf32>
    %69 = mhlo.dynamic_reshape %68, %29 {disc.device = "gpu"} : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %70 = bufferization.to_memref %69 : memref<?x?xf32>
    %c1_9 = arith.constant 1 : index
    %71 = arith.muli %c1_9, %5 : index
    %72 = arith.muli %71, %4 : index
    %73 = memref.reinterpret_cast %70 to offset: [0], sizes: [%4, %5], strides: [%71, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
    %74 = bufferization.to_tensor %73 : memref<?x?xf32>
    %75 = "mhlo.dynamic_broadcast_in_dim"(%74, %36) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x?xf32>, tensor<3xindex>) -> tensor<?x?x?xf32>
    %76 = bufferization.to_memref %75 : memref<?x?x?xf32>
    %c1_10 = arith.constant 1 : index
    %77 = arith.muli %c1_10, %6 : index
    %78 = arith.muli %77, %5 : index
    %79 = arith.muli %78, %4 : index
    %80 = memref.reinterpret_cast %76 to offset: [0], sizes: [%4, %5, %6], strides: [%78, %77, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %81 = bufferization.to_tensor %80 : memref<?x?x?xf32>
    %82 = mhlo.divide %57, %81 {disc.device = "gpu"} : tensor<?x?x?xf32>
    %83 = bufferization.to_memref %82 : memref<?x?x?xf32>
    %c1_11 = arith.constant 1 : index
    %84 = arith.muli %c1_11, %6 : index
    %85 = arith.muli %84, %5 : index
    %86 = arith.muli %85, %4 : index
    %87 = memref.reinterpret_cast %83 to offset: [0], sizes: [%4, %5, %6], strides: [%85, %84, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %88 = bufferization.to_tensor %87 : memref<?x?x?xf32>
    %89 = bufferization.to_memref %88 : memref<?x?x?xf32>
    return %89 : memref<?x?x?xf32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After HloLegalizeToLhloPass (hlo-legalize-to-lhlo) //----- //
module {
  func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = bufferization.to_tensor %arg0 : memref<?x?x?xf32>
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %1 = memref.alloc() : memref<f32>
    "lmhlo.constant"(%1) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32>) -> ()
    %2 = memref.alloc() : memref<f32>
    "lmhlo.constant"(%2) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
    %3 = tensor.dim %0, %c0 : tensor<?x?x?xf32>
    %4 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
    %5 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
    %c1_0 = arith.constant 1 : index
    %6 = arith.muli %c1_0, %5 : index
    %7 = arith.muli %6, %4 : index
    %8 = arith.muli %7, %3 : index
    %9 = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%3, %4, %5], strides: [%7, %6, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %10 = bufferization.to_tensor %9 : memref<?x?x?xf32>
    %11 = bufferization.to_memref %10 : memref<?x?x?xf32>
    %12 = arith.index_cast %5 : index to i32
    %13 = arith.index_cast %3 : index to i32
    %14 = arith.index_cast %4 : index to i32
    %15 = arith.muli %13, %14 : i32
    %16 = tensor.from_elements %15, %12 {disc.shape_op = true} : tensor<2xi32>
    %17 = bufferization.to_memref %16 : memref<2xi32>
    %18 = arith.index_cast %15 : i32 to index
    %19 = bufferization.to_tensor %11 : memref<?x?x?xf32>
    %20 = bufferization.to_tensor %17 : memref<2xi32>
    %21 = arith.index_cast %20 : tensor<2xi32> to tensor<2xindex>
    %c0_1 = arith.constant 0 : index
    %22 = tensor.extract %21[%c0_1] : tensor<2xindex>
    %c1_2 = arith.constant 1 : index
    %23 = tensor.extract %21[%c1_2] : tensor<2xindex>
    %24 = memref.alloc(%22, %23) : memref<?x?xf32>
    "lmhlo.dynamic_reshape"(%11, %17, %24) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
    %25 = bufferization.to_tensor %24 : memref<?x?xf32>
    %26 = bufferization.to_memref %25 : memref<?x?xf32>
    %c1_3 = arith.constant 1 : index
    %27 = arith.muli %c1_3, %5 : index
    %28 = arith.muli %27, %18 : index
    %29 = memref.reinterpret_cast %26 to offset: [0], sizes: [%18, %5], strides: [%27, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
    %30 = bufferization.to_tensor %29 : memref<?x?xf32>
    %31 = bufferization.to_memref %30 : memref<?x?xf32>
    %32 = bufferization.to_tensor %31 : memref<?x?xf32>
    %33 = bufferization.to_tensor %2 : memref<f32>
    %c0_4 = arith.constant 0 : index
    %34 = tensor.dim %32, %c0_4 : tensor<?x?xf32>
    %35 = tensor.from_elements %34 : tensor<1xindex>
    %c0_5 = arith.constant 0 : index
    %36 = tensor.extract %35[%c0_5] : tensor<1xindex>
    %37 = memref.alloc(%36) : memref<?xf32>
    "lmhlo.reduce"(%31, %2, %37) ({
    ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
      %170 = memref.alloc() : memref<f32>
      "lmhlo.maximum"(%arg1, %arg2, %170) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.copy"(%170, %arg3) : (memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
    %38 = bufferization.to_tensor %37 : memref<?xf32>
    %39 = bufferization.to_memref %38 : memref<?xf32>
    %c1_6 = arith.constant 1 : index
    %40 = arith.muli %c1_6, %18 : index
    %41 = memref.reinterpret_cast %39 to offset: [0], sizes: [%18], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
    %42 = bufferization.to_tensor %41 : memref<?xf32>
    %43 = bufferization.to_memref %42 : memref<?xf32>
    %44 = tensor.from_elements %13, %14 {disc.shape_op = true} : tensor<2xi32>
    %45 = bufferization.to_memref %44 : memref<2xi32>
    %46 = bufferization.to_tensor %43 : memref<?xf32>
    %47 = bufferization.to_tensor %45 : memref<2xi32>
    %48 = arith.index_cast %47 : tensor<2xi32> to tensor<2xindex>
    %c0_7 = arith.constant 0 : index
    %49 = tensor.extract %48[%c0_7] : tensor<2xindex>
    %c1_8 = arith.constant 1 : index
    %50 = tensor.extract %48[%c1_8] : tensor<2xindex>
    %51 = memref.alloc(%49, %50) : memref<?x?xf32>
    "lmhlo.dynamic_reshape"(%43, %45, %51) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
    %52 = bufferization.to_tensor %51 : memref<?x?xf32>
    %53 = bufferization.to_memref %52 : memref<?x?xf32>
    %c1_9 = arith.constant 1 : index
    %54 = arith.muli %c1_9, %4 : index
    %55 = arith.muli %54, %3 : index
    %56 = memref.reinterpret_cast %53 to offset: [0], sizes: [%3, %4], strides: [%54, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
    %57 = bufferization.to_tensor %56 : memref<?x?xf32>
    %58 = bufferization.to_memref %57 : memref<?x?xf32>
    %59 = tensor.from_elements %3, %4, %5 {disc.shape_op = true} : tensor<3xindex>
    %60 = bufferization.to_memref %59 : memref<3xindex>
    %61 = bufferization.to_tensor %58 : memref<?x?xf32>
    %62 = bufferization.to_tensor %60 : memref<3xindex>
    %c0_10 = arith.constant 0 : index
    %63 = tensor.extract %62[%c0_10] : tensor<3xindex>
    %c1_11 = arith.constant 1 : index
    %64 = tensor.extract %62[%c1_11] : tensor<3xindex>
    %c2_12 = arith.constant 2 : index
    %65 = tensor.extract %62[%c2_12] : tensor<3xindex>
    %66 = memref.alloc(%63, %64, %65) : memref<?x?x?xf32>
    "lmhlo.dynamic_broadcast_in_dim"(%58, %60, %66) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
    %67 = bufferization.to_tensor %66 : memref<?x?x?xf32>
    %68 = bufferization.to_memref %67 : memref<?x?x?xf32>
    %c1_13 = arith.constant 1 : index
    %69 = arith.muli %c1_13, %5 : index
    %70 = arith.muli %69, %4 : index
    %71 = arith.muli %70, %3 : index
    %72 = memref.reinterpret_cast %68 to offset: [0], sizes: [%3, %4, %5], strides: [%70, %69, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %73 = bufferization.to_tensor %72 : memref<?x?x?xf32>
    %74 = bufferization.to_memref %73 : memref<?x?x?xf32>
    %75 = bufferization.to_tensor %11 : memref<?x?x?xf32>
    %76 = bufferization.to_tensor %74 : memref<?x?x?xf32>
    %77 = shape.shape_of %75 : tensor<?x?x?xf32> -> tensor<3xindex>
    %c0_14 = arith.constant 0 : index
    %78 = tensor.extract %77[%c0_14] : tensor<3xindex>
    %c1_15 = arith.constant 1 : index
    %79 = tensor.extract %77[%c1_15] : tensor<3xindex>
    %c2_16 = arith.constant 2 : index
    %80 = tensor.extract %77[%c2_16] : tensor<3xindex>
    %81 = memref.alloc(%78, %79, %80) : memref<?x?x?xf32>
    "lmhlo.subtract"(%11, %74, %81) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
    %82 = bufferization.to_tensor %81 : memref<?x?x?xf32>
    %83 = bufferization.to_memref %82 : memref<?x?x?xf32>
    %c1_17 = arith.constant 1 : index
    %84 = arith.muli %c1_17, %5 : index
    %85 = arith.muli %84, %4 : index
    %86 = arith.muli %85, %3 : index
    %87 = memref.reinterpret_cast %83 to offset: [0], sizes: [%3, %4, %5], strides: [%85, %84, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %88 = bufferization.to_tensor %87 : memref<?x?x?xf32>
    %89 = bufferization.to_memref %88 : memref<?x?x?xf32>
    %90 = bufferization.to_tensor %89 : memref<?x?x?xf32>
    %91 = shape.shape_of %90 : tensor<?x?x?xf32> -> tensor<3xindex>
    %c0_18 = arith.constant 0 : index
    %92 = tensor.extract %91[%c0_18] : tensor<3xindex>
    %c1_19 = arith.constant 1 : index
    %93 = tensor.extract %91[%c1_19] : tensor<3xindex>
    %c2_20 = arith.constant 2 : index
    %94 = tensor.extract %91[%c2_20] : tensor<3xindex>
    %95 = memref.alloc(%92, %93, %94) : memref<?x?x?xf32>
    "lmhlo.exponential"(%89, %95) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
    %96 = bufferization.to_tensor %95 : memref<?x?x?xf32>
    %97 = bufferization.to_memref %96 : memref<?x?x?xf32>
    %c1_21 = arith.constant 1 : index
    %98 = arith.muli %c1_21, %5 : index
    %99 = arith.muli %98, %4 : index
    %100 = arith.muli %99, %3 : index
    %101 = memref.reinterpret_cast %97 to offset: [0], sizes: [%3, %4, %5], strides: [%99, %98, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %102 = bufferization.to_tensor %101 : memref<?x?x?xf32>
    %103 = bufferization.to_memref %102 : memref<?x?x?xf32>
    %104 = bufferization.to_tensor %103 : memref<?x?x?xf32>
    %105 = bufferization.to_tensor %17 : memref<2xi32>
    %106 = arith.index_cast %105 : tensor<2xi32> to tensor<2xindex>
    %c0_22 = arith.constant 0 : index
    %107 = tensor.extract %106[%c0_22] : tensor<2xindex>
    %c1_23 = arith.constant 1 : index
    %108 = tensor.extract %106[%c1_23] : tensor<2xindex>
    %109 = memref.alloc(%107, %108) : memref<?x?xf32>
    "lmhlo.dynamic_reshape"(%103, %17, %109) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
    %110 = bufferization.to_tensor %109 : memref<?x?xf32>
    %111 = bufferization.to_memref %110 : memref<?x?xf32>
    %c1_24 = arith.constant 1 : index
    %112 = arith.muli %c1_24, %5 : index
    %113 = arith.muli %112, %18 : index
    %114 = memref.reinterpret_cast %111 to offset: [0], sizes: [%18, %5], strides: [%112, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
    %115 = bufferization.to_tensor %114 : memref<?x?xf32>
    %116 = bufferization.to_memref %115 : memref<?x?xf32>
    %117 = bufferization.to_tensor %116 : memref<?x?xf32>
    %118 = bufferization.to_tensor %1 : memref<f32>
    %c0_25 = arith.constant 0 : index
    %119 = tensor.dim %117, %c0_25 : tensor<?x?xf32>
    %120 = tensor.from_elements %119 : tensor<1xindex>
    %c0_26 = arith.constant 0 : index
    %121 = tensor.extract %120[%c0_26] : tensor<1xindex>
    %122 = memref.alloc(%121) : memref<?xf32>
    "lmhlo.reduce"(%116, %1, %122) ({
    ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
      %170 = memref.alloc() : memref<f32>
      "lmhlo.add"(%arg1, %arg2, %170) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.copy"(%170, %arg3) : (memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
    %123 = bufferization.to_tensor %122 : memref<?xf32>
    %124 = bufferization.to_memref %123 : memref<?xf32>
    %c1_27 = arith.constant 1 : index
    %125 = arith.muli %c1_27, %18 : index
    %126 = memref.reinterpret_cast %124 to offset: [0], sizes: [%18], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
    %127 = bufferization.to_tensor %126 : memref<?xf32>
    %128 = bufferization.to_memref %127 : memref<?xf32>
    %129 = bufferization.to_tensor %128 : memref<?xf32>
    %130 = bufferization.to_tensor %45 : memref<2xi32>
    %131 = arith.index_cast %130 : tensor<2xi32> to tensor<2xindex>
    %c0_28 = arith.constant 0 : index
    %132 = tensor.extract %131[%c0_28] : tensor<2xindex>
    %c1_29 = arith.constant 1 : index
    %133 = tensor.extract %131[%c1_29] : tensor<2xindex>
    %134 = memref.alloc(%132, %133) : memref<?x?xf32>
    "lmhlo.dynamic_reshape"(%128, %45, %134) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
    %135 = bufferization.to_tensor %134 : memref<?x?xf32>
    %136 = bufferization.to_memref %135 : memref<?x?xf32>
    %c1_30 = arith.constant 1 : index
    %137 = arith.muli %c1_30, %4 : index
    %138 = arith.muli %137, %3 : index
    %139 = memref.reinterpret_cast %136 to offset: [0], sizes: [%3, %4], strides: [%137, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
    %140 = bufferization.to_tensor %139 : memref<?x?xf32>
    %141 = bufferization.to_memref %140 : memref<?x?xf32>
    %142 = bufferization.to_tensor %141 : memref<?x?xf32>
    %143 = bufferization.to_tensor %60 : memref<3xindex>
    %c0_31 = arith.constant 0 : index
    %144 = tensor.extract %143[%c0_31] : tensor<3xindex>
    %c1_32 = arith.constant 1 : index
    %145 = tensor.extract %143[%c1_32] : tensor<3xindex>
    %c2_33 = arith.constant 2 : index
    %146 = tensor.extract %143[%c2_33] : tensor<3xindex>
    %147 = memref.alloc(%144, %145, %146) : memref<?x?x?xf32>
    "lmhlo.dynamic_broadcast_in_dim"(%141, %60, %147) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
    %148 = bufferization.to_tensor %147 : memref<?x?x?xf32>
    %149 = bufferization.to_memref %148 : memref<?x?x?xf32>
    %c1_34 = arith.constant 1 : index
    %150 = arith.muli %c1_34, %5 : index
    %151 = arith.muli %150, %4 : index
    %152 = arith.muli %151, %3 : index
    %153 = memref.reinterpret_cast %149 to offset: [0], sizes: [%3, %4, %5], strides: [%151, %150, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %154 = bufferization.to_tensor %153 : memref<?x?x?xf32>
    %155 = bufferization.to_memref %154 : memref<?x?x?xf32>
    %156 = bufferization.to_tensor %103 : memref<?x?x?xf32>
    %157 = bufferization.to_tensor %155 : memref<?x?x?xf32>
    %158 = shape.shape_of %156 : tensor<?x?x?xf32> -> tensor<3xindex>
    %c0_35 = arith.constant 0 : index
    %159 = tensor.extract %158[%c0_35] : tensor<3xindex>
    %c1_36 = arith.constant 1 : index
    %160 = tensor.extract %158[%c1_36] : tensor<3xindex>
    %c2_37 = arith.constant 2 : index
    %161 = tensor.extract %158[%c2_37] : tensor<3xindex>
    %162 = memref.alloc(%159, %160, %161) : memref<?x?x?xf32>
    "lmhlo.divide"(%103, %155, %162) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
    %163 = bufferization.to_tensor %162 : memref<?x?x?xf32>
    %164 = bufferization.to_memref %163 : memref<?x?x?xf32>
    %c1_38 = arith.constant 1 : index
    %165 = arith.muli %c1_38, %5 : index
    %166 = arith.muli %165, %4 : index
    %167 = arith.muli %166, %3 : index
    %168 = memref.reinterpret_cast %164 to offset: [0], sizes: [%3, %4, %5], strides: [%166, %165, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %169 = bufferization.to_tensor %168 : memref<?x?x?xf32>
    return %168 : memref<?x?x?xf32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c2 = arith.constant 2 : index
  %0 = memref.alloc() : memref<f32>
  "lmhlo.constant"(%0) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32>) -> ()
  %1 = memref.alloc() : memref<f32>
  "lmhlo.constant"(%1) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %2 = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %3 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %4 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %5 = arith.muli %4, %3 : index
  %6 = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %7 = arith.index_cast %4 : index to i32
  %8 = arith.index_cast %2 : index to i32
  %9 = arith.index_cast %3 : index to i32
  %10 = arith.muli %8, %9 : i32
  %11 = tensor.from_elements %10, %7 {disc.shape_op = true} : tensor<2xi32>
  %12 = bufferization.to_memref %11 : memref<2xi32>
  %13 = arith.index_cast %10 : i32 to index
  %14 = bufferization.to_tensor %12 : memref<2xi32>
  %15 = tensor.extract %14[%c0] : tensor<2xi32>
  %16 = arith.index_cast %15 : i32 to index
  %17 = tensor.extract %14[%c1] : tensor<2xi32>
  %18 = arith.index_cast %17 : i32 to index
  %19 = memref.alloc(%16, %18) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%6, %12, %19) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %20 = memref.reinterpret_cast %19 to offset: [0], sizes: [%13, %4], strides: [%4, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  %21 = memref.alloc(%13) : memref<?xf32>
  "lmhlo.reduce"(%20, %1, %21) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %22 = memref.reinterpret_cast %21 to offset: [0], sizes: [%13], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %23 = tensor.from_elements %8, %9 {disc.shape_op = true} : tensor<2xi32>
  %24 = bufferization.to_memref %23 : memref<2xi32>
  %25 = memref.alloc(%2, %3) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%22, %24, %25) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %26 = memref.reinterpret_cast %25 to offset: [0], sizes: [%2, %3], strides: [%3, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
  %27 = tensor.from_elements %2, %3, %4 {disc.shape_op = true} : tensor<3xindex>
  %28 = bufferization.to_memref %27 : memref<3xindex>
  %29 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%26, %28, %29) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
  %30 = arith.muli %4, %3 : index
  %31 = memref.reinterpret_cast %29 to offset: [0], sizes: [%2, %3, %4], strides: [%30, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %32 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.subtract"(%6, %31, %32) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %33 = arith.muli %4, %3 : index
  %34 = memref.reinterpret_cast %32 to offset: [0], sizes: [%2, %3, %4], strides: [%33, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %35 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.exponential"(%34, %35) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %36 = arith.muli %4, %3 : index
  %37 = memref.reinterpret_cast %35 to offset: [0], sizes: [%2, %3, %4], strides: [%36, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %38 = bufferization.to_tensor %12 : memref<2xi32>
  %39 = tensor.extract %38[%c0] : tensor<2xi32>
  %40 = arith.index_cast %39 : i32 to index
  %41 = tensor.extract %38[%c1] : tensor<2xi32>
  %42 = arith.index_cast %41 : i32 to index
  %43 = memref.alloc(%40, %42) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%37, %12, %43) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %44 = memref.reinterpret_cast %43 to offset: [0], sizes: [%13, %4], strides: [%4, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  %45 = memref.alloc(%13) : memref<?xf32>
  "lmhlo.reduce"(%44, %0, %45) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %46 = memref.reinterpret_cast %45 to offset: [0], sizes: [%13], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %47 = bufferization.to_tensor %24 : memref<2xi32>
  %48 = tensor.extract %47[%c0] : tensor<2xi32>
  %49 = arith.index_cast %48 : i32 to index
  %50 = tensor.extract %47[%c1] : tensor<2xi32>
  %51 = arith.index_cast %50 : i32 to index
  %52 = memref.alloc(%49, %51) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%46, %24, %52) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %53 = memref.reinterpret_cast %52 to offset: [0], sizes: [%2, %3], strides: [%3, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
  %54 = bufferization.to_tensor %28 : memref<3xindex>
  %55 = tensor.extract %54[%c0] : tensor<3xindex>
  %56 = tensor.extract %54[%c1] : tensor<3xindex>
  %57 = tensor.extract %54[%c2] : tensor<3xindex>
  %58 = memref.alloc(%55, %56, %57) : memref<?x?x?xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%53, %28, %58) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
  %59 = arith.muli %4, %3 : index
  %60 = memref.reinterpret_cast %58 to offset: [0], sizes: [%2, %3, %4], strides: [%59, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %61 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.divide"(%37, %60, %61) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %62 = arith.muli %4, %3 : index
  %63 = memref.reinterpret_cast %61 to offset: [0], sizes: [%2, %3, %4], strides: [%62, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  return %63 : memref<?x?x?xf32>
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c2 = arith.constant 2 : index
  %0 = memref.alloc() : memref<f32>
  "lmhlo.constant"(%0) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32>) -> ()
  %1 = memref.alloc() : memref<f32>
  "lmhlo.constant"(%1) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %2 = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %3 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %4 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %5 = arith.muli %4, %3 : index
  %6 = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %7 = arith.index_cast %4 : index to i32
  %8 = arith.index_cast %2 : index to i32
  %9 = arith.index_cast %3 : index to i32
  %10 = arith.muli %8, %9 : i32
  %11 = tensor.from_elements %10, %7 {disc.shape_op = true} : tensor<2xi32>
  %12 = bufferization.to_memref %11 : memref<2xi32>
  %13 = arith.index_cast %10 : i32 to index
  %14 = bufferization.to_tensor %12 : memref<2xi32>
  %15 = tensor.extract %14[%c0] : tensor<2xi32>
  %16 = arith.index_cast %15 : i32 to index
  %17 = tensor.extract %14[%c1] : tensor<2xi32>
  %18 = arith.index_cast %17 : i32 to index
  %19 = memref.alloc(%16, %18) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%6, %12, %19) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %20 = memref.reinterpret_cast %19 to offset: [0], sizes: [%13, %4], strides: [%4, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  %21 = memref.alloc(%13) : memref<?xf32>
  "lmhlo.reduce"(%20, %1, %21) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %22 = memref.reinterpret_cast %21 to offset: [0], sizes: [%13], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %23 = tensor.from_elements %8, %9 {disc.shape_op = true} : tensor<2xi32>
  %24 = bufferization.to_memref %23 : memref<2xi32>
  %25 = memref.alloc(%2, %3) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%22, %24, %25) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %26 = memref.reinterpret_cast %25 to offset: [0], sizes: [%2, %3], strides: [%3, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
  %27 = tensor.from_elements %2, %3, %4 {disc.shape_op = true} : tensor<3xindex>
  %28 = bufferization.to_memref %27 : memref<3xindex>
  %29 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%26, %28, %29) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
  %30 = memref.reinterpret_cast %29 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %31 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.subtract"(%6, %30, %31) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %32 = memref.reinterpret_cast %31 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %33 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.exponential"(%32, %33) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %34 = memref.reinterpret_cast %33 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %35 = bufferization.to_tensor %12 : memref<2xi32>
  %36 = tensor.extract %35[%c0] : tensor<2xi32>
  %37 = arith.index_cast %36 : i32 to index
  %38 = tensor.extract %35[%c1] : tensor<2xi32>
  %39 = arith.index_cast %38 : i32 to index
  %40 = memref.alloc(%37, %39) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%34, %12, %40) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %41 = memref.reinterpret_cast %40 to offset: [0], sizes: [%13, %4], strides: [%4, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  %42 = memref.alloc(%13) : memref<?xf32>
  "lmhlo.reduce"(%41, %0, %42) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %43 = memref.reinterpret_cast %42 to offset: [0], sizes: [%13], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %44 = bufferization.to_tensor %24 : memref<2xi32>
  %45 = tensor.extract %44[%c0] : tensor<2xi32>
  %46 = arith.index_cast %45 : i32 to index
  %47 = tensor.extract %44[%c1] : tensor<2xi32>
  %48 = arith.index_cast %47 : i32 to index
  %49 = memref.alloc(%46, %48) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%43, %24, %49) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %50 = memref.reinterpret_cast %49 to offset: [0], sizes: [%2, %3], strides: [%3, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
  %51 = bufferization.to_tensor %28 : memref<3xindex>
  %52 = tensor.extract %51[%c0] : tensor<3xindex>
  %53 = tensor.extract %51[%c1] : tensor<3xindex>
  %54 = tensor.extract %51[%c2] : tensor<3xindex>
  %55 = memref.alloc(%52, %53, %54) : memref<?x?x?xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%50, %28, %55) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
  %56 = memref.reinterpret_cast %55 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %57 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.divide"(%34, %56, %57) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %58 = memref.reinterpret_cast %57 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  return %58 : memref<?x?x?xf32>
}

// -----// IR Dump After LegalizeToTensorOpPass (lhlo-legalize-to-tensor-op) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c2 = arith.constant 2 : index
  %0 = memref.alloc() : memref<f32>
  "lmhlo.constant"(%0) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32>) -> ()
  %1 = memref.alloc() : memref<f32>
  "lmhlo.constant"(%1) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %2 = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %3 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %4 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %5 = arith.muli %4, %3 : index
  %6 = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %7 = arith.index_cast %4 : index to i32
  %8 = arith.index_cast %2 : index to i32
  %9 = arith.index_cast %3 : index to i32
  %10 = arith.muli %8, %9 : i32
  %11 = tensor.from_elements %10, %7 {disc.shape_op = true} : tensor<2xi32>
  %12 = bufferization.to_memref %11 : memref<2xi32>
  %13 = arith.index_cast %10 : i32 to index
  %14 = memref.load %12[%c0] : memref<2xi32>
  %15 = arith.index_cast %14 : i32 to index
  %16 = memref.load %12[%c1] : memref<2xi32>
  %17 = arith.index_cast %16 : i32 to index
  %18 = memref.alloc(%15, %17) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%6, %12, %18) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %19 = memref.reinterpret_cast %18 to offset: [0], sizes: [%13, %4], strides: [%4, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  %20 = memref.alloc(%13) : memref<?xf32>
  "lmhlo.reduce"(%19, %1, %20) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %21 = memref.reinterpret_cast %20 to offset: [0], sizes: [%13], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %22 = tensor.from_elements %8, %9 {disc.shape_op = true} : tensor<2xi32>
  %23 = bufferization.to_memref %22 : memref<2xi32>
  %24 = memref.alloc(%2, %3) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%21, %23, %24) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %25 = memref.reinterpret_cast %24 to offset: [0], sizes: [%2, %3], strides: [%3, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
  %26 = tensor.from_elements %2, %3, %4 {disc.shape_op = true} : tensor<3xindex>
  %27 = bufferization.to_memref %26 : memref<3xindex>
  %28 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%25, %27, %28) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
  %29 = memref.reinterpret_cast %28 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %30 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.subtract"(%6, %29, %30) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %31 = memref.reinterpret_cast %30 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %32 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.exponential"(%31, %32) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %33 = memref.reinterpret_cast %32 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %34 = memref.load %12[%c0] : memref<2xi32>
  %35 = arith.index_cast %34 : i32 to index
  %36 = memref.load %12[%c1] : memref<2xi32>
  %37 = arith.index_cast %36 : i32 to index
  %38 = memref.alloc(%35, %37) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%33, %12, %38) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %39 = memref.reinterpret_cast %38 to offset: [0], sizes: [%13, %4], strides: [%4, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  %40 = memref.alloc(%13) : memref<?xf32>
  "lmhlo.reduce"(%39, %0, %40) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %41 = memref.reinterpret_cast %40 to offset: [0], sizes: [%13], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %42 = memref.load %23[%c0] : memref<2xi32>
  %43 = arith.index_cast %42 : i32 to index
  %44 = memref.load %23[%c1] : memref<2xi32>
  %45 = arith.index_cast %44 : i32 to index
  %46 = memref.alloc(%43, %45) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%41, %23, %46) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %47 = memref.reinterpret_cast %46 to offset: [0], sizes: [%2, %3], strides: [%3, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
  %48 = memref.load %27[%c0] : memref<3xindex>
  %49 = memref.load %27[%c1] : memref<3xindex>
  %50 = memref.load %27[%c2] : memref<3xindex>
  %51 = memref.alloc(%48, %49, %50) : memref<?x?x?xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%47, %27, %51) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
  %52 = memref.reinterpret_cast %51 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %53 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.divide"(%33, %52, %53) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %54 = memref.reinterpret_cast %53 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  return %54 : memref<?x?x?xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c2 = arith.constant 2 : index
  %0 = memref.alloc() : memref<f32>
  "lmhlo.constant"(%0) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32>) -> ()
  %1 = memref.alloc() : memref<f32>
  "lmhlo.constant"(%1) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %2 = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %3 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %4 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %5 = arith.muli %4, %3 : index
  %6 = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %7 = arith.index_cast %4 : index to i32
  %8 = arith.index_cast %2 : index to i32
  %9 = arith.index_cast %3 : index to i32
  %10 = arith.muli %8, %9 : i32
  %11 = tensor.from_elements %10, %7 {disc.shape_op = true} : tensor<2xi32>
  %12 = bufferization.to_memref %11 : memref<2xi32>
  %13 = arith.index_cast %10 : i32 to index
  %14 = arith.index_cast %10 : i32 to index
  %15 = memref.alloc(%14, %4) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%6, %12, %15) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %16 = memref.reinterpret_cast %15 to offset: [0], sizes: [%13, %4], strides: [%4, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  %17 = memref.alloc(%13) : memref<?xf32>
  "lmhlo.reduce"(%16, %1, %17) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %18 = memref.reinterpret_cast %17 to offset: [0], sizes: [%13], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %19 = tensor.from_elements %8, %9 {disc.shape_op = true} : tensor<2xi32>
  %20 = bufferization.to_memref %19 : memref<2xi32>
  %21 = memref.alloc(%2, %3) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%18, %20, %21) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %22 = memref.reinterpret_cast %21 to offset: [0], sizes: [%2, %3], strides: [%3, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
  %23 = tensor.from_elements %2, %3, %4 {disc.shape_op = true} : tensor<3xindex>
  %24 = bufferization.to_memref %23 : memref<3xindex>
  %25 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%22, %24, %25) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
  %26 = memref.reinterpret_cast %25 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %27 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.subtract"(%6, %26, %27) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %28 = memref.reinterpret_cast %27 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %29 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.exponential"(%28, %29) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %30 = memref.reinterpret_cast %29 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %31 = arith.index_cast %10 : i32 to index
  %32 = memref.alloc(%31, %4) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%30, %12, %32) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %33 = memref.reinterpret_cast %32 to offset: [0], sizes: [%13, %4], strides: [%4, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  %34 = memref.alloc(%13) : memref<?xf32>
  "lmhlo.reduce"(%33, %0, %34) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %35 = memref.reinterpret_cast %34 to offset: [0], sizes: [%13], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %36 = memref.alloc(%2, %3) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%35, %20, %36) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %37 = memref.reinterpret_cast %36 to offset: [0], sizes: [%2, %3], strides: [%3, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
  %38 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%37, %24, %38) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
  %39 = memref.reinterpret_cast %38 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %40 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.divide"(%30, %39, %40) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %41 = memref.reinterpret_cast %40 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  return %41 : memref<?x?x?xf32>
}

// -----// IR Dump After TensorBufferize (tensor-bufferize) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c2 = arith.constant 2 : index
  %0 = memref.alloc() : memref<f32>
  "lmhlo.constant"(%0) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32>) -> ()
  %1 = memref.alloc() : memref<f32>
  "lmhlo.constant"(%1) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %2 = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %3 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %4 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %5 = arith.muli %4, %3 : index
  %6 = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %7 = arith.index_cast %4 : index to i32
  %8 = arith.index_cast %2 : index to i32
  %9 = arith.index_cast %3 : index to i32
  %10 = arith.muli %8, %9 : i32
  %11 = memref.alloc() {alignment = 128 : i64} : memref<2xi32>
  %12 = bufferization.to_tensor %11 : memref<2xi32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  memref.store %10, %11[%c0_0] : memref<2xi32>
  memref.store %7, %11[%c1_1] : memref<2xi32>
  %13 = bufferization.to_tensor %11 : memref<2xi32>
  %14 = arith.index_cast %10 : i32 to index
  %15 = arith.index_cast %10 : i32 to index
  %16 = memref.alloc(%15, %4) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%6, %11, %16) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %17 = memref.reinterpret_cast %16 to offset: [0], sizes: [%14, %4], strides: [%4, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  %18 = memref.alloc(%14) : memref<?xf32>
  "lmhlo.reduce"(%17, %1, %18) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %19 = memref.reinterpret_cast %18 to offset: [0], sizes: [%14], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %20 = memref.alloc() {alignment = 128 : i64} : memref<2xi32>
  %21 = bufferization.to_tensor %20 : memref<2xi32>
  %c0_2 = arith.constant 0 : index
  %c1_3 = arith.constant 1 : index
  memref.store %8, %20[%c0_2] : memref<2xi32>
  memref.store %9, %20[%c1_3] : memref<2xi32>
  %22 = bufferization.to_tensor %20 : memref<2xi32>
  %23 = memref.alloc(%2, %3) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%19, %20, %23) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %24 = memref.reinterpret_cast %23 to offset: [0], sizes: [%2, %3], strides: [%3, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
  %25 = memref.alloc() {alignment = 128 : i64} : memref<3xindex>
  %26 = bufferization.to_tensor %25 : memref<3xindex>
  %c0_4 = arith.constant 0 : index
  %c1_5 = arith.constant 1 : index
  %c2_6 = arith.constant 2 : index
  memref.store %2, %25[%c0_4] : memref<3xindex>
  memref.store %3, %25[%c1_5] : memref<3xindex>
  memref.store %4, %25[%c2_6] : memref<3xindex>
  %27 = bufferization.to_tensor %25 : memref<3xindex>
  %28 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%24, %25, %28) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
  %29 = memref.reinterpret_cast %28 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %30 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.subtract"(%6, %29, %30) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %31 = memref.reinterpret_cast %30 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %32 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.exponential"(%31, %32) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %33 = memref.reinterpret_cast %32 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %34 = arith.index_cast %10 : i32 to index
  %35 = memref.alloc(%34, %4) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%33, %11, %35) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %36 = memref.reinterpret_cast %35 to offset: [0], sizes: [%14, %4], strides: [%4, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  %37 = memref.alloc(%14) : memref<?xf32>
  "lmhlo.reduce"(%36, %0, %37) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %38 = memref.reinterpret_cast %37 to offset: [0], sizes: [%14], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %39 = memref.alloc(%2, %3) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%38, %20, %39) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %40 = memref.reinterpret_cast %39 to offset: [0], sizes: [%2, %3], strides: [%3, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
  %41 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%40, %25, %41) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
  %42 = memref.reinterpret_cast %41 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %43 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.divide"(%33, %42, %43) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %44 = memref.reinterpret_cast %43 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  return %44 : memref<?x?x?xf32>
}

// -----// IR Dump After FinalizingBufferize (finalizing-bufferize) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c2 = arith.constant 2 : index
  %0 = memref.alloc() : memref<f32>
  "lmhlo.constant"(%0) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32>) -> ()
  %1 = memref.alloc() : memref<f32>
  "lmhlo.constant"(%1) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %2 = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %3 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %4 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %5 = arith.muli %4, %3 : index
  %6 = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %7 = arith.index_cast %4 : index to i32
  %8 = arith.index_cast %2 : index to i32
  %9 = arith.index_cast %3 : index to i32
  %10 = arith.muli %8, %9 : i32
  %11 = memref.alloc() {alignment = 128 : i64} : memref<2xi32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  memref.store %10, %11[%c0_0] : memref<2xi32>
  memref.store %7, %11[%c1_1] : memref<2xi32>
  %12 = arith.index_cast %10 : i32 to index
  %13 = arith.index_cast %10 : i32 to index
  %14 = memref.alloc(%13, %4) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%6, %11, %14) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %15 = memref.reinterpret_cast %14 to offset: [0], sizes: [%12, %4], strides: [%4, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  %16 = memref.alloc(%12) : memref<?xf32>
  "lmhlo.reduce"(%15, %1, %16) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %17 = memref.reinterpret_cast %16 to offset: [0], sizes: [%12], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %18 = memref.alloc() {alignment = 128 : i64} : memref<2xi32>
  %c0_2 = arith.constant 0 : index
  %c1_3 = arith.constant 1 : index
  memref.store %8, %18[%c0_2] : memref<2xi32>
  memref.store %9, %18[%c1_3] : memref<2xi32>
  %19 = memref.alloc(%2, %3) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%17, %18, %19) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %20 = memref.reinterpret_cast %19 to offset: [0], sizes: [%2, %3], strides: [%3, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
  %21 = memref.alloc() {alignment = 128 : i64} : memref<3xindex>
  %c0_4 = arith.constant 0 : index
  %c1_5 = arith.constant 1 : index
  %c2_6 = arith.constant 2 : index
  memref.store %2, %21[%c0_4] : memref<3xindex>
  memref.store %3, %21[%c1_5] : memref<3xindex>
  memref.store %4, %21[%c2_6] : memref<3xindex>
  %22 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%20, %21, %22) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
  %23 = memref.reinterpret_cast %22 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %24 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.subtract"(%6, %23, %24) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %25 = memref.reinterpret_cast %24 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %26 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.exponential"(%25, %26) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %27 = memref.reinterpret_cast %26 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %28 = arith.index_cast %10 : i32 to index
  %29 = memref.alloc(%28, %4) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%27, %11, %29) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %30 = memref.reinterpret_cast %29 to offset: [0], sizes: [%12, %4], strides: [%4, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  %31 = memref.alloc(%12) : memref<?xf32>
  "lmhlo.reduce"(%30, %0, %31) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %32 = memref.reinterpret_cast %31 to offset: [0], sizes: [%12], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %33 = memref.alloc(%2, %3) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%32, %18, %33) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %34 = memref.reinterpret_cast %33 to offset: [0], sizes: [%2, %3], strides: [%3, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
  %35 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%34, %21, %35) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
  %36 = memref.reinterpret_cast %35 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %37 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.divide"(%27, %36, %37) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %38 = memref.reinterpret_cast %37 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  return %38 : memref<?x?x?xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c2 = arith.constant 2 : index
  %0 = memref.alloc() : memref<f32>
  "lmhlo.constant"(%0) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32>) -> ()
  %1 = memref.alloc() : memref<f32>
  "lmhlo.constant"(%1) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %2 = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %3 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %4 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %5 = arith.muli %4, %3 : index
  %6 = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %7 = arith.index_cast %4 : index to i32
  %8 = arith.index_cast %2 : index to i32
  %9 = arith.index_cast %3 : index to i32
  %10 = arith.muli %8, %9 : i32
  %11 = memref.alloc() {alignment = 128 : i64} : memref<2xi32>
  memref.store %10, %11[%c0] : memref<2xi32>
  memref.store %7, %11[%c1] : memref<2xi32>
  %12 = arith.index_cast %10 : i32 to index
  %13 = arith.index_cast %10 : i32 to index
  %14 = memref.alloc(%13, %4) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%6, %11, %14) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %15 = memref.reinterpret_cast %14 to offset: [0], sizes: [%12, %4], strides: [%4, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  %16 = memref.alloc(%12) : memref<?xf32>
  "lmhlo.reduce"(%15, %1, %16) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %17 = memref.reinterpret_cast %16 to offset: [0], sizes: [%12], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %18 = memref.alloc() {alignment = 128 : i64} : memref<2xi32>
  memref.store %8, %18[%c0] : memref<2xi32>
  memref.store %9, %18[%c1] : memref<2xi32>
  %19 = memref.alloc(%2, %3) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%17, %18, %19) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %20 = memref.reinterpret_cast %19 to offset: [0], sizes: [%2, %3], strides: [%3, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
  %21 = memref.alloc() {alignment = 128 : i64} : memref<3xindex>
  memref.store %2, %21[%c0] : memref<3xindex>
  memref.store %3, %21[%c1] : memref<3xindex>
  memref.store %4, %21[%c2] : memref<3xindex>
  %22 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%20, %21, %22) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
  %23 = memref.reinterpret_cast %22 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %24 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.subtract"(%6, %23, %24) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %25 = memref.reinterpret_cast %24 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %26 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.exponential"(%25, %26) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %27 = memref.reinterpret_cast %26 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %28 = arith.index_cast %10 : i32 to index
  %29 = memref.alloc(%28, %4) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%27, %11, %29) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %30 = memref.reinterpret_cast %29 to offset: [0], sizes: [%12, %4], strides: [%4, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  %31 = memref.alloc(%12) : memref<?xf32>
  "lmhlo.reduce"(%30, %0, %31) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %32 = memref.reinterpret_cast %31 to offset: [0], sizes: [%12], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %33 = memref.alloc(%2, %3) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%32, %18, %33) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %34 = memref.reinterpret_cast %33 to offset: [0], sizes: [%2, %3], strides: [%3, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
  %35 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%34, %21, %35) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
  %36 = memref.reinterpret_cast %35 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %37 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.divide"(%27, %36, %37) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %38 = memref.reinterpret_cast %37 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  return %38 : memref<?x?x?xf32>
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c2 = arith.constant 2 : index
  %0 = memref.alloc() : memref<f32>
  "lmhlo.constant"(%0) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32>) -> ()
  %1 = memref.alloc() : memref<f32>
  "lmhlo.constant"(%1) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %2 = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %3 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %4 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %5 = arith.muli %4, %3 : index
  %6 = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %7 = arith.index_cast %4 : index to i32
  %8 = arith.index_cast %2 : index to i32
  %9 = arith.index_cast %3 : index to i32
  %10 = arith.muli %8, %9 : i32
  %11 = memref.alloc() {alignment = 128 : i64} : memref<2xi32>
  memref.store %10, %11[%c0] : memref<2xi32>
  memref.store %7, %11[%c1] : memref<2xi32>
  %12 = arith.index_cast %10 : i32 to index
  %13 = memref.alloc(%12, %4) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%6, %11, %13) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %14 = memref.reinterpret_cast %13 to offset: [0], sizes: [%12, %4], strides: [%4, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  %15 = memref.alloc(%12) : memref<?xf32>
  "lmhlo.reduce"(%14, %1, %15) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %16 = memref.reinterpret_cast %15 to offset: [0], sizes: [%12], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %17 = memref.alloc() {alignment = 128 : i64} : memref<2xi32>
  memref.store %8, %17[%c0] : memref<2xi32>
  memref.store %9, %17[%c1] : memref<2xi32>
  %18 = memref.alloc(%2, %3) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%16, %17, %18) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %19 = memref.reinterpret_cast %18 to offset: [0], sizes: [%2, %3], strides: [%3, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
  %20 = memref.alloc() {alignment = 128 : i64} : memref<3xindex>
  memref.store %2, %20[%c0] : memref<3xindex>
  memref.store %3, %20[%c1] : memref<3xindex>
  memref.store %4, %20[%c2] : memref<3xindex>
  %21 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%19, %20, %21) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
  %22 = memref.reinterpret_cast %21 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %23 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.subtract"(%6, %22, %23) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %24 = memref.reinterpret_cast %23 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %25 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.exponential"(%24, %25) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %26 = memref.reinterpret_cast %25 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %27 = memref.alloc(%12, %4) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%26, %11, %27) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %28 = memref.reinterpret_cast %27 to offset: [0], sizes: [%12, %4], strides: [%4, 1] {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  %29 = memref.alloc(%12) : memref<?xf32>
  "lmhlo.reduce"(%28, %0, %29) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %30 = memref.reinterpret_cast %29 to offset: [0], sizes: [%12], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %31 = memref.alloc(%2, %3) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%30, %17, %31) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %32 = memref.reinterpret_cast %31 to offset: [0], sizes: [%2, %3], strides: [%3, 1] {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32> to memref<?x?xf32>
  %33 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%32, %20, %33) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
  %34 = memref.reinterpret_cast %33 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %35 = memref.alloc(%2, %3, %4) : memref<?x?x?xf32>
  "lmhlo.divide"(%26, %34, %35) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %36 = memref.reinterpret_cast %35 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  return %36 : memref<?x?x?xf32>
}

// -----// IR Dump After DiscMemrefCanonicalizer (disc-memref-canonicalize) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c2 = arith.constant 2 : index
  %0 = memref.alloc() : memref<f32>
  "lmhlo.constant"(%0) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32>) -> ()
  %1 = memref.alloc() : memref<f32>
  "lmhlo.constant"(%1) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %2 = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %3 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %4 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %5 = arith.muli %4, %3 : index
  %6 = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %7 = arith.index_cast %4 : index to i32
  %8 = arith.index_cast %2 : index to i32
  %9 = arith.index_cast %3 : index to i32
  %10 = arith.muli %8, %9 : i32
  %11 = memref.alloc() {alignment = 128 : i64} : memref<2xi32>
  memref.store %10, %11[%c0] : memref<2xi32>
  memref.store %7, %11[%c1] : memref<2xi32>
  %12 = arith.index_cast %10 : i32 to index
  %13 = memref.alloc(%12, %4) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%6, %11, %13) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %14 = memref.alloc(%12) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32>
  "lmhlo.reduce"(%13, %1, %14) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %15 = memref.alloc() {alignment = 128 : i64} : memref<2xi32>
  memref.store %8, %15[%c0] : memref<2xi32>
  memref.store %9, %15[%c1] : memref<2xi32>
  %16 = memref.alloc(%2, %3) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%14, %15, %16) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %17 = memref.alloc() {alignment = 128 : i64} : memref<3xindex>
  memref.store %2, %17[%c0] : memref<3xindex>
  memref.store %3, %17[%c1] : memref<3xindex>
  memref.store %4, %17[%c2] : memref<3xindex>
  %18 = memref.alloc(%2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%16, %17, %18) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
  %19 = memref.alloc(%2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32>
  "lmhlo.subtract"(%6, %18, %19) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %20 = memref.alloc(%2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32>
  "lmhlo.exponential"(%19, %20) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %21 = memref.alloc(%12, %4) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%20, %11, %21) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %22 = memref.alloc(%12) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32>
  "lmhlo.reduce"(%21, %0, %22) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %23 = memref.alloc(%2, %3) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%22, %15, %23) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %24 = memref.alloc(%2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%23, %17, %24) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<3xindex>, memref<?x?x?xf32>) -> ()
  %25 = memref.alloc(%2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32>
  "lmhlo.divide"(%20, %24, %25) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  return %25 : memref<?x?x?xf32>
}

// -----// IR Dump After DiscAssignMemorySpacePass (disc-assign-memory-space) //----- //
module {
  func.func @main(%arg0: memref<?x?x?xf32, "gpu">) -> memref<?x?x?xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %0 = memref.alloc() : memref<f32, "gpu">
    "lmhlo.constant"(%0) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32, "gpu">) -> ()
    %1 = memref.alloc() : memref<f32, "gpu">
    "lmhlo.constant"(%1) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
    %2 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
    %3 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
    %4 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
    %5 = arith.muli %4, %3 : index
    %6 = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
    %7 = arith.index_cast %4 : index to i32
    %8 = arith.index_cast %2 : index to i32
    %9 = arith.index_cast %3 : index to i32
    %10 = arith.muli %8, %9 : i32
    %11 = memref.alloc() {alignment = 128 : i64} : memref<2xi32, "cpu">
    memref.store %10, %11[%c0] : memref<2xi32, "cpu">
    memref.store %7, %11[%c1] : memref<2xi32, "cpu">
    %12 = arith.index_cast %10 : i32 to index
    %13 = memref.alloc(%12, %4) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
    "lmhlo.dynamic_reshape"(%6, %11, %13) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
    %14 = memref.alloc(%12) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    "lmhlo.reduce"(%13, %1, %14) ({
    ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
      "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
    %15 = memref.alloc() {alignment = 128 : i64} : memref<2xi32, "cpu">
    memref.store %8, %15[%c0] : memref<2xi32, "cpu">
    memref.store %9, %15[%c1] : memref<2xi32, "cpu">
    %16 = memref.alloc(%2, %3) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
    "lmhlo.dynamic_reshape"(%14, %15, %16) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
    %17 = memref.alloc() {alignment = 128 : i64} : memref<3xindex, "cpu">
    memref.store %2, %17[%c0] : memref<3xindex, "cpu">
    memref.store %3, %17[%c1] : memref<3xindex, "cpu">
    memref.store %4, %17[%c2] : memref<3xindex, "cpu">
    %18 = memref.alloc(%2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    "lmhlo.dynamic_broadcast_in_dim"(%16, %17, %18) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
    %19 = memref.alloc(%2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    "lmhlo.subtract"(%6, %18, %19) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
    %20 = memref.alloc(%2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    "lmhlo.exponential"(%19, %20) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
    %21 = memref.alloc(%12, %4) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
    "lmhlo.dynamic_reshape"(%20, %11, %21) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
    %22 = memref.alloc(%12) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    "lmhlo.reduce"(%21, %0, %22) ({
    ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
      "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
    %23 = memref.alloc(%2, %3) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
    "lmhlo.dynamic_reshape"(%22, %15, %23) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
    %24 = memref.alloc(%2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    "lmhlo.dynamic_broadcast_in_dim"(%23, %17, %24) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
    %25 = memref.alloc(%2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    "lmhlo.divide"(%20, %24, %25) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
    return %25 : memref<?x?x?xf32, "gpu">
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After PromoteBuffersToStack (promote-buffers-to-stack) //----- //
func.func @main(%arg0: memref<?x?x?xf32, "gpu">) -> memref<?x?x?xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c2 = arith.constant 2 : index
  %0 = memref.alloc() : memref<f32, "gpu">
  "lmhlo.constant"(%0) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32, "gpu">) -> ()
  %1 = memref.alloc() : memref<f32, "gpu">
  "lmhlo.constant"(%1) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
  %2 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
  %3 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
  %4 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
  %5 = arith.muli %4, %3 : index
  %6 = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%2, %3, %4], strides: [%5, %4, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %7 = arith.index_cast %4 : index to i32
  %8 = arith.index_cast %2 : index to i32
  %9 = arith.index_cast %3 : index to i32
  %10 = arith.muli %8, %9 : i32
  %11 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %10, %11[%c0] : memref<2xi32, "cpu">
  memref.store %7, %11[%c1] : memref<2xi32, "cpu">
  %12 = arith.index_cast %10 : i32 to index
  %13 = memref.alloc(%12, %4) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  "lmhlo.dynamic_reshape"(%6, %11, %13) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
  %14 = memref.alloc(%12) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  "lmhlo.reduce"(%13, %1, %14) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
  %15 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %8, %15[%c0] : memref<2xi32, "cpu">
  memref.store %9, %15[%c1] : memref<2xi32, "cpu">
  %16 = memref.alloc(%2, %3) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  "lmhlo.dynamic_reshape"(%14, %15, %16) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
  %17 = memref.alloca() : memref<3xindex, "cpu">
  memref.store %2, %17[%c0] : memref<3xindex, "cpu">
  memref.store %3, %17[%c1] : memref<3xindex, "cpu">
  memref.store %4, %17[%c2] : memref<3xindex, "cpu">
  %18 = memref.alloc(%2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  "lmhlo.dynamic_broadcast_in_dim"(%16, %17, %18) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
  %19 = memref.alloc(%2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  "lmhlo.subtract"(%6, %18, %19) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
  %20 = memref.alloc(%2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  "lmhlo.exponential"(%19, %20) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
  %21 = memref.alloc(%12, %4) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  "lmhlo.dynamic_reshape"(%20, %11, %21) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
  %22 = memref.alloc(%12) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  "lmhlo.reduce"(%21, %0, %22) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
  %23 = memref.alloc(%2, %3) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  "lmhlo.dynamic_reshape"(%22, %15, %23) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
  %24 = memref.alloc(%2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  "lmhlo.dynamic_broadcast_in_dim"(%23, %17, %24) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
  %25 = memref.alloc(%2, %3, %4) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  "lmhlo.divide"(%20, %24, %25) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
  return %25 : memref<?x?x?xf32, "gpu">
}

// -----// IR Dump After DiscFusionPass (disc-fusion) //----- //
func.func @main(%arg0: memref<?x?x?xf32, "gpu">) -> memref<?x?x?xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c1 = arith.constant 1 : index
  %0 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
  %c0 = arith.constant 0 : index
  %1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
  %c2 = arith.constant 2 : index
  %2 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
  %3 = memref.alloc() : memref<f32, "gpu">
  %4 = memref.alloc() : memref<f32, "gpu">
  %5 = arith.muli %2, %0 : index
  %6 = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%1, %0, %2], strides: [%5, %2, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %7 = arith.index_cast %2 : index to i32
  %8 = arith.index_cast %1 : index to i32
  %9 = arith.index_cast %0 : index to i32
  %10 = arith.muli %8, %9 : i32
  %11 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %10, %11[%c0] : memref<2xi32, "cpu">
  memref.store %7, %11[%c1] : memref<2xi32, "cpu">
  %12 = arith.index_cast %10 : i32 to index
  %13 = memref.alloc(%12, %2) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %14 = memref.alloc(%12) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %15 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %8, %15[%c0] : memref<2xi32, "cpu">
  memref.store %9, %15[%c1] : memref<2xi32, "cpu">
  %16 = memref.alloc(%1, %0) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %17 = memref.alloca() : memref<3xindex, "cpu">
  memref.store %1, %17[%c0] : memref<3xindex, "cpu">
  memref.store %0, %17[%c1] : memref<3xindex, "cpu">
  memref.store %2, %17[%c2] : memref<3xindex, "cpu">
  %18 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %19 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %20 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %21 = memref.alloc(%12, %2) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %22 = memref.alloc(%12) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %23 = memref.alloc(%1, %0) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %24 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %25 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  "lmhlo.fusion"() ({
    "lmhlo.constant"(%3) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32, "gpu">) -> ()
    "lmhlo.constant"(%4) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
    "lmhlo.dynamic_reshape"(%6, %11, %13) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
    "lmhlo.reduce"(%13, %4, %14) ({
    ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
      "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
    "lmhlo.dynamic_reshape"(%14, %15, %16) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
    "lmhlo.dynamic_broadcast_in_dim"(%16, %17, %18) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
    "lmhlo.subtract"(%6, %18, %19) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
    "lmhlo.exponential"(%19, %20) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
    "lmhlo.dynamic_reshape"(%20, %11, %21) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
    "lmhlo.reduce"(%21, %3, %22) ({
    ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
      "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
    "lmhlo.dynamic_reshape"(%22, %15, %23) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
    "lmhlo.dynamic_broadcast_in_dim"(%23, %17, %24) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
    "lmhlo.divide"(%20, %24, %25) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion_type = "kStitch"} : () -> ()
  return %25 : memref<?x?x?xf32, "gpu">
}

// -----// IR Dump After DiscSpecializeFusionWithSpeculationPass (disc-specialize-fusion-with-speculation) //----- //
func.func @main(%arg0: memref<?x?x?xf32, "gpu">) -> memref<?x?x?xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c1 = arith.constant 1 : index
  %0 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
  %c0 = arith.constant 0 : index
  %1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
  %c2 = arith.constant 2 : index
  %2 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
  %3 = memref.alloc() : memref<f32, "gpu">
  %4 = memref.alloc() : memref<f32, "gpu">
  %5 = arith.muli %2, %0 : index
  %6 = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%1, %0, %2], strides: [%5, %2, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %7 = arith.index_cast %2 : index to i32
  %8 = arith.index_cast %1 : index to i32
  %9 = arith.index_cast %0 : index to i32
  %10 = arith.muli %8, %9 : i32
  %11 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %10, %11[%c0] : memref<2xi32, "cpu">
  memref.store %7, %11[%c1] : memref<2xi32, "cpu">
  %12 = arith.index_cast %10 : i32 to index
  %13 = memref.alloc(%12, %2) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %14 = memref.alloc(%12) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %15 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %8, %15[%c0] : memref<2xi32, "cpu">
  memref.store %9, %15[%c1] : memref<2xi32, "cpu">
  %16 = memref.alloc(%1, %0) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %17 = memref.alloca() : memref<3xindex, "cpu">
  memref.store %1, %17[%c0] : memref<3xindex, "cpu">
  memref.store %0, %17[%c1] : memref<3xindex, "cpu">
  memref.store %2, %17[%c2] : memref<3xindex, "cpu">
  %18 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %19 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %20 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %21 = memref.alloc(%12, %2) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %22 = memref.alloc(%12) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %23 = memref.alloc(%1, %0) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %24 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %25 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %c1_0 = arith.constant 1 : index
  %26 = memref.dim %13, %c1_0 : memref<?x?xf32, "gpu">
  %c0_1 = arith.constant 0 : index
  %27 = memref.dim %13, %c0_1 : memref<?x?xf32, "gpu">
  %c1_2 = arith.constant 1 : index
  %28 = memref.dim %13, %c1_2 : memref<?x?xf32, "gpu">
  %c3 = arith.constant 3 : index
  %29 = arith.cmpi slt, %27, %c3 : index
  %c1024 = arith.constant 1024 : index
  %30 = arith.cmpi sge, %28, %c1024 : index
  %c6 = arith.constant 6 : index
  %31 = arith.cmpi slt, %27, %c6 : index
  %32 = arith.andi %31, %30 : i1
  %c512 = arith.constant 512 : index
  %33 = arith.cmpi sge, %28, %c512 : index
  %c6_3 = arith.constant 6 : index
  %34 = arith.cmpi sge, %27, %c6_3 : index
  %35 = arith.andi %34, %33 : i1
  %36 = arith.ori %29, %32 : i1
  %37 = arith.ori %36, %35 : i1
  scf.if %37 {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%3) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.constant"(%4) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%6, %11, %13) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%13, %4, %14) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%14, %15, %16) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_broadcast_in_dim"(%16, %17, %18) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.subtract"(%6, %18, %19) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.exponential"(%19, %20) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%20, %11, %21) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%21, %3, %22) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%22, %15, %23) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_broadcast_in_dim"(%23, %17, %24) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.divide"(%20, %24, %25) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%3) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.constant"(%4) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%6, %11, %13) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%13, %4, %14) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%14, %15, %16) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_broadcast_in_dim"(%16, %17, %18) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.subtract"(%6, %18, %19) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.exponential"(%19, %20) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%20, %11, %21) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%21, %3, %22) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%22, %15, %23) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_broadcast_in_dim"(%23, %17, %24) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.divide"(%20, %24, %25) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  return %25 : memref<?x?x?xf32, "gpu">
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: memref<?x?x?xf32, "gpu">) -> memref<?x?x?xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c512 = arith.constant 512 : index
  %c6 = arith.constant 6 : index
  %c1024 = arith.constant 1024 : index
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
  %1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
  %2 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
  %3 = memref.alloc() : memref<f32, "gpu">
  %4 = memref.alloc() : memref<f32, "gpu">
  %5 = arith.muli %2, %0 : index
  %6 = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%1, %0, %2], strides: [%5, %2, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %7 = arith.index_cast %2 : index to i32
  %8 = arith.index_cast %1 : index to i32
  %9 = arith.index_cast %0 : index to i32
  %10 = arith.muli %8, %9 : i32
  %11 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %10, %11[%c0] : memref<2xi32, "cpu">
  memref.store %7, %11[%c1] : memref<2xi32, "cpu">
  %12 = arith.index_cast %10 : i32 to index
  %13 = memref.alloc(%12, %2) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %14 = memref.alloc(%12) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %15 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %8, %15[%c0] : memref<2xi32, "cpu">
  memref.store %9, %15[%c1] : memref<2xi32, "cpu">
  %16 = memref.alloc(%1, %0) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %17 = memref.alloca() : memref<3xindex, "cpu">
  memref.store %1, %17[%c0] : memref<3xindex, "cpu">
  memref.store %0, %17[%c1] : memref<3xindex, "cpu">
  memref.store %2, %17[%c2] : memref<3xindex, "cpu">
  %18 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %19 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %20 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %21 = memref.alloc(%12, %2) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %22 = memref.alloc(%12) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %23 = memref.alloc(%1, %0) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %24 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %25 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %26 = arith.cmpi slt, %12, %c3 : index
  %27 = arith.cmpi sge, %2, %c1024 : index
  %28 = arith.cmpi slt, %12, %c6 : index
  %29 = arith.andi %28, %27 : i1
  %30 = arith.cmpi sge, %2, %c512 : index
  %31 = arith.cmpi sge, %12, %c6 : index
  %32 = arith.andi %31, %30 : i1
  %33 = arith.ori %26, %29 : i1
  %34 = arith.ori %33, %32 : i1
  scf.if %34 {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%3) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.constant"(%4) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%6, %11, %13) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%13, %4, %14) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%14, %15, %16) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_broadcast_in_dim"(%16, %17, %18) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.subtract"(%6, %18, %19) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.exponential"(%19, %20) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%20, %11, %21) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%21, %3, %22) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%22, %15, %23) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_broadcast_in_dim"(%23, %17, %24) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.divide"(%20, %24, %25) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%3) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.constant"(%4) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%6, %11, %13) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%13, %4, %14) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%14, %15, %16) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_broadcast_in_dim"(%16, %17, %18) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.subtract"(%6, %18, %19) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.exponential"(%19, %20) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%20, %11, %21) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%21, %3, %22) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%22, %15, %23) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_broadcast_in_dim"(%23, %17, %24) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.divide"(%20, %24, %25) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  return %25 : memref<?x?x?xf32, "gpu">
}

// -----// IR Dump After BufferDeallocation (buffer-deallocation) //----- //
func.func @main(%arg0: memref<?x?x?xf32, "gpu">) -> memref<?x?x?xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c512 = arith.constant 512 : index
  %c6 = arith.constant 6 : index
  %c1024 = arith.constant 1024 : index
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
  %1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
  %2 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
  %3 = memref.alloc() : memref<f32, "gpu">
  %4 = memref.alloc() : memref<f32, "gpu">
  %5 = arith.muli %2, %0 : index
  %6 = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%1, %0, %2], strides: [%5, %2, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %7 = arith.index_cast %2 : index to i32
  %8 = arith.index_cast %1 : index to i32
  %9 = arith.index_cast %0 : index to i32
  %10 = arith.muli %8, %9 : i32
  %11 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %10, %11[%c0] : memref<2xi32, "cpu">
  memref.store %7, %11[%c1] : memref<2xi32, "cpu">
  %12 = arith.index_cast %10 : i32 to index
  %13 = memref.alloc(%12, %2) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %14 = memref.alloc(%12) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %15 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %8, %15[%c0] : memref<2xi32, "cpu">
  memref.store %9, %15[%c1] : memref<2xi32, "cpu">
  %16 = memref.alloc(%1, %0) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %17 = memref.alloca() : memref<3xindex, "cpu">
  memref.store %1, %17[%c0] : memref<3xindex, "cpu">
  memref.store %0, %17[%c1] : memref<3xindex, "cpu">
  memref.store %2, %17[%c2] : memref<3xindex, "cpu">
  %18 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %19 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %20 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %21 = memref.alloc(%12, %2) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %22 = memref.alloc(%12) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %23 = memref.alloc(%1, %0) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %24 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %25 = memref.alloc(%1, %0, %2) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %26 = arith.cmpi slt, %12, %c3 : index
  %27 = arith.cmpi sge, %2, %c1024 : index
  %28 = arith.cmpi slt, %12, %c6 : index
  %29 = arith.andi %28, %27 : i1
  %30 = arith.cmpi sge, %2, %c512 : index
  %31 = arith.cmpi sge, %12, %c6 : index
  %32 = arith.andi %31, %30 : i1
  %33 = arith.ori %26, %29 : i1
  %34 = arith.ori %33, %32 : i1
  scf.if %34 {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%3) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.constant"(%4) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%6, %11, %13) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%13, %4, %14) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%14, %15, %16) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_broadcast_in_dim"(%16, %17, %18) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.subtract"(%6, %18, %19) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.exponential"(%19, %20) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%20, %11, %21) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%21, %3, %22) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%22, %15, %23) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_broadcast_in_dim"(%23, %17, %24) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.divide"(%20, %24, %25) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%3) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.constant"(%4) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%6, %11, %13) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%13, %4, %14) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%14, %15, %16) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_broadcast_in_dim"(%16, %17, %18) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.subtract"(%6, %18, %19) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.exponential"(%19, %20) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%20, %11, %21) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%21, %3, %22) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%22, %15, %23) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_broadcast_in_dim"(%23, %17, %24) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.divide"(%20, %24, %25) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  memref.dealloc %24 : memref<?x?x?xf32, "gpu">
  memref.dealloc %23 : memref<?x?xf32, "gpu">
  memref.dealloc %22 : memref<?xf32, "gpu">
  memref.dealloc %21 : memref<?x?xf32, "gpu">
  memref.dealloc %20 : memref<?x?x?xf32, "gpu">
  memref.dealloc %19 : memref<?x?x?xf32, "gpu">
  memref.dealloc %18 : memref<?x?x?xf32, "gpu">
  memref.dealloc %16 : memref<?x?xf32, "gpu">
  memref.dealloc %14 : memref<?xf32, "gpu">
  memref.dealloc %13 : memref<?x?xf32, "gpu">
  memref.dealloc %4 : memref<f32, "gpu">
  memref.dealloc %3 : memref<f32, "gpu">
  return %25 : memref<?x?x?xf32, "gpu">
}

// -----// IR Dump After RalInjectExecutionContextPass (disc-ral-inject-execution-context) //----- //
module {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %0 = "disc_ral.recv_input"(%arg0, %c0) : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %c512 = arith.constant 512 : index
    %c6 = arith.constant 6 : index
    %c1024 = arith.constant 1024 : index
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c0_0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
    %2 = memref.dim %0, %c0_0 : memref<?x?x?xf32, "gpu">
    %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
    %4 = memref.alloc() : memref<f32, "gpu">
    %5 = memref.alloc() : memref<f32, "gpu">
    %6 = arith.muli %3, %1 : index
    %7 = memref.reinterpret_cast %0 to offset: [0], sizes: [%2, %1, %3], strides: [%6, %3, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
    %8 = arith.index_cast %3 : index to i32
    %9 = arith.index_cast %2 : index to i32
    %10 = arith.index_cast %1 : index to i32
    %11 = arith.muli %9, %10 : i32
    %12 = memref.alloca() : memref<2xi32, "cpu">
    memref.store %11, %12[%c0_0] : memref<2xi32, "cpu">
    memref.store %8, %12[%c1] : memref<2xi32, "cpu">
    %13 = arith.index_cast %11 : i32 to index
    %14 = memref.alloc(%13, %3) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
    %15 = memref.alloc(%13) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %16 = memref.alloca() : memref<2xi32, "cpu">
    memref.store %9, %16[%c0_0] : memref<2xi32, "cpu">
    memref.store %10, %16[%c1] : memref<2xi32, "cpu">
    %17 = memref.alloc(%2, %1) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
    %18 = memref.alloca() : memref<3xindex, "cpu">
    memref.store %2, %18[%c0_0] : memref<3xindex, "cpu">
    memref.store %1, %18[%c1] : memref<3xindex, "cpu">
    memref.store %3, %18[%c2] : memref<3xindex, "cpu">
    %19 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    %20 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    %21 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    %22 = memref.alloc(%13, %3) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
    %23 = memref.alloc(%13) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %24 = memref.alloc(%2, %1) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
    %25 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    %26 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    %27 = arith.cmpi slt, %13, %c3 : index
    %28 = arith.cmpi sge, %3, %c1024 : index
    %29 = arith.cmpi slt, %13, %c6 : index
    %30 = arith.andi %29, %28 : i1
    %31 = arith.cmpi sge, %3, %c512 : index
    %32 = arith.cmpi sge, %13, %c6 : index
    %33 = arith.andi %32, %31 : i1
    %34 = arith.ori %27, %30 : i1
    %35 = arith.ori %34, %33 : i1
    scf.if %35 {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%4) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32, "gpu">) -> ()
        "lmhlo.constant"(%5) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
        "lmhlo.dynamic_reshape"(%7, %12, %14) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
        "lmhlo.reduce"(%14, %5, %15) ({
        ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
          "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
        "lmhlo.dynamic_reshape"(%15, %16, %17) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%17, %18, %19) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
        "lmhlo.subtract"(%7, %19, %20) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
        "lmhlo.exponential"(%20, %21) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
        "lmhlo.dynamic_reshape"(%21, %12, %22) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
        "lmhlo.reduce"(%22, %4, %23) ({
        ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
          "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
        "lmhlo.dynamic_reshape"(%23, %16, %24) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%24, %18, %25) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
        "lmhlo.divide"(%21, %25, %26) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%4) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32, "gpu">) -> ()
        "lmhlo.constant"(%5) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
        "lmhlo.dynamic_reshape"(%7, %12, %14) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
        "lmhlo.reduce"(%14, %5, %15) ({
        ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
          "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
        "lmhlo.dynamic_reshape"(%15, %16, %17) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%17, %18, %19) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
        "lmhlo.subtract"(%7, %19, %20) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
        "lmhlo.exponential"(%20, %21) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
        "lmhlo.dynamic_reshape"(%21, %12, %22) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
        "lmhlo.reduce"(%22, %4, %23) ({
        ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
          "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
        "lmhlo.dynamic_reshape"(%23, %16, %24) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%24, %18, %25) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
        "lmhlo.divide"(%21, %25, %26) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    }
    memref.dealloc %25 : memref<?x?x?xf32, "gpu">
    memref.dealloc %24 : memref<?x?xf32, "gpu">
    memref.dealloc %23 : memref<?xf32, "gpu">
    memref.dealloc %22 : memref<?x?xf32, "gpu">
    memref.dealloc %21 : memref<?x?x?xf32, "gpu">
    memref.dealloc %20 : memref<?x?x?xf32, "gpu">
    memref.dealloc %19 : memref<?x?x?xf32, "gpu">
    memref.dealloc %17 : memref<?x?xf32, "gpu">
    memref.dealloc %15 : memref<?xf32, "gpu">
    memref.dealloc %14 : memref<?x?xf32, "gpu">
    memref.dealloc %5 : memref<f32, "gpu">
    memref.dealloc %4 : memref<f32, "gpu">
    %c0_1 = arith.constant 0 : index
    "disc_ral.send_output"(%arg0, %c0_1, %26) : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
    return
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscLowerToLibraryCallPass (disc-lower-to-library-call) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c1024 = arith.constant 1024 : index
  %c6 = arith.constant 6 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
  %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
  %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
  %4 = memref.alloc() : memref<f32, "gpu">
  %5 = memref.alloc() : memref<f32, "gpu">
  %6 = arith.muli %3, %1 : index
  %7 = memref.reinterpret_cast %0 to offset: [0], sizes: [%2, %1, %3], strides: [%6, %3, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %8 = arith.index_cast %3 : index to i32
  %9 = arith.index_cast %2 : index to i32
  %10 = arith.index_cast %1 : index to i32
  %11 = arith.muli %9, %10 : i32
  %12 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %11, %12[%c0] : memref<2xi32, "cpu">
  memref.store %8, %12[%c1] : memref<2xi32, "cpu">
  %13 = arith.index_cast %11 : i32 to index
  %14 = memref.alloc(%13, %3) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %15 = memref.alloc(%13) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %16 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %9, %16[%c0] : memref<2xi32, "cpu">
  memref.store %10, %16[%c1] : memref<2xi32, "cpu">
  %17 = memref.alloc(%2, %1) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %18 = memref.alloca() : memref<3xindex, "cpu">
  memref.store %2, %18[%c0] : memref<3xindex, "cpu">
  memref.store %1, %18[%c1] : memref<3xindex, "cpu">
  memref.store %3, %18[%c2] : memref<3xindex, "cpu">
  %19 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %20 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %21 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %22 = memref.alloc(%13, %3) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %23 = memref.alloc(%13) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %24 = memref.alloc(%2, %1) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %25 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %26 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %27 = arith.cmpi slt, %13, %c3 : index
  %28 = arith.cmpi sge, %3, %c1024 : index
  %29 = arith.cmpi slt, %13, %c6 : index
  %30 = arith.andi %29, %28 : i1
  %31 = arith.cmpi sge, %3, %c512 : index
  %32 = arith.cmpi sge, %13, %c6 : index
  %33 = arith.andi %32, %31 : i1
  %34 = arith.ori %27, %30 : i1
  %35 = arith.ori %34, %33 : i1
  scf.if %35 {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%4) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.constant"(%5) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%7, %12, %14) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%14, %5, %15) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%15, %16, %17) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_broadcast_in_dim"(%17, %18, %19) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.subtract"(%7, %19, %20) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.exponential"(%20, %21) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%21, %12, %22) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%22, %4, %23) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%23, %16, %24) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_broadcast_in_dim"(%24, %18, %25) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.divide"(%21, %25, %26) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%4) {disc.device = "gpu", value = dense<-0.000000e+00> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.constant"(%5) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%7, %12, %14) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%14, %5, %15) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%15, %16, %17) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_broadcast_in_dim"(%17, %18, %19) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.subtract"(%7, %19, %20) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.exponential"(%20, %21) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%21, %12, %22) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%22, %4, %23) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<1> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%23, %16, %24) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_broadcast_in_dim"(%24, %18, %25) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<3xindex, "cpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.divide"(%21, %25, %26) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  memref.dealloc %25 : memref<?x?x?xf32, "gpu">
  memref.dealloc %24 : memref<?x?xf32, "gpu">
  memref.dealloc %23 : memref<?xf32, "gpu">
  memref.dealloc %22 : memref<?x?xf32, "gpu">
  memref.dealloc %21 : memref<?x?x?xf32, "gpu">
  memref.dealloc %20 : memref<?x?x?xf32, "gpu">
  memref.dealloc %19 : memref<?x?x?xf32, "gpu">
  memref.dealloc %17 : memref<?x?xf32, "gpu">
  memref.dealloc %15 : memref<?xf32, "gpu">
  memref.dealloc %14 : memref<?x?xf32, "gpu">
  memref.dealloc %5 : memref<f32, "gpu">
  memref.dealloc %4 : memref<f32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %26) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After DiscLhloLegalizeRootsToParallelLoopsPass (disc-lhlo-legalize-roots-to-parallel-loops) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant -0.000000e+00 : f32
  %cst_0 = arith.constant 0xFF800000 : f32
  %c8 = arith.constant 8 : index
  %c16_i32 = arith.constant 16 : i32
  %c8_i32 = arith.constant 8 : i32
  %c4_i32 = arith.constant 4 : i32
  %c2_i32 = arith.constant 2 : i32
  %c1_i32 = arith.constant 1 : i32
  %c32_i32 = arith.constant 32 : i32
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c1024 = arith.constant 1024 : index
  %c6 = arith.constant 6 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
  %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
  %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
  %4 = memref.alloc() : memref<f32, "gpu">
  %5 = memref.alloc() : memref<f32, "gpu">
  %6 = arith.muli %3, %1 : index
  %7 = memref.reinterpret_cast %0 to offset: [0], sizes: [%2, %1, %3], strides: [%6, %3, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %8 = arith.index_cast %3 : index to i32
  %9 = arith.index_cast %2 : index to i32
  %10 = arith.index_cast %1 : index to i32
  %11 = arith.muli %9, %10 : i32
  %12 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %11, %12[%c0] : memref<2xi32, "cpu">
  memref.store %8, %12[%c1] : memref<2xi32, "cpu">
  %13 = arith.index_cast %11 : i32 to index
  %14 = memref.alloc(%13, %3) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %15 = memref.alloc(%13) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %16 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %9, %16[%c0] : memref<2xi32, "cpu">
  memref.store %10, %16[%c1] : memref<2xi32, "cpu">
  %17 = memref.alloc(%2, %1) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %18 = memref.alloca() : memref<3xindex, "cpu">
  memref.store %2, %18[%c0] : memref<3xindex, "cpu">
  memref.store %1, %18[%c1] : memref<3xindex, "cpu">
  memref.store %3, %18[%c2] : memref<3xindex, "cpu">
  %19 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %20 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %21 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %22 = memref.alloc(%13, %3) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %23 = memref.alloc(%13) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %24 = memref.alloc(%2, %1) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %25 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %26 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %27 = arith.cmpi slt, %13, %c3 : index
  %28 = arith.cmpi sge, %3, %c1024 : index
  %29 = arith.cmpi slt, %13, %c6 : index
  %30 = arith.andi %29, %28 : i1
  %31 = arith.cmpi sge, %3, %c512 : index
  %32 = arith.cmpi sge, %13, %c6 : index
  %33 = arith.andi %32, %31 : i1
  %34 = arith.ori %27, %30 : i1
  %35 = arith.ori %34, %33 : i1
  scf.if %35 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%13) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %36 = gpu.block_id  x
          %37 = gpu.thread_id  x
          %38 = memref.alloc() : memref<32xf32, 3>
          %39 = gpu.block_id  x
          %40 = gpu.thread_id  x
          %41 = arith.divui %40, %c32 : index
          %42 = arith.remui %40, %c32 : index
          %43 = scf.for %arg3 = %40 to %3 step %c256 iter_args(%arg4 = %cst_0) -> (f32) {
            %66 = "disc_shape.linearize"(%39, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %67:3 = "disc_shape.delinearize"(%66, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %68 = memref.load %7[%67#0, %67#1, %67#2] : memref<?x?x?xf32, "gpu">
            %69 = arith.maxf %arg4, %68 : f32
            scf.yield %69 : f32
          }
          %result, %valid = gpu.shuffle  xor %43, %c1_i32, %c32_i32 : f32
          %44 = arith.maxf %43, %result : f32
          %result_1, %valid_2 = gpu.shuffle  xor %44, %c2_i32, %c32_i32 : f32
          %45 = arith.maxf %44, %result_1 : f32
          %result_3, %valid_4 = gpu.shuffle  xor %45, %c4_i32, %c32_i32 : f32
          %46 = arith.maxf %45, %result_3 : f32
          %result_5, %valid_6 = gpu.shuffle  xor %46, %c8_i32, %c32_i32 : f32
          %47 = arith.maxf %46, %result_5 : f32
          %result_7, %valid_8 = gpu.shuffle  xor %47, %c16_i32, %c32_i32 : f32
          %48 = arith.maxf %47, %result_7 : f32
          %49 = memref.alloc() : memref<8xf32, 3>
          %50 = arith.cmpi eq, %42, %c0 : index
          scf.if %50 {
            memref.store %48, %49[%41] : memref<8xf32, 3>
          }
          gpu.barrier
          %51 = arith.cmpi slt, %40, %c32 : index
          scf.if %51 {
            %66 = arith.cmpi slt, %42, %c8 : index
            %67 = scf.if %66 -> (f32) {
              %71 = memref.load %49[%42] : memref<8xf32, 3>
              scf.yield %71 : f32
            } else {
              scf.yield %cst_0 : f32
            }
            %result_19, %valid_20 = gpu.shuffle  xor %67, %c1_i32, %c8_i32 : f32
            %68 = arith.maxf %67, %result_19 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %68, %c2_i32, %c8_i32 : f32
            %69 = arith.maxf %68, %result_21 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %69, %c4_i32, %c8_i32 : f32
            %70 = arith.maxf %69, %result_23 : f32
            scf.if %50 {
              memref.store %70, %38[%c0] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %52 = memref.alloc() : memref<32xf32, 3>
          %53 = gpu.block_id  x
          %54 = gpu.thread_id  x
          %55 = arith.divui %54, %c32 : index
          %56 = arith.remui %54, %c32 : index
          %57 = scf.for %arg3 = %54 to %3 step %c256 iter_args(%arg4 = %cst) -> (f32) {
            %66 = "disc_shape.linearize"(%53, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %67:3 = "disc_shape.delinearize"(%66, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %68 = memref.load %7[%67#0, %67#1, %67#2] : memref<?x?x?xf32, "gpu">
            %69 = memref.load %38[%c0] : memref<32xf32, 3>
            %70 = arith.subf %68, %69 : f32
            %71 = math.exp %70 : f32
            %72 = arith.addf %arg4, %71 : f32
            scf.yield %72 : f32
          }
          %result_9, %valid_10 = gpu.shuffle  xor %57, %c1_i32, %c32_i32 : f32
          %58 = arith.addf %57, %result_9 : f32
          %result_11, %valid_12 = gpu.shuffle  xor %58, %c2_i32, %c32_i32 : f32
          %59 = arith.addf %58, %result_11 : f32
          %result_13, %valid_14 = gpu.shuffle  xor %59, %c4_i32, %c32_i32 : f32
          %60 = arith.addf %59, %result_13 : f32
          %result_15, %valid_16 = gpu.shuffle  xor %60, %c8_i32, %c32_i32 : f32
          %61 = arith.addf %60, %result_15 : f32
          %result_17, %valid_18 = gpu.shuffle  xor %61, %c16_i32, %c32_i32 : f32
          %62 = arith.addf %61, %result_17 : f32
          %63 = memref.alloc() : memref<8xf32, 3>
          %64 = arith.cmpi eq, %56, %c0 : index
          scf.if %64 {
            memref.store %62, %63[%55] : memref<8xf32, 3>
          }
          gpu.barrier
          %65 = arith.cmpi slt, %54, %c32 : index
          scf.if %65 {
            %66 = arith.cmpi slt, %56, %c8 : index
            %67 = scf.if %66 -> (f32) {
              %71 = memref.load %63[%56] : memref<8xf32, 3>
              scf.yield %71 : f32
            } else {
              scf.yield %cst : f32
            }
            %result_19, %valid_20 = gpu.shuffle  xor %67, %c1_i32, %c8_i32 : f32
            %68 = arith.addf %67, %result_19 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %68, %c2_i32, %c8_i32 : f32
            %69 = arith.addf %68, %result_21 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %69, %c4_i32, %c8_i32 : f32
            %70 = arith.addf %69, %result_23 : f32
            scf.if %64 {
              memref.store %70, %52[%c0] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          scf.for %arg3 = %37 to %3 step %c256 {
            %66 = arith.muli %36, %3 : index
            %67 = arith.addi %66, %arg3 : index
            %68 = arith.muli %2, %1 : index
            %69 = arith.muli %68, %3 : index
            %70 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%69], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %71:3 = "disc_shape.delinearize"(%67, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %72 = memref.load %7[%71#0, %71#1, %71#2] : memref<?x?x?xf32, "gpu">
            %73 = memref.load %38[%c0] : memref<32xf32, 3>
            %74 = arith.subf %72, %73 : f32
            %75 = math.exp %74 : f32
            %76 = memref.load %52[%c0] : memref<32xf32, 3>
            %77 = arith.divf %75, %76 : f32
            memref.store %77, %70[%67] : memref<?xf32, "gpu">
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      %36 = arith.ceildivui %13, %c8 : index
      scf.parallel (%arg1) = (%c0) to (%36) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %37 = gpu.block_id  x
          %38 = gpu.thread_id  x
          %39 = arith.divui %38, %c32 : index
          %40 = arith.remui %38, %c32 : index
          %41 = arith.muli %37, %c8 : index
          %42 = arith.addi %41, %39 : index
          %43 = arith.cmpi slt, %42, %13 : index
          %44 = memref.alloc() : memref<32xf32, 3>
          %45 = gpu.block_id  x
          %46 = gpu.thread_id  x
          %47 = arith.divui %46, %c32 : index
          %48 = arith.remui %46, %c32 : index
          %49 = arith.muli %45, %c8 : index
          %50 = arith.addi %49, %47 : index
          %51 = arith.cmpi slt, %50, %13 : index
          scf.if %51 {
            %60 = scf.for %arg3 = %48 to %3 step %c32 iter_args(%arg4 = %cst_0) -> (f32) {
              %67 = "disc_shape.linearize"(%50, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %68:3 = "disc_shape.delinearize"(%67, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %69 = memref.load %7[%68#0, %68#1, %68#2] : memref<?x?x?xf32, "gpu">
              %70 = arith.maxf %arg4, %69 : f32
              scf.yield %70 : f32
            }
            %result, %valid = gpu.shuffle  xor %60, %c1_i32, %c32_i32 : f32
            %61 = arith.maxf %60, %result : f32
            %result_1, %valid_2 = gpu.shuffle  xor %61, %c2_i32, %c32_i32 : f32
            %62 = arith.maxf %61, %result_1 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %62, %c4_i32, %c32_i32 : f32
            %63 = arith.maxf %62, %result_3 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %63, %c8_i32, %c32_i32 : f32
            %64 = arith.maxf %63, %result_5 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %64, %c16_i32, %c32_i32 : f32
            %65 = arith.maxf %64, %result_7 : f32
            %66 = arith.cmpi eq, %48, %c0 : index
            scf.if %66 {
              memref.store %65, %44[%47] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %52 = memref.alloc() : memref<32xf32, 3>
          %53 = gpu.block_id  x
          %54 = gpu.thread_id  x
          %55 = arith.divui %54, %c32 : index
          %56 = arith.remui %54, %c32 : index
          %57 = arith.muli %53, %c8 : index
          %58 = arith.addi %57, %55 : index
          %59 = arith.cmpi slt, %58, %13 : index
          scf.if %59 {
            %60 = scf.for %arg3 = %56 to %3 step %c32 iter_args(%arg4 = %cst) -> (f32) {
              %67 = "disc_shape.linearize"(%58, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %68:3 = "disc_shape.delinearize"(%67, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %69 = memref.load %7[%68#0, %68#1, %68#2] : memref<?x?x?xf32, "gpu">
              %70 = "disc_shape.linearize"(%68#0, %68#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %71 = "disc_shape.delinearize"(%70, %13) : (index, index) -> index
              %72 = "disc_shape.linearize"(%71, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %73 = arith.remui %72, %c8 : index
              %74 = memref.load %44[%73] : memref<32xf32, 3>
              %75 = arith.subf %69, %74 : f32
              %76 = math.exp %75 : f32
              %77 = arith.addf %arg4, %76 : f32
              scf.yield %77 : f32
            }
            %result, %valid = gpu.shuffle  xor %60, %c1_i32, %c32_i32 : f32
            %61 = arith.addf %60, %result : f32
            %result_1, %valid_2 = gpu.shuffle  xor %61, %c2_i32, %c32_i32 : f32
            %62 = arith.addf %61, %result_1 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %62, %c4_i32, %c32_i32 : f32
            %63 = arith.addf %62, %result_3 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %63, %c8_i32, %c32_i32 : f32
            %64 = arith.addf %63, %result_5 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %64, %c16_i32, %c32_i32 : f32
            %65 = arith.addf %64, %result_7 : f32
            %66 = arith.cmpi eq, %56, %c0 : index
            scf.if %66 {
              memref.store %65, %52[%55] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          scf.if %43 {
            scf.for %arg3 = %40 to %3 step %c32 {
              %60 = arith.muli %42, %3 : index
              %61 = arith.addi %60, %arg3 : index
              %62 = arith.muli %2, %1 : index
              %63 = arith.muli %62, %3 : index
              %64 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%63], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %65:3 = "disc_shape.delinearize"(%61, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %66 = memref.load %7[%65#0, %65#1, %65#2] : memref<?x?x?xf32, "gpu">
              %67 = "disc_shape.linearize"(%65#0, %65#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %68 = "disc_shape.delinearize"(%67, %13) : (index, index) -> index
              %69 = "disc_shape.linearize"(%68, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %70 = arith.remui %69, %c8 : index
              %71 = memref.load %44[%70] : memref<32xf32, 3>
              %72 = arith.subf %66, %71 : f32
              %73 = math.exp %72 : f32
              %74 = "disc_shape.linearize"(%65#0, %65#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %75 = "disc_shape.delinearize"(%74, %13) : (index, index) -> index
              %76 = "disc_shape.linearize"(%75, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %77 = arith.remui %76, %c8 : index
              %78 = memref.load %52[%77] : memref<32xf32, 3>
              %79 = arith.divf %73, %78 : f32
              memref.store %79, %64[%61] : memref<?xf32, "gpu">
            }
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  memref.dealloc %25 : memref<?x?x?xf32, "gpu">
  memref.dealloc %24 : memref<?x?xf32, "gpu">
  memref.dealloc %23 : memref<?xf32, "gpu">
  memref.dealloc %22 : memref<?x?xf32, "gpu">
  memref.dealloc %21 : memref<?x?x?xf32, "gpu">
  memref.dealloc %20 : memref<?x?x?xf32, "gpu">
  memref.dealloc %19 : memref<?x?x?xf32, "gpu">
  memref.dealloc %17 : memref<?x?xf32, "gpu">
  memref.dealloc %15 : memref<?xf32, "gpu">
  memref.dealloc %14 : memref<?x?xf32, "gpu">
  memref.dealloc %5 : memref<f32, "gpu">
  memref.dealloc %4 : memref<f32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %26) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ForLoopUnrollInterleave (disc-for-loop-unroll-interleave) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant -0.000000e+00 : f32
  %cst_0 = arith.constant 0xFF800000 : f32
  %c8 = arith.constant 8 : index
  %c16_i32 = arith.constant 16 : i32
  %c8_i32 = arith.constant 8 : i32
  %c4_i32 = arith.constant 4 : i32
  %c2_i32 = arith.constant 2 : i32
  %c1_i32 = arith.constant 1 : i32
  %c32_i32 = arith.constant 32 : i32
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c1024 = arith.constant 1024 : index
  %c6 = arith.constant 6 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
  %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
  %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
  %4 = memref.alloc() : memref<f32, "gpu">
  %5 = memref.alloc() : memref<f32, "gpu">
  %6 = arith.muli %3, %1 : index
  %7 = memref.reinterpret_cast %0 to offset: [0], sizes: [%2, %1, %3], strides: [%6, %3, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %8 = arith.index_cast %3 : index to i32
  %9 = arith.index_cast %2 : index to i32
  %10 = arith.index_cast %1 : index to i32
  %11 = arith.muli %9, %10 : i32
  %12 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %11, %12[%c0] : memref<2xi32, "cpu">
  memref.store %8, %12[%c1] : memref<2xi32, "cpu">
  %13 = arith.index_cast %11 : i32 to index
  %14 = memref.alloc(%13, %3) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %15 = memref.alloc(%13) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %16 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %9, %16[%c0] : memref<2xi32, "cpu">
  memref.store %10, %16[%c1] : memref<2xi32, "cpu">
  %17 = memref.alloc(%2, %1) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %18 = memref.alloca() : memref<3xindex, "cpu">
  memref.store %2, %18[%c0] : memref<3xindex, "cpu">
  memref.store %1, %18[%c1] : memref<3xindex, "cpu">
  memref.store %3, %18[%c2] : memref<3xindex, "cpu">
  %19 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %20 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %21 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %22 = memref.alloc(%13, %3) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %23 = memref.alloc(%13) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %24 = memref.alloc(%2, %1) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %25 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %26 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %27 = arith.cmpi slt, %13, %c3 : index
  %28 = arith.cmpi sge, %3, %c1024 : index
  %29 = arith.cmpi slt, %13, %c6 : index
  %30 = arith.andi %29, %28 : i1
  %31 = arith.cmpi sge, %3, %c512 : index
  %32 = arith.cmpi sge, %13, %c6 : index
  %33 = arith.andi %32, %31 : i1
  %34 = arith.ori %27, %30 : i1
  %35 = arith.ori %34, %33 : i1
  scf.if %35 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%13) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %36 = gpu.block_id  x
          %37 = gpu.thread_id  x
          %38 = memref.alloc() : memref<32xf32, 3>
          %39 = gpu.block_id  x
          %40 = gpu.thread_id  x
          %41 = arith.divui %40, %c32 : index
          %42 = arith.remui %40, %c32 : index
          %43 = arith.subi %3, %40 : index
          %44 = arith.ceildivui %43, %c256 : index
          %c4 = arith.constant 4 : index
          %45 = arith.remsi %44, %c4 : index
          %46 = arith.subi %44, %45 : index
          %47 = arith.muli %46, %c256 : index
          %48 = arith.addi %40, %47 : index
          %49 = arith.muli %c256, %c4 : index
          %50 = scf.for %arg3 = %40 to %48 step %49 iter_args(%arg4 = %cst_0) -> (f32) {
            %c1_21 = arith.constant 1 : index
            %89 = arith.muli %c256, %c1_21 : index
            %90 = arith.addi %arg3, %89 : index
            %c2_22 = arith.constant 2 : index
            %91 = arith.muli %c256, %c2_22 : index
            %92 = arith.addi %arg3, %91 : index
            %c3_23 = arith.constant 3 : index
            %93 = arith.muli %c256, %c3_23 : index
            %94 = arith.addi %arg3, %93 : index
            %95 = "disc_shape.linearize"(%39, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %96 = "disc_shape.linearize"(%39, %90, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %97 = "disc_shape.linearize"(%39, %92, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %98 = "disc_shape.linearize"(%39, %94, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %99:3 = "disc_shape.delinearize"(%95, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %100:3 = "disc_shape.delinearize"(%96, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %101:3 = "disc_shape.delinearize"(%97, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %102:3 = "disc_shape.delinearize"(%98, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %103 = memref.load %7[%99#0, %99#1, %99#2] : memref<?x?x?xf32, "gpu">
            %104 = memref.load %7[%100#0, %100#1, %100#2] : memref<?x?x?xf32, "gpu">
            %105 = memref.load %7[%101#0, %101#1, %101#2] : memref<?x?x?xf32, "gpu">
            %106 = memref.load %7[%102#0, %102#1, %102#2] : memref<?x?x?xf32, "gpu">
            %107 = arith.maxf %arg4, %103 : f32
            %108 = arith.maxf %107, %104 : f32
            %109 = arith.maxf %108, %105 : f32
            %110 = arith.maxf %109, %106 : f32
            scf.yield %110 : f32
          }
          %51 = scf.for %arg3 = %48 to %3 step %c256 iter_args(%arg4 = %50) -> (f32) {
            %89 = "disc_shape.linearize"(%39, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %90:3 = "disc_shape.delinearize"(%89, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %91 = memref.load %7[%90#0, %90#1, %90#2] : memref<?x?x?xf32, "gpu">
            %92 = arith.maxf %arg4, %91 : f32
            scf.yield %92 : f32
          }
          %result, %valid = gpu.shuffle  xor %51, %c1_i32, %c32_i32 : f32
          %52 = arith.maxf %51, %result : f32
          %result_1, %valid_2 = gpu.shuffle  xor %52, %c2_i32, %c32_i32 : f32
          %53 = arith.maxf %52, %result_1 : f32
          %result_3, %valid_4 = gpu.shuffle  xor %53, %c4_i32, %c32_i32 : f32
          %54 = arith.maxf %53, %result_3 : f32
          %result_5, %valid_6 = gpu.shuffle  xor %54, %c8_i32, %c32_i32 : f32
          %55 = arith.maxf %54, %result_5 : f32
          %result_7, %valid_8 = gpu.shuffle  xor %55, %c16_i32, %c32_i32 : f32
          %56 = arith.maxf %55, %result_7 : f32
          %57 = memref.alloc() : memref<8xf32, 3>
          %58 = arith.cmpi eq, %42, %c0 : index
          scf.if %58 {
            memref.store %56, %57[%41] : memref<8xf32, 3>
          }
          gpu.barrier
          %59 = arith.cmpi slt, %40, %c32 : index
          scf.if %59 {
            %89 = arith.cmpi slt, %42, %c8 : index
            %90 = scf.if %89 -> (f32) {
              %94 = memref.load %57[%42] : memref<8xf32, 3>
              scf.yield %94 : f32
            } else {
              scf.yield %cst_0 : f32
            }
            %result_21, %valid_22 = gpu.shuffle  xor %90, %c1_i32, %c8_i32 : f32
            %91 = arith.maxf %90, %result_21 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %91, %c2_i32, %c8_i32 : f32
            %92 = arith.maxf %91, %result_23 : f32
            %result_25, %valid_26 = gpu.shuffle  xor %92, %c4_i32, %c8_i32 : f32
            %93 = arith.maxf %92, %result_25 : f32
            scf.if %58 {
              memref.store %93, %38[%c0] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %60 = memref.alloc() : memref<32xf32, 3>
          %61 = gpu.block_id  x
          %62 = gpu.thread_id  x
          %63 = arith.divui %62, %c32 : index
          %64 = arith.remui %62, %c32 : index
          %65 = arith.subi %3, %62 : index
          %66 = arith.ceildivui %65, %c256 : index
          %c4_9 = arith.constant 4 : index
          %67 = arith.remsi %66, %c4_9 : index
          %68 = arith.subi %66, %67 : index
          %69 = arith.muli %68, %c256 : index
          %70 = arith.addi %62, %69 : index
          %71 = arith.muli %c256, %c4_9 : index
          %72 = scf.for %arg3 = %62 to %70 step %71 iter_args(%arg4 = %cst) -> (f32) {
            %c1_21 = arith.constant 1 : index
            %89 = arith.muli %c256, %c1_21 : index
            %90 = arith.addi %arg3, %89 : index
            %c2_22 = arith.constant 2 : index
            %91 = arith.muli %c256, %c2_22 : index
            %92 = arith.addi %arg3, %91 : index
            %c3_23 = arith.constant 3 : index
            %93 = arith.muli %c256, %c3_23 : index
            %94 = arith.addi %arg3, %93 : index
            %95 = "disc_shape.linearize"(%61, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %96 = "disc_shape.linearize"(%61, %90, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %97 = "disc_shape.linearize"(%61, %92, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %98 = "disc_shape.linearize"(%61, %94, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %99:3 = "disc_shape.delinearize"(%95, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %100:3 = "disc_shape.delinearize"(%96, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %101:3 = "disc_shape.delinearize"(%97, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %102:3 = "disc_shape.delinearize"(%98, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %103 = memref.load %7[%99#0, %99#1, %99#2] : memref<?x?x?xf32, "gpu">
            %104 = memref.load %7[%100#0, %100#1, %100#2] : memref<?x?x?xf32, "gpu">
            %105 = memref.load %7[%101#0, %101#1, %101#2] : memref<?x?x?xf32, "gpu">
            %106 = memref.load %7[%102#0, %102#1, %102#2] : memref<?x?x?xf32, "gpu">
            %107 = memref.load %38[%c0] : memref<32xf32, 3>
            %108 = memref.load %38[%c0] : memref<32xf32, 3>
            %109 = memref.load %38[%c0] : memref<32xf32, 3>
            %110 = memref.load %38[%c0] : memref<32xf32, 3>
            %111 = arith.subf %103, %107 : f32
            %112 = arith.subf %104, %108 : f32
            %113 = arith.subf %105, %109 : f32
            %114 = arith.subf %106, %110 : f32
            %115 = math.exp %111 : f32
            %116 = math.exp %112 : f32
            %117 = math.exp %113 : f32
            %118 = math.exp %114 : f32
            %119 = arith.addf %arg4, %115 : f32
            %120 = arith.addf %119, %116 : f32
            %121 = arith.addf %120, %117 : f32
            %122 = arith.addf %121, %118 : f32
            scf.yield %122 : f32
          }
          %73 = scf.for %arg3 = %70 to %3 step %c256 iter_args(%arg4 = %72) -> (f32) {
            %89 = "disc_shape.linearize"(%61, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %90:3 = "disc_shape.delinearize"(%89, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %91 = memref.load %7[%90#0, %90#1, %90#2] : memref<?x?x?xf32, "gpu">
            %92 = memref.load %38[%c0] : memref<32xf32, 3>
            %93 = arith.subf %91, %92 : f32
            %94 = math.exp %93 : f32
            %95 = arith.addf %arg4, %94 : f32
            scf.yield %95 : f32
          }
          %result_10, %valid_11 = gpu.shuffle  xor %73, %c1_i32, %c32_i32 : f32
          %74 = arith.addf %73, %result_10 : f32
          %result_12, %valid_13 = gpu.shuffle  xor %74, %c2_i32, %c32_i32 : f32
          %75 = arith.addf %74, %result_12 : f32
          %result_14, %valid_15 = gpu.shuffle  xor %75, %c4_i32, %c32_i32 : f32
          %76 = arith.addf %75, %result_14 : f32
          %result_16, %valid_17 = gpu.shuffle  xor %76, %c8_i32, %c32_i32 : f32
          %77 = arith.addf %76, %result_16 : f32
          %result_18, %valid_19 = gpu.shuffle  xor %77, %c16_i32, %c32_i32 : f32
          %78 = arith.addf %77, %result_18 : f32
          %79 = memref.alloc() : memref<8xf32, 3>
          %80 = arith.cmpi eq, %64, %c0 : index
          scf.if %80 {
            memref.store %78, %79[%63] : memref<8xf32, 3>
          }
          gpu.barrier
          %81 = arith.cmpi slt, %62, %c32 : index
          scf.if %81 {
            %89 = arith.cmpi slt, %64, %c8 : index
            %90 = scf.if %89 -> (f32) {
              %94 = memref.load %79[%64] : memref<8xf32, 3>
              scf.yield %94 : f32
            } else {
              scf.yield %cst : f32
            }
            %result_21, %valid_22 = gpu.shuffle  xor %90, %c1_i32, %c8_i32 : f32
            %91 = arith.addf %90, %result_21 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %91, %c2_i32, %c8_i32 : f32
            %92 = arith.addf %91, %result_23 : f32
            %result_25, %valid_26 = gpu.shuffle  xor %92, %c4_i32, %c8_i32 : f32
            %93 = arith.addf %92, %result_25 : f32
            scf.if %80 {
              memref.store %93, %60[%c0] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %82 = arith.subi %3, %37 : index
          %83 = arith.ceildivui %82, %c256 : index
          %c4_20 = arith.constant 4 : index
          %84 = arith.remsi %83, %c4_20 : index
          %85 = arith.subi %83, %84 : index
          %86 = arith.muli %85, %c256 : index
          %87 = arith.addi %37, %86 : index
          %88 = arith.muli %c256, %c4_20 : index
          scf.for %arg3 = %37 to %87 step %88 {
            %c1_21 = arith.constant 1 : index
            %89 = arith.muli %c256, %c1_21 : index
            %90 = arith.addi %arg3, %89 : index
            %c2_22 = arith.constant 2 : index
            %91 = arith.muli %c256, %c2_22 : index
            %92 = arith.addi %arg3, %91 : index
            %c3_23 = arith.constant 3 : index
            %93 = arith.muli %c256, %c3_23 : index
            %94 = arith.addi %arg3, %93 : index
            %95 = arith.muli %36, %3 : index
            %96 = arith.muli %36, %3 : index
            %97 = arith.muli %36, %3 : index
            %98 = arith.muli %36, %3 : index
            %99 = arith.addi %95, %arg3 : index
            %100 = arith.addi %96, %90 : index
            %101 = arith.addi %97, %92 : index
            %102 = arith.addi %98, %94 : index
            %103 = arith.muli %2, %1 : index
            %104 = arith.muli %2, %1 : index
            %105 = arith.muli %2, %1 : index
            %106 = arith.muli %2, %1 : index
            %107 = arith.muli %103, %3 : index
            %108 = arith.muli %104, %3 : index
            %109 = arith.muli %105, %3 : index
            %110 = arith.muli %106, %3 : index
            %111 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%107], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %112 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%108], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %113 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%109], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %114 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%110], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %115:3 = "disc_shape.delinearize"(%99, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %116:3 = "disc_shape.delinearize"(%100, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %117:3 = "disc_shape.delinearize"(%101, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %118:3 = "disc_shape.delinearize"(%102, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %119 = memref.load %7[%115#0, %115#1, %115#2] : memref<?x?x?xf32, "gpu">
            %120 = memref.load %7[%116#0, %116#1, %116#2] : memref<?x?x?xf32, "gpu">
            %121 = memref.load %7[%117#0, %117#1, %117#2] : memref<?x?x?xf32, "gpu">
            %122 = memref.load %7[%118#0, %118#1, %118#2] : memref<?x?x?xf32, "gpu">
            %123 = memref.load %38[%c0] : memref<32xf32, 3>
            %124 = memref.load %38[%c0] : memref<32xf32, 3>
            %125 = memref.load %38[%c0] : memref<32xf32, 3>
            %126 = memref.load %38[%c0] : memref<32xf32, 3>
            %127 = arith.subf %119, %123 : f32
            %128 = arith.subf %120, %124 : f32
            %129 = arith.subf %121, %125 : f32
            %130 = arith.subf %122, %126 : f32
            %131 = math.exp %127 : f32
            %132 = math.exp %128 : f32
            %133 = math.exp %129 : f32
            %134 = math.exp %130 : f32
            %135 = memref.load %60[%c0] : memref<32xf32, 3>
            %136 = memref.load %60[%c0] : memref<32xf32, 3>
            %137 = memref.load %60[%c0] : memref<32xf32, 3>
            %138 = memref.load %60[%c0] : memref<32xf32, 3>
            %139 = arith.divf %131, %135 : f32
            %140 = arith.divf %132, %136 : f32
            %141 = arith.divf %133, %137 : f32
            %142 = arith.divf %134, %138 : f32
            memref.store %139, %111[%99] : memref<?xf32, "gpu">
            memref.store %140, %112[%100] : memref<?xf32, "gpu">
            memref.store %141, %113[%101] : memref<?xf32, "gpu">
            memref.store %142, %114[%102] : memref<?xf32, "gpu">
          }
          scf.for %arg3 = %87 to %3 step %c256 {
            %89 = arith.muli %36, %3 : index
            %90 = arith.addi %89, %arg3 : index
            %91 = arith.muli %2, %1 : index
            %92 = arith.muli %91, %3 : index
            %93 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%92], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %94:3 = "disc_shape.delinearize"(%90, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %95 = memref.load %7[%94#0, %94#1, %94#2] : memref<?x?x?xf32, "gpu">
            %96 = memref.load %38[%c0] : memref<32xf32, 3>
            %97 = arith.subf %95, %96 : f32
            %98 = math.exp %97 : f32
            %99 = memref.load %60[%c0] : memref<32xf32, 3>
            %100 = arith.divf %98, %99 : f32
            memref.store %100, %93[%90] : memref<?xf32, "gpu">
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      %36 = arith.ceildivui %13, %c8 : index
      scf.parallel (%arg1) = (%c0) to (%36) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %37 = gpu.block_id  x
          %38 = gpu.thread_id  x
          %39 = arith.divui %38, %c32 : index
          %40 = arith.remui %38, %c32 : index
          %41 = arith.muli %37, %c8 : index
          %42 = arith.addi %41, %39 : index
          %43 = arith.cmpi slt, %42, %13 : index
          %44 = memref.alloc() : memref<32xf32, 3>
          %45 = gpu.block_id  x
          %46 = gpu.thread_id  x
          %47 = arith.divui %46, %c32 : index
          %48 = arith.remui %46, %c32 : index
          %49 = arith.muli %45, %c8 : index
          %50 = arith.addi %49, %47 : index
          %51 = arith.cmpi slt, %50, %13 : index
          scf.if %51 {
            %60 = arith.subi %3, %48 : index
            %61 = arith.ceildivui %60, %c32 : index
            %c4 = arith.constant 4 : index
            %62 = arith.remsi %61, %c4 : index
            %63 = arith.subi %61, %62 : index
            %64 = arith.muli %63, %c32 : index
            %65 = arith.addi %48, %64 : index
            %66 = arith.muli %c32, %c4 : index
            %67 = scf.for %arg3 = %48 to %65 step %66 iter_args(%arg4 = %cst_0) -> (f32) {
              %c1_9 = arith.constant 1 : index
              %75 = arith.muli %c32, %c1_9 : index
              %76 = arith.addi %arg3, %75 : index
              %c2_10 = arith.constant 2 : index
              %77 = arith.muli %c32, %c2_10 : index
              %78 = arith.addi %arg3, %77 : index
              %c3_11 = arith.constant 3 : index
              %79 = arith.muli %c32, %c3_11 : index
              %80 = arith.addi %arg3, %79 : index
              %81 = "disc_shape.linearize"(%50, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %82 = "disc_shape.linearize"(%50, %76, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %83 = "disc_shape.linearize"(%50, %78, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %84 = "disc_shape.linearize"(%50, %80, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %85:3 = "disc_shape.delinearize"(%81, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %86:3 = "disc_shape.delinearize"(%82, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %87:3 = "disc_shape.delinearize"(%83, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %88:3 = "disc_shape.delinearize"(%84, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %89 = memref.load %7[%85#0, %85#1, %85#2] : memref<?x?x?xf32, "gpu">
              %90 = memref.load %7[%86#0, %86#1, %86#2] : memref<?x?x?xf32, "gpu">
              %91 = memref.load %7[%87#0, %87#1, %87#2] : memref<?x?x?xf32, "gpu">
              %92 = memref.load %7[%88#0, %88#1, %88#2] : memref<?x?x?xf32, "gpu">
              %93 = arith.maxf %arg4, %89 : f32
              %94 = arith.maxf %93, %90 : f32
              %95 = arith.maxf %94, %91 : f32
              %96 = arith.maxf %95, %92 : f32
              scf.yield %96 : f32
            }
            %68 = scf.for %arg3 = %65 to %3 step %c32 iter_args(%arg4 = %67) -> (f32) {
              %75 = "disc_shape.linearize"(%50, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %76:3 = "disc_shape.delinearize"(%75, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %77 = memref.load %7[%76#0, %76#1, %76#2] : memref<?x?x?xf32, "gpu">
              %78 = arith.maxf %arg4, %77 : f32
              scf.yield %78 : f32
            }
            %result, %valid = gpu.shuffle  xor %68, %c1_i32, %c32_i32 : f32
            %69 = arith.maxf %68, %result : f32
            %result_1, %valid_2 = gpu.shuffle  xor %69, %c2_i32, %c32_i32 : f32
            %70 = arith.maxf %69, %result_1 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %70, %c4_i32, %c32_i32 : f32
            %71 = arith.maxf %70, %result_3 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %71, %c8_i32, %c32_i32 : f32
            %72 = arith.maxf %71, %result_5 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %72, %c16_i32, %c32_i32 : f32
            %73 = arith.maxf %72, %result_7 : f32
            %74 = arith.cmpi eq, %48, %c0 : index
            scf.if %74 {
              memref.store %73, %44[%47] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %52 = memref.alloc() : memref<32xf32, 3>
          %53 = gpu.block_id  x
          %54 = gpu.thread_id  x
          %55 = arith.divui %54, %c32 : index
          %56 = arith.remui %54, %c32 : index
          %57 = arith.muli %53, %c8 : index
          %58 = arith.addi %57, %55 : index
          %59 = arith.cmpi slt, %58, %13 : index
          scf.if %59 {
            %60 = arith.subi %3, %56 : index
            %61 = arith.ceildivui %60, %c32 : index
            %c4 = arith.constant 4 : index
            %62 = arith.remsi %61, %c4 : index
            %63 = arith.subi %61, %62 : index
            %64 = arith.muli %63, %c32 : index
            %65 = arith.addi %56, %64 : index
            %66 = arith.muli %c32, %c4 : index
            %67 = scf.for %arg3 = %56 to %65 step %66 iter_args(%arg4 = %cst) -> (f32) {
              %c1_9 = arith.constant 1 : index
              %75 = arith.muli %c32, %c1_9 : index
              %76 = arith.addi %arg3, %75 : index
              %c2_10 = arith.constant 2 : index
              %77 = arith.muli %c32, %c2_10 : index
              %78 = arith.addi %arg3, %77 : index
              %c3_11 = arith.constant 3 : index
              %79 = arith.muli %c32, %c3_11 : index
              %80 = arith.addi %arg3, %79 : index
              %81 = "disc_shape.linearize"(%58, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %82 = "disc_shape.linearize"(%58, %76, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %83 = "disc_shape.linearize"(%58, %78, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %84 = "disc_shape.linearize"(%58, %80, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %85:3 = "disc_shape.delinearize"(%81, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %86:3 = "disc_shape.delinearize"(%82, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %87:3 = "disc_shape.delinearize"(%83, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %88:3 = "disc_shape.delinearize"(%84, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %89 = memref.load %7[%85#0, %85#1, %85#2] : memref<?x?x?xf32, "gpu">
              %90 = memref.load %7[%86#0, %86#1, %86#2] : memref<?x?x?xf32, "gpu">
              %91 = memref.load %7[%87#0, %87#1, %87#2] : memref<?x?x?xf32, "gpu">
              %92 = memref.load %7[%88#0, %88#1, %88#2] : memref<?x?x?xf32, "gpu">
              %93 = "disc_shape.linearize"(%85#0, %85#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %94 = "disc_shape.linearize"(%86#0, %86#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %95 = "disc_shape.linearize"(%87#0, %87#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %96 = "disc_shape.linearize"(%88#0, %88#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %97 = "disc_shape.delinearize"(%93, %13) : (index, index) -> index
              %98 = "disc_shape.delinearize"(%94, %13) : (index, index) -> index
              %99 = "disc_shape.delinearize"(%95, %13) : (index, index) -> index
              %100 = "disc_shape.delinearize"(%96, %13) : (index, index) -> index
              %101 = "disc_shape.linearize"(%97, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %102 = "disc_shape.linearize"(%98, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %103 = "disc_shape.linearize"(%99, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %104 = "disc_shape.linearize"(%100, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %105 = arith.remui %101, %c8 : index
              %106 = arith.remui %102, %c8 : index
              %107 = arith.remui %103, %c8 : index
              %108 = arith.remui %104, %c8 : index
              %109 = memref.load %44[%105] : memref<32xf32, 3>
              %110 = memref.load %44[%106] : memref<32xf32, 3>
              %111 = memref.load %44[%107] : memref<32xf32, 3>
              %112 = memref.load %44[%108] : memref<32xf32, 3>
              %113 = arith.subf %89, %109 : f32
              %114 = arith.subf %90, %110 : f32
              %115 = arith.subf %91, %111 : f32
              %116 = arith.subf %92, %112 : f32
              %117 = math.exp %113 : f32
              %118 = math.exp %114 : f32
              %119 = math.exp %115 : f32
              %120 = math.exp %116 : f32
              %121 = arith.addf %arg4, %117 : f32
              %122 = arith.addf %121, %118 : f32
              %123 = arith.addf %122, %119 : f32
              %124 = arith.addf %123, %120 : f32
              scf.yield %124 : f32
            }
            %68 = scf.for %arg3 = %65 to %3 step %c32 iter_args(%arg4 = %67) -> (f32) {
              %75 = "disc_shape.linearize"(%58, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %76:3 = "disc_shape.delinearize"(%75, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %77 = memref.load %7[%76#0, %76#1, %76#2] : memref<?x?x?xf32, "gpu">
              %78 = "disc_shape.linearize"(%76#0, %76#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %79 = "disc_shape.delinearize"(%78, %13) : (index, index) -> index
              %80 = "disc_shape.linearize"(%79, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %81 = arith.remui %80, %c8 : index
              %82 = memref.load %44[%81] : memref<32xf32, 3>
              %83 = arith.subf %77, %82 : f32
              %84 = math.exp %83 : f32
              %85 = arith.addf %arg4, %84 : f32
              scf.yield %85 : f32
            }
            %result, %valid = gpu.shuffle  xor %68, %c1_i32, %c32_i32 : f32
            %69 = arith.addf %68, %result : f32
            %result_1, %valid_2 = gpu.shuffle  xor %69, %c2_i32, %c32_i32 : f32
            %70 = arith.addf %69, %result_1 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %70, %c4_i32, %c32_i32 : f32
            %71 = arith.addf %70, %result_3 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %71, %c8_i32, %c32_i32 : f32
            %72 = arith.addf %71, %result_5 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %72, %c16_i32, %c32_i32 : f32
            %73 = arith.addf %72, %result_7 : f32
            %74 = arith.cmpi eq, %56, %c0 : index
            scf.if %74 {
              memref.store %73, %52[%55] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          scf.if %43 {
            %60 = arith.subi %3, %40 : index
            %61 = arith.ceildivui %60, %c32 : index
            %c4 = arith.constant 4 : index
            %62 = arith.remsi %61, %c4 : index
            %63 = arith.subi %61, %62 : index
            %64 = arith.muli %63, %c32 : index
            %65 = arith.addi %40, %64 : index
            %66 = arith.muli %c32, %c4 : index
            scf.for %arg3 = %40 to %65 step %66 {
              %c1_1 = arith.constant 1 : index
              %67 = arith.muli %c32, %c1_1 : index
              %68 = arith.addi %arg3, %67 : index
              %c2_2 = arith.constant 2 : index
              %69 = arith.muli %c32, %c2_2 : index
              %70 = arith.addi %arg3, %69 : index
              %c3_3 = arith.constant 3 : index
              %71 = arith.muli %c32, %c3_3 : index
              %72 = arith.addi %arg3, %71 : index
              %73 = arith.muli %42, %3 : index
              %74 = arith.muli %42, %3 : index
              %75 = arith.muli %42, %3 : index
              %76 = arith.muli %42, %3 : index
              %77 = arith.addi %73, %arg3 : index
              %78 = arith.addi %74, %68 : index
              %79 = arith.addi %75, %70 : index
              %80 = arith.addi %76, %72 : index
              %81 = arith.muli %2, %1 : index
              %82 = arith.muli %2, %1 : index
              %83 = arith.muli %2, %1 : index
              %84 = arith.muli %2, %1 : index
              %85 = arith.muli %81, %3 : index
              %86 = arith.muli %82, %3 : index
              %87 = arith.muli %83, %3 : index
              %88 = arith.muli %84, %3 : index
              %89 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%85], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %90 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%86], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %91 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%87], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %92 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%88], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %93:3 = "disc_shape.delinearize"(%77, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %94:3 = "disc_shape.delinearize"(%78, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %95:3 = "disc_shape.delinearize"(%79, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %96:3 = "disc_shape.delinearize"(%80, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %97 = memref.load %7[%93#0, %93#1, %93#2] : memref<?x?x?xf32, "gpu">
              %98 = memref.load %7[%94#0, %94#1, %94#2] : memref<?x?x?xf32, "gpu">
              %99 = memref.load %7[%95#0, %95#1, %95#2] : memref<?x?x?xf32, "gpu">
              %100 = memref.load %7[%96#0, %96#1, %96#2] : memref<?x?x?xf32, "gpu">
              %101 = "disc_shape.linearize"(%93#0, %93#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %102 = "disc_shape.linearize"(%94#0, %94#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %103 = "disc_shape.linearize"(%95#0, %95#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %104 = "disc_shape.linearize"(%96#0, %96#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %105 = "disc_shape.delinearize"(%101, %13) : (index, index) -> index
              %106 = "disc_shape.delinearize"(%102, %13) : (index, index) -> index
              %107 = "disc_shape.delinearize"(%103, %13) : (index, index) -> index
              %108 = "disc_shape.delinearize"(%104, %13) : (index, index) -> index
              %109 = "disc_shape.linearize"(%105, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %110 = "disc_shape.linearize"(%106, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %111 = "disc_shape.linearize"(%107, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %112 = "disc_shape.linearize"(%108, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %113 = arith.remui %109, %c8 : index
              %114 = arith.remui %110, %c8 : index
              %115 = arith.remui %111, %c8 : index
              %116 = arith.remui %112, %c8 : index
              %117 = memref.load %44[%113] : memref<32xf32, 3>
              %118 = memref.load %44[%114] : memref<32xf32, 3>
              %119 = memref.load %44[%115] : memref<32xf32, 3>
              %120 = memref.load %44[%116] : memref<32xf32, 3>
              %121 = arith.subf %97, %117 : f32
              %122 = arith.subf %98, %118 : f32
              %123 = arith.subf %99, %119 : f32
              %124 = arith.subf %100, %120 : f32
              %125 = math.exp %121 : f32
              %126 = math.exp %122 : f32
              %127 = math.exp %123 : f32
              %128 = math.exp %124 : f32
              %129 = "disc_shape.linearize"(%93#0, %93#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %130 = "disc_shape.linearize"(%94#0, %94#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %131 = "disc_shape.linearize"(%95#0, %95#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %132 = "disc_shape.linearize"(%96#0, %96#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %133 = "disc_shape.delinearize"(%129, %13) : (index, index) -> index
              %134 = "disc_shape.delinearize"(%130, %13) : (index, index) -> index
              %135 = "disc_shape.delinearize"(%131, %13) : (index, index) -> index
              %136 = "disc_shape.delinearize"(%132, %13) : (index, index) -> index
              %137 = "disc_shape.linearize"(%133, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %138 = "disc_shape.linearize"(%134, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %139 = "disc_shape.linearize"(%135, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %140 = "disc_shape.linearize"(%136, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %141 = arith.remui %137, %c8 : index
              %142 = arith.remui %138, %c8 : index
              %143 = arith.remui %139, %c8 : index
              %144 = arith.remui %140, %c8 : index
              %145 = memref.load %52[%141] : memref<32xf32, 3>
              %146 = memref.load %52[%142] : memref<32xf32, 3>
              %147 = memref.load %52[%143] : memref<32xf32, 3>
              %148 = memref.load %52[%144] : memref<32xf32, 3>
              %149 = arith.divf %125, %145 : f32
              %150 = arith.divf %126, %146 : f32
              %151 = arith.divf %127, %147 : f32
              %152 = arith.divf %128, %148 : f32
              memref.store %149, %89[%77] : memref<?xf32, "gpu">
              memref.store %150, %90[%78] : memref<?xf32, "gpu">
              memref.store %151, %91[%79] : memref<?xf32, "gpu">
              memref.store %152, %92[%80] : memref<?xf32, "gpu">
            }
            scf.for %arg3 = %65 to %3 step %c32 {
              %67 = arith.muli %42, %3 : index
              %68 = arith.addi %67, %arg3 : index
              %69 = arith.muli %2, %1 : index
              %70 = arith.muli %69, %3 : index
              %71 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %72:3 = "disc_shape.delinearize"(%68, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %73 = memref.load %7[%72#0, %72#1, %72#2] : memref<?x?x?xf32, "gpu">
              %74 = "disc_shape.linearize"(%72#0, %72#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %75 = "disc_shape.delinearize"(%74, %13) : (index, index) -> index
              %76 = "disc_shape.linearize"(%75, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %77 = arith.remui %76, %c8 : index
              %78 = memref.load %44[%77] : memref<32xf32, 3>
              %79 = arith.subf %73, %78 : f32
              %80 = math.exp %79 : f32
              %81 = "disc_shape.linearize"(%72#0, %72#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %82 = "disc_shape.delinearize"(%81, %13) : (index, index) -> index
              %83 = "disc_shape.linearize"(%82, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %84 = arith.remui %83, %c8 : index
              %85 = memref.load %52[%84] : memref<32xf32, 3>
              %86 = arith.divf %80, %85 : f32
              memref.store %86, %71[%68] : memref<?xf32, "gpu">
            }
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  memref.dealloc %25 : memref<?x?x?xf32, "gpu">
  memref.dealloc %24 : memref<?x?xf32, "gpu">
  memref.dealloc %23 : memref<?xf32, "gpu">
  memref.dealloc %22 : memref<?x?xf32, "gpu">
  memref.dealloc %21 : memref<?x?x?xf32, "gpu">
  memref.dealloc %20 : memref<?x?x?xf32, "gpu">
  memref.dealloc %19 : memref<?x?x?xf32, "gpu">
  memref.dealloc %17 : memref<?x?xf32, "gpu">
  memref.dealloc %15 : memref<?xf32, "gpu">
  memref.dealloc %14 : memref<?x?xf32, "gpu">
  memref.dealloc %5 : memref<f32, "gpu">
  memref.dealloc %4 : memref<f32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %26) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ArithmeticExpandOps (arith-expand) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant -0.000000e+00 : f32
  %cst_0 = arith.constant 0xFF800000 : f32
  %c8 = arith.constant 8 : index
  %c16_i32 = arith.constant 16 : i32
  %c8_i32 = arith.constant 8 : i32
  %c4_i32 = arith.constant 4 : i32
  %c2_i32 = arith.constant 2 : i32
  %c1_i32 = arith.constant 1 : i32
  %c32_i32 = arith.constant 32 : i32
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c1024 = arith.constant 1024 : index
  %c6 = arith.constant 6 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
  %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
  %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
  %4 = memref.alloc() : memref<f32, "gpu">
  %5 = memref.alloc() : memref<f32, "gpu">
  %6 = arith.muli %3, %1 : index
  %7 = memref.reinterpret_cast %0 to offset: [0], sizes: [%2, %1, %3], strides: [%6, %3, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %8 = arith.index_cast %3 : index to i32
  %9 = arith.index_cast %2 : index to i32
  %10 = arith.index_cast %1 : index to i32
  %11 = arith.muli %9, %10 : i32
  %12 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %11, %12[%c0] : memref<2xi32, "cpu">
  memref.store %8, %12[%c1] : memref<2xi32, "cpu">
  %13 = arith.index_cast %11 : i32 to index
  %14 = memref.alloc(%13, %3) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %15 = memref.alloc(%13) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %16 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %9, %16[%c0] : memref<2xi32, "cpu">
  memref.store %10, %16[%c1] : memref<2xi32, "cpu">
  %17 = memref.alloc(%2, %1) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %18 = memref.alloca() : memref<3xindex, "cpu">
  memref.store %2, %18[%c0] : memref<3xindex, "cpu">
  memref.store %1, %18[%c1] : memref<3xindex, "cpu">
  memref.store %3, %18[%c2] : memref<3xindex, "cpu">
  %19 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %20 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %21 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %22 = memref.alloc(%13, %3) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %23 = memref.alloc(%13) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %24 = memref.alloc(%2, %1) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %25 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %26 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %27 = arith.cmpi slt, %13, %c3 : index
  %28 = arith.cmpi sge, %3, %c1024 : index
  %29 = arith.cmpi slt, %13, %c6 : index
  %30 = arith.andi %29, %28 : i1
  %31 = arith.cmpi sge, %3, %c512 : index
  %32 = arith.cmpi sge, %13, %c6 : index
  %33 = arith.andi %32, %31 : i1
  %34 = arith.ori %27, %30 : i1
  %35 = arith.ori %34, %33 : i1
  scf.if %35 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%13) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %36 = gpu.block_id  x
          %37 = gpu.thread_id  x
          %38 = memref.alloc() : memref<32xf32, 3>
          %39 = gpu.block_id  x
          %40 = gpu.thread_id  x
          %41 = arith.divui %40, %c32 : index
          %42 = arith.remui %40, %c32 : index
          %43 = arith.subi %3, %40 : index
          %c0_1 = arith.constant 0 : index
          %44 = arith.cmpi eq, %43, %c0_1 : index
          %c1_2 = arith.constant 1 : index
          %45 = arith.subi %43, %c1_2 : index
          %46 = arith.divui %45, %c256 : index
          %47 = arith.addi %46, %c1_2 : index
          %48 = arith.select %44, %c0_1, %47 : index
          %c4 = arith.constant 4 : index
          %49 = arith.remsi %48, %c4 : index
          %50 = arith.subi %48, %49 : index
          %51 = arith.muli %50, %c256 : index
          %52 = arith.addi %40, %51 : index
          %53 = arith.muli %c256, %c4 : index
          %54 = scf.for %arg3 = %40 to %52 step %53 iter_args(%arg4 = %cst_0) -> (f32) {
            %c1_27 = arith.constant 1 : index
            %116 = arith.muli %c256, %c1_27 : index
            %117 = arith.addi %arg3, %116 : index
            %c2_28 = arith.constant 2 : index
            %118 = arith.muli %c256, %c2_28 : index
            %119 = arith.addi %arg3, %118 : index
            %c3_29 = arith.constant 3 : index
            %120 = arith.muli %c256, %c3_29 : index
            %121 = arith.addi %arg3, %120 : index
            %122 = "disc_shape.linearize"(%39, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %123 = "disc_shape.linearize"(%39, %117, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %124 = "disc_shape.linearize"(%39, %119, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %125 = "disc_shape.linearize"(%39, %121, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %126:3 = "disc_shape.delinearize"(%122, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %127:3 = "disc_shape.delinearize"(%123, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %128:3 = "disc_shape.delinearize"(%124, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %129:3 = "disc_shape.delinearize"(%125, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %130 = memref.load %7[%126#0, %126#1, %126#2] : memref<?x?x?xf32, "gpu">
            %131 = memref.load %7[%127#0, %127#1, %127#2] : memref<?x?x?xf32, "gpu">
            %132 = memref.load %7[%128#0, %128#1, %128#2] : memref<?x?x?xf32, "gpu">
            %133 = memref.load %7[%129#0, %129#1, %129#2] : memref<?x?x?xf32, "gpu">
            %134 = arith.cmpf ugt, %arg4, %130 : f32
            %135 = arith.select %134, %arg4, %130 : f32
            %136 = arith.cmpf uno, %130, %130 : f32
            %137 = arith.select %136, %130, %135 : f32
            %138 = arith.cmpf ugt, %137, %131 : f32
            %139 = arith.select %138, %137, %131 : f32
            %140 = arith.cmpf uno, %131, %131 : f32
            %141 = arith.select %140, %131, %139 : f32
            %142 = arith.cmpf ugt, %141, %132 : f32
            %143 = arith.select %142, %141, %132 : f32
            %144 = arith.cmpf uno, %132, %132 : f32
            %145 = arith.select %144, %132, %143 : f32
            %146 = arith.cmpf ugt, %145, %133 : f32
            %147 = arith.select %146, %145, %133 : f32
            %148 = arith.cmpf uno, %133, %133 : f32
            %149 = arith.select %148, %133, %147 : f32
            scf.yield %149 : f32
          }
          %55 = scf.for %arg3 = %52 to %3 step %c256 iter_args(%arg4 = %54) -> (f32) {
            %116 = "disc_shape.linearize"(%39, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %117:3 = "disc_shape.delinearize"(%116, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %118 = memref.load %7[%117#0, %117#1, %117#2] : memref<?x?x?xf32, "gpu">
            %119 = arith.cmpf ugt, %arg4, %118 : f32
            %120 = arith.select %119, %arg4, %118 : f32
            %121 = arith.cmpf uno, %118, %118 : f32
            %122 = arith.select %121, %118, %120 : f32
            scf.yield %122 : f32
          }
          %result, %valid = gpu.shuffle  xor %55, %c1_i32, %c32_i32 : f32
          %56 = arith.cmpf ugt, %55, %result : f32
          %57 = arith.select %56, %55, %result : f32
          %58 = arith.cmpf uno, %result, %result : f32
          %59 = arith.select %58, %result, %57 : f32
          %result_3, %valid_4 = gpu.shuffle  xor %59, %c2_i32, %c32_i32 : f32
          %60 = arith.cmpf ugt, %59, %result_3 : f32
          %61 = arith.select %60, %59, %result_3 : f32
          %62 = arith.cmpf uno, %result_3, %result_3 : f32
          %63 = arith.select %62, %result_3, %61 : f32
          %result_5, %valid_6 = gpu.shuffle  xor %63, %c4_i32, %c32_i32 : f32
          %64 = arith.cmpf ugt, %63, %result_5 : f32
          %65 = arith.select %64, %63, %result_5 : f32
          %66 = arith.cmpf uno, %result_5, %result_5 : f32
          %67 = arith.select %66, %result_5, %65 : f32
          %result_7, %valid_8 = gpu.shuffle  xor %67, %c8_i32, %c32_i32 : f32
          %68 = arith.cmpf ugt, %67, %result_7 : f32
          %69 = arith.select %68, %67, %result_7 : f32
          %70 = arith.cmpf uno, %result_7, %result_7 : f32
          %71 = arith.select %70, %result_7, %69 : f32
          %result_9, %valid_10 = gpu.shuffle  xor %71, %c16_i32, %c32_i32 : f32
          %72 = arith.cmpf ugt, %71, %result_9 : f32
          %73 = arith.select %72, %71, %result_9 : f32
          %74 = arith.cmpf uno, %result_9, %result_9 : f32
          %75 = arith.select %74, %result_9, %73 : f32
          %76 = memref.alloc() : memref<8xf32, 3>
          %77 = arith.cmpi eq, %42, %c0 : index
          scf.if %77 {
            memref.store %75, %76[%41] : memref<8xf32, 3>
          }
          gpu.barrier
          %78 = arith.cmpi slt, %40, %c32 : index
          scf.if %78 {
            %116 = arith.cmpi slt, %42, %c8 : index
            %117 = scf.if %116 -> (f32) {
              %130 = memref.load %76[%42] : memref<8xf32, 3>
              scf.yield %130 : f32
            } else {
              scf.yield %cst_0 : f32
            }
            %result_27, %valid_28 = gpu.shuffle  xor %117, %c1_i32, %c8_i32 : f32
            %118 = arith.cmpf ugt, %117, %result_27 : f32
            %119 = arith.select %118, %117, %result_27 : f32
            %120 = arith.cmpf uno, %result_27, %result_27 : f32
            %121 = arith.select %120, %result_27, %119 : f32
            %result_29, %valid_30 = gpu.shuffle  xor %121, %c2_i32, %c8_i32 : f32
            %122 = arith.cmpf ugt, %121, %result_29 : f32
            %123 = arith.select %122, %121, %result_29 : f32
            %124 = arith.cmpf uno, %result_29, %result_29 : f32
            %125 = arith.select %124, %result_29, %123 : f32
            %result_31, %valid_32 = gpu.shuffle  xor %125, %c4_i32, %c8_i32 : f32
            %126 = arith.cmpf ugt, %125, %result_31 : f32
            %127 = arith.select %126, %125, %result_31 : f32
            %128 = arith.cmpf uno, %result_31, %result_31 : f32
            %129 = arith.select %128, %result_31, %127 : f32
            scf.if %77 {
              memref.store %129, %38[%c0] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %79 = memref.alloc() : memref<32xf32, 3>
          %80 = gpu.block_id  x
          %81 = gpu.thread_id  x
          %82 = arith.divui %81, %c32 : index
          %83 = arith.remui %81, %c32 : index
          %84 = arith.subi %3, %81 : index
          %c0_11 = arith.constant 0 : index
          %85 = arith.cmpi eq, %84, %c0_11 : index
          %c1_12 = arith.constant 1 : index
          %86 = arith.subi %84, %c1_12 : index
          %87 = arith.divui %86, %c256 : index
          %88 = arith.addi %87, %c1_12 : index
          %89 = arith.select %85, %c0_11, %88 : index
          %c4_13 = arith.constant 4 : index
          %90 = arith.remsi %89, %c4_13 : index
          %91 = arith.subi %89, %90 : index
          %92 = arith.muli %91, %c256 : index
          %93 = arith.addi %81, %92 : index
          %94 = arith.muli %c256, %c4_13 : index
          %95 = scf.for %arg3 = %81 to %93 step %94 iter_args(%arg4 = %cst) -> (f32) {
            %c1_27 = arith.constant 1 : index
            %116 = arith.muli %c256, %c1_27 : index
            %117 = arith.addi %arg3, %116 : index
            %c2_28 = arith.constant 2 : index
            %118 = arith.muli %c256, %c2_28 : index
            %119 = arith.addi %arg3, %118 : index
            %c3_29 = arith.constant 3 : index
            %120 = arith.muli %c256, %c3_29 : index
            %121 = arith.addi %arg3, %120 : index
            %122 = "disc_shape.linearize"(%80, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %123 = "disc_shape.linearize"(%80, %117, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %124 = "disc_shape.linearize"(%80, %119, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %125 = "disc_shape.linearize"(%80, %121, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %126:3 = "disc_shape.delinearize"(%122, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %127:3 = "disc_shape.delinearize"(%123, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %128:3 = "disc_shape.delinearize"(%124, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %129:3 = "disc_shape.delinearize"(%125, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %130 = memref.load %7[%126#0, %126#1, %126#2] : memref<?x?x?xf32, "gpu">
            %131 = memref.load %7[%127#0, %127#1, %127#2] : memref<?x?x?xf32, "gpu">
            %132 = memref.load %7[%128#0, %128#1, %128#2] : memref<?x?x?xf32, "gpu">
            %133 = memref.load %7[%129#0, %129#1, %129#2] : memref<?x?x?xf32, "gpu">
            %134 = memref.load %38[%c0] : memref<32xf32, 3>
            %135 = memref.load %38[%c0] : memref<32xf32, 3>
            %136 = memref.load %38[%c0] : memref<32xf32, 3>
            %137 = memref.load %38[%c0] : memref<32xf32, 3>
            %138 = arith.subf %130, %134 : f32
            %139 = arith.subf %131, %135 : f32
            %140 = arith.subf %132, %136 : f32
            %141 = arith.subf %133, %137 : f32
            %142 = math.exp %138 : f32
            %143 = math.exp %139 : f32
            %144 = math.exp %140 : f32
            %145 = math.exp %141 : f32
            %146 = arith.addf %arg4, %142 : f32
            %147 = arith.addf %146, %143 : f32
            %148 = arith.addf %147, %144 : f32
            %149 = arith.addf %148, %145 : f32
            scf.yield %149 : f32
          }
          %96 = scf.for %arg3 = %93 to %3 step %c256 iter_args(%arg4 = %95) -> (f32) {
            %116 = "disc_shape.linearize"(%80, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %117:3 = "disc_shape.delinearize"(%116, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %118 = memref.load %7[%117#0, %117#1, %117#2] : memref<?x?x?xf32, "gpu">
            %119 = memref.load %38[%c0] : memref<32xf32, 3>
            %120 = arith.subf %118, %119 : f32
            %121 = math.exp %120 : f32
            %122 = arith.addf %arg4, %121 : f32
            scf.yield %122 : f32
          }
          %result_14, %valid_15 = gpu.shuffle  xor %96, %c1_i32, %c32_i32 : f32
          %97 = arith.addf %96, %result_14 : f32
          %result_16, %valid_17 = gpu.shuffle  xor %97, %c2_i32, %c32_i32 : f32
          %98 = arith.addf %97, %result_16 : f32
          %result_18, %valid_19 = gpu.shuffle  xor %98, %c4_i32, %c32_i32 : f32
          %99 = arith.addf %98, %result_18 : f32
          %result_20, %valid_21 = gpu.shuffle  xor %99, %c8_i32, %c32_i32 : f32
          %100 = arith.addf %99, %result_20 : f32
          %result_22, %valid_23 = gpu.shuffle  xor %100, %c16_i32, %c32_i32 : f32
          %101 = arith.addf %100, %result_22 : f32
          %102 = memref.alloc() : memref<8xf32, 3>
          %103 = arith.cmpi eq, %83, %c0 : index
          scf.if %103 {
            memref.store %101, %102[%82] : memref<8xf32, 3>
          }
          gpu.barrier
          %104 = arith.cmpi slt, %81, %c32 : index
          scf.if %104 {
            %116 = arith.cmpi slt, %83, %c8 : index
            %117 = scf.if %116 -> (f32) {
              %121 = memref.load %102[%83] : memref<8xf32, 3>
              scf.yield %121 : f32
            } else {
              scf.yield %cst : f32
            }
            %result_27, %valid_28 = gpu.shuffle  xor %117, %c1_i32, %c8_i32 : f32
            %118 = arith.addf %117, %result_27 : f32
            %result_29, %valid_30 = gpu.shuffle  xor %118, %c2_i32, %c8_i32 : f32
            %119 = arith.addf %118, %result_29 : f32
            %result_31, %valid_32 = gpu.shuffle  xor %119, %c4_i32, %c8_i32 : f32
            %120 = arith.addf %119, %result_31 : f32
            scf.if %103 {
              memref.store %120, %79[%c0] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %105 = arith.subi %3, %37 : index
          %c0_24 = arith.constant 0 : index
          %106 = arith.cmpi eq, %105, %c0_24 : index
          %c1_25 = arith.constant 1 : index
          %107 = arith.subi %105, %c1_25 : index
          %108 = arith.divui %107, %c256 : index
          %109 = arith.addi %108, %c1_25 : index
          %110 = arith.select %106, %c0_24, %109 : index
          %c4_26 = arith.constant 4 : index
          %111 = arith.remsi %110, %c4_26 : index
          %112 = arith.subi %110, %111 : index
          %113 = arith.muli %112, %c256 : index
          %114 = arith.addi %37, %113 : index
          %115 = arith.muli %c256, %c4_26 : index
          scf.for %arg3 = %37 to %114 step %115 {
            %c1_27 = arith.constant 1 : index
            %116 = arith.muli %c256, %c1_27 : index
            %117 = arith.addi %arg3, %116 : index
            %c2_28 = arith.constant 2 : index
            %118 = arith.muli %c256, %c2_28 : index
            %119 = arith.addi %arg3, %118 : index
            %c3_29 = arith.constant 3 : index
            %120 = arith.muli %c256, %c3_29 : index
            %121 = arith.addi %arg3, %120 : index
            %122 = arith.muli %36, %3 : index
            %123 = arith.muli %36, %3 : index
            %124 = arith.muli %36, %3 : index
            %125 = arith.muli %36, %3 : index
            %126 = arith.addi %122, %arg3 : index
            %127 = arith.addi %123, %117 : index
            %128 = arith.addi %124, %119 : index
            %129 = arith.addi %125, %121 : index
            %130 = arith.muli %2, %1 : index
            %131 = arith.muli %2, %1 : index
            %132 = arith.muli %2, %1 : index
            %133 = arith.muli %2, %1 : index
            %134 = arith.muli %130, %3 : index
            %135 = arith.muli %131, %3 : index
            %136 = arith.muli %132, %3 : index
            %137 = arith.muli %133, %3 : index
            %138 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%134], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %139 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%135], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %140 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%136], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %141 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%137], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %142:3 = "disc_shape.delinearize"(%126, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %143:3 = "disc_shape.delinearize"(%127, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %144:3 = "disc_shape.delinearize"(%128, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %145:3 = "disc_shape.delinearize"(%129, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %146 = memref.load %7[%142#0, %142#1, %142#2] : memref<?x?x?xf32, "gpu">
            %147 = memref.load %7[%143#0, %143#1, %143#2] : memref<?x?x?xf32, "gpu">
            %148 = memref.load %7[%144#0, %144#1, %144#2] : memref<?x?x?xf32, "gpu">
            %149 = memref.load %7[%145#0, %145#1, %145#2] : memref<?x?x?xf32, "gpu">
            %150 = memref.load %38[%c0] : memref<32xf32, 3>
            %151 = memref.load %38[%c0] : memref<32xf32, 3>
            %152 = memref.load %38[%c0] : memref<32xf32, 3>
            %153 = memref.load %38[%c0] : memref<32xf32, 3>
            %154 = arith.subf %146, %150 : f32
            %155 = arith.subf %147, %151 : f32
            %156 = arith.subf %148, %152 : f32
            %157 = arith.subf %149, %153 : f32
            %158 = math.exp %154 : f32
            %159 = math.exp %155 : f32
            %160 = math.exp %156 : f32
            %161 = math.exp %157 : f32
            %162 = memref.load %79[%c0] : memref<32xf32, 3>
            %163 = memref.load %79[%c0] : memref<32xf32, 3>
            %164 = memref.load %79[%c0] : memref<32xf32, 3>
            %165 = memref.load %79[%c0] : memref<32xf32, 3>
            %166 = arith.divf %158, %162 : f32
            %167 = arith.divf %159, %163 : f32
            %168 = arith.divf %160, %164 : f32
            %169 = arith.divf %161, %165 : f32
            memref.store %166, %138[%126] : memref<?xf32, "gpu">
            memref.store %167, %139[%127] : memref<?xf32, "gpu">
            memref.store %168, %140[%128] : memref<?xf32, "gpu">
            memref.store %169, %141[%129] : memref<?xf32, "gpu">
          }
          scf.for %arg3 = %114 to %3 step %c256 {
            %116 = arith.muli %36, %3 : index
            %117 = arith.addi %116, %arg3 : index
            %118 = arith.muli %2, %1 : index
            %119 = arith.muli %118, %3 : index
            %120 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%119], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %121:3 = "disc_shape.delinearize"(%117, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %122 = memref.load %7[%121#0, %121#1, %121#2] : memref<?x?x?xf32, "gpu">
            %123 = memref.load %38[%c0] : memref<32xf32, 3>
            %124 = arith.subf %122, %123 : f32
            %125 = math.exp %124 : f32
            %126 = memref.load %79[%c0] : memref<32xf32, 3>
            %127 = arith.divf %125, %126 : f32
            memref.store %127, %120[%117] : memref<?xf32, "gpu">
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      %c0_1 = arith.constant 0 : index
      %36 = arith.cmpi eq, %13, %c0_1 : index
      %c1_2 = arith.constant 1 : index
      %37 = arith.subi %13, %c1_2 : index
      %38 = arith.divui %37, %c8 : index
      %39 = arith.addi %38, %c1_2 : index
      %40 = arith.select %36, %c0_1, %39 : index
      scf.parallel (%arg1) = (%c0) to (%40) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %41 = gpu.block_id  x
          %42 = gpu.thread_id  x
          %43 = arith.divui %42, %c32 : index
          %44 = arith.remui %42, %c32 : index
          %45 = arith.muli %41, %c8 : index
          %46 = arith.addi %45, %43 : index
          %47 = arith.cmpi slt, %46, %13 : index
          %48 = memref.alloc() : memref<32xf32, 3>
          %49 = gpu.block_id  x
          %50 = gpu.thread_id  x
          %51 = arith.divui %50, %c32 : index
          %52 = arith.remui %50, %c32 : index
          %53 = arith.muli %49, %c8 : index
          %54 = arith.addi %53, %51 : index
          %55 = arith.cmpi slt, %54, %13 : index
          scf.if %55 {
            %64 = arith.subi %3, %52 : index
            %c0_3 = arith.constant 0 : index
            %65 = arith.cmpi eq, %64, %c0_3 : index
            %c1_4 = arith.constant 1 : index
            %66 = arith.subi %64, %c1_4 : index
            %67 = arith.divui %66, %c32 : index
            %68 = arith.addi %67, %c1_4 : index
            %69 = arith.select %65, %c0_3, %68 : index
            %c4 = arith.constant 4 : index
            %70 = arith.remsi %69, %c4 : index
            %71 = arith.subi %69, %70 : index
            %72 = arith.muli %71, %c32 : index
            %73 = arith.addi %52, %72 : index
            %74 = arith.muli %c32, %c4 : index
            %75 = scf.for %arg3 = %52 to %73 step %74 iter_args(%arg4 = %cst_0) -> (f32) {
              %c1_13 = arith.constant 1 : index
              %98 = arith.muli %c32, %c1_13 : index
              %99 = arith.addi %arg3, %98 : index
              %c2_14 = arith.constant 2 : index
              %100 = arith.muli %c32, %c2_14 : index
              %101 = arith.addi %arg3, %100 : index
              %c3_15 = arith.constant 3 : index
              %102 = arith.muli %c32, %c3_15 : index
              %103 = arith.addi %arg3, %102 : index
              %104 = "disc_shape.linearize"(%54, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %105 = "disc_shape.linearize"(%54, %99, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %106 = "disc_shape.linearize"(%54, %101, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %107 = "disc_shape.linearize"(%54, %103, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %108:3 = "disc_shape.delinearize"(%104, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %109:3 = "disc_shape.delinearize"(%105, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %110:3 = "disc_shape.delinearize"(%106, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %111:3 = "disc_shape.delinearize"(%107, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %112 = memref.load %7[%108#0, %108#1, %108#2] : memref<?x?x?xf32, "gpu">
              %113 = memref.load %7[%109#0, %109#1, %109#2] : memref<?x?x?xf32, "gpu">
              %114 = memref.load %7[%110#0, %110#1, %110#2] : memref<?x?x?xf32, "gpu">
              %115 = memref.load %7[%111#0, %111#1, %111#2] : memref<?x?x?xf32, "gpu">
              %116 = arith.cmpf ugt, %arg4, %112 : f32
              %117 = arith.select %116, %arg4, %112 : f32
              %118 = arith.cmpf uno, %112, %112 : f32
              %119 = arith.select %118, %112, %117 : f32
              %120 = arith.cmpf ugt, %119, %113 : f32
              %121 = arith.select %120, %119, %113 : f32
              %122 = arith.cmpf uno, %113, %113 : f32
              %123 = arith.select %122, %113, %121 : f32
              %124 = arith.cmpf ugt, %123, %114 : f32
              %125 = arith.select %124, %123, %114 : f32
              %126 = arith.cmpf uno, %114, %114 : f32
              %127 = arith.select %126, %114, %125 : f32
              %128 = arith.cmpf ugt, %127, %115 : f32
              %129 = arith.select %128, %127, %115 : f32
              %130 = arith.cmpf uno, %115, %115 : f32
              %131 = arith.select %130, %115, %129 : f32
              scf.yield %131 : f32
            }
            %76 = scf.for %arg3 = %73 to %3 step %c32 iter_args(%arg4 = %75) -> (f32) {
              %98 = "disc_shape.linearize"(%54, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %99:3 = "disc_shape.delinearize"(%98, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %100 = memref.load %7[%99#0, %99#1, %99#2] : memref<?x?x?xf32, "gpu">
              %101 = arith.cmpf ugt, %arg4, %100 : f32
              %102 = arith.select %101, %arg4, %100 : f32
              %103 = arith.cmpf uno, %100, %100 : f32
              %104 = arith.select %103, %100, %102 : f32
              scf.yield %104 : f32
            }
            %result, %valid = gpu.shuffle  xor %76, %c1_i32, %c32_i32 : f32
            %77 = arith.cmpf ugt, %76, %result : f32
            %78 = arith.select %77, %76, %result : f32
            %79 = arith.cmpf uno, %result, %result : f32
            %80 = arith.select %79, %result, %78 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %80, %c2_i32, %c32_i32 : f32
            %81 = arith.cmpf ugt, %80, %result_5 : f32
            %82 = arith.select %81, %80, %result_5 : f32
            %83 = arith.cmpf uno, %result_5, %result_5 : f32
            %84 = arith.select %83, %result_5, %82 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %84, %c4_i32, %c32_i32 : f32
            %85 = arith.cmpf ugt, %84, %result_7 : f32
            %86 = arith.select %85, %84, %result_7 : f32
            %87 = arith.cmpf uno, %result_7, %result_7 : f32
            %88 = arith.select %87, %result_7, %86 : f32
            %result_9, %valid_10 = gpu.shuffle  xor %88, %c8_i32, %c32_i32 : f32
            %89 = arith.cmpf ugt, %88, %result_9 : f32
            %90 = arith.select %89, %88, %result_9 : f32
            %91 = arith.cmpf uno, %result_9, %result_9 : f32
            %92 = arith.select %91, %result_9, %90 : f32
            %result_11, %valid_12 = gpu.shuffle  xor %92, %c16_i32, %c32_i32 : f32
            %93 = arith.cmpf ugt, %92, %result_11 : f32
            %94 = arith.select %93, %92, %result_11 : f32
            %95 = arith.cmpf uno, %result_11, %result_11 : f32
            %96 = arith.select %95, %result_11, %94 : f32
            %97 = arith.cmpi eq, %52, %c0 : index
            scf.if %97 {
              memref.store %96, %48[%51] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %56 = memref.alloc() : memref<32xf32, 3>
          %57 = gpu.block_id  x
          %58 = gpu.thread_id  x
          %59 = arith.divui %58, %c32 : index
          %60 = arith.remui %58, %c32 : index
          %61 = arith.muli %57, %c8 : index
          %62 = arith.addi %61, %59 : index
          %63 = arith.cmpi slt, %62, %13 : index
          scf.if %63 {
            %64 = arith.subi %3, %60 : index
            %c0_3 = arith.constant 0 : index
            %65 = arith.cmpi eq, %64, %c0_3 : index
            %c1_4 = arith.constant 1 : index
            %66 = arith.subi %64, %c1_4 : index
            %67 = arith.divui %66, %c32 : index
            %68 = arith.addi %67, %c1_4 : index
            %69 = arith.select %65, %c0_3, %68 : index
            %c4 = arith.constant 4 : index
            %70 = arith.remsi %69, %c4 : index
            %71 = arith.subi %69, %70 : index
            %72 = arith.muli %71, %c32 : index
            %73 = arith.addi %60, %72 : index
            %74 = arith.muli %c32, %c4 : index
            %75 = scf.for %arg3 = %60 to %73 step %74 iter_args(%arg4 = %cst) -> (f32) {
              %c1_13 = arith.constant 1 : index
              %83 = arith.muli %c32, %c1_13 : index
              %84 = arith.addi %arg3, %83 : index
              %c2_14 = arith.constant 2 : index
              %85 = arith.muli %c32, %c2_14 : index
              %86 = arith.addi %arg3, %85 : index
              %c3_15 = arith.constant 3 : index
              %87 = arith.muli %c32, %c3_15 : index
              %88 = arith.addi %arg3, %87 : index
              %89 = "disc_shape.linearize"(%62, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %90 = "disc_shape.linearize"(%62, %84, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %91 = "disc_shape.linearize"(%62, %86, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %92 = "disc_shape.linearize"(%62, %88, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %93:3 = "disc_shape.delinearize"(%89, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %94:3 = "disc_shape.delinearize"(%90, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %95:3 = "disc_shape.delinearize"(%91, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %96:3 = "disc_shape.delinearize"(%92, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %97 = memref.load %7[%93#0, %93#1, %93#2] : memref<?x?x?xf32, "gpu">
              %98 = memref.load %7[%94#0, %94#1, %94#2] : memref<?x?x?xf32, "gpu">
              %99 = memref.load %7[%95#0, %95#1, %95#2] : memref<?x?x?xf32, "gpu">
              %100 = memref.load %7[%96#0, %96#1, %96#2] : memref<?x?x?xf32, "gpu">
              %101 = "disc_shape.linearize"(%93#0, %93#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %102 = "disc_shape.linearize"(%94#0, %94#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %103 = "disc_shape.linearize"(%95#0, %95#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %104 = "disc_shape.linearize"(%96#0, %96#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %105 = "disc_shape.delinearize"(%101, %13) : (index, index) -> index
              %106 = "disc_shape.delinearize"(%102, %13) : (index, index) -> index
              %107 = "disc_shape.delinearize"(%103, %13) : (index, index) -> index
              %108 = "disc_shape.delinearize"(%104, %13) : (index, index) -> index
              %109 = "disc_shape.linearize"(%105, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %110 = "disc_shape.linearize"(%106, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %111 = "disc_shape.linearize"(%107, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %112 = "disc_shape.linearize"(%108, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %113 = arith.remui %109, %c8 : index
              %114 = arith.remui %110, %c8 : index
              %115 = arith.remui %111, %c8 : index
              %116 = arith.remui %112, %c8 : index
              %117 = memref.load %48[%113] : memref<32xf32, 3>
              %118 = memref.load %48[%114] : memref<32xf32, 3>
              %119 = memref.load %48[%115] : memref<32xf32, 3>
              %120 = memref.load %48[%116] : memref<32xf32, 3>
              %121 = arith.subf %97, %117 : f32
              %122 = arith.subf %98, %118 : f32
              %123 = arith.subf %99, %119 : f32
              %124 = arith.subf %100, %120 : f32
              %125 = math.exp %121 : f32
              %126 = math.exp %122 : f32
              %127 = math.exp %123 : f32
              %128 = math.exp %124 : f32
              %129 = arith.addf %arg4, %125 : f32
              %130 = arith.addf %129, %126 : f32
              %131 = arith.addf %130, %127 : f32
              %132 = arith.addf %131, %128 : f32
              scf.yield %132 : f32
            }
            %76 = scf.for %arg3 = %73 to %3 step %c32 iter_args(%arg4 = %75) -> (f32) {
              %83 = "disc_shape.linearize"(%62, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %84:3 = "disc_shape.delinearize"(%83, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %85 = memref.load %7[%84#0, %84#1, %84#2] : memref<?x?x?xf32, "gpu">
              %86 = "disc_shape.linearize"(%84#0, %84#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %87 = "disc_shape.delinearize"(%86, %13) : (index, index) -> index
              %88 = "disc_shape.linearize"(%87, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %89 = arith.remui %88, %c8 : index
              %90 = memref.load %48[%89] : memref<32xf32, 3>
              %91 = arith.subf %85, %90 : f32
              %92 = math.exp %91 : f32
              %93 = arith.addf %arg4, %92 : f32
              scf.yield %93 : f32
            }
            %result, %valid = gpu.shuffle  xor %76, %c1_i32, %c32_i32 : f32
            %77 = arith.addf %76, %result : f32
            %result_5, %valid_6 = gpu.shuffle  xor %77, %c2_i32, %c32_i32 : f32
            %78 = arith.addf %77, %result_5 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %78, %c4_i32, %c32_i32 : f32
            %79 = arith.addf %78, %result_7 : f32
            %result_9, %valid_10 = gpu.shuffle  xor %79, %c8_i32, %c32_i32 : f32
            %80 = arith.addf %79, %result_9 : f32
            %result_11, %valid_12 = gpu.shuffle  xor %80, %c16_i32, %c32_i32 : f32
            %81 = arith.addf %80, %result_11 : f32
            %82 = arith.cmpi eq, %60, %c0 : index
            scf.if %82 {
              memref.store %81, %56[%59] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          scf.if %47 {
            %64 = arith.subi %3, %44 : index
            %c0_3 = arith.constant 0 : index
            %65 = arith.cmpi eq, %64, %c0_3 : index
            %c1_4 = arith.constant 1 : index
            %66 = arith.subi %64, %c1_4 : index
            %67 = arith.divui %66, %c32 : index
            %68 = arith.addi %67, %c1_4 : index
            %69 = arith.select %65, %c0_3, %68 : index
            %c4 = arith.constant 4 : index
            %70 = arith.remsi %69, %c4 : index
            %71 = arith.subi %69, %70 : index
            %72 = arith.muli %71, %c32 : index
            %73 = arith.addi %44, %72 : index
            %74 = arith.muli %c32, %c4 : index
            scf.for %arg3 = %44 to %73 step %74 {
              %c1_5 = arith.constant 1 : index
              %75 = arith.muli %c32, %c1_5 : index
              %76 = arith.addi %arg3, %75 : index
              %c2_6 = arith.constant 2 : index
              %77 = arith.muli %c32, %c2_6 : index
              %78 = arith.addi %arg3, %77 : index
              %c3_7 = arith.constant 3 : index
              %79 = arith.muli %c32, %c3_7 : index
              %80 = arith.addi %arg3, %79 : index
              %81 = arith.muli %46, %3 : index
              %82 = arith.muli %46, %3 : index
              %83 = arith.muli %46, %3 : index
              %84 = arith.muli %46, %3 : index
              %85 = arith.addi %81, %arg3 : index
              %86 = arith.addi %82, %76 : index
              %87 = arith.addi %83, %78 : index
              %88 = arith.addi %84, %80 : index
              %89 = arith.muli %2, %1 : index
              %90 = arith.muli %2, %1 : index
              %91 = arith.muli %2, %1 : index
              %92 = arith.muli %2, %1 : index
              %93 = arith.muli %89, %3 : index
              %94 = arith.muli %90, %3 : index
              %95 = arith.muli %91, %3 : index
              %96 = arith.muli %92, %3 : index
              %97 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%93], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %98 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%94], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %99 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%95], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %100 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%96], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %101:3 = "disc_shape.delinearize"(%85, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %102:3 = "disc_shape.delinearize"(%86, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %103:3 = "disc_shape.delinearize"(%87, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %104:3 = "disc_shape.delinearize"(%88, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %105 = memref.load %7[%101#0, %101#1, %101#2] : memref<?x?x?xf32, "gpu">
              %106 = memref.load %7[%102#0, %102#1, %102#2] : memref<?x?x?xf32, "gpu">
              %107 = memref.load %7[%103#0, %103#1, %103#2] : memref<?x?x?xf32, "gpu">
              %108 = memref.load %7[%104#0, %104#1, %104#2] : memref<?x?x?xf32, "gpu">
              %109 = "disc_shape.linearize"(%101#0, %101#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %110 = "disc_shape.linearize"(%102#0, %102#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %111 = "disc_shape.linearize"(%103#0, %103#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %112 = "disc_shape.linearize"(%104#0, %104#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %113 = "disc_shape.delinearize"(%109, %13) : (index, index) -> index
              %114 = "disc_shape.delinearize"(%110, %13) : (index, index) -> index
              %115 = "disc_shape.delinearize"(%111, %13) : (index, index) -> index
              %116 = "disc_shape.delinearize"(%112, %13) : (index, index) -> index
              %117 = "disc_shape.linearize"(%113, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %118 = "disc_shape.linearize"(%114, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %119 = "disc_shape.linearize"(%115, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %120 = "disc_shape.linearize"(%116, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %121 = arith.remui %117, %c8 : index
              %122 = arith.remui %118, %c8 : index
              %123 = arith.remui %119, %c8 : index
              %124 = arith.remui %120, %c8 : index
              %125 = memref.load %48[%121] : memref<32xf32, 3>
              %126 = memref.load %48[%122] : memref<32xf32, 3>
              %127 = memref.load %48[%123] : memref<32xf32, 3>
              %128 = memref.load %48[%124] : memref<32xf32, 3>
              %129 = arith.subf %105, %125 : f32
              %130 = arith.subf %106, %126 : f32
              %131 = arith.subf %107, %127 : f32
              %132 = arith.subf %108, %128 : f32
              %133 = math.exp %129 : f32
              %134 = math.exp %130 : f32
              %135 = math.exp %131 : f32
              %136 = math.exp %132 : f32
              %137 = "disc_shape.linearize"(%101#0, %101#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %138 = "disc_shape.linearize"(%102#0, %102#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %139 = "disc_shape.linearize"(%103#0, %103#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %140 = "disc_shape.linearize"(%104#0, %104#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %141 = "disc_shape.delinearize"(%137, %13) : (index, index) -> index
              %142 = "disc_shape.delinearize"(%138, %13) : (index, index) -> index
              %143 = "disc_shape.delinearize"(%139, %13) : (index, index) -> index
              %144 = "disc_shape.delinearize"(%140, %13) : (index, index) -> index
              %145 = "disc_shape.linearize"(%141, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %146 = "disc_shape.linearize"(%142, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %147 = "disc_shape.linearize"(%143, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %148 = "disc_shape.linearize"(%144, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %149 = arith.remui %145, %c8 : index
              %150 = arith.remui %146, %c8 : index
              %151 = arith.remui %147, %c8 : index
              %152 = arith.remui %148, %c8 : index
              %153 = memref.load %56[%149] : memref<32xf32, 3>
              %154 = memref.load %56[%150] : memref<32xf32, 3>
              %155 = memref.load %56[%151] : memref<32xf32, 3>
              %156 = memref.load %56[%152] : memref<32xf32, 3>
              %157 = arith.divf %133, %153 : f32
              %158 = arith.divf %134, %154 : f32
              %159 = arith.divf %135, %155 : f32
              %160 = arith.divf %136, %156 : f32
              memref.store %157, %97[%85] : memref<?xf32, "gpu">
              memref.store %158, %98[%86] : memref<?xf32, "gpu">
              memref.store %159, %99[%87] : memref<?xf32, "gpu">
              memref.store %160, %100[%88] : memref<?xf32, "gpu">
            }
            scf.for %arg3 = %73 to %3 step %c32 {
              %75 = arith.muli %46, %3 : index
              %76 = arith.addi %75, %arg3 : index
              %77 = arith.muli %2, %1 : index
              %78 = arith.muli %77, %3 : index
              %79 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%78], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %80:3 = "disc_shape.delinearize"(%76, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %81 = memref.load %7[%80#0, %80#1, %80#2] : memref<?x?x?xf32, "gpu">
              %82 = "disc_shape.linearize"(%80#0, %80#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %83 = "disc_shape.delinearize"(%82, %13) : (index, index) -> index
              %84 = "disc_shape.linearize"(%83, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %85 = arith.remui %84, %c8 : index
              %86 = memref.load %48[%85] : memref<32xf32, 3>
              %87 = arith.subf %81, %86 : f32
              %88 = math.exp %87 : f32
              %89 = "disc_shape.linearize"(%80#0, %80#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %90 = "disc_shape.delinearize"(%89, %13) : (index, index) -> index
              %91 = "disc_shape.linearize"(%90, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %92 = arith.remui %91, %c8 : index
              %93 = memref.load %56[%92] : memref<32xf32, 3>
              %94 = arith.divf %88, %93 : f32
              memref.store %94, %79[%76] : memref<?xf32, "gpu">
            }
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  memref.dealloc %25 : memref<?x?x?xf32, "gpu">
  memref.dealloc %24 : memref<?x?xf32, "gpu">
  memref.dealloc %23 : memref<?xf32, "gpu">
  memref.dealloc %22 : memref<?x?xf32, "gpu">
  memref.dealloc %21 : memref<?x?x?xf32, "gpu">
  memref.dealloc %20 : memref<?x?x?xf32, "gpu">
  memref.dealloc %19 : memref<?x?x?xf32, "gpu">
  memref.dealloc %17 : memref<?x?xf32, "gpu">
  memref.dealloc %15 : memref<?xf32, "gpu">
  memref.dealloc %14 : memref<?x?xf32, "gpu">
  memref.dealloc %5 : memref<f32, "gpu">
  memref.dealloc %4 : memref<f32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %26) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After FoldSubViewOps (fold-memref-subview-ops) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c768 = arith.constant 768 : index
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant -0.000000e+00 : f32
  %cst_0 = arith.constant 0xFF800000 : f32
  %c8 = arith.constant 8 : index
  %c16_i32 = arith.constant 16 : i32
  %c8_i32 = arith.constant 8 : i32
  %c4_i32 = arith.constant 4 : i32
  %c2_i32 = arith.constant 2 : i32
  %c1_i32 = arith.constant 1 : i32
  %c32_i32 = arith.constant 32 : i32
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c1024 = arith.constant 1024 : index
  %c6 = arith.constant 6 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
  %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
  %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
  %4 = memref.alloc() : memref<f32, "gpu">
  %5 = memref.alloc() : memref<f32, "gpu">
  %6 = arith.muli %3, %1 : index
  %7 = memref.reinterpret_cast %0 to offset: [0], sizes: [%2, %1, %3], strides: [%6, %3, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %8 = arith.index_cast %3 : index to i32
  %9 = arith.index_cast %2 : index to i32
  %10 = arith.index_cast %1 : index to i32
  %11 = arith.muli %9, %10 : i32
  %12 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %11, %12[%c0] : memref<2xi32, "cpu">
  memref.store %8, %12[%c1] : memref<2xi32, "cpu">
  %13 = arith.index_cast %11 : i32 to index
  %14 = memref.alloc(%13, %3) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %15 = memref.alloc(%13) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %16 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %9, %16[%c0] : memref<2xi32, "cpu">
  memref.store %10, %16[%c1] : memref<2xi32, "cpu">
  %17 = memref.alloc(%2, %1) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %18 = memref.alloca() : memref<3xindex, "cpu">
  memref.store %2, %18[%c0] : memref<3xindex, "cpu">
  memref.store %1, %18[%c1] : memref<3xindex, "cpu">
  memref.store %3, %18[%c2] : memref<3xindex, "cpu">
  %19 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %20 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %21 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %22 = memref.alloc(%13, %3) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %23 = memref.alloc(%13) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %24 = memref.alloc(%2, %1) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %25 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %26 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %27 = arith.cmpi slt, %13, %c3 : index
  %28 = arith.cmpi sge, %3, %c1024 : index
  %29 = arith.cmpi slt, %13, %c6 : index
  %30 = arith.andi %29, %28 : i1
  %31 = arith.cmpi sge, %3, %c512 : index
  %32 = arith.cmpi sge, %13, %c6 : index
  %33 = arith.andi %32, %31 : i1
  %34 = arith.ori %27, %30 : i1
  %35 = arith.ori %34, %33 : i1
  scf.if %35 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%13) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %36 = gpu.block_id  x
          %37 = gpu.thread_id  x
          %38 = memref.alloc() : memref<32xf32, 3>
          %39 = gpu.block_id  x
          %40 = gpu.thread_id  x
          %41 = arith.divui %40, %c32 : index
          %42 = arith.remui %40, %c32 : index
          %43 = arith.subi %3, %40 : index
          %44 = arith.cmpi eq, %43, %c0 : index
          %45 = arith.subi %43, %c1 : index
          %46 = arith.divui %45, %c256 : index
          %47 = arith.addi %46, %c1 : index
          %48 = arith.select %44, %c0, %47 : index
          %49 = arith.remsi %48, %c4 : index
          %50 = arith.subi %48, %49 : index
          %51 = arith.muli %50, %c256 : index
          %52 = arith.addi %40, %51 : index
          %53 = scf.for %arg3 = %40 to %52 step %c1024 iter_args(%arg4 = %cst_0) -> (f32) {
            %113 = arith.addi %arg3, %c256 : index
            %114 = arith.addi %arg3, %c512 : index
            %115 = arith.addi %arg3, %c768 : index
            %116 = "disc_shape.linearize"(%39, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %117 = "disc_shape.linearize"(%39, %113, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %118 = "disc_shape.linearize"(%39, %114, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %119 = "disc_shape.linearize"(%39, %115, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %120:3 = "disc_shape.delinearize"(%116, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %121:3 = "disc_shape.delinearize"(%117, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %122:3 = "disc_shape.delinearize"(%118, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %123:3 = "disc_shape.delinearize"(%119, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %124 = memref.load %7[%120#0, %120#1, %120#2] : memref<?x?x?xf32, "gpu">
            %125 = memref.load %7[%121#0, %121#1, %121#2] : memref<?x?x?xf32, "gpu">
            %126 = memref.load %7[%122#0, %122#1, %122#2] : memref<?x?x?xf32, "gpu">
            %127 = memref.load %7[%123#0, %123#1, %123#2] : memref<?x?x?xf32, "gpu">
            %128 = arith.cmpf ugt, %arg4, %124 : f32
            %129 = arith.select %128, %arg4, %124 : f32
            %130 = arith.cmpf uno, %124, %124 : f32
            %131 = arith.select %130, %124, %129 : f32
            %132 = arith.cmpf ugt, %131, %125 : f32
            %133 = arith.select %132, %131, %125 : f32
            %134 = arith.cmpf uno, %125, %125 : f32
            %135 = arith.select %134, %125, %133 : f32
            %136 = arith.cmpf ugt, %135, %126 : f32
            %137 = arith.select %136, %135, %126 : f32
            %138 = arith.cmpf uno, %126, %126 : f32
            %139 = arith.select %138, %126, %137 : f32
            %140 = arith.cmpf ugt, %139, %127 : f32
            %141 = arith.select %140, %139, %127 : f32
            %142 = arith.cmpf uno, %127, %127 : f32
            %143 = arith.select %142, %127, %141 : f32
            scf.yield %143 : f32
          }
          %54 = scf.for %arg3 = %52 to %3 step %c256 iter_args(%arg4 = %53) -> (f32) {
            %113 = "disc_shape.linearize"(%39, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %114:3 = "disc_shape.delinearize"(%113, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %115 = memref.load %7[%114#0, %114#1, %114#2] : memref<?x?x?xf32, "gpu">
            %116 = arith.cmpf ugt, %arg4, %115 : f32
            %117 = arith.select %116, %arg4, %115 : f32
            %118 = arith.cmpf uno, %115, %115 : f32
            %119 = arith.select %118, %115, %117 : f32
            scf.yield %119 : f32
          }
          %result, %valid = gpu.shuffle  xor %54, %c1_i32, %c32_i32 : f32
          %55 = arith.cmpf ugt, %54, %result : f32
          %56 = arith.select %55, %54, %result : f32
          %57 = arith.cmpf uno, %result, %result : f32
          %58 = arith.select %57, %result, %56 : f32
          %result_1, %valid_2 = gpu.shuffle  xor %58, %c2_i32, %c32_i32 : f32
          %59 = arith.cmpf ugt, %58, %result_1 : f32
          %60 = arith.select %59, %58, %result_1 : f32
          %61 = arith.cmpf uno, %result_1, %result_1 : f32
          %62 = arith.select %61, %result_1, %60 : f32
          %result_3, %valid_4 = gpu.shuffle  xor %62, %c4_i32, %c32_i32 : f32
          %63 = arith.cmpf ugt, %62, %result_3 : f32
          %64 = arith.select %63, %62, %result_3 : f32
          %65 = arith.cmpf uno, %result_3, %result_3 : f32
          %66 = arith.select %65, %result_3, %64 : f32
          %result_5, %valid_6 = gpu.shuffle  xor %66, %c8_i32, %c32_i32 : f32
          %67 = arith.cmpf ugt, %66, %result_5 : f32
          %68 = arith.select %67, %66, %result_5 : f32
          %69 = arith.cmpf uno, %result_5, %result_5 : f32
          %70 = arith.select %69, %result_5, %68 : f32
          %result_7, %valid_8 = gpu.shuffle  xor %70, %c16_i32, %c32_i32 : f32
          %71 = arith.cmpf ugt, %70, %result_7 : f32
          %72 = arith.select %71, %70, %result_7 : f32
          %73 = arith.cmpf uno, %result_7, %result_7 : f32
          %74 = arith.select %73, %result_7, %72 : f32
          %75 = memref.alloc() : memref<8xf32, 3>
          %76 = arith.cmpi eq, %42, %c0 : index
          scf.if %76 {
            memref.store %74, %75[%41] : memref<8xf32, 3>
          }
          gpu.barrier
          %77 = arith.cmpi slt, %40, %c32 : index
          scf.if %77 {
            %113 = arith.cmpi slt, %42, %c8 : index
            %114 = scf.if %113 -> (f32) {
              %127 = memref.load %75[%42] : memref<8xf32, 3>
              scf.yield %127 : f32
            } else {
              scf.yield %cst_0 : f32
            }
            %result_19, %valid_20 = gpu.shuffle  xor %114, %c1_i32, %c8_i32 : f32
            %115 = arith.cmpf ugt, %114, %result_19 : f32
            %116 = arith.select %115, %114, %result_19 : f32
            %117 = arith.cmpf uno, %result_19, %result_19 : f32
            %118 = arith.select %117, %result_19, %116 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %118, %c2_i32, %c8_i32 : f32
            %119 = arith.cmpf ugt, %118, %result_21 : f32
            %120 = arith.select %119, %118, %result_21 : f32
            %121 = arith.cmpf uno, %result_21, %result_21 : f32
            %122 = arith.select %121, %result_21, %120 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %122, %c4_i32, %c8_i32 : f32
            %123 = arith.cmpf ugt, %122, %result_23 : f32
            %124 = arith.select %123, %122, %result_23 : f32
            %125 = arith.cmpf uno, %result_23, %result_23 : f32
            %126 = arith.select %125, %result_23, %124 : f32
            scf.if %76 {
              memref.store %126, %38[%c0] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %78 = memref.alloc() : memref<32xf32, 3>
          %79 = gpu.block_id  x
          %80 = gpu.thread_id  x
          %81 = arith.divui %80, %c32 : index
          %82 = arith.remui %80, %c32 : index
          %83 = arith.subi %3, %80 : index
          %84 = arith.cmpi eq, %83, %c0 : index
          %85 = arith.subi %83, %c1 : index
          %86 = arith.divui %85, %c256 : index
          %87 = arith.addi %86, %c1 : index
          %88 = arith.select %84, %c0, %87 : index
          %89 = arith.remsi %88, %c4 : index
          %90 = arith.subi %88, %89 : index
          %91 = arith.muli %90, %c256 : index
          %92 = arith.addi %80, %91 : index
          %93 = scf.for %arg3 = %80 to %92 step %c1024 iter_args(%arg4 = %cst) -> (f32) {
            %113 = arith.addi %arg3, %c256 : index
            %114 = arith.addi %arg3, %c512 : index
            %115 = arith.addi %arg3, %c768 : index
            %116 = "disc_shape.linearize"(%79, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %117 = "disc_shape.linearize"(%79, %113, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %118 = "disc_shape.linearize"(%79, %114, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %119 = "disc_shape.linearize"(%79, %115, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %120:3 = "disc_shape.delinearize"(%116, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %121:3 = "disc_shape.delinearize"(%117, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %122:3 = "disc_shape.delinearize"(%118, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %123:3 = "disc_shape.delinearize"(%119, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %124 = memref.load %7[%120#0, %120#1, %120#2] : memref<?x?x?xf32, "gpu">
            %125 = memref.load %7[%121#0, %121#1, %121#2] : memref<?x?x?xf32, "gpu">
            %126 = memref.load %7[%122#0, %122#1, %122#2] : memref<?x?x?xf32, "gpu">
            %127 = memref.load %7[%123#0, %123#1, %123#2] : memref<?x?x?xf32, "gpu">
            %128 = memref.load %38[%c0] : memref<32xf32, 3>
            %129 = memref.load %38[%c0] : memref<32xf32, 3>
            %130 = memref.load %38[%c0] : memref<32xf32, 3>
            %131 = memref.load %38[%c0] : memref<32xf32, 3>
            %132 = arith.subf %124, %128 : f32
            %133 = arith.subf %125, %129 : f32
            %134 = arith.subf %126, %130 : f32
            %135 = arith.subf %127, %131 : f32
            %136 = math.exp %132 : f32
            %137 = math.exp %133 : f32
            %138 = math.exp %134 : f32
            %139 = math.exp %135 : f32
            %140 = arith.addf %arg4, %136 : f32
            %141 = arith.addf %140, %137 : f32
            %142 = arith.addf %141, %138 : f32
            %143 = arith.addf %142, %139 : f32
            scf.yield %143 : f32
          }
          %94 = scf.for %arg3 = %92 to %3 step %c256 iter_args(%arg4 = %93) -> (f32) {
            %113 = "disc_shape.linearize"(%79, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %114:3 = "disc_shape.delinearize"(%113, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %115 = memref.load %7[%114#0, %114#1, %114#2] : memref<?x?x?xf32, "gpu">
            %116 = memref.load %38[%c0] : memref<32xf32, 3>
            %117 = arith.subf %115, %116 : f32
            %118 = math.exp %117 : f32
            %119 = arith.addf %arg4, %118 : f32
            scf.yield %119 : f32
          }
          %result_9, %valid_10 = gpu.shuffle  xor %94, %c1_i32, %c32_i32 : f32
          %95 = arith.addf %94, %result_9 : f32
          %result_11, %valid_12 = gpu.shuffle  xor %95, %c2_i32, %c32_i32 : f32
          %96 = arith.addf %95, %result_11 : f32
          %result_13, %valid_14 = gpu.shuffle  xor %96, %c4_i32, %c32_i32 : f32
          %97 = arith.addf %96, %result_13 : f32
          %result_15, %valid_16 = gpu.shuffle  xor %97, %c8_i32, %c32_i32 : f32
          %98 = arith.addf %97, %result_15 : f32
          %result_17, %valid_18 = gpu.shuffle  xor %98, %c16_i32, %c32_i32 : f32
          %99 = arith.addf %98, %result_17 : f32
          %100 = memref.alloc() : memref<8xf32, 3>
          %101 = arith.cmpi eq, %82, %c0 : index
          scf.if %101 {
            memref.store %99, %100[%81] : memref<8xf32, 3>
          }
          gpu.barrier
          %102 = arith.cmpi slt, %80, %c32 : index
          scf.if %102 {
            %113 = arith.cmpi slt, %82, %c8 : index
            %114 = scf.if %113 -> (f32) {
              %118 = memref.load %100[%82] : memref<8xf32, 3>
              scf.yield %118 : f32
            } else {
              scf.yield %cst : f32
            }
            %result_19, %valid_20 = gpu.shuffle  xor %114, %c1_i32, %c8_i32 : f32
            %115 = arith.addf %114, %result_19 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %115, %c2_i32, %c8_i32 : f32
            %116 = arith.addf %115, %result_21 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %116, %c4_i32, %c8_i32 : f32
            %117 = arith.addf %116, %result_23 : f32
            scf.if %101 {
              memref.store %117, %78[%c0] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %103 = arith.subi %3, %37 : index
          %104 = arith.cmpi eq, %103, %c0 : index
          %105 = arith.subi %103, %c1 : index
          %106 = arith.divui %105, %c256 : index
          %107 = arith.addi %106, %c1 : index
          %108 = arith.select %104, %c0, %107 : index
          %109 = arith.remsi %108, %c4 : index
          %110 = arith.subi %108, %109 : index
          %111 = arith.muli %110, %c256 : index
          %112 = arith.addi %37, %111 : index
          scf.for %arg3 = %37 to %112 step %c1024 {
            %113 = arith.addi %arg3, %c256 : index
            %114 = arith.addi %arg3, %c512 : index
            %115 = arith.addi %arg3, %c768 : index
            %116 = arith.muli %36, %3 : index
            %117 = arith.muli %36, %3 : index
            %118 = arith.muli %36, %3 : index
            %119 = arith.muli %36, %3 : index
            %120 = arith.addi %116, %arg3 : index
            %121 = arith.addi %117, %113 : index
            %122 = arith.addi %118, %114 : index
            %123 = arith.addi %119, %115 : index
            %124 = arith.muli %2, %1 : index
            %125 = arith.muli %2, %1 : index
            %126 = arith.muli %2, %1 : index
            %127 = arith.muli %2, %1 : index
            %128 = arith.muli %124, %3 : index
            %129 = arith.muli %125, %3 : index
            %130 = arith.muli %126, %3 : index
            %131 = arith.muli %127, %3 : index
            %132 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%128], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %133 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%129], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %134 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%130], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %135 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%131], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %136:3 = "disc_shape.delinearize"(%120, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %137:3 = "disc_shape.delinearize"(%121, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %138:3 = "disc_shape.delinearize"(%122, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %139:3 = "disc_shape.delinearize"(%123, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %140 = memref.load %7[%136#0, %136#1, %136#2] : memref<?x?x?xf32, "gpu">
            %141 = memref.load %7[%137#0, %137#1, %137#2] : memref<?x?x?xf32, "gpu">
            %142 = memref.load %7[%138#0, %138#1, %138#2] : memref<?x?x?xf32, "gpu">
            %143 = memref.load %7[%139#0, %139#1, %139#2] : memref<?x?x?xf32, "gpu">
            %144 = memref.load %38[%c0] : memref<32xf32, 3>
            %145 = memref.load %38[%c0] : memref<32xf32, 3>
            %146 = memref.load %38[%c0] : memref<32xf32, 3>
            %147 = memref.load %38[%c0] : memref<32xf32, 3>
            %148 = arith.subf %140, %144 : f32
            %149 = arith.subf %141, %145 : f32
            %150 = arith.subf %142, %146 : f32
            %151 = arith.subf %143, %147 : f32
            %152 = math.exp %148 : f32
            %153 = math.exp %149 : f32
            %154 = math.exp %150 : f32
            %155 = math.exp %151 : f32
            %156 = memref.load %78[%c0] : memref<32xf32, 3>
            %157 = memref.load %78[%c0] : memref<32xf32, 3>
            %158 = memref.load %78[%c0] : memref<32xf32, 3>
            %159 = memref.load %78[%c0] : memref<32xf32, 3>
            %160 = arith.divf %152, %156 : f32
            %161 = arith.divf %153, %157 : f32
            %162 = arith.divf %154, %158 : f32
            %163 = arith.divf %155, %159 : f32
            memref.store %160, %132[%120] : memref<?xf32, "gpu">
            memref.store %161, %133[%121] : memref<?xf32, "gpu">
            memref.store %162, %134[%122] : memref<?xf32, "gpu">
            memref.store %163, %135[%123] : memref<?xf32, "gpu">
          }
          scf.for %arg3 = %112 to %3 step %c256 {
            %113 = arith.muli %36, %3 : index
            %114 = arith.addi %113, %arg3 : index
            %115 = arith.muli %2, %1 : index
            %116 = arith.muli %115, %3 : index
            %117 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%116], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %118:3 = "disc_shape.delinearize"(%114, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %119 = memref.load %7[%118#0, %118#1, %118#2] : memref<?x?x?xf32, "gpu">
            %120 = memref.load %38[%c0] : memref<32xf32, 3>
            %121 = arith.subf %119, %120 : f32
            %122 = math.exp %121 : f32
            %123 = memref.load %78[%c0] : memref<32xf32, 3>
            %124 = arith.divf %122, %123 : f32
            memref.store %124, %117[%114] : memref<?xf32, "gpu">
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      %36 = arith.cmpi eq, %13, %c0 : index
      %37 = arith.subi %13, %c1 : index
      %38 = arith.divui %37, %c8 : index
      %39 = arith.addi %38, %c1 : index
      %40 = arith.select %36, %c0, %39 : index
      scf.parallel (%arg1) = (%c0) to (%40) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %41 = gpu.block_id  x
          %42 = gpu.thread_id  x
          %43 = arith.divui %42, %c32 : index
          %44 = arith.remui %42, %c32 : index
          %45 = arith.muli %41, %c8 : index
          %46 = arith.addi %45, %43 : index
          %47 = arith.cmpi slt, %46, %13 : index
          %48 = memref.alloc() : memref<32xf32, 3>
          %49 = gpu.block_id  x
          %50 = gpu.thread_id  x
          %51 = arith.divui %50, %c32 : index
          %52 = arith.remui %50, %c32 : index
          %53 = arith.muli %49, %c8 : index
          %54 = arith.addi %53, %51 : index
          %55 = arith.cmpi slt, %54, %13 : index
          scf.if %55 {
            %64 = arith.subi %3, %52 : index
            %65 = arith.cmpi eq, %64, %c0 : index
            %66 = arith.subi %64, %c1 : index
            %67 = arith.divui %66, %c32 : index
            %68 = arith.addi %67, %c1 : index
            %69 = arith.select %65, %c0, %68 : index
            %70 = arith.remsi %69, %c4 : index
            %71 = arith.subi %69, %70 : index
            %72 = arith.muli %71, %c32 : index
            %73 = arith.addi %52, %72 : index
            %74 = scf.for %arg3 = %52 to %73 step %c128 iter_args(%arg4 = %cst_0) -> (f32) {
              %97 = arith.addi %arg3, %c32 : index
              %98 = arith.addi %arg3, %c64 : index
              %99 = arith.addi %arg3, %c96 : index
              %100 = "disc_shape.linearize"(%54, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %101 = "disc_shape.linearize"(%54, %97, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %102 = "disc_shape.linearize"(%54, %98, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %103 = "disc_shape.linearize"(%54, %99, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %104:3 = "disc_shape.delinearize"(%100, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %105:3 = "disc_shape.delinearize"(%101, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %106:3 = "disc_shape.delinearize"(%102, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %107:3 = "disc_shape.delinearize"(%103, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %108 = memref.load %7[%104#0, %104#1, %104#2] : memref<?x?x?xf32, "gpu">
              %109 = memref.load %7[%105#0, %105#1, %105#2] : memref<?x?x?xf32, "gpu">
              %110 = memref.load %7[%106#0, %106#1, %106#2] : memref<?x?x?xf32, "gpu">
              %111 = memref.load %7[%107#0, %107#1, %107#2] : memref<?x?x?xf32, "gpu">
              %112 = arith.cmpf ugt, %arg4, %108 : f32
              %113 = arith.select %112, %arg4, %108 : f32
              %114 = arith.cmpf uno, %108, %108 : f32
              %115 = arith.select %114, %108, %113 : f32
              %116 = arith.cmpf ugt, %115, %109 : f32
              %117 = arith.select %116, %115, %109 : f32
              %118 = arith.cmpf uno, %109, %109 : f32
              %119 = arith.select %118, %109, %117 : f32
              %120 = arith.cmpf ugt, %119, %110 : f32
              %121 = arith.select %120, %119, %110 : f32
              %122 = arith.cmpf uno, %110, %110 : f32
              %123 = arith.select %122, %110, %121 : f32
              %124 = arith.cmpf ugt, %123, %111 : f32
              %125 = arith.select %124, %123, %111 : f32
              %126 = arith.cmpf uno, %111, %111 : f32
              %127 = arith.select %126, %111, %125 : f32
              scf.yield %127 : f32
            }
            %75 = scf.for %arg3 = %73 to %3 step %c32 iter_args(%arg4 = %74) -> (f32) {
              %97 = "disc_shape.linearize"(%54, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %98:3 = "disc_shape.delinearize"(%97, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %99 = memref.load %7[%98#0, %98#1, %98#2] : memref<?x?x?xf32, "gpu">
              %100 = arith.cmpf ugt, %arg4, %99 : f32
              %101 = arith.select %100, %arg4, %99 : f32
              %102 = arith.cmpf uno, %99, %99 : f32
              %103 = arith.select %102, %99, %101 : f32
              scf.yield %103 : f32
            }
            %result, %valid = gpu.shuffle  xor %75, %c1_i32, %c32_i32 : f32
            %76 = arith.cmpf ugt, %75, %result : f32
            %77 = arith.select %76, %75, %result : f32
            %78 = arith.cmpf uno, %result, %result : f32
            %79 = arith.select %78, %result, %77 : f32
            %result_1, %valid_2 = gpu.shuffle  xor %79, %c2_i32, %c32_i32 : f32
            %80 = arith.cmpf ugt, %79, %result_1 : f32
            %81 = arith.select %80, %79, %result_1 : f32
            %82 = arith.cmpf uno, %result_1, %result_1 : f32
            %83 = arith.select %82, %result_1, %81 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %83, %c4_i32, %c32_i32 : f32
            %84 = arith.cmpf ugt, %83, %result_3 : f32
            %85 = arith.select %84, %83, %result_3 : f32
            %86 = arith.cmpf uno, %result_3, %result_3 : f32
            %87 = arith.select %86, %result_3, %85 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %87, %c8_i32, %c32_i32 : f32
            %88 = arith.cmpf ugt, %87, %result_5 : f32
            %89 = arith.select %88, %87, %result_5 : f32
            %90 = arith.cmpf uno, %result_5, %result_5 : f32
            %91 = arith.select %90, %result_5, %89 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %91, %c16_i32, %c32_i32 : f32
            %92 = arith.cmpf ugt, %91, %result_7 : f32
            %93 = arith.select %92, %91, %result_7 : f32
            %94 = arith.cmpf uno, %result_7, %result_7 : f32
            %95 = arith.select %94, %result_7, %93 : f32
            %96 = arith.cmpi eq, %52, %c0 : index
            scf.if %96 {
              memref.store %95, %48[%51] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %56 = memref.alloc() : memref<32xf32, 3>
          %57 = gpu.block_id  x
          %58 = gpu.thread_id  x
          %59 = arith.divui %58, %c32 : index
          %60 = arith.remui %58, %c32 : index
          %61 = arith.muli %57, %c8 : index
          %62 = arith.addi %61, %59 : index
          %63 = arith.cmpi slt, %62, %13 : index
          scf.if %63 {
            %64 = arith.subi %3, %60 : index
            %65 = arith.cmpi eq, %64, %c0 : index
            %66 = arith.subi %64, %c1 : index
            %67 = arith.divui %66, %c32 : index
            %68 = arith.addi %67, %c1 : index
            %69 = arith.select %65, %c0, %68 : index
            %70 = arith.remsi %69, %c4 : index
            %71 = arith.subi %69, %70 : index
            %72 = arith.muli %71, %c32 : index
            %73 = arith.addi %60, %72 : index
            %74 = scf.for %arg3 = %60 to %73 step %c128 iter_args(%arg4 = %cst) -> (f32) {
              %82 = arith.addi %arg3, %c32 : index
              %83 = arith.addi %arg3, %c64 : index
              %84 = arith.addi %arg3, %c96 : index
              %85 = "disc_shape.linearize"(%62, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %86 = "disc_shape.linearize"(%62, %82, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %87 = "disc_shape.linearize"(%62, %83, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %88 = "disc_shape.linearize"(%62, %84, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %89:3 = "disc_shape.delinearize"(%85, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %90:3 = "disc_shape.delinearize"(%86, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %91:3 = "disc_shape.delinearize"(%87, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %92:3 = "disc_shape.delinearize"(%88, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %93 = memref.load %7[%89#0, %89#1, %89#2] : memref<?x?x?xf32, "gpu">
              %94 = memref.load %7[%90#0, %90#1, %90#2] : memref<?x?x?xf32, "gpu">
              %95 = memref.load %7[%91#0, %91#1, %91#2] : memref<?x?x?xf32, "gpu">
              %96 = memref.load %7[%92#0, %92#1, %92#2] : memref<?x?x?xf32, "gpu">
              %97 = "disc_shape.linearize"(%89#0, %89#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %98 = "disc_shape.linearize"(%90#0, %90#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %99 = "disc_shape.linearize"(%91#0, %91#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %100 = "disc_shape.linearize"(%92#0, %92#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %101 = "disc_shape.delinearize"(%97, %13) : (index, index) -> index
              %102 = "disc_shape.delinearize"(%98, %13) : (index, index) -> index
              %103 = "disc_shape.delinearize"(%99, %13) : (index, index) -> index
              %104 = "disc_shape.delinearize"(%100, %13) : (index, index) -> index
              %105 = "disc_shape.linearize"(%101, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %106 = "disc_shape.linearize"(%102, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %107 = "disc_shape.linearize"(%103, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %108 = "disc_shape.linearize"(%104, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %109 = arith.remui %105, %c8 : index
              %110 = arith.remui %106, %c8 : index
              %111 = arith.remui %107, %c8 : index
              %112 = arith.remui %108, %c8 : index
              %113 = memref.load %48[%109] : memref<32xf32, 3>
              %114 = memref.load %48[%110] : memref<32xf32, 3>
              %115 = memref.load %48[%111] : memref<32xf32, 3>
              %116 = memref.load %48[%112] : memref<32xf32, 3>
              %117 = arith.subf %93, %113 : f32
              %118 = arith.subf %94, %114 : f32
              %119 = arith.subf %95, %115 : f32
              %120 = arith.subf %96, %116 : f32
              %121 = math.exp %117 : f32
              %122 = math.exp %118 : f32
              %123 = math.exp %119 : f32
              %124 = math.exp %120 : f32
              %125 = arith.addf %arg4, %121 : f32
              %126 = arith.addf %125, %122 : f32
              %127 = arith.addf %126, %123 : f32
              %128 = arith.addf %127, %124 : f32
              scf.yield %128 : f32
            }
            %75 = scf.for %arg3 = %73 to %3 step %c32 iter_args(%arg4 = %74) -> (f32) {
              %82 = "disc_shape.linearize"(%62, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %83:3 = "disc_shape.delinearize"(%82, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %84 = memref.load %7[%83#0, %83#1, %83#2] : memref<?x?x?xf32, "gpu">
              %85 = "disc_shape.linearize"(%83#0, %83#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %86 = "disc_shape.delinearize"(%85, %13) : (index, index) -> index
              %87 = "disc_shape.linearize"(%86, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %88 = arith.remui %87, %c8 : index
              %89 = memref.load %48[%88] : memref<32xf32, 3>
              %90 = arith.subf %84, %89 : f32
              %91 = math.exp %90 : f32
              %92 = arith.addf %arg4, %91 : f32
              scf.yield %92 : f32
            }
            %result, %valid = gpu.shuffle  xor %75, %c1_i32, %c32_i32 : f32
            %76 = arith.addf %75, %result : f32
            %result_1, %valid_2 = gpu.shuffle  xor %76, %c2_i32, %c32_i32 : f32
            %77 = arith.addf %76, %result_1 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %77, %c4_i32, %c32_i32 : f32
            %78 = arith.addf %77, %result_3 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %78, %c8_i32, %c32_i32 : f32
            %79 = arith.addf %78, %result_5 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %79, %c16_i32, %c32_i32 : f32
            %80 = arith.addf %79, %result_7 : f32
            %81 = arith.cmpi eq, %60, %c0 : index
            scf.if %81 {
              memref.store %80, %56[%59] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          scf.if %47 {
            %64 = arith.subi %3, %44 : index
            %65 = arith.cmpi eq, %64, %c0 : index
            %66 = arith.subi %64, %c1 : index
            %67 = arith.divui %66, %c32 : index
            %68 = arith.addi %67, %c1 : index
            %69 = arith.select %65, %c0, %68 : index
            %70 = arith.remsi %69, %c4 : index
            %71 = arith.subi %69, %70 : index
            %72 = arith.muli %71, %c32 : index
            %73 = arith.addi %44, %72 : index
            scf.for %arg3 = %44 to %73 step %c128 {
              %74 = arith.addi %arg3, %c32 : index
              %75 = arith.addi %arg3, %c64 : index
              %76 = arith.addi %arg3, %c96 : index
              %77 = arith.muli %46, %3 : index
              %78 = arith.muli %46, %3 : index
              %79 = arith.muli %46, %3 : index
              %80 = arith.muli %46, %3 : index
              %81 = arith.addi %77, %arg3 : index
              %82 = arith.addi %78, %74 : index
              %83 = arith.addi %79, %75 : index
              %84 = arith.addi %80, %76 : index
              %85 = arith.muli %2, %1 : index
              %86 = arith.muli %2, %1 : index
              %87 = arith.muli %2, %1 : index
              %88 = arith.muli %2, %1 : index
              %89 = arith.muli %85, %3 : index
              %90 = arith.muli %86, %3 : index
              %91 = arith.muli %87, %3 : index
              %92 = arith.muli %88, %3 : index
              %93 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%89], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %94 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%90], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %95 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%91], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %96 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%92], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %97:3 = "disc_shape.delinearize"(%81, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %98:3 = "disc_shape.delinearize"(%82, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %99:3 = "disc_shape.delinearize"(%83, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %100:3 = "disc_shape.delinearize"(%84, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %101 = memref.load %7[%97#0, %97#1, %97#2] : memref<?x?x?xf32, "gpu">
              %102 = memref.load %7[%98#0, %98#1, %98#2] : memref<?x?x?xf32, "gpu">
              %103 = memref.load %7[%99#0, %99#1, %99#2] : memref<?x?x?xf32, "gpu">
              %104 = memref.load %7[%100#0, %100#1, %100#2] : memref<?x?x?xf32, "gpu">
              %105 = "disc_shape.linearize"(%97#0, %97#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %106 = "disc_shape.linearize"(%98#0, %98#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %107 = "disc_shape.linearize"(%99#0, %99#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %108 = "disc_shape.linearize"(%100#0, %100#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %109 = "disc_shape.delinearize"(%105, %13) : (index, index) -> index
              %110 = "disc_shape.delinearize"(%106, %13) : (index, index) -> index
              %111 = "disc_shape.delinearize"(%107, %13) : (index, index) -> index
              %112 = "disc_shape.delinearize"(%108, %13) : (index, index) -> index
              %113 = "disc_shape.linearize"(%109, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %114 = "disc_shape.linearize"(%110, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %115 = "disc_shape.linearize"(%111, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %116 = "disc_shape.linearize"(%112, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %117 = arith.remui %113, %c8 : index
              %118 = arith.remui %114, %c8 : index
              %119 = arith.remui %115, %c8 : index
              %120 = arith.remui %116, %c8 : index
              %121 = memref.load %48[%117] : memref<32xf32, 3>
              %122 = memref.load %48[%118] : memref<32xf32, 3>
              %123 = memref.load %48[%119] : memref<32xf32, 3>
              %124 = memref.load %48[%120] : memref<32xf32, 3>
              %125 = arith.subf %101, %121 : f32
              %126 = arith.subf %102, %122 : f32
              %127 = arith.subf %103, %123 : f32
              %128 = arith.subf %104, %124 : f32
              %129 = math.exp %125 : f32
              %130 = math.exp %126 : f32
              %131 = math.exp %127 : f32
              %132 = math.exp %128 : f32
              %133 = "disc_shape.linearize"(%97#0, %97#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %134 = "disc_shape.linearize"(%98#0, %98#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %135 = "disc_shape.linearize"(%99#0, %99#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %136 = "disc_shape.linearize"(%100#0, %100#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %137 = "disc_shape.delinearize"(%133, %13) : (index, index) -> index
              %138 = "disc_shape.delinearize"(%134, %13) : (index, index) -> index
              %139 = "disc_shape.delinearize"(%135, %13) : (index, index) -> index
              %140 = "disc_shape.delinearize"(%136, %13) : (index, index) -> index
              %141 = "disc_shape.linearize"(%137, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %142 = "disc_shape.linearize"(%138, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %143 = "disc_shape.linearize"(%139, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %144 = "disc_shape.linearize"(%140, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %145 = arith.remui %141, %c8 : index
              %146 = arith.remui %142, %c8 : index
              %147 = arith.remui %143, %c8 : index
              %148 = arith.remui %144, %c8 : index
              %149 = memref.load %56[%145] : memref<32xf32, 3>
              %150 = memref.load %56[%146] : memref<32xf32, 3>
              %151 = memref.load %56[%147] : memref<32xf32, 3>
              %152 = memref.load %56[%148] : memref<32xf32, 3>
              %153 = arith.divf %129, %149 : f32
              %154 = arith.divf %130, %150 : f32
              %155 = arith.divf %131, %151 : f32
              %156 = arith.divf %132, %152 : f32
              memref.store %153, %93[%81] : memref<?xf32, "gpu">
              memref.store %154, %94[%82] : memref<?xf32, "gpu">
              memref.store %155, %95[%83] : memref<?xf32, "gpu">
              memref.store %156, %96[%84] : memref<?xf32, "gpu">
            }
            scf.for %arg3 = %73 to %3 step %c32 {
              %74 = arith.muli %46, %3 : index
              %75 = arith.addi %74, %arg3 : index
              %76 = arith.muli %2, %1 : index
              %77 = arith.muli %76, %3 : index
              %78 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%77], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %79:3 = "disc_shape.delinearize"(%75, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %80 = memref.load %7[%79#0, %79#1, %79#2] : memref<?x?x?xf32, "gpu">
              %81 = "disc_shape.linearize"(%79#0, %79#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %82 = "disc_shape.delinearize"(%81, %13) : (index, index) -> index
              %83 = "disc_shape.linearize"(%82, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %84 = arith.remui %83, %c8 : index
              %85 = memref.load %48[%84] : memref<32xf32, 3>
              %86 = arith.subf %80, %85 : f32
              %87 = math.exp %86 : f32
              %88 = "disc_shape.linearize"(%79#0, %79#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %89 = "disc_shape.delinearize"(%88, %13) : (index, index) -> index
              %90 = "disc_shape.linearize"(%89, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %91 = arith.remui %90, %c8 : index
              %92 = memref.load %56[%91] : memref<32xf32, 3>
              %93 = arith.divf %87, %92 : f32
              memref.store %93, %78[%75] : memref<?xf32, "gpu">
            }
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  memref.dealloc %25 : memref<?x?x?xf32, "gpu">
  memref.dealloc %24 : memref<?x?xf32, "gpu">
  memref.dealloc %23 : memref<?xf32, "gpu">
  memref.dealloc %22 : memref<?x?xf32, "gpu">
  memref.dealloc %21 : memref<?x?x?xf32, "gpu">
  memref.dealloc %20 : memref<?x?x?xf32, "gpu">
  memref.dealloc %19 : memref<?x?x?xf32, "gpu">
  memref.dealloc %17 : memref<?x?xf32, "gpu">
  memref.dealloc %15 : memref<?xf32, "gpu">
  memref.dealloc %14 : memref<?x?xf32, "gpu">
  memref.dealloc %5 : memref<f32, "gpu">
  memref.dealloc %4 : memref<f32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %26) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After DiscFlattenMemrefAccessPass (disc-flatten-memref-access) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c768 = arith.constant 768 : index
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant -0.000000e+00 : f32
  %cst_0 = arith.constant 0xFF800000 : f32
  %c8 = arith.constant 8 : index
  %c16_i32 = arith.constant 16 : i32
  %c8_i32 = arith.constant 8 : i32
  %c4_i32 = arith.constant 4 : i32
  %c2_i32 = arith.constant 2 : i32
  %c1_i32 = arith.constant 1 : i32
  %c32_i32 = arith.constant 32 : i32
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c1024 = arith.constant 1024 : index
  %c6 = arith.constant 6 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
  %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
  %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
  %4 = memref.alloc() : memref<f32, "gpu">
  %5 = memref.alloc() : memref<f32, "gpu">
  %6 = arith.muli %3, %1 : index
  %7 = memref.reinterpret_cast %0 to offset: [0], sizes: [%2, %1, %3], strides: [%6, %3, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %8 = arith.index_cast %3 : index to i32
  %9 = arith.index_cast %2 : index to i32
  %10 = arith.index_cast %1 : index to i32
  %11 = arith.muli %9, %10 : i32
  %12 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %11, %12[%c0] : memref<2xi32, "cpu">
  memref.store %8, %12[%c1] : memref<2xi32, "cpu">
  %13 = arith.index_cast %11 : i32 to index
  %14 = memref.alloc(%13, %3) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %15 = memref.alloc(%13) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %16 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %9, %16[%c0] : memref<2xi32, "cpu">
  memref.store %10, %16[%c1] : memref<2xi32, "cpu">
  %17 = memref.alloc(%2, %1) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %18 = memref.alloca() : memref<3xindex, "cpu">
  memref.store %2, %18[%c0] : memref<3xindex, "cpu">
  memref.store %1, %18[%c1] : memref<3xindex, "cpu">
  memref.store %3, %18[%c2] : memref<3xindex, "cpu">
  %19 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %20 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %21 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %22 = memref.alloc(%13, %3) {kDiscSymbolicDimAttr = [@S3, @S2]} : memref<?x?xf32, "gpu">
  %23 = memref.alloc(%13) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %24 = memref.alloc(%2, %1) {kDiscSymbolicDimAttr = [@S0, @S1]} : memref<?x?xf32, "gpu">
  %25 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %26 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %27 = arith.cmpi slt, %13, %c3 : index
  %28 = arith.cmpi sge, %3, %c1024 : index
  %29 = arith.cmpi slt, %13, %c6 : index
  %30 = arith.andi %29, %28 : i1
  %31 = arith.cmpi sge, %3, %c512 : index
  %32 = arith.cmpi sge, %13, %c6 : index
  %33 = arith.andi %32, %31 : i1
  %34 = arith.ori %27, %30 : i1
  %35 = arith.ori %34, %33 : i1
  scf.if %35 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%13) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %36 = gpu.block_id  x
          %37 = gpu.thread_id  x
          %38 = memref.alloc() : memref<32xf32, 3>
          %39 = gpu.block_id  x
          %40 = gpu.thread_id  x
          %41 = arith.divui %40, %c32 : index
          %42 = arith.remui %40, %c32 : index
          %43 = arith.subi %3, %40 : index
          %44 = arith.cmpi eq, %43, %c0 : index
          %45 = arith.subi %43, %c1 : index
          %46 = arith.divui %45, %c256 : index
          %47 = arith.addi %46, %c1 : index
          %48 = arith.select %44, %c0, %47 : index
          %49 = arith.remsi %48, %c4 : index
          %50 = arith.subi %48, %49 : index
          %51 = arith.muli %50, %c256 : index
          %52 = arith.addi %40, %51 : index
          %53 = scf.for %arg3 = %40 to %52 step %c1024 iter_args(%arg4 = %cst_0) -> (f32) {
            %113 = arith.addi %arg3, %c256 : index
            %114 = arith.addi %arg3, %c512 : index
            %115 = arith.addi %arg3, %c768 : index
            %116 = "disc_shape.linearize"(%39, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %117 = "disc_shape.linearize"(%39, %113, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %118 = "disc_shape.linearize"(%39, %114, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %119 = "disc_shape.linearize"(%39, %115, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %120:3 = "disc_shape.delinearize"(%116, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %121:3 = "disc_shape.delinearize"(%117, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %122:3 = "disc_shape.delinearize"(%118, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %123:3 = "disc_shape.delinearize"(%119, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %c0_19 = arith.constant 0 : index
            %124 = memref.dim %7, %c0_19 : memref<?x?x?xf32, "gpu">
            %c1_20 = arith.constant 1 : index
            %125 = memref.dim %7, %c1_20 : memref<?x?x?xf32, "gpu">
            %c2_21 = arith.constant 2 : index
            %126 = memref.dim %7, %c2_21 : memref<?x?x?xf32, "gpu">
            %127 = "disc_shape.linearize"(%120#0, %120#1, %120#2, %124, %125, %126) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
            %c1_22 = arith.constant 1 : index
            %c0_23 = arith.constant 0 : index
            %128 = memref.dim %7, %c0_23 : memref<?x?x?xf32, "gpu">
            %129 = arith.muli %c1_22, %128 : index
            %c1_24 = arith.constant 1 : index
            %130 = memref.dim %7, %c1_24 : memref<?x?x?xf32, "gpu">
            %131 = arith.muli %129, %130 : index
            %c2_25 = arith.constant 2 : index
            %132 = memref.dim %7, %c2_25 : memref<?x?x?xf32, "gpu">
            %133 = arith.muli %131, %132 : index
            %c1_26 = arith.constant 1 : index
            %c0_27 = arith.constant 0 : index
            %134 = memref.reinterpret_cast %7 to offset: [%c0_27], sizes: [%133], strides: [%c1_26] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %135 = memref.load %134[%127] : memref<?xf32, "gpu">
            %c0_28 = arith.constant 0 : index
            %136 = memref.dim %7, %c0_28 : memref<?x?x?xf32, "gpu">
            %c1_29 = arith.constant 1 : index
            %137 = memref.dim %7, %c1_29 : memref<?x?x?xf32, "gpu">
            %c2_30 = arith.constant 2 : index
            %138 = memref.dim %7, %c2_30 : memref<?x?x?xf32, "gpu">
            %139 = "disc_shape.linearize"(%121#0, %121#1, %121#2, %136, %137, %138) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
            %c1_31 = arith.constant 1 : index
            %c0_32 = arith.constant 0 : index
            %140 = memref.dim %7, %c0_32 : memref<?x?x?xf32, "gpu">
            %141 = arith.muli %c1_31, %140 : index
            %c1_33 = arith.constant 1 : index
            %142 = memref.dim %7, %c1_33 : memref<?x?x?xf32, "gpu">
            %143 = arith.muli %141, %142 : index
            %c2_34 = arith.constant 2 : index
            %144 = memref.dim %7, %c2_34 : memref<?x?x?xf32, "gpu">
            %145 = arith.muli %143, %144 : index
            %c1_35 = arith.constant 1 : index
            %c0_36 = arith.constant 0 : index
            %146 = memref.reinterpret_cast %7 to offset: [%c0_36], sizes: [%145], strides: [%c1_35] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %147 = memref.load %146[%139] : memref<?xf32, "gpu">
            %c0_37 = arith.constant 0 : index
            %148 = memref.dim %7, %c0_37 : memref<?x?x?xf32, "gpu">
            %c1_38 = arith.constant 1 : index
            %149 = memref.dim %7, %c1_38 : memref<?x?x?xf32, "gpu">
            %c2_39 = arith.constant 2 : index
            %150 = memref.dim %7, %c2_39 : memref<?x?x?xf32, "gpu">
            %151 = "disc_shape.linearize"(%122#0, %122#1, %122#2, %148, %149, %150) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
            %c1_40 = arith.constant 1 : index
            %c0_41 = arith.constant 0 : index
            %152 = memref.dim %7, %c0_41 : memref<?x?x?xf32, "gpu">
            %153 = arith.muli %c1_40, %152 : index
            %c1_42 = arith.constant 1 : index
            %154 = memref.dim %7, %c1_42 : memref<?x?x?xf32, "gpu">
            %155 = arith.muli %153, %154 : index
            %c2_43 = arith.constant 2 : index
            %156 = memref.dim %7, %c2_43 : memref<?x?x?xf32, "gpu">
            %157 = arith.muli %155, %156 : index
            %c1_44 = arith.constant 1 : index
            %c0_45 = arith.constant 0 : index
            %158 = memref.reinterpret_cast %7 to offset: [%c0_45], sizes: [%157], strides: [%c1_44] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %159 = memref.load %158[%151] : memref<?xf32, "gpu">
            %c0_46 = arith.constant 0 : index
            %160 = memref.dim %7, %c0_46 : memref<?x?x?xf32, "gpu">
            %c1_47 = arith.constant 1 : index
            %161 = memref.dim %7, %c1_47 : memref<?x?x?xf32, "gpu">
            %c2_48 = arith.constant 2 : index
            %162 = memref.dim %7, %c2_48 : memref<?x?x?xf32, "gpu">
            %163 = "disc_shape.linearize"(%123#0, %123#1, %123#2, %160, %161, %162) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
            %c1_49 = arith.constant 1 : index
            %c0_50 = arith.constant 0 : index
            %164 = memref.dim %7, %c0_50 : memref<?x?x?xf32, "gpu">
            %165 = arith.muli %c1_49, %164 : index
            %c1_51 = arith.constant 1 : index
            %166 = memref.dim %7, %c1_51 : memref<?x?x?xf32, "gpu">
            %167 = arith.muli %165, %166 : index
            %c2_52 = arith.constant 2 : index
            %168 = memref.dim %7, %c2_52 : memref<?x?x?xf32, "gpu">
            %169 = arith.muli %167, %168 : index
            %c1_53 = arith.constant 1 : index
            %c0_54 = arith.constant 0 : index
            %170 = memref.reinterpret_cast %7 to offset: [%c0_54], sizes: [%169], strides: [%c1_53] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %171 = memref.load %170[%163] : memref<?xf32, "gpu">
            %172 = arith.cmpf ugt, %arg4, %135 : f32
            %173 = arith.select %172, %arg4, %135 : f32
            %174 = arith.cmpf uno, %135, %135 : f32
            %175 = arith.select %174, %135, %173 : f32
            %176 = arith.cmpf ugt, %175, %147 : f32
            %177 = arith.select %176, %175, %147 : f32
            %178 = arith.cmpf uno, %147, %147 : f32
            %179 = arith.select %178, %147, %177 : f32
            %180 = arith.cmpf ugt, %179, %159 : f32
            %181 = arith.select %180, %179, %159 : f32
            %182 = arith.cmpf uno, %159, %159 : f32
            %183 = arith.select %182, %159, %181 : f32
            %184 = arith.cmpf ugt, %183, %171 : f32
            %185 = arith.select %184, %183, %171 : f32
            %186 = arith.cmpf uno, %171, %171 : f32
            %187 = arith.select %186, %171, %185 : f32
            scf.yield %187 : f32
          }
          %54 = scf.for %arg3 = %52 to %3 step %c256 iter_args(%arg4 = %53) -> (f32) {
            %113 = "disc_shape.linearize"(%39, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %114:3 = "disc_shape.delinearize"(%113, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %c0_19 = arith.constant 0 : index
            %115 = memref.dim %7, %c0_19 : memref<?x?x?xf32, "gpu">
            %c1_20 = arith.constant 1 : index
            %116 = memref.dim %7, %c1_20 : memref<?x?x?xf32, "gpu">
            %c2_21 = arith.constant 2 : index
            %117 = memref.dim %7, %c2_21 : memref<?x?x?xf32, "gpu">
            %118 = "disc_shape.linearize"(%114#0, %114#1, %114#2, %115, %116, %117) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
            %c1_22 = arith.constant 1 : index
            %c0_23 = arith.constant 0 : index
            %119 = memref.dim %7, %c0_23 : memref<?x?x?xf32, "gpu">
            %120 = arith.muli %c1_22, %119 : index
            %c1_24 = arith.constant 1 : index
            %121 = memref.dim %7, %c1_24 : memref<?x?x?xf32, "gpu">
            %122 = arith.muli %120, %121 : index
            %c2_25 = arith.constant 2 : index
            %123 = memref.dim %7, %c2_25 : memref<?x?x?xf32, "gpu">
            %124 = arith.muli %122, %123 : index
            %c1_26 = arith.constant 1 : index
            %c0_27 = arith.constant 0 : index
            %125 = memref.reinterpret_cast %7 to offset: [%c0_27], sizes: [%124], strides: [%c1_26] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %126 = memref.load %125[%118] : memref<?xf32, "gpu">
            %127 = arith.cmpf ugt, %arg4, %126 : f32
            %128 = arith.select %127, %arg4, %126 : f32
            %129 = arith.cmpf uno, %126, %126 : f32
            %130 = arith.select %129, %126, %128 : f32
            scf.yield %130 : f32
          }
          %result, %valid = gpu.shuffle  xor %54, %c1_i32, %c32_i32 : f32
          %55 = arith.cmpf ugt, %54, %result : f32
          %56 = arith.select %55, %54, %result : f32
          %57 = arith.cmpf uno, %result, %result : f32
          %58 = arith.select %57, %result, %56 : f32
          %result_1, %valid_2 = gpu.shuffle  xor %58, %c2_i32, %c32_i32 : f32
          %59 = arith.cmpf ugt, %58, %result_1 : f32
          %60 = arith.select %59, %58, %result_1 : f32
          %61 = arith.cmpf uno, %result_1, %result_1 : f32
          %62 = arith.select %61, %result_1, %60 : f32
          %result_3, %valid_4 = gpu.shuffle  xor %62, %c4_i32, %c32_i32 : f32
          %63 = arith.cmpf ugt, %62, %result_3 : f32
          %64 = arith.select %63, %62, %result_3 : f32
          %65 = arith.cmpf uno, %result_3, %result_3 : f32
          %66 = arith.select %65, %result_3, %64 : f32
          %result_5, %valid_6 = gpu.shuffle  xor %66, %c8_i32, %c32_i32 : f32
          %67 = arith.cmpf ugt, %66, %result_5 : f32
          %68 = arith.select %67, %66, %result_5 : f32
          %69 = arith.cmpf uno, %result_5, %result_5 : f32
          %70 = arith.select %69, %result_5, %68 : f32
          %result_7, %valid_8 = gpu.shuffle  xor %70, %c16_i32, %c32_i32 : f32
          %71 = arith.cmpf ugt, %70, %result_7 : f32
          %72 = arith.select %71, %70, %result_7 : f32
          %73 = arith.cmpf uno, %result_7, %result_7 : f32
          %74 = arith.select %73, %result_7, %72 : f32
          %75 = memref.alloc() : memref<8xf32, 3>
          %76 = arith.cmpi eq, %42, %c0 : index
          scf.if %76 {
            %c8_19 = arith.constant 8 : index
            %113 = "disc_shape.linearize"(%41, %c8_19) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %114 = memref.reinterpret_cast %75 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
            memref.store %74, %114[%113] : memref<8xf32, 3>
          }
          gpu.barrier
          %77 = arith.cmpi slt, %40, %c32 : index
          scf.if %77 {
            %113 = arith.cmpi slt, %42, %c8 : index
            %114 = scf.if %113 -> (f32) {
              %c8_25 = arith.constant 8 : index
              %127 = "disc_shape.linearize"(%42, %c8_25) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %128 = memref.reinterpret_cast %75 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
              %129 = memref.load %128[%127] : memref<8xf32, 3>
              scf.yield %129 : f32
            } else {
              scf.yield %cst_0 : f32
            }
            %result_19, %valid_20 = gpu.shuffle  xor %114, %c1_i32, %c8_i32 : f32
            %115 = arith.cmpf ugt, %114, %result_19 : f32
            %116 = arith.select %115, %114, %result_19 : f32
            %117 = arith.cmpf uno, %result_19, %result_19 : f32
            %118 = arith.select %117, %result_19, %116 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %118, %c2_i32, %c8_i32 : f32
            %119 = arith.cmpf ugt, %118, %result_21 : f32
            %120 = arith.select %119, %118, %result_21 : f32
            %121 = arith.cmpf uno, %result_21, %result_21 : f32
            %122 = arith.select %121, %result_21, %120 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %122, %c4_i32, %c8_i32 : f32
            %123 = arith.cmpf ugt, %122, %result_23 : f32
            %124 = arith.select %123, %122, %result_23 : f32
            %125 = arith.cmpf uno, %result_23, %result_23 : f32
            %126 = arith.select %125, %result_23, %124 : f32
            scf.if %76 {
              %c32_25 = arith.constant 32 : index
              %127 = "disc_shape.linearize"(%c0, %c32_25) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %128 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %126, %128[%127] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %78 = memref.alloc() : memref<32xf32, 3>
          %79 = gpu.block_id  x
          %80 = gpu.thread_id  x
          %81 = arith.divui %80, %c32 : index
          %82 = arith.remui %80, %c32 : index
          %83 = arith.subi %3, %80 : index
          %84 = arith.cmpi eq, %83, %c0 : index
          %85 = arith.subi %83, %c1 : index
          %86 = arith.divui %85, %c256 : index
          %87 = arith.addi %86, %c1 : index
          %88 = arith.select %84, %c0, %87 : index
          %89 = arith.remsi %88, %c4 : index
          %90 = arith.subi %88, %89 : index
          %91 = arith.muli %90, %c256 : index
          %92 = arith.addi %80, %91 : index
          %93 = scf.for %arg3 = %80 to %92 step %c1024 iter_args(%arg4 = %cst) -> (f32) {
            %113 = arith.addi %arg3, %c256 : index
            %114 = arith.addi %arg3, %c512 : index
            %115 = arith.addi %arg3, %c768 : index
            %116 = "disc_shape.linearize"(%79, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %117 = "disc_shape.linearize"(%79, %113, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %118 = "disc_shape.linearize"(%79, %114, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %119 = "disc_shape.linearize"(%79, %115, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %120:3 = "disc_shape.delinearize"(%116, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %121:3 = "disc_shape.delinearize"(%117, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %122:3 = "disc_shape.delinearize"(%118, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %123:3 = "disc_shape.delinearize"(%119, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %c0_19 = arith.constant 0 : index
            %124 = memref.dim %7, %c0_19 : memref<?x?x?xf32, "gpu">
            %c1_20 = arith.constant 1 : index
            %125 = memref.dim %7, %c1_20 : memref<?x?x?xf32, "gpu">
            %c2_21 = arith.constant 2 : index
            %126 = memref.dim %7, %c2_21 : memref<?x?x?xf32, "gpu">
            %127 = "disc_shape.linearize"(%120#0, %120#1, %120#2, %124, %125, %126) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
            %c1_22 = arith.constant 1 : index
            %c0_23 = arith.constant 0 : index
            %128 = memref.dim %7, %c0_23 : memref<?x?x?xf32, "gpu">
            %129 = arith.muli %c1_22, %128 : index
            %c1_24 = arith.constant 1 : index
            %130 = memref.dim %7, %c1_24 : memref<?x?x?xf32, "gpu">
            %131 = arith.muli %129, %130 : index
            %c2_25 = arith.constant 2 : index
            %132 = memref.dim %7, %c2_25 : memref<?x?x?xf32, "gpu">
            %133 = arith.muli %131, %132 : index
            %c1_26 = arith.constant 1 : index
            %c0_27 = arith.constant 0 : index
            %134 = memref.reinterpret_cast %7 to offset: [%c0_27], sizes: [%133], strides: [%c1_26] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %135 = memref.load %134[%127] : memref<?xf32, "gpu">
            %c0_28 = arith.constant 0 : index
            %136 = memref.dim %7, %c0_28 : memref<?x?x?xf32, "gpu">
            %c1_29 = arith.constant 1 : index
            %137 = memref.dim %7, %c1_29 : memref<?x?x?xf32, "gpu">
            %c2_30 = arith.constant 2 : index
            %138 = memref.dim %7, %c2_30 : memref<?x?x?xf32, "gpu">
            %139 = "disc_shape.linearize"(%121#0, %121#1, %121#2, %136, %137, %138) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
            %c1_31 = arith.constant 1 : index
            %c0_32 = arith.constant 0 : index
            %140 = memref.dim %7, %c0_32 : memref<?x?x?xf32, "gpu">
            %141 = arith.muli %c1_31, %140 : index
            %c1_33 = arith.constant 1 : index
            %142 = memref.dim %7, %c1_33 : memref<?x?x?xf32, "gpu">
            %143 = arith.muli %141, %142 : index
            %c2_34 = arith.constant 2 : index
            %144 = memref.dim %7, %c2_34 : memref<?x?x?xf32, "gpu">
            %145 = arith.muli %143, %144 : index
            %c1_35 = arith.constant 1 : index
            %c0_36 = arith.constant 0 : index
            %146 = memref.reinterpret_cast %7 to offset: [%c0_36], sizes: [%145], strides: [%c1_35] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %147 = memref.load %146[%139] : memref<?xf32, "gpu">
            %c0_37 = arith.constant 0 : index
            %148 = memref.dim %7, %c0_37 : memref<?x?x?xf32, "gpu">
            %c1_38 = arith.constant 1 : index
            %149 = memref.dim %7, %c1_38 : memref<?x?x?xf32, "gpu">
            %c2_39 = arith.constant 2 : index
            %150 = memref.dim %7, %c2_39 : memref<?x?x?xf32, "gpu">
            %151 = "disc_shape.linearize"(%122#0, %122#1, %122#2, %148, %149, %150) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
            %c1_40 = arith.constant 1 : index
            %c0_41 = arith.constant 0 : index
            %152 = memref.dim %7, %c0_41 : memref<?x?x?xf32, "gpu">
            %153 = arith.muli %c1_40, %152 : index
            %c1_42 = arith.constant 1 : index
            %154 = memref.dim %7, %c1_42 : memref<?x?x?xf32, "gpu">
            %155 = arith.muli %153, %154 : index
            %c2_43 = arith.constant 2 : index
            %156 = memref.dim %7, %c2_43 : memref<?x?x?xf32, "gpu">
            %157 = arith.muli %155, %156 : index
            %c1_44 = arith.constant 1 : index
            %c0_45 = arith.constant 0 : index
            %158 = memref.reinterpret_cast %7 to offset: [%c0_45], sizes: [%157], strides: [%c1_44] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %159 = memref.load %158[%151] : memref<?xf32, "gpu">
            %c0_46 = arith.constant 0 : index
            %160 = memref.dim %7, %c0_46 : memref<?x?x?xf32, "gpu">
            %c1_47 = arith.constant 1 : index
            %161 = memref.dim %7, %c1_47 : memref<?x?x?xf32, "gpu">
            %c2_48 = arith.constant 2 : index
            %162 = memref.dim %7, %c2_48 : memref<?x?x?xf32, "gpu">
            %163 = "disc_shape.linearize"(%123#0, %123#1, %123#2, %160, %161, %162) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
            %c1_49 = arith.constant 1 : index
            %c0_50 = arith.constant 0 : index
            %164 = memref.dim %7, %c0_50 : memref<?x?x?xf32, "gpu">
            %165 = arith.muli %c1_49, %164 : index
            %c1_51 = arith.constant 1 : index
            %166 = memref.dim %7, %c1_51 : memref<?x?x?xf32, "gpu">
            %167 = arith.muli %165, %166 : index
            %c2_52 = arith.constant 2 : index
            %168 = memref.dim %7, %c2_52 : memref<?x?x?xf32, "gpu">
            %169 = arith.muli %167, %168 : index
            %c1_53 = arith.constant 1 : index
            %c0_54 = arith.constant 0 : index
            %170 = memref.reinterpret_cast %7 to offset: [%c0_54], sizes: [%169], strides: [%c1_53] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %171 = memref.load %170[%163] : memref<?xf32, "gpu">
            %c32_55 = arith.constant 32 : index
            %172 = "disc_shape.linearize"(%c0, %c32_55) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %173 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %174 = memref.load %173[%172] : memref<32xf32, 3>
            %c32_56 = arith.constant 32 : index
            %175 = "disc_shape.linearize"(%c0, %c32_56) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %176 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %177 = memref.load %176[%175] : memref<32xf32, 3>
            %c32_57 = arith.constant 32 : index
            %178 = "disc_shape.linearize"(%c0, %c32_57) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %179 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %180 = memref.load %179[%178] : memref<32xf32, 3>
            %c32_58 = arith.constant 32 : index
            %181 = "disc_shape.linearize"(%c0, %c32_58) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %182 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %183 = memref.load %182[%181] : memref<32xf32, 3>
            %184 = arith.subf %135, %174 : f32
            %185 = arith.subf %147, %177 : f32
            %186 = arith.subf %159, %180 : f32
            %187 = arith.subf %171, %183 : f32
            %188 = math.exp %184 : f32
            %189 = math.exp %185 : f32
            %190 = math.exp %186 : f32
            %191 = math.exp %187 : f32
            %192 = arith.addf %arg4, %188 : f32
            %193 = arith.addf %192, %189 : f32
            %194 = arith.addf %193, %190 : f32
            %195 = arith.addf %194, %191 : f32
            scf.yield %195 : f32
          }
          %94 = scf.for %arg3 = %92 to %3 step %c256 iter_args(%arg4 = %93) -> (f32) {
            %113 = "disc_shape.linearize"(%79, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %114:3 = "disc_shape.delinearize"(%113, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %c0_19 = arith.constant 0 : index
            %115 = memref.dim %7, %c0_19 : memref<?x?x?xf32, "gpu">
            %c1_20 = arith.constant 1 : index
            %116 = memref.dim %7, %c1_20 : memref<?x?x?xf32, "gpu">
            %c2_21 = arith.constant 2 : index
            %117 = memref.dim %7, %c2_21 : memref<?x?x?xf32, "gpu">
            %118 = "disc_shape.linearize"(%114#0, %114#1, %114#2, %115, %116, %117) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
            %c1_22 = arith.constant 1 : index
            %c0_23 = arith.constant 0 : index
            %119 = memref.dim %7, %c0_23 : memref<?x?x?xf32, "gpu">
            %120 = arith.muli %c1_22, %119 : index
            %c1_24 = arith.constant 1 : index
            %121 = memref.dim %7, %c1_24 : memref<?x?x?xf32, "gpu">
            %122 = arith.muli %120, %121 : index
            %c2_25 = arith.constant 2 : index
            %123 = memref.dim %7, %c2_25 : memref<?x?x?xf32, "gpu">
            %124 = arith.muli %122, %123 : index
            %c1_26 = arith.constant 1 : index
            %c0_27 = arith.constant 0 : index
            %125 = memref.reinterpret_cast %7 to offset: [%c0_27], sizes: [%124], strides: [%c1_26] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %126 = memref.load %125[%118] : memref<?xf32, "gpu">
            %c32_28 = arith.constant 32 : index
            %127 = "disc_shape.linearize"(%c0, %c32_28) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %128 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %129 = memref.load %128[%127] : memref<32xf32, 3>
            %130 = arith.subf %126, %129 : f32
            %131 = math.exp %130 : f32
            %132 = arith.addf %arg4, %131 : f32
            scf.yield %132 : f32
          }
          %result_9, %valid_10 = gpu.shuffle  xor %94, %c1_i32, %c32_i32 : f32
          %95 = arith.addf %94, %result_9 : f32
          %result_11, %valid_12 = gpu.shuffle  xor %95, %c2_i32, %c32_i32 : f32
          %96 = arith.addf %95, %result_11 : f32
          %result_13, %valid_14 = gpu.shuffle  xor %96, %c4_i32, %c32_i32 : f32
          %97 = arith.addf %96, %result_13 : f32
          %result_15, %valid_16 = gpu.shuffle  xor %97, %c8_i32, %c32_i32 : f32
          %98 = arith.addf %97, %result_15 : f32
          %result_17, %valid_18 = gpu.shuffle  xor %98, %c16_i32, %c32_i32 : f32
          %99 = arith.addf %98, %result_17 : f32
          %100 = memref.alloc() : memref<8xf32, 3>
          %101 = arith.cmpi eq, %82, %c0 : index
          scf.if %101 {
            %c8_19 = arith.constant 8 : index
            %113 = "disc_shape.linearize"(%81, %c8_19) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %114 = memref.reinterpret_cast %100 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
            memref.store %99, %114[%113] : memref<8xf32, 3>
          }
          gpu.barrier
          %102 = arith.cmpi slt, %80, %c32 : index
          scf.if %102 {
            %113 = arith.cmpi slt, %82, %c8 : index
            %114 = scf.if %113 -> (f32) {
              %c8_25 = arith.constant 8 : index
              %118 = "disc_shape.linearize"(%82, %c8_25) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %119 = memref.reinterpret_cast %100 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
              %120 = memref.load %119[%118] : memref<8xf32, 3>
              scf.yield %120 : f32
            } else {
              scf.yield %cst : f32
            }
            %result_19, %valid_20 = gpu.shuffle  xor %114, %c1_i32, %c8_i32 : f32
            %115 = arith.addf %114, %result_19 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %115, %c2_i32, %c8_i32 : f32
            %116 = arith.addf %115, %result_21 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %116, %c4_i32, %c8_i32 : f32
            %117 = arith.addf %116, %result_23 : f32
            scf.if %101 {
              %c32_25 = arith.constant 32 : index
              %118 = "disc_shape.linearize"(%c0, %c32_25) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %119 = memref.reinterpret_cast %78 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %117, %119[%118] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %103 = arith.subi %3, %37 : index
          %104 = arith.cmpi eq, %103, %c0 : index
          %105 = arith.subi %103, %c1 : index
          %106 = arith.divui %105, %c256 : index
          %107 = arith.addi %106, %c1 : index
          %108 = arith.select %104, %c0, %107 : index
          %109 = arith.remsi %108, %c4 : index
          %110 = arith.subi %108, %109 : index
          %111 = arith.muli %110, %c256 : index
          %112 = arith.addi %37, %111 : index
          scf.for %arg3 = %37 to %112 step %c1024 {
            %113 = arith.addi %arg3, %c256 : index
            %114 = arith.addi %arg3, %c512 : index
            %115 = arith.addi %arg3, %c768 : index
            %116 = arith.muli %36, %3 : index
            %117 = arith.muli %36, %3 : index
            %118 = arith.muli %36, %3 : index
            %119 = arith.muli %36, %3 : index
            %120 = arith.addi %116, %arg3 : index
            %121 = arith.addi %117, %113 : index
            %122 = arith.addi %118, %114 : index
            %123 = arith.addi %119, %115 : index
            %124 = arith.muli %2, %1 : index
            %125 = arith.muli %2, %1 : index
            %126 = arith.muli %2, %1 : index
            %127 = arith.muli %2, %1 : index
            %128 = arith.muli %124, %3 : index
            %129 = arith.muli %125, %3 : index
            %130 = arith.muli %126, %3 : index
            %131 = arith.muli %127, %3 : index
            %132 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%128], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %133 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%129], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %134 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%130], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %135 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%131], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %136:3 = "disc_shape.delinearize"(%120, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %137:3 = "disc_shape.delinearize"(%121, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %138:3 = "disc_shape.delinearize"(%122, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %139:3 = "disc_shape.delinearize"(%123, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %c0_19 = arith.constant 0 : index
            %140 = memref.dim %7, %c0_19 : memref<?x?x?xf32, "gpu">
            %c1_20 = arith.constant 1 : index
            %141 = memref.dim %7, %c1_20 : memref<?x?x?xf32, "gpu">
            %c2_21 = arith.constant 2 : index
            %142 = memref.dim %7, %c2_21 : memref<?x?x?xf32, "gpu">
            %143 = "disc_shape.linearize"(%136#0, %136#1, %136#2, %140, %141, %142) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
            %c1_22 = arith.constant 1 : index
            %c0_23 = arith.constant 0 : index
            %144 = memref.dim %7, %c0_23 : memref<?x?x?xf32, "gpu">
            %145 = arith.muli %c1_22, %144 : index
            %c1_24 = arith.constant 1 : index
            %146 = memref.dim %7, %c1_24 : memref<?x?x?xf32, "gpu">
            %147 = arith.muli %145, %146 : index
            %c2_25 = arith.constant 2 : index
            %148 = memref.dim %7, %c2_25 : memref<?x?x?xf32, "gpu">
            %149 = arith.muli %147, %148 : index
            %c1_26 = arith.constant 1 : index
            %c0_27 = arith.constant 0 : index
            %150 = memref.reinterpret_cast %7 to offset: [%c0_27], sizes: [%149], strides: [%c1_26] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %151 = memref.load %150[%143] : memref<?xf32, "gpu">
            %c0_28 = arith.constant 0 : index
            %152 = memref.dim %7, %c0_28 : memref<?x?x?xf32, "gpu">
            %c1_29 = arith.constant 1 : index
            %153 = memref.dim %7, %c1_29 : memref<?x?x?xf32, "gpu">
            %c2_30 = arith.constant 2 : index
            %154 = memref.dim %7, %c2_30 : memref<?x?x?xf32, "gpu">
            %155 = "disc_shape.linearize"(%137#0, %137#1, %137#2, %152, %153, %154) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
            %c1_31 = arith.constant 1 : index
            %c0_32 = arith.constant 0 : index
            %156 = memref.dim %7, %c0_32 : memref<?x?x?xf32, "gpu">
            %157 = arith.muli %c1_31, %156 : index
            %c1_33 = arith.constant 1 : index
            %158 = memref.dim %7, %c1_33 : memref<?x?x?xf32, "gpu">
            %159 = arith.muli %157, %158 : index
            %c2_34 = arith.constant 2 : index
            %160 = memref.dim %7, %c2_34 : memref<?x?x?xf32, "gpu">
            %161 = arith.muli %159, %160 : index
            %c1_35 = arith.constant 1 : index
            %c0_36 = arith.constant 0 : index
            %162 = memref.reinterpret_cast %7 to offset: [%c0_36], sizes: [%161], strides: [%c1_35] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %163 = memref.load %162[%155] : memref<?xf32, "gpu">
            %c0_37 = arith.constant 0 : index
            %164 = memref.dim %7, %c0_37 : memref<?x?x?xf32, "gpu">
            %c1_38 = arith.constant 1 : index
            %165 = memref.dim %7, %c1_38 : memref<?x?x?xf32, "gpu">
            %c2_39 = arith.constant 2 : index
            %166 = memref.dim %7, %c2_39 : memref<?x?x?xf32, "gpu">
            %167 = "disc_shape.linearize"(%138#0, %138#1, %138#2, %164, %165, %166) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
            %c1_40 = arith.constant 1 : index
            %c0_41 = arith.constant 0 : index
            %168 = memref.dim %7, %c0_41 : memref<?x?x?xf32, "gpu">
            %169 = arith.muli %c1_40, %168 : index
            %c1_42 = arith.constant 1 : index
            %170 = memref.dim %7, %c1_42 : memref<?x?x?xf32, "gpu">
            %171 = arith.muli %169, %170 : index
            %c2_43 = arith.constant 2 : index
            %172 = memref.dim %7, %c2_43 : memref<?x?x?xf32, "gpu">
            %173 = arith.muli %171, %172 : index
            %c1_44 = arith.constant 1 : index
            %c0_45 = arith.constant 0 : index
            %174 = memref.reinterpret_cast %7 to offset: [%c0_45], sizes: [%173], strides: [%c1_44] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %175 = memref.load %174[%167] : memref<?xf32, "gpu">
            %c0_46 = arith.constant 0 : index
            %176 = memref.dim %7, %c0_46 : memref<?x?x?xf32, "gpu">
            %c1_47 = arith.constant 1 : index
            %177 = memref.dim %7, %c1_47 : memref<?x?x?xf32, "gpu">
            %c2_48 = arith.constant 2 : index
            %178 = memref.dim %7, %c2_48 : memref<?x?x?xf32, "gpu">
            %179 = "disc_shape.linearize"(%139#0, %139#1, %139#2, %176, %177, %178) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
            %c1_49 = arith.constant 1 : index
            %c0_50 = arith.constant 0 : index
            %180 = memref.dim %7, %c0_50 : memref<?x?x?xf32, "gpu">
            %181 = arith.muli %c1_49, %180 : index
            %c1_51 = arith.constant 1 : index
            %182 = memref.dim %7, %c1_51 : memref<?x?x?xf32, "gpu">
            %183 = arith.muli %181, %182 : index
            %c2_52 = arith.constant 2 : index
            %184 = memref.dim %7, %c2_52 : memref<?x?x?xf32, "gpu">
            %185 = arith.muli %183, %184 : index
            %c1_53 = arith.constant 1 : index
            %c0_54 = arith.constant 0 : index
            %186 = memref.reinterpret_cast %7 to offset: [%c0_54], sizes: [%185], strides: [%c1_53] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %187 = memref.load %186[%179] : memref<?xf32, "gpu">
            %c32_55 = arith.constant 32 : index
            %188 = "disc_shape.linearize"(%c0, %c32_55) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %189 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %190 = memref.load %189[%188] : memref<32xf32, 3>
            %c32_56 = arith.constant 32 : index
            %191 = "disc_shape.linearize"(%c0, %c32_56) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %192 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %193 = memref.load %192[%191] : memref<32xf32, 3>
            %c32_57 = arith.constant 32 : index
            %194 = "disc_shape.linearize"(%c0, %c32_57) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %195 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %196 = memref.load %195[%194] : memref<32xf32, 3>
            %c32_58 = arith.constant 32 : index
            %197 = "disc_shape.linearize"(%c0, %c32_58) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %198 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %199 = memref.load %198[%197] : memref<32xf32, 3>
            %200 = arith.subf %151, %190 : f32
            %201 = arith.subf %163, %193 : f32
            %202 = arith.subf %175, %196 : f32
            %203 = arith.subf %187, %199 : f32
            %204 = math.exp %200 : f32
            %205 = math.exp %201 : f32
            %206 = math.exp %202 : f32
            %207 = math.exp %203 : f32
            %c32_59 = arith.constant 32 : index
            %208 = "disc_shape.linearize"(%c0, %c32_59) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %209 = memref.reinterpret_cast %78 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %210 = memref.load %209[%208] : memref<32xf32, 3>
            %c32_60 = arith.constant 32 : index
            %211 = "disc_shape.linearize"(%c0, %c32_60) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %212 = memref.reinterpret_cast %78 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %213 = memref.load %212[%211] : memref<32xf32, 3>
            %c32_61 = arith.constant 32 : index
            %214 = "disc_shape.linearize"(%c0, %c32_61) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %215 = memref.reinterpret_cast %78 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %216 = memref.load %215[%214] : memref<32xf32, 3>
            %c32_62 = arith.constant 32 : index
            %217 = "disc_shape.linearize"(%c0, %c32_62) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %218 = memref.reinterpret_cast %78 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %219 = memref.load %218[%217] : memref<32xf32, 3>
            %220 = arith.divf %204, %210 : f32
            %221 = arith.divf %205, %213 : f32
            %222 = arith.divf %206, %216 : f32
            %223 = arith.divf %207, %219 : f32
            %c0_63 = arith.constant 0 : index
            %224 = memref.dim %132, %c0_63 : memref<?xf32, "gpu">
            %225 = "disc_shape.linearize"(%120, %224) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %c1_64 = arith.constant 1 : index
            %c0_65 = arith.constant 0 : index
            %226 = memref.dim %132, %c0_65 : memref<?xf32, "gpu">
            %227 = arith.muli %c1_64, %226 : index
            %c1_66 = arith.constant 1 : index
            %c0_67 = arith.constant 0 : index
            %228 = memref.reinterpret_cast %132 to offset: [%c0_67], sizes: [%227], strides: [%c1_66] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %220, %228[%225] : memref<?xf32, "gpu">
            %c0_68 = arith.constant 0 : index
            %229 = memref.dim %133, %c0_68 : memref<?xf32, "gpu">
            %230 = "disc_shape.linearize"(%121, %229) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %c1_69 = arith.constant 1 : index
            %c0_70 = arith.constant 0 : index
            %231 = memref.dim %133, %c0_70 : memref<?xf32, "gpu">
            %232 = arith.muli %c1_69, %231 : index
            %c1_71 = arith.constant 1 : index
            %c0_72 = arith.constant 0 : index
            %233 = memref.reinterpret_cast %133 to offset: [%c0_72], sizes: [%232], strides: [%c1_71] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %221, %233[%230] : memref<?xf32, "gpu">
            %c0_73 = arith.constant 0 : index
            %234 = memref.dim %134, %c0_73 : memref<?xf32, "gpu">
            %235 = "disc_shape.linearize"(%122, %234) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %c1_74 = arith.constant 1 : index
            %c0_75 = arith.constant 0 : index
            %236 = memref.dim %134, %c0_75 : memref<?xf32, "gpu">
            %237 = arith.muli %c1_74, %236 : index
            %c1_76 = arith.constant 1 : index
            %c0_77 = arith.constant 0 : index
            %238 = memref.reinterpret_cast %134 to offset: [%c0_77], sizes: [%237], strides: [%c1_76] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %222, %238[%235] : memref<?xf32, "gpu">
            %c0_78 = arith.constant 0 : index
            %239 = memref.dim %135, %c0_78 : memref<?xf32, "gpu">
            %240 = "disc_shape.linearize"(%123, %239) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %c1_79 = arith.constant 1 : index
            %c0_80 = arith.constant 0 : index
            %241 = memref.dim %135, %c0_80 : memref<?xf32, "gpu">
            %242 = arith.muli %c1_79, %241 : index
            %c1_81 = arith.constant 1 : index
            %c0_82 = arith.constant 0 : index
            %243 = memref.reinterpret_cast %135 to offset: [%c0_82], sizes: [%242], strides: [%c1_81] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %223, %243[%240] : memref<?xf32, "gpu">
          }
          scf.for %arg3 = %112 to %3 step %c256 {
            %113 = arith.muli %36, %3 : index
            %114 = arith.addi %113, %arg3 : index
            %115 = arith.muli %2, %1 : index
            %116 = arith.muli %115, %3 : index
            %117 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%116], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %118:3 = "disc_shape.delinearize"(%114, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
            %c0_19 = arith.constant 0 : index
            %119 = memref.dim %7, %c0_19 : memref<?x?x?xf32, "gpu">
            %c1_20 = arith.constant 1 : index
            %120 = memref.dim %7, %c1_20 : memref<?x?x?xf32, "gpu">
            %c2_21 = arith.constant 2 : index
            %121 = memref.dim %7, %c2_21 : memref<?x?x?xf32, "gpu">
            %122 = "disc_shape.linearize"(%118#0, %118#1, %118#2, %119, %120, %121) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
            %c1_22 = arith.constant 1 : index
            %c0_23 = arith.constant 0 : index
            %123 = memref.dim %7, %c0_23 : memref<?x?x?xf32, "gpu">
            %124 = arith.muli %c1_22, %123 : index
            %c1_24 = arith.constant 1 : index
            %125 = memref.dim %7, %c1_24 : memref<?x?x?xf32, "gpu">
            %126 = arith.muli %124, %125 : index
            %c2_25 = arith.constant 2 : index
            %127 = memref.dim %7, %c2_25 : memref<?x?x?xf32, "gpu">
            %128 = arith.muli %126, %127 : index
            %c1_26 = arith.constant 1 : index
            %c0_27 = arith.constant 0 : index
            %129 = memref.reinterpret_cast %7 to offset: [%c0_27], sizes: [%128], strides: [%c1_26] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %130 = memref.load %129[%122] : memref<?xf32, "gpu">
            %c32_28 = arith.constant 32 : index
            %131 = "disc_shape.linearize"(%c0, %c32_28) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %132 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %133 = memref.load %132[%131] : memref<32xf32, 3>
            %134 = arith.subf %130, %133 : f32
            %135 = math.exp %134 : f32
            %c32_29 = arith.constant 32 : index
            %136 = "disc_shape.linearize"(%c0, %c32_29) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %137 = memref.reinterpret_cast %78 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %138 = memref.load %137[%136] : memref<32xf32, 3>
            %139 = arith.divf %135, %138 : f32
            %c0_30 = arith.constant 0 : index
            %140 = memref.dim %117, %c0_30 : memref<?xf32, "gpu">
            %141 = "disc_shape.linearize"(%114, %140) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %c1_31 = arith.constant 1 : index
            %c0_32 = arith.constant 0 : index
            %142 = memref.dim %117, %c0_32 : memref<?xf32, "gpu">
            %143 = arith.muli %c1_31, %142 : index
            %c1_33 = arith.constant 1 : index
            %c0_34 = arith.constant 0 : index
            %144 = memref.reinterpret_cast %117 to offset: [%c0_34], sizes: [%143], strides: [%c1_33] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %139, %144[%141] : memref<?xf32, "gpu">
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      %36 = arith.cmpi eq, %13, %c0 : index
      %37 = arith.subi %13, %c1 : index
      %38 = arith.divui %37, %c8 : index
      %39 = arith.addi %38, %c1 : index
      %40 = arith.select %36, %c0, %39 : index
      scf.parallel (%arg1) = (%c0) to (%40) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %41 = gpu.block_id  x
          %42 = gpu.thread_id  x
          %43 = arith.divui %42, %c32 : index
          %44 = arith.remui %42, %c32 : index
          %45 = arith.muli %41, %c8 : index
          %46 = arith.addi %45, %43 : index
          %47 = arith.cmpi slt, %46, %13 : index
          %48 = memref.alloc() : memref<32xf32, 3>
          %49 = gpu.block_id  x
          %50 = gpu.thread_id  x
          %51 = arith.divui %50, %c32 : index
          %52 = arith.remui %50, %c32 : index
          %53 = arith.muli %49, %c8 : index
          %54 = arith.addi %53, %51 : index
          %55 = arith.cmpi slt, %54, %13 : index
          scf.if %55 {
            %64 = arith.subi %3, %52 : index
            %65 = arith.cmpi eq, %64, %c0 : index
            %66 = arith.subi %64, %c1 : index
            %67 = arith.divui %66, %c32 : index
            %68 = arith.addi %67, %c1 : index
            %69 = arith.select %65, %c0, %68 : index
            %70 = arith.remsi %69, %c4 : index
            %71 = arith.subi %69, %70 : index
            %72 = arith.muli %71, %c32 : index
            %73 = arith.addi %52, %72 : index
            %74 = scf.for %arg3 = %52 to %73 step %c128 iter_args(%arg4 = %cst_0) -> (f32) {
              %97 = arith.addi %arg3, %c32 : index
              %98 = arith.addi %arg3, %c64 : index
              %99 = arith.addi %arg3, %c96 : index
              %100 = "disc_shape.linearize"(%54, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %101 = "disc_shape.linearize"(%54, %97, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %102 = "disc_shape.linearize"(%54, %98, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %103 = "disc_shape.linearize"(%54, %99, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %104:3 = "disc_shape.delinearize"(%100, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %105:3 = "disc_shape.delinearize"(%101, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %106:3 = "disc_shape.delinearize"(%102, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %107:3 = "disc_shape.delinearize"(%103, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %c0_9 = arith.constant 0 : index
              %108 = memref.dim %7, %c0_9 : memref<?x?x?xf32, "gpu">
              %c1_10 = arith.constant 1 : index
              %109 = memref.dim %7, %c1_10 : memref<?x?x?xf32, "gpu">
              %c2_11 = arith.constant 2 : index
              %110 = memref.dim %7, %c2_11 : memref<?x?x?xf32, "gpu">
              %111 = "disc_shape.linearize"(%104#0, %104#1, %104#2, %108, %109, %110) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
              %c1_12 = arith.constant 1 : index
              %c0_13 = arith.constant 0 : index
              %112 = memref.dim %7, %c0_13 : memref<?x?x?xf32, "gpu">
              %113 = arith.muli %c1_12, %112 : index
              %c1_14 = arith.constant 1 : index
              %114 = memref.dim %7, %c1_14 : memref<?x?x?xf32, "gpu">
              %115 = arith.muli %113, %114 : index
              %c2_15 = arith.constant 2 : index
              %116 = memref.dim %7, %c2_15 : memref<?x?x?xf32, "gpu">
              %117 = arith.muli %115, %116 : index
              %c1_16 = arith.constant 1 : index
              %c0_17 = arith.constant 0 : index
              %118 = memref.reinterpret_cast %7 to offset: [%c0_17], sizes: [%117], strides: [%c1_16] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %119 = memref.load %118[%111] : memref<?xf32, "gpu">
              %c0_18 = arith.constant 0 : index
              %120 = memref.dim %7, %c0_18 : memref<?x?x?xf32, "gpu">
              %c1_19 = arith.constant 1 : index
              %121 = memref.dim %7, %c1_19 : memref<?x?x?xf32, "gpu">
              %c2_20 = arith.constant 2 : index
              %122 = memref.dim %7, %c2_20 : memref<?x?x?xf32, "gpu">
              %123 = "disc_shape.linearize"(%105#0, %105#1, %105#2, %120, %121, %122) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
              %c1_21 = arith.constant 1 : index
              %c0_22 = arith.constant 0 : index
              %124 = memref.dim %7, %c0_22 : memref<?x?x?xf32, "gpu">
              %125 = arith.muli %c1_21, %124 : index
              %c1_23 = arith.constant 1 : index
              %126 = memref.dim %7, %c1_23 : memref<?x?x?xf32, "gpu">
              %127 = arith.muli %125, %126 : index
              %c2_24 = arith.constant 2 : index
              %128 = memref.dim %7, %c2_24 : memref<?x?x?xf32, "gpu">
              %129 = arith.muli %127, %128 : index
              %c1_25 = arith.constant 1 : index
              %c0_26 = arith.constant 0 : index
              %130 = memref.reinterpret_cast %7 to offset: [%c0_26], sizes: [%129], strides: [%c1_25] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %131 = memref.load %130[%123] : memref<?xf32, "gpu">
              %c0_27 = arith.constant 0 : index
              %132 = memref.dim %7, %c0_27 : memref<?x?x?xf32, "gpu">
              %c1_28 = arith.constant 1 : index
              %133 = memref.dim %7, %c1_28 : memref<?x?x?xf32, "gpu">
              %c2_29 = arith.constant 2 : index
              %134 = memref.dim %7, %c2_29 : memref<?x?x?xf32, "gpu">
              %135 = "disc_shape.linearize"(%106#0, %106#1, %106#2, %132, %133, %134) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
              %c1_30 = arith.constant 1 : index
              %c0_31 = arith.constant 0 : index
              %136 = memref.dim %7, %c0_31 : memref<?x?x?xf32, "gpu">
              %137 = arith.muli %c1_30, %136 : index
              %c1_32 = arith.constant 1 : index
              %138 = memref.dim %7, %c1_32 : memref<?x?x?xf32, "gpu">
              %139 = arith.muli %137, %138 : index
              %c2_33 = arith.constant 2 : index
              %140 = memref.dim %7, %c2_33 : memref<?x?x?xf32, "gpu">
              %141 = arith.muli %139, %140 : index
              %c1_34 = arith.constant 1 : index
              %c0_35 = arith.constant 0 : index
              %142 = memref.reinterpret_cast %7 to offset: [%c0_35], sizes: [%141], strides: [%c1_34] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %143 = memref.load %142[%135] : memref<?xf32, "gpu">
              %c0_36 = arith.constant 0 : index
              %144 = memref.dim %7, %c0_36 : memref<?x?x?xf32, "gpu">
              %c1_37 = arith.constant 1 : index
              %145 = memref.dim %7, %c1_37 : memref<?x?x?xf32, "gpu">
              %c2_38 = arith.constant 2 : index
              %146 = memref.dim %7, %c2_38 : memref<?x?x?xf32, "gpu">
              %147 = "disc_shape.linearize"(%107#0, %107#1, %107#2, %144, %145, %146) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
              %c1_39 = arith.constant 1 : index
              %c0_40 = arith.constant 0 : index
              %148 = memref.dim %7, %c0_40 : memref<?x?x?xf32, "gpu">
              %149 = arith.muli %c1_39, %148 : index
              %c1_41 = arith.constant 1 : index
              %150 = memref.dim %7, %c1_41 : memref<?x?x?xf32, "gpu">
              %151 = arith.muli %149, %150 : index
              %c2_42 = arith.constant 2 : index
              %152 = memref.dim %7, %c2_42 : memref<?x?x?xf32, "gpu">
              %153 = arith.muli %151, %152 : index
              %c1_43 = arith.constant 1 : index
              %c0_44 = arith.constant 0 : index
              %154 = memref.reinterpret_cast %7 to offset: [%c0_44], sizes: [%153], strides: [%c1_43] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %155 = memref.load %154[%147] : memref<?xf32, "gpu">
              %156 = arith.cmpf ugt, %arg4, %119 : f32
              %157 = arith.select %156, %arg4, %119 : f32
              %158 = arith.cmpf uno, %119, %119 : f32
              %159 = arith.select %158, %119, %157 : f32
              %160 = arith.cmpf ugt, %159, %131 : f32
              %161 = arith.select %160, %159, %131 : f32
              %162 = arith.cmpf uno, %131, %131 : f32
              %163 = arith.select %162, %131, %161 : f32
              %164 = arith.cmpf ugt, %163, %143 : f32
              %165 = arith.select %164, %163, %143 : f32
              %166 = arith.cmpf uno, %143, %143 : f32
              %167 = arith.select %166, %143, %165 : f32
              %168 = arith.cmpf ugt, %167, %155 : f32
              %169 = arith.select %168, %167, %155 : f32
              %170 = arith.cmpf uno, %155, %155 : f32
              %171 = arith.select %170, %155, %169 : f32
              scf.yield %171 : f32
            }
            %75 = scf.for %arg3 = %73 to %3 step %c32 iter_args(%arg4 = %74) -> (f32) {
              %97 = "disc_shape.linearize"(%54, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %98:3 = "disc_shape.delinearize"(%97, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %c0_9 = arith.constant 0 : index
              %99 = memref.dim %7, %c0_9 : memref<?x?x?xf32, "gpu">
              %c1_10 = arith.constant 1 : index
              %100 = memref.dim %7, %c1_10 : memref<?x?x?xf32, "gpu">
              %c2_11 = arith.constant 2 : index
              %101 = memref.dim %7, %c2_11 : memref<?x?x?xf32, "gpu">
              %102 = "disc_shape.linearize"(%98#0, %98#1, %98#2, %99, %100, %101) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
              %c1_12 = arith.constant 1 : index
              %c0_13 = arith.constant 0 : index
              %103 = memref.dim %7, %c0_13 : memref<?x?x?xf32, "gpu">
              %104 = arith.muli %c1_12, %103 : index
              %c1_14 = arith.constant 1 : index
              %105 = memref.dim %7, %c1_14 : memref<?x?x?xf32, "gpu">
              %106 = arith.muli %104, %105 : index
              %c2_15 = arith.constant 2 : index
              %107 = memref.dim %7, %c2_15 : memref<?x?x?xf32, "gpu">
              %108 = arith.muli %106, %107 : index
              %c1_16 = arith.constant 1 : index
              %c0_17 = arith.constant 0 : index
              %109 = memref.reinterpret_cast %7 to offset: [%c0_17], sizes: [%108], strides: [%c1_16] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %110 = memref.load %109[%102] : memref<?xf32, "gpu">
              %111 = arith.cmpf ugt, %arg4, %110 : f32
              %112 = arith.select %111, %arg4, %110 : f32
              %113 = arith.cmpf uno, %110, %110 : f32
              %114 = arith.select %113, %110, %112 : f32
              scf.yield %114 : f32
            }
            %result, %valid = gpu.shuffle  xor %75, %c1_i32, %c32_i32 : f32
            %76 = arith.cmpf ugt, %75, %result : f32
            %77 = arith.select %76, %75, %result : f32
            %78 = arith.cmpf uno, %result, %result : f32
            %79 = arith.select %78, %result, %77 : f32
            %result_1, %valid_2 = gpu.shuffle  xor %79, %c2_i32, %c32_i32 : f32
            %80 = arith.cmpf ugt, %79, %result_1 : f32
            %81 = arith.select %80, %79, %result_1 : f32
            %82 = arith.cmpf uno, %result_1, %result_1 : f32
            %83 = arith.select %82, %result_1, %81 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %83, %c4_i32, %c32_i32 : f32
            %84 = arith.cmpf ugt, %83, %result_3 : f32
            %85 = arith.select %84, %83, %result_3 : f32
            %86 = arith.cmpf uno, %result_3, %result_3 : f32
            %87 = arith.select %86, %result_3, %85 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %87, %c8_i32, %c32_i32 : f32
            %88 = arith.cmpf ugt, %87, %result_5 : f32
            %89 = arith.select %88, %87, %result_5 : f32
            %90 = arith.cmpf uno, %result_5, %result_5 : f32
            %91 = arith.select %90, %result_5, %89 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %91, %c16_i32, %c32_i32 : f32
            %92 = arith.cmpf ugt, %91, %result_7 : f32
            %93 = arith.select %92, %91, %result_7 : f32
            %94 = arith.cmpf uno, %result_7, %result_7 : f32
            %95 = arith.select %94, %result_7, %93 : f32
            %96 = arith.cmpi eq, %52, %c0 : index
            scf.if %96 {
              %c32_9 = arith.constant 32 : index
              %97 = "disc_shape.linearize"(%51, %c32_9) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %98 = memref.reinterpret_cast %48 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %95, %98[%97] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %56 = memref.alloc() : memref<32xf32, 3>
          %57 = gpu.block_id  x
          %58 = gpu.thread_id  x
          %59 = arith.divui %58, %c32 : index
          %60 = arith.remui %58, %c32 : index
          %61 = arith.muli %57, %c8 : index
          %62 = arith.addi %61, %59 : index
          %63 = arith.cmpi slt, %62, %13 : index
          scf.if %63 {
            %64 = arith.subi %3, %60 : index
            %65 = arith.cmpi eq, %64, %c0 : index
            %66 = arith.subi %64, %c1 : index
            %67 = arith.divui %66, %c32 : index
            %68 = arith.addi %67, %c1 : index
            %69 = arith.select %65, %c0, %68 : index
            %70 = arith.remsi %69, %c4 : index
            %71 = arith.subi %69, %70 : index
            %72 = arith.muli %71, %c32 : index
            %73 = arith.addi %60, %72 : index
            %74 = scf.for %arg3 = %60 to %73 step %c128 iter_args(%arg4 = %cst) -> (f32) {
              %82 = arith.addi %arg3, %c32 : index
              %83 = arith.addi %arg3, %c64 : index
              %84 = arith.addi %arg3, %c96 : index
              %85 = "disc_shape.linearize"(%62, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %86 = "disc_shape.linearize"(%62, %82, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %87 = "disc_shape.linearize"(%62, %83, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %88 = "disc_shape.linearize"(%62, %84, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %89:3 = "disc_shape.delinearize"(%85, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %90:3 = "disc_shape.delinearize"(%86, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %91:3 = "disc_shape.delinearize"(%87, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %92:3 = "disc_shape.delinearize"(%88, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %c0_9 = arith.constant 0 : index
              %93 = memref.dim %7, %c0_9 : memref<?x?x?xf32, "gpu">
              %c1_10 = arith.constant 1 : index
              %94 = memref.dim %7, %c1_10 : memref<?x?x?xf32, "gpu">
              %c2_11 = arith.constant 2 : index
              %95 = memref.dim %7, %c2_11 : memref<?x?x?xf32, "gpu">
              %96 = "disc_shape.linearize"(%89#0, %89#1, %89#2, %93, %94, %95) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
              %c1_12 = arith.constant 1 : index
              %c0_13 = arith.constant 0 : index
              %97 = memref.dim %7, %c0_13 : memref<?x?x?xf32, "gpu">
              %98 = arith.muli %c1_12, %97 : index
              %c1_14 = arith.constant 1 : index
              %99 = memref.dim %7, %c1_14 : memref<?x?x?xf32, "gpu">
              %100 = arith.muli %98, %99 : index
              %c2_15 = arith.constant 2 : index
              %101 = memref.dim %7, %c2_15 : memref<?x?x?xf32, "gpu">
              %102 = arith.muli %100, %101 : index
              %c1_16 = arith.constant 1 : index
              %c0_17 = arith.constant 0 : index
              %103 = memref.reinterpret_cast %7 to offset: [%c0_17], sizes: [%102], strides: [%c1_16] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %104 = memref.load %103[%96] : memref<?xf32, "gpu">
              %c0_18 = arith.constant 0 : index
              %105 = memref.dim %7, %c0_18 : memref<?x?x?xf32, "gpu">
              %c1_19 = arith.constant 1 : index
              %106 = memref.dim %7, %c1_19 : memref<?x?x?xf32, "gpu">
              %c2_20 = arith.constant 2 : index
              %107 = memref.dim %7, %c2_20 : memref<?x?x?xf32, "gpu">
              %108 = "disc_shape.linearize"(%90#0, %90#1, %90#2, %105, %106, %107) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
              %c1_21 = arith.constant 1 : index
              %c0_22 = arith.constant 0 : index
              %109 = memref.dim %7, %c0_22 : memref<?x?x?xf32, "gpu">
              %110 = arith.muli %c1_21, %109 : index
              %c1_23 = arith.constant 1 : index
              %111 = memref.dim %7, %c1_23 : memref<?x?x?xf32, "gpu">
              %112 = arith.muli %110, %111 : index
              %c2_24 = arith.constant 2 : index
              %113 = memref.dim %7, %c2_24 : memref<?x?x?xf32, "gpu">
              %114 = arith.muli %112, %113 : index
              %c1_25 = arith.constant 1 : index
              %c0_26 = arith.constant 0 : index
              %115 = memref.reinterpret_cast %7 to offset: [%c0_26], sizes: [%114], strides: [%c1_25] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %116 = memref.load %115[%108] : memref<?xf32, "gpu">
              %c0_27 = arith.constant 0 : index
              %117 = memref.dim %7, %c0_27 : memref<?x?x?xf32, "gpu">
              %c1_28 = arith.constant 1 : index
              %118 = memref.dim %7, %c1_28 : memref<?x?x?xf32, "gpu">
              %c2_29 = arith.constant 2 : index
              %119 = memref.dim %7, %c2_29 : memref<?x?x?xf32, "gpu">
              %120 = "disc_shape.linearize"(%91#0, %91#1, %91#2, %117, %118, %119) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
              %c1_30 = arith.constant 1 : index
              %c0_31 = arith.constant 0 : index
              %121 = memref.dim %7, %c0_31 : memref<?x?x?xf32, "gpu">
              %122 = arith.muli %c1_30, %121 : index
              %c1_32 = arith.constant 1 : index
              %123 = memref.dim %7, %c1_32 : memref<?x?x?xf32, "gpu">
              %124 = arith.muli %122, %123 : index
              %c2_33 = arith.constant 2 : index
              %125 = memref.dim %7, %c2_33 : memref<?x?x?xf32, "gpu">
              %126 = arith.muli %124, %125 : index
              %c1_34 = arith.constant 1 : index
              %c0_35 = arith.constant 0 : index
              %127 = memref.reinterpret_cast %7 to offset: [%c0_35], sizes: [%126], strides: [%c1_34] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %128 = memref.load %127[%120] : memref<?xf32, "gpu">
              %c0_36 = arith.constant 0 : index
              %129 = memref.dim %7, %c0_36 : memref<?x?x?xf32, "gpu">
              %c1_37 = arith.constant 1 : index
              %130 = memref.dim %7, %c1_37 : memref<?x?x?xf32, "gpu">
              %c2_38 = arith.constant 2 : index
              %131 = memref.dim %7, %c2_38 : memref<?x?x?xf32, "gpu">
              %132 = "disc_shape.linearize"(%92#0, %92#1, %92#2, %129, %130, %131) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
              %c1_39 = arith.constant 1 : index
              %c0_40 = arith.constant 0 : index
              %133 = memref.dim %7, %c0_40 : memref<?x?x?xf32, "gpu">
              %134 = arith.muli %c1_39, %133 : index
              %c1_41 = arith.constant 1 : index
              %135 = memref.dim %7, %c1_41 : memref<?x?x?xf32, "gpu">
              %136 = arith.muli %134, %135 : index
              %c2_42 = arith.constant 2 : index
              %137 = memref.dim %7, %c2_42 : memref<?x?x?xf32, "gpu">
              %138 = arith.muli %136, %137 : index
              %c1_43 = arith.constant 1 : index
              %c0_44 = arith.constant 0 : index
              %139 = memref.reinterpret_cast %7 to offset: [%c0_44], sizes: [%138], strides: [%c1_43] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %140 = memref.load %139[%132] : memref<?xf32, "gpu">
              %141 = "disc_shape.linearize"(%89#0, %89#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %142 = "disc_shape.linearize"(%90#0, %90#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %143 = "disc_shape.linearize"(%91#0, %91#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %144 = "disc_shape.linearize"(%92#0, %92#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %145 = "disc_shape.delinearize"(%141, %13) : (index, index) -> index
              %146 = "disc_shape.delinearize"(%142, %13) : (index, index) -> index
              %147 = "disc_shape.delinearize"(%143, %13) : (index, index) -> index
              %148 = "disc_shape.delinearize"(%144, %13) : (index, index) -> index
              %149 = "disc_shape.linearize"(%145, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %150 = "disc_shape.linearize"(%146, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %151 = "disc_shape.linearize"(%147, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %152 = "disc_shape.linearize"(%148, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %153 = arith.remui %149, %c8 : index
              %154 = arith.remui %150, %c8 : index
              %155 = arith.remui %151, %c8 : index
              %156 = arith.remui %152, %c8 : index
              %c32_45 = arith.constant 32 : index
              %157 = "disc_shape.linearize"(%153, %c32_45) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %158 = memref.reinterpret_cast %48 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %159 = memref.load %158[%157] : memref<32xf32, 3>
              %c32_46 = arith.constant 32 : index
              %160 = "disc_shape.linearize"(%154, %c32_46) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %161 = memref.reinterpret_cast %48 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %162 = memref.load %161[%160] : memref<32xf32, 3>
              %c32_47 = arith.constant 32 : index
              %163 = "disc_shape.linearize"(%155, %c32_47) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %164 = memref.reinterpret_cast %48 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %165 = memref.load %164[%163] : memref<32xf32, 3>
              %c32_48 = arith.constant 32 : index
              %166 = "disc_shape.linearize"(%156, %c32_48) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %167 = memref.reinterpret_cast %48 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %168 = memref.load %167[%166] : memref<32xf32, 3>
              %169 = arith.subf %104, %159 : f32
              %170 = arith.subf %116, %162 : f32
              %171 = arith.subf %128, %165 : f32
              %172 = arith.subf %140, %168 : f32
              %173 = math.exp %169 : f32
              %174 = math.exp %170 : f32
              %175 = math.exp %171 : f32
              %176 = math.exp %172 : f32
              %177 = arith.addf %arg4, %173 : f32
              %178 = arith.addf %177, %174 : f32
              %179 = arith.addf %178, %175 : f32
              %180 = arith.addf %179, %176 : f32
              scf.yield %180 : f32
            }
            %75 = scf.for %arg3 = %73 to %3 step %c32 iter_args(%arg4 = %74) -> (f32) {
              %82 = "disc_shape.linearize"(%62, %arg3, %13, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %83:3 = "disc_shape.delinearize"(%82, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %c0_9 = arith.constant 0 : index
              %84 = memref.dim %7, %c0_9 : memref<?x?x?xf32, "gpu">
              %c1_10 = arith.constant 1 : index
              %85 = memref.dim %7, %c1_10 : memref<?x?x?xf32, "gpu">
              %c2_11 = arith.constant 2 : index
              %86 = memref.dim %7, %c2_11 : memref<?x?x?xf32, "gpu">
              %87 = "disc_shape.linearize"(%83#0, %83#1, %83#2, %84, %85, %86) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
              %c1_12 = arith.constant 1 : index
              %c0_13 = arith.constant 0 : index
              %88 = memref.dim %7, %c0_13 : memref<?x?x?xf32, "gpu">
              %89 = arith.muli %c1_12, %88 : index
              %c1_14 = arith.constant 1 : index
              %90 = memref.dim %7, %c1_14 : memref<?x?x?xf32, "gpu">
              %91 = arith.muli %89, %90 : index
              %c2_15 = arith.constant 2 : index
              %92 = memref.dim %7, %c2_15 : memref<?x?x?xf32, "gpu">
              %93 = arith.muli %91, %92 : index
              %c1_16 = arith.constant 1 : index
              %c0_17 = arith.constant 0 : index
              %94 = memref.reinterpret_cast %7 to offset: [%c0_17], sizes: [%93], strides: [%c1_16] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %95 = memref.load %94[%87] : memref<?xf32, "gpu">
              %96 = "disc_shape.linearize"(%83#0, %83#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %97 = "disc_shape.delinearize"(%96, %13) : (index, index) -> index
              %98 = "disc_shape.linearize"(%97, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %99 = arith.remui %98, %c8 : index
              %c32_18 = arith.constant 32 : index
              %100 = "disc_shape.linearize"(%99, %c32_18) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %101 = memref.reinterpret_cast %48 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %102 = memref.load %101[%100] : memref<32xf32, 3>
              %103 = arith.subf %95, %102 : f32
              %104 = math.exp %103 : f32
              %105 = arith.addf %arg4, %104 : f32
              scf.yield %105 : f32
            }
            %result, %valid = gpu.shuffle  xor %75, %c1_i32, %c32_i32 : f32
            %76 = arith.addf %75, %result : f32
            %result_1, %valid_2 = gpu.shuffle  xor %76, %c2_i32, %c32_i32 : f32
            %77 = arith.addf %76, %result_1 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %77, %c4_i32, %c32_i32 : f32
            %78 = arith.addf %77, %result_3 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %78, %c8_i32, %c32_i32 : f32
            %79 = arith.addf %78, %result_5 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %79, %c16_i32, %c32_i32 : f32
            %80 = arith.addf %79, %result_7 : f32
            %81 = arith.cmpi eq, %60, %c0 : index
            scf.if %81 {
              %c32_9 = arith.constant 32 : index
              %82 = "disc_shape.linearize"(%59, %c32_9) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %83 = memref.reinterpret_cast %56 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %80, %83[%82] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          scf.if %47 {
            %64 = arith.subi %3, %44 : index
            %65 = arith.cmpi eq, %64, %c0 : index
            %66 = arith.subi %64, %c1 : index
            %67 = arith.divui %66, %c32 : index
            %68 = arith.addi %67, %c1 : index
            %69 = arith.select %65, %c0, %68 : index
            %70 = arith.remsi %69, %c4 : index
            %71 = arith.subi %69, %70 : index
            %72 = arith.muli %71, %c32 : index
            %73 = arith.addi %44, %72 : index
            scf.for %arg3 = %44 to %73 step %c128 {
              %74 = arith.addi %arg3, %c32 : index
              %75 = arith.addi %arg3, %c64 : index
              %76 = arith.addi %arg3, %c96 : index
              %77 = arith.muli %46, %3 : index
              %78 = arith.muli %46, %3 : index
              %79 = arith.muli %46, %3 : index
              %80 = arith.muli %46, %3 : index
              %81 = arith.addi %77, %arg3 : index
              %82 = arith.addi %78, %74 : index
              %83 = arith.addi %79, %75 : index
              %84 = arith.addi %80, %76 : index
              %85 = arith.muli %2, %1 : index
              %86 = arith.muli %2, %1 : index
              %87 = arith.muli %2, %1 : index
              %88 = arith.muli %2, %1 : index
              %89 = arith.muli %85, %3 : index
              %90 = arith.muli %86, %3 : index
              %91 = arith.muli %87, %3 : index
              %92 = arith.muli %88, %3 : index
              %93 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%89], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %94 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%90], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %95 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%91], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %96 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%92], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %97:3 = "disc_shape.delinearize"(%81, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %98:3 = "disc_shape.delinearize"(%82, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %99:3 = "disc_shape.delinearize"(%83, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %100:3 = "disc_shape.delinearize"(%84, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %c0_1 = arith.constant 0 : index
              %101 = memref.dim %7, %c0_1 : memref<?x?x?xf32, "gpu">
              %c1_2 = arith.constant 1 : index
              %102 = memref.dim %7, %c1_2 : memref<?x?x?xf32, "gpu">
              %c2_3 = arith.constant 2 : index
              %103 = memref.dim %7, %c2_3 : memref<?x?x?xf32, "gpu">
              %104 = "disc_shape.linearize"(%97#0, %97#1, %97#2, %101, %102, %103) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
              %c1_4 = arith.constant 1 : index
              %c0_5 = arith.constant 0 : index
              %105 = memref.dim %7, %c0_5 : memref<?x?x?xf32, "gpu">
              %106 = arith.muli %c1_4, %105 : index
              %c1_6 = arith.constant 1 : index
              %107 = memref.dim %7, %c1_6 : memref<?x?x?xf32, "gpu">
              %108 = arith.muli %106, %107 : index
              %c2_7 = arith.constant 2 : index
              %109 = memref.dim %7, %c2_7 : memref<?x?x?xf32, "gpu">
              %110 = arith.muli %108, %109 : index
              %c1_8 = arith.constant 1 : index
              %c0_9 = arith.constant 0 : index
              %111 = memref.reinterpret_cast %7 to offset: [%c0_9], sizes: [%110], strides: [%c1_8] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %112 = memref.load %111[%104] : memref<?xf32, "gpu">
              %c0_10 = arith.constant 0 : index
              %113 = memref.dim %7, %c0_10 : memref<?x?x?xf32, "gpu">
              %c1_11 = arith.constant 1 : index
              %114 = memref.dim %7, %c1_11 : memref<?x?x?xf32, "gpu">
              %c2_12 = arith.constant 2 : index
              %115 = memref.dim %7, %c2_12 : memref<?x?x?xf32, "gpu">
              %116 = "disc_shape.linearize"(%98#0, %98#1, %98#2, %113, %114, %115) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
              %c1_13 = arith.constant 1 : index
              %c0_14 = arith.constant 0 : index
              %117 = memref.dim %7, %c0_14 : memref<?x?x?xf32, "gpu">
              %118 = arith.muli %c1_13, %117 : index
              %c1_15 = arith.constant 1 : index
              %119 = memref.dim %7, %c1_15 : memref<?x?x?xf32, "gpu">
              %120 = arith.muli %118, %119 : index
              %c2_16 = arith.constant 2 : index
              %121 = memref.dim %7, %c2_16 : memref<?x?x?xf32, "gpu">
              %122 = arith.muli %120, %121 : index
              %c1_17 = arith.constant 1 : index
              %c0_18 = arith.constant 0 : index
              %123 = memref.reinterpret_cast %7 to offset: [%c0_18], sizes: [%122], strides: [%c1_17] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %124 = memref.load %123[%116] : memref<?xf32, "gpu">
              %c0_19 = arith.constant 0 : index
              %125 = memref.dim %7, %c0_19 : memref<?x?x?xf32, "gpu">
              %c1_20 = arith.constant 1 : index
              %126 = memref.dim %7, %c1_20 : memref<?x?x?xf32, "gpu">
              %c2_21 = arith.constant 2 : index
              %127 = memref.dim %7, %c2_21 : memref<?x?x?xf32, "gpu">
              %128 = "disc_shape.linearize"(%99#0, %99#1, %99#2, %125, %126, %127) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
              %c1_22 = arith.constant 1 : index
              %c0_23 = arith.constant 0 : index
              %129 = memref.dim %7, %c0_23 : memref<?x?x?xf32, "gpu">
              %130 = arith.muli %c1_22, %129 : index
              %c1_24 = arith.constant 1 : index
              %131 = memref.dim %7, %c1_24 : memref<?x?x?xf32, "gpu">
              %132 = arith.muli %130, %131 : index
              %c2_25 = arith.constant 2 : index
              %133 = memref.dim %7, %c2_25 : memref<?x?x?xf32, "gpu">
              %134 = arith.muli %132, %133 : index
              %c1_26 = arith.constant 1 : index
              %c0_27 = arith.constant 0 : index
              %135 = memref.reinterpret_cast %7 to offset: [%c0_27], sizes: [%134], strides: [%c1_26] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %136 = memref.load %135[%128] : memref<?xf32, "gpu">
              %c0_28 = arith.constant 0 : index
              %137 = memref.dim %7, %c0_28 : memref<?x?x?xf32, "gpu">
              %c1_29 = arith.constant 1 : index
              %138 = memref.dim %7, %c1_29 : memref<?x?x?xf32, "gpu">
              %c2_30 = arith.constant 2 : index
              %139 = memref.dim %7, %c2_30 : memref<?x?x?xf32, "gpu">
              %140 = "disc_shape.linearize"(%100#0, %100#1, %100#2, %137, %138, %139) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
              %c1_31 = arith.constant 1 : index
              %c0_32 = arith.constant 0 : index
              %141 = memref.dim %7, %c0_32 : memref<?x?x?xf32, "gpu">
              %142 = arith.muli %c1_31, %141 : index
              %c1_33 = arith.constant 1 : index
              %143 = memref.dim %7, %c1_33 : memref<?x?x?xf32, "gpu">
              %144 = arith.muli %142, %143 : index
              %c2_34 = arith.constant 2 : index
              %145 = memref.dim %7, %c2_34 : memref<?x?x?xf32, "gpu">
              %146 = arith.muli %144, %145 : index
              %c1_35 = arith.constant 1 : index
              %c0_36 = arith.constant 0 : index
              %147 = memref.reinterpret_cast %7 to offset: [%c0_36], sizes: [%146], strides: [%c1_35] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %148 = memref.load %147[%140] : memref<?xf32, "gpu">
              %149 = "disc_shape.linearize"(%97#0, %97#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %150 = "disc_shape.linearize"(%98#0, %98#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %151 = "disc_shape.linearize"(%99#0, %99#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %152 = "disc_shape.linearize"(%100#0, %100#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %153 = "disc_shape.delinearize"(%149, %13) : (index, index) -> index
              %154 = "disc_shape.delinearize"(%150, %13) : (index, index) -> index
              %155 = "disc_shape.delinearize"(%151, %13) : (index, index) -> index
              %156 = "disc_shape.delinearize"(%152, %13) : (index, index) -> index
              %157 = "disc_shape.linearize"(%153, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %158 = "disc_shape.linearize"(%154, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %159 = "disc_shape.linearize"(%155, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %160 = "disc_shape.linearize"(%156, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %161 = arith.remui %157, %c8 : index
              %162 = arith.remui %158, %c8 : index
              %163 = arith.remui %159, %c8 : index
              %164 = arith.remui %160, %c8 : index
              %c32_37 = arith.constant 32 : index
              %165 = "disc_shape.linearize"(%161, %c32_37) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %166 = memref.reinterpret_cast %48 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %167 = memref.load %166[%165] : memref<32xf32, 3>
              %c32_38 = arith.constant 32 : index
              %168 = "disc_shape.linearize"(%162, %c32_38) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %169 = memref.reinterpret_cast %48 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %170 = memref.load %169[%168] : memref<32xf32, 3>
              %c32_39 = arith.constant 32 : index
              %171 = "disc_shape.linearize"(%163, %c32_39) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %172 = memref.reinterpret_cast %48 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %173 = memref.load %172[%171] : memref<32xf32, 3>
              %c32_40 = arith.constant 32 : index
              %174 = "disc_shape.linearize"(%164, %c32_40) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %175 = memref.reinterpret_cast %48 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %176 = memref.load %175[%174] : memref<32xf32, 3>
              %177 = arith.subf %112, %167 : f32
              %178 = arith.subf %124, %170 : f32
              %179 = arith.subf %136, %173 : f32
              %180 = arith.subf %148, %176 : f32
              %181 = math.exp %177 : f32
              %182 = math.exp %178 : f32
              %183 = math.exp %179 : f32
              %184 = math.exp %180 : f32
              %185 = "disc_shape.linearize"(%97#0, %97#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %186 = "disc_shape.linearize"(%98#0, %98#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %187 = "disc_shape.linearize"(%99#0, %99#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %188 = "disc_shape.linearize"(%100#0, %100#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %189 = "disc_shape.delinearize"(%185, %13) : (index, index) -> index
              %190 = "disc_shape.delinearize"(%186, %13) : (index, index) -> index
              %191 = "disc_shape.delinearize"(%187, %13) : (index, index) -> index
              %192 = "disc_shape.delinearize"(%188, %13) : (index, index) -> index
              %193 = "disc_shape.linearize"(%189, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %194 = "disc_shape.linearize"(%190, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %195 = "disc_shape.linearize"(%191, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %196 = "disc_shape.linearize"(%192, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %197 = arith.remui %193, %c8 : index
              %198 = arith.remui %194, %c8 : index
              %199 = arith.remui %195, %c8 : index
              %200 = arith.remui %196, %c8 : index
              %c32_41 = arith.constant 32 : index
              %201 = "disc_shape.linearize"(%197, %c32_41) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %202 = memref.reinterpret_cast %56 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %203 = memref.load %202[%201] : memref<32xf32, 3>
              %c32_42 = arith.constant 32 : index
              %204 = "disc_shape.linearize"(%198, %c32_42) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %205 = memref.reinterpret_cast %56 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %206 = memref.load %205[%204] : memref<32xf32, 3>
              %c32_43 = arith.constant 32 : index
              %207 = "disc_shape.linearize"(%199, %c32_43) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %208 = memref.reinterpret_cast %56 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %209 = memref.load %208[%207] : memref<32xf32, 3>
              %c32_44 = arith.constant 32 : index
              %210 = "disc_shape.linearize"(%200, %c32_44) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %211 = memref.reinterpret_cast %56 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %212 = memref.load %211[%210] : memref<32xf32, 3>
              %213 = arith.divf %181, %203 : f32
              %214 = arith.divf %182, %206 : f32
              %215 = arith.divf %183, %209 : f32
              %216 = arith.divf %184, %212 : f32
              %c0_45 = arith.constant 0 : index
              %217 = memref.dim %93, %c0_45 : memref<?xf32, "gpu">
              %218 = "disc_shape.linearize"(%81, %217) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %c1_46 = arith.constant 1 : index
              %c0_47 = arith.constant 0 : index
              %219 = memref.dim %93, %c0_47 : memref<?xf32, "gpu">
              %220 = arith.muli %c1_46, %219 : index
              %c1_48 = arith.constant 1 : index
              %c0_49 = arith.constant 0 : index
              %221 = memref.reinterpret_cast %93 to offset: [%c0_49], sizes: [%220], strides: [%c1_48] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %213, %221[%218] : memref<?xf32, "gpu">
              %c0_50 = arith.constant 0 : index
              %222 = memref.dim %94, %c0_50 : memref<?xf32, "gpu">
              %223 = "disc_shape.linearize"(%82, %222) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %c1_51 = arith.constant 1 : index
              %c0_52 = arith.constant 0 : index
              %224 = memref.dim %94, %c0_52 : memref<?xf32, "gpu">
              %225 = arith.muli %c1_51, %224 : index
              %c1_53 = arith.constant 1 : index
              %c0_54 = arith.constant 0 : index
              %226 = memref.reinterpret_cast %94 to offset: [%c0_54], sizes: [%225], strides: [%c1_53] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %214, %226[%223] : memref<?xf32, "gpu">
              %c0_55 = arith.constant 0 : index
              %227 = memref.dim %95, %c0_55 : memref<?xf32, "gpu">
              %228 = "disc_shape.linearize"(%83, %227) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %c1_56 = arith.constant 1 : index
              %c0_57 = arith.constant 0 : index
              %229 = memref.dim %95, %c0_57 : memref<?xf32, "gpu">
              %230 = arith.muli %c1_56, %229 : index
              %c1_58 = arith.constant 1 : index
              %c0_59 = arith.constant 0 : index
              %231 = memref.reinterpret_cast %95 to offset: [%c0_59], sizes: [%230], strides: [%c1_58] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %215, %231[%228] : memref<?xf32, "gpu">
              %c0_60 = arith.constant 0 : index
              %232 = memref.dim %96, %c0_60 : memref<?xf32, "gpu">
              %233 = "disc_shape.linearize"(%84, %232) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %c1_61 = arith.constant 1 : index
              %c0_62 = arith.constant 0 : index
              %234 = memref.dim %96, %c0_62 : memref<?xf32, "gpu">
              %235 = arith.muli %c1_61, %234 : index
              %c1_63 = arith.constant 1 : index
              %c0_64 = arith.constant 0 : index
              %236 = memref.reinterpret_cast %96 to offset: [%c0_64], sizes: [%235], strides: [%c1_63] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %216, %236[%233] : memref<?xf32, "gpu">
            }
            scf.for %arg3 = %73 to %3 step %c32 {
              %74 = arith.muli %46, %3 : index
              %75 = arith.addi %74, %arg3 : index
              %76 = arith.muli %2, %1 : index
              %77 = arith.muli %76, %3 : index
              %78 = memref.reinterpret_cast %26 to offset: [%c0], sizes: [%77], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %79:3 = "disc_shape.delinearize"(%75, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %c0_1 = arith.constant 0 : index
              %80 = memref.dim %7, %c0_1 : memref<?x?x?xf32, "gpu">
              %c1_2 = arith.constant 1 : index
              %81 = memref.dim %7, %c1_2 : memref<?x?x?xf32, "gpu">
              %c2_3 = arith.constant 2 : index
              %82 = memref.dim %7, %c2_3 : memref<?x?x?xf32, "gpu">
              %83 = "disc_shape.linearize"(%79#0, %79#1, %79#2, %80, %81, %82) {operand_segment_sizes = dense<3> : vector<2xi32>} : (index, index, index, index, index, index) -> index
              %c1_4 = arith.constant 1 : index
              %c0_5 = arith.constant 0 : index
              %84 = memref.dim %7, %c0_5 : memref<?x?x?xf32, "gpu">
              %85 = arith.muli %c1_4, %84 : index
              %c1_6 = arith.constant 1 : index
              %86 = memref.dim %7, %c1_6 : memref<?x?x?xf32, "gpu">
              %87 = arith.muli %85, %86 : index
              %c2_7 = arith.constant 2 : index
              %88 = memref.dim %7, %c2_7 : memref<?x?x?xf32, "gpu">
              %89 = arith.muli %87, %88 : index
              %c1_8 = arith.constant 1 : index
              %c0_9 = arith.constant 0 : index
              %90 = memref.reinterpret_cast %7 to offset: [%c0_9], sizes: [%89], strides: [%c1_8] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %91 = memref.load %90[%83] : memref<?xf32, "gpu">
              %92 = "disc_shape.linearize"(%79#0, %79#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %93 = "disc_shape.delinearize"(%92, %13) : (index, index) -> index
              %94 = "disc_shape.linearize"(%93, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %95 = arith.remui %94, %c8 : index
              %c32_10 = arith.constant 32 : index
              %96 = "disc_shape.linearize"(%95, %c32_10) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %97 = memref.reinterpret_cast %48 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %98 = memref.load %97[%96] : memref<32xf32, 3>
              %99 = arith.subf %91, %98 : f32
              %100 = math.exp %99 : f32
              %101 = "disc_shape.linearize"(%79#0, %79#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %102 = "disc_shape.delinearize"(%101, %13) : (index, index) -> index
              %103 = "disc_shape.linearize"(%102, %13) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %104 = arith.remui %103, %c8 : index
              %c32_11 = arith.constant 32 : index
              %105 = "disc_shape.linearize"(%104, %c32_11) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %106 = memref.reinterpret_cast %56 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %107 = memref.load %106[%105] : memref<32xf32, 3>
              %108 = arith.divf %100, %107 : f32
              %c0_12 = arith.constant 0 : index
              %109 = memref.dim %78, %c0_12 : memref<?xf32, "gpu">
              %110 = "disc_shape.linearize"(%75, %109) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %c1_13 = arith.constant 1 : index
              %c0_14 = arith.constant 0 : index
              %111 = memref.dim %78, %c0_14 : memref<?xf32, "gpu">
              %112 = arith.muli %c1_13, %111 : index
              %c1_15 = arith.constant 1 : index
              %c0_16 = arith.constant 0 : index
              %113 = memref.reinterpret_cast %78 to offset: [%c0_16], sizes: [%112], strides: [%c1_15] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %108, %113[%110] : memref<?xf32, "gpu">
            }
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  memref.dealloc %25 : memref<?x?x?xf32, "gpu">
  memref.dealloc %24 : memref<?x?xf32, "gpu">
  memref.dealloc %23 : memref<?xf32, "gpu">
  memref.dealloc %22 : memref<?x?xf32, "gpu">
  memref.dealloc %21 : memref<?x?x?xf32, "gpu">
  memref.dealloc %20 : memref<?x?x?xf32, "gpu">
  memref.dealloc %19 : memref<?x?x?xf32, "gpu">
  memref.dealloc %17 : memref<?x?xf32, "gpu">
  memref.dealloc %15 : memref<?xf32, "gpu">
  memref.dealloc %14 : memref<?x?xf32, "gpu">
  memref.dealloc %5 : memref<f32, "gpu">
  memref.dealloc %4 : memref<f32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %26) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After Canonicalizer (disc-canonicalize) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c768 = arith.constant 768 : index
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant -0.000000e+00 : f32
  %cst_0 = arith.constant 0xFF800000 : f32
  %c8 = arith.constant 8 : index
  %c16_i32 = arith.constant 16 : i32
  %c8_i32 = arith.constant 8 : i32
  %c4_i32 = arith.constant 4 : i32
  %c2_i32 = arith.constant 2 : i32
  %c1_i32 = arith.constant 1 : i32
  %c32_i32 = arith.constant 32 : i32
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c1024 = arith.constant 1024 : index
  %c6 = arith.constant 6 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
  %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
  %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
  %4 = arith.index_cast %2 : index to i32
  %5 = arith.index_cast %1 : index to i32
  %6 = arith.muli %4, %5 : i32
  %7 = arith.index_cast %6 : i32 to index
  %8 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %9 = arith.cmpi slt, %7, %c3 : index
  %10 = arith.cmpi sge, %3, %c1024 : index
  %11 = arith.cmpi slt, %7, %c6 : index
  %12 = arith.andi %11, %10 : i1
  %13 = arith.cmpi sge, %3, %c512 : index
  %14 = arith.cmpi sge, %7, %c6 : index
  %15 = arith.andi %14, %13 : i1
  %16 = arith.ori %9, %12 : i1
  %17 = arith.ori %16, %15 : i1
  scf.if %17 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %18 = gpu.block_id  x
          %19 = gpu.thread_id  x
          %20 = memref.alloc() : memref<32xf32, 3>
          %21 = gpu.block_id  x
          %22 = gpu.thread_id  x
          %23 = arith.divui %22, %c32 : index
          %24 = arith.remui %22, %c32 : index
          %25 = arith.subi %3, %22 : index
          %26 = arith.cmpi eq, %25, %c0 : index
          %27 = arith.subi %25, %c1 : index
          %28 = arith.divui %27, %c256 : index
          %29 = arith.addi %28, %c1 : index
          %30 = arith.select %26, %c0, %29 : index
          %31 = arith.remsi %30, %c4 : index
          %32 = arith.subi %30, %31 : index
          %33 = arith.muli %32, %c256 : index
          %34 = arith.addi %22, %33 : index
          %35 = scf.for %arg3 = %22 to %34 step %c1024 iter_args(%arg4 = %cst_0) -> (f32) {
            %95 = arith.addi %arg3, %c256 : index
            %96 = arith.addi %arg3, %c512 : index
            %97 = arith.addi %arg3, %c768 : index
            %98 = "disc_shape.linearize"(%21, %arg3, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %99 = "disc_shape.linearize"(%21, %95, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %100 = "disc_shape.linearize"(%21, %96, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %101 = "disc_shape.linearize"(%21, %97, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %102 = arith.muli %2, %1 : index
            %103 = arith.muli %102, %3 : index
            %104 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%103], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %105 = memref.load %104[%98] : memref<?xf32, "gpu">
            %106 = arith.muli %2, %1 : index
            %107 = arith.muli %106, %3 : index
            %108 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%107], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %109 = memref.load %108[%99] : memref<?xf32, "gpu">
            %110 = arith.muli %2, %1 : index
            %111 = arith.muli %110, %3 : index
            %112 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%111], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %113 = memref.load %112[%100] : memref<?xf32, "gpu">
            %114 = arith.muli %2, %1 : index
            %115 = arith.muli %114, %3 : index
            %116 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%115], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %117 = memref.load %116[%101] : memref<?xf32, "gpu">
            %118 = arith.cmpf ugt, %arg4, %105 : f32
            %119 = arith.select %118, %arg4, %105 : f32
            %120 = arith.cmpf uno, %105, %105 : f32
            %121 = arith.select %120, %105, %119 : f32
            %122 = arith.cmpf ugt, %121, %109 : f32
            %123 = arith.select %122, %121, %109 : f32
            %124 = arith.cmpf uno, %109, %109 : f32
            %125 = arith.select %124, %109, %123 : f32
            %126 = arith.cmpf ugt, %125, %113 : f32
            %127 = arith.select %126, %125, %113 : f32
            %128 = arith.cmpf uno, %113, %113 : f32
            %129 = arith.select %128, %113, %127 : f32
            %130 = arith.cmpf ugt, %129, %117 : f32
            %131 = arith.select %130, %129, %117 : f32
            %132 = arith.cmpf uno, %117, %117 : f32
            %133 = arith.select %132, %117, %131 : f32
            scf.yield %133 : f32
          }
          %36 = scf.for %arg3 = %34 to %3 step %c256 iter_args(%arg4 = %35) -> (f32) {
            %95 = "disc_shape.linearize"(%21, %arg3, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %96 = arith.muli %2, %1 : index
            %97 = arith.muli %96, %3 : index
            %98 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%97], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %99 = memref.load %98[%95] : memref<?xf32, "gpu">
            %100 = arith.cmpf ugt, %arg4, %99 : f32
            %101 = arith.select %100, %arg4, %99 : f32
            %102 = arith.cmpf uno, %99, %99 : f32
            %103 = arith.select %102, %99, %101 : f32
            scf.yield %103 : f32
          }
          %result, %valid = gpu.shuffle  xor %36, %c1_i32, %c32_i32 : f32
          %37 = arith.cmpf ugt, %36, %result : f32
          %38 = arith.select %37, %36, %result : f32
          %39 = arith.cmpf uno, %result, %result : f32
          %40 = arith.select %39, %result, %38 : f32
          %result_1, %valid_2 = gpu.shuffle  xor %40, %c2_i32, %c32_i32 : f32
          %41 = arith.cmpf ugt, %40, %result_1 : f32
          %42 = arith.select %41, %40, %result_1 : f32
          %43 = arith.cmpf uno, %result_1, %result_1 : f32
          %44 = arith.select %43, %result_1, %42 : f32
          %result_3, %valid_4 = gpu.shuffle  xor %44, %c4_i32, %c32_i32 : f32
          %45 = arith.cmpf ugt, %44, %result_3 : f32
          %46 = arith.select %45, %44, %result_3 : f32
          %47 = arith.cmpf uno, %result_3, %result_3 : f32
          %48 = arith.select %47, %result_3, %46 : f32
          %result_5, %valid_6 = gpu.shuffle  xor %48, %c8_i32, %c32_i32 : f32
          %49 = arith.cmpf ugt, %48, %result_5 : f32
          %50 = arith.select %49, %48, %result_5 : f32
          %51 = arith.cmpf uno, %result_5, %result_5 : f32
          %52 = arith.select %51, %result_5, %50 : f32
          %result_7, %valid_8 = gpu.shuffle  xor %52, %c16_i32, %c32_i32 : f32
          %53 = arith.cmpf ugt, %52, %result_7 : f32
          %54 = arith.select %53, %52, %result_7 : f32
          %55 = arith.cmpf uno, %result_7, %result_7 : f32
          %56 = arith.select %55, %result_7, %54 : f32
          %57 = memref.alloc() : memref<8xf32, 3>
          %58 = arith.cmpi eq, %24, %c0 : index
          scf.if %58 {
            %95 = "disc_shape.linearize"(%23, %c8) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %96 = memref.reinterpret_cast %57 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
            memref.store %56, %96[%95] : memref<8xf32, 3>
          }
          gpu.barrier
          %59 = arith.cmpi slt, %22, %c32 : index
          scf.if %59 {
            %95 = arith.cmpi slt, %24, %c8 : index
            %96 = scf.if %95 -> (f32) {
              %109 = "disc_shape.linearize"(%24, %c8) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %110 = memref.reinterpret_cast %57 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
              %111 = memref.load %110[%109] : memref<8xf32, 3>
              scf.yield %111 : f32
            } else {
              scf.yield %cst_0 : f32
            }
            %result_19, %valid_20 = gpu.shuffle  xor %96, %c1_i32, %c8_i32 : f32
            %97 = arith.cmpf ugt, %96, %result_19 : f32
            %98 = arith.select %97, %96, %result_19 : f32
            %99 = arith.cmpf uno, %result_19, %result_19 : f32
            %100 = arith.select %99, %result_19, %98 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %100, %c2_i32, %c8_i32 : f32
            %101 = arith.cmpf ugt, %100, %result_21 : f32
            %102 = arith.select %101, %100, %result_21 : f32
            %103 = arith.cmpf uno, %result_21, %result_21 : f32
            %104 = arith.select %103, %result_21, %102 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %104, %c4_i32, %c8_i32 : f32
            %105 = arith.cmpf ugt, %104, %result_23 : f32
            %106 = arith.select %105, %104, %result_23 : f32
            %107 = arith.cmpf uno, %result_23, %result_23 : f32
            %108 = arith.select %107, %result_23, %106 : f32
            scf.if %58 {
              %109 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %110 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %108, %110[%109] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %60 = memref.alloc() : memref<32xf32, 3>
          %61 = gpu.block_id  x
          %62 = gpu.thread_id  x
          %63 = arith.divui %62, %c32 : index
          %64 = arith.remui %62, %c32 : index
          %65 = arith.subi %3, %62 : index
          %66 = arith.cmpi eq, %65, %c0 : index
          %67 = arith.subi %65, %c1 : index
          %68 = arith.divui %67, %c256 : index
          %69 = arith.addi %68, %c1 : index
          %70 = arith.select %66, %c0, %69 : index
          %71 = arith.remsi %70, %c4 : index
          %72 = arith.subi %70, %71 : index
          %73 = arith.muli %72, %c256 : index
          %74 = arith.addi %62, %73 : index
          %75 = scf.for %arg3 = %62 to %74 step %c1024 iter_args(%arg4 = %cst) -> (f32) {
            %95 = arith.addi %arg3, %c256 : index
            %96 = arith.addi %arg3, %c512 : index
            %97 = arith.addi %arg3, %c768 : index
            %98 = "disc_shape.linearize"(%61, %arg3, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %99 = "disc_shape.linearize"(%61, %95, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %100 = "disc_shape.linearize"(%61, %96, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %101 = "disc_shape.linearize"(%61, %97, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %102 = arith.muli %2, %1 : index
            %103 = arith.muli %102, %3 : index
            %104 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%103], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %105 = memref.load %104[%98] : memref<?xf32, "gpu">
            %106 = arith.muli %2, %1 : index
            %107 = arith.muli %106, %3 : index
            %108 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%107], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %109 = memref.load %108[%99] : memref<?xf32, "gpu">
            %110 = arith.muli %2, %1 : index
            %111 = arith.muli %110, %3 : index
            %112 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%111], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %113 = memref.load %112[%100] : memref<?xf32, "gpu">
            %114 = arith.muli %2, %1 : index
            %115 = arith.muli %114, %3 : index
            %116 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%115], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %117 = memref.load %116[%101] : memref<?xf32, "gpu">
            %118 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %119 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %120 = memref.load %119[%118] : memref<32xf32, 3>
            %121 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %122 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %123 = memref.load %122[%121] : memref<32xf32, 3>
            %124 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %125 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %126 = memref.load %125[%124] : memref<32xf32, 3>
            %127 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %128 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %129 = memref.load %128[%127] : memref<32xf32, 3>
            %130 = arith.subf %105, %120 : f32
            %131 = arith.subf %109, %123 : f32
            %132 = arith.subf %113, %126 : f32
            %133 = arith.subf %117, %129 : f32
            %134 = math.exp %130 : f32
            %135 = math.exp %131 : f32
            %136 = math.exp %132 : f32
            %137 = math.exp %133 : f32
            %138 = arith.addf %arg4, %134 : f32
            %139 = arith.addf %138, %135 : f32
            %140 = arith.addf %139, %136 : f32
            %141 = arith.addf %140, %137 : f32
            scf.yield %141 : f32
          }
          %76 = scf.for %arg3 = %74 to %3 step %c256 iter_args(%arg4 = %75) -> (f32) {
            %95 = "disc_shape.linearize"(%61, %arg3, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %96 = arith.muli %2, %1 : index
            %97 = arith.muli %96, %3 : index
            %98 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%97], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %99 = memref.load %98[%95] : memref<?xf32, "gpu">
            %100 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %101 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %102 = memref.load %101[%100] : memref<32xf32, 3>
            %103 = arith.subf %99, %102 : f32
            %104 = math.exp %103 : f32
            %105 = arith.addf %arg4, %104 : f32
            scf.yield %105 : f32
          }
          %result_9, %valid_10 = gpu.shuffle  xor %76, %c1_i32, %c32_i32 : f32
          %77 = arith.addf %76, %result_9 : f32
          %result_11, %valid_12 = gpu.shuffle  xor %77, %c2_i32, %c32_i32 : f32
          %78 = arith.addf %77, %result_11 : f32
          %result_13, %valid_14 = gpu.shuffle  xor %78, %c4_i32, %c32_i32 : f32
          %79 = arith.addf %78, %result_13 : f32
          %result_15, %valid_16 = gpu.shuffle  xor %79, %c8_i32, %c32_i32 : f32
          %80 = arith.addf %79, %result_15 : f32
          %result_17, %valid_18 = gpu.shuffle  xor %80, %c16_i32, %c32_i32 : f32
          %81 = arith.addf %80, %result_17 : f32
          %82 = memref.alloc() : memref<8xf32, 3>
          %83 = arith.cmpi eq, %64, %c0 : index
          scf.if %83 {
            %95 = "disc_shape.linearize"(%63, %c8) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %96 = memref.reinterpret_cast %82 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
            memref.store %81, %96[%95] : memref<8xf32, 3>
          }
          gpu.barrier
          %84 = arith.cmpi slt, %62, %c32 : index
          scf.if %84 {
            %95 = arith.cmpi slt, %64, %c8 : index
            %96 = scf.if %95 -> (f32) {
              %100 = "disc_shape.linearize"(%64, %c8) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %101 = memref.reinterpret_cast %82 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
              %102 = memref.load %101[%100] : memref<8xf32, 3>
              scf.yield %102 : f32
            } else {
              scf.yield %cst : f32
            }
            %result_19, %valid_20 = gpu.shuffle  xor %96, %c1_i32, %c8_i32 : f32
            %97 = arith.addf %96, %result_19 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %97, %c2_i32, %c8_i32 : f32
            %98 = arith.addf %97, %result_21 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %98, %c4_i32, %c8_i32 : f32
            %99 = arith.addf %98, %result_23 : f32
            scf.if %83 {
              %100 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %101 = memref.reinterpret_cast %60 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %99, %101[%100] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %85 = arith.subi %3, %19 : index
          %86 = arith.cmpi eq, %85, %c0 : index
          %87 = arith.subi %85, %c1 : index
          %88 = arith.divui %87, %c256 : index
          %89 = arith.addi %88, %c1 : index
          %90 = arith.select %86, %c0, %89 : index
          %91 = arith.remsi %90, %c4 : index
          %92 = arith.subi %90, %91 : index
          %93 = arith.muli %92, %c256 : index
          %94 = arith.addi %19, %93 : index
          scf.for %arg3 = %19 to %94 step %c1024 {
            %95 = arith.addi %arg3, %c256 : index
            %96 = arith.addi %arg3, %c512 : index
            %97 = arith.addi %arg3, %c768 : index
            %98 = arith.muli %18, %3 : index
            %99 = arith.muli %18, %3 : index
            %100 = arith.muli %18, %3 : index
            %101 = arith.muli %18, %3 : index
            %102 = arith.addi %98, %arg3 : index
            %103 = arith.addi %99, %95 : index
            %104 = arith.addi %100, %96 : index
            %105 = arith.addi %101, %97 : index
            %106 = arith.muli %2, %1 : index
            %107 = arith.muli %2, %1 : index
            %108 = arith.muli %2, %1 : index
            %109 = arith.muli %2, %1 : index
            %110 = arith.muli %106, %3 : index
            %111 = arith.muli %107, %3 : index
            %112 = arith.muli %108, %3 : index
            %113 = arith.muli %109, %3 : index
            %114 = arith.muli %2, %1 : index
            %115 = arith.muli %114, %3 : index
            %116 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%115], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %117 = memref.load %116[%102] : memref<?xf32, "gpu">
            %118 = arith.muli %2, %1 : index
            %119 = arith.muli %118, %3 : index
            %120 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%119], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %121 = memref.load %120[%103] : memref<?xf32, "gpu">
            %122 = arith.muli %2, %1 : index
            %123 = arith.muli %122, %3 : index
            %124 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%123], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %125 = memref.load %124[%104] : memref<?xf32, "gpu">
            %126 = arith.muli %2, %1 : index
            %127 = arith.muli %126, %3 : index
            %128 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%127], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %129 = memref.load %128[%105] : memref<?xf32, "gpu">
            %130 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %131 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %132 = memref.load %131[%130] : memref<32xf32, 3>
            %133 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %134 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %135 = memref.load %134[%133] : memref<32xf32, 3>
            %136 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %137 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %138 = memref.load %137[%136] : memref<32xf32, 3>
            %139 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %140 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %141 = memref.load %140[%139] : memref<32xf32, 3>
            %142 = arith.subf %117, %132 : f32
            %143 = arith.subf %121, %135 : f32
            %144 = arith.subf %125, %138 : f32
            %145 = arith.subf %129, %141 : f32
            %146 = math.exp %142 : f32
            %147 = math.exp %143 : f32
            %148 = math.exp %144 : f32
            %149 = math.exp %145 : f32
            %150 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %151 = memref.reinterpret_cast %60 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %152 = memref.load %151[%150] : memref<32xf32, 3>
            %153 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %154 = memref.reinterpret_cast %60 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %155 = memref.load %154[%153] : memref<32xf32, 3>
            %156 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %157 = memref.reinterpret_cast %60 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %158 = memref.load %157[%156] : memref<32xf32, 3>
            %159 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %160 = memref.reinterpret_cast %60 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %161 = memref.load %160[%159] : memref<32xf32, 3>
            %162 = arith.divf %146, %152 : f32
            %163 = arith.divf %147, %155 : f32
            %164 = arith.divf %148, %158 : f32
            %165 = arith.divf %149, %161 : f32
            %166 = "disc_shape.linearize"(%102, %110) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %167 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%110], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %162, %167[%166] : memref<?xf32, "gpu">
            %168 = "disc_shape.linearize"(%103, %111) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %169 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%111], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %163, %169[%168] : memref<?xf32, "gpu">
            %170 = "disc_shape.linearize"(%104, %112) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %171 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%112], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %164, %171[%170] : memref<?xf32, "gpu">
            %172 = "disc_shape.linearize"(%105, %113) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %173 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%113], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %165, %173[%172] : memref<?xf32, "gpu">
          }
          scf.for %arg3 = %94 to %3 step %c256 {
            %95 = arith.muli %18, %3 : index
            %96 = arith.addi %95, %arg3 : index
            %97 = arith.muli %2, %1 : index
            %98 = arith.muli %97, %3 : index
            %99 = arith.muli %2, %1 : index
            %100 = arith.muli %99, %3 : index
            %101 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%100], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %102 = memref.load %101[%96] : memref<?xf32, "gpu">
            %103 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %104 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %105 = memref.load %104[%103] : memref<32xf32, 3>
            %106 = arith.subf %102, %105 : f32
            %107 = math.exp %106 : f32
            %108 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %109 = memref.reinterpret_cast %60 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %110 = memref.load %109[%108] : memref<32xf32, 3>
            %111 = arith.divf %107, %110 : f32
            %112 = "disc_shape.linearize"(%96, %98) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %113 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%98], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %111, %113[%112] : memref<?xf32, "gpu">
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      %18 = arith.cmpi eq, %7, %c0 : index
      %19 = arith.subi %7, %c1 : index
      %20 = arith.divui %19, %c8 : index
      %21 = arith.addi %20, %c1 : index
      %22 = arith.select %18, %c0, %21 : index
      scf.parallel (%arg1) = (%c0) to (%22) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %23 = gpu.block_id  x
          %24 = gpu.thread_id  x
          %25 = arith.divui %24, %c32 : index
          %26 = arith.remui %24, %c32 : index
          %27 = arith.muli %23, %c8 : index
          %28 = arith.addi %27, %25 : index
          %29 = arith.cmpi slt, %28, %7 : index
          %30 = memref.alloc() : memref<32xf32, 3>
          %31 = gpu.block_id  x
          %32 = gpu.thread_id  x
          %33 = arith.divui %32, %c32 : index
          %34 = arith.remui %32, %c32 : index
          %35 = arith.muli %31, %c8 : index
          %36 = arith.addi %35, %33 : index
          %37 = arith.cmpi slt, %36, %7 : index
          scf.if %37 {
            %46 = arith.subi %3, %34 : index
            %47 = arith.cmpi eq, %46, %c0 : index
            %48 = arith.subi %46, %c1 : index
            %49 = arith.divui %48, %c32 : index
            %50 = arith.addi %49, %c1 : index
            %51 = arith.select %47, %c0, %50 : index
            %52 = arith.remsi %51, %c4 : index
            %53 = arith.subi %51, %52 : index
            %54 = arith.muli %53, %c32 : index
            %55 = arith.addi %34, %54 : index
            %56 = scf.for %arg3 = %34 to %55 step %c128 iter_args(%arg4 = %cst_0) -> (f32) {
              %79 = arith.addi %arg3, %c32 : index
              %80 = arith.addi %arg3, %c64 : index
              %81 = arith.addi %arg3, %c96 : index
              %82 = "disc_shape.linearize"(%36, %arg3, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %83 = "disc_shape.linearize"(%36, %79, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %84 = "disc_shape.linearize"(%36, %80, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %85 = "disc_shape.linearize"(%36, %81, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %86 = arith.muli %2, %1 : index
              %87 = arith.muli %86, %3 : index
              %88 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%87], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %89 = memref.load %88[%82] : memref<?xf32, "gpu">
              %90 = arith.muli %2, %1 : index
              %91 = arith.muli %90, %3 : index
              %92 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%91], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %93 = memref.load %92[%83] : memref<?xf32, "gpu">
              %94 = arith.muli %2, %1 : index
              %95 = arith.muli %94, %3 : index
              %96 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%95], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %97 = memref.load %96[%84] : memref<?xf32, "gpu">
              %98 = arith.muli %2, %1 : index
              %99 = arith.muli %98, %3 : index
              %100 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%99], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %101 = memref.load %100[%85] : memref<?xf32, "gpu">
              %102 = arith.cmpf ugt, %arg4, %89 : f32
              %103 = arith.select %102, %arg4, %89 : f32
              %104 = arith.cmpf uno, %89, %89 : f32
              %105 = arith.select %104, %89, %103 : f32
              %106 = arith.cmpf ugt, %105, %93 : f32
              %107 = arith.select %106, %105, %93 : f32
              %108 = arith.cmpf uno, %93, %93 : f32
              %109 = arith.select %108, %93, %107 : f32
              %110 = arith.cmpf ugt, %109, %97 : f32
              %111 = arith.select %110, %109, %97 : f32
              %112 = arith.cmpf uno, %97, %97 : f32
              %113 = arith.select %112, %97, %111 : f32
              %114 = arith.cmpf ugt, %113, %101 : f32
              %115 = arith.select %114, %113, %101 : f32
              %116 = arith.cmpf uno, %101, %101 : f32
              %117 = arith.select %116, %101, %115 : f32
              scf.yield %117 : f32
            }
            %57 = scf.for %arg3 = %55 to %3 step %c32 iter_args(%arg4 = %56) -> (f32) {
              %79 = "disc_shape.linearize"(%36, %arg3, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %80 = arith.muli %2, %1 : index
              %81 = arith.muli %80, %3 : index
              %82 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%81], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %83 = memref.load %82[%79] : memref<?xf32, "gpu">
              %84 = arith.cmpf ugt, %arg4, %83 : f32
              %85 = arith.select %84, %arg4, %83 : f32
              %86 = arith.cmpf uno, %83, %83 : f32
              %87 = arith.select %86, %83, %85 : f32
              scf.yield %87 : f32
            }
            %result, %valid = gpu.shuffle  xor %57, %c1_i32, %c32_i32 : f32
            %58 = arith.cmpf ugt, %57, %result : f32
            %59 = arith.select %58, %57, %result : f32
            %60 = arith.cmpf uno, %result, %result : f32
            %61 = arith.select %60, %result, %59 : f32
            %result_1, %valid_2 = gpu.shuffle  xor %61, %c2_i32, %c32_i32 : f32
            %62 = arith.cmpf ugt, %61, %result_1 : f32
            %63 = arith.select %62, %61, %result_1 : f32
            %64 = arith.cmpf uno, %result_1, %result_1 : f32
            %65 = arith.select %64, %result_1, %63 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %65, %c4_i32, %c32_i32 : f32
            %66 = arith.cmpf ugt, %65, %result_3 : f32
            %67 = arith.select %66, %65, %result_3 : f32
            %68 = arith.cmpf uno, %result_3, %result_3 : f32
            %69 = arith.select %68, %result_3, %67 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %69, %c8_i32, %c32_i32 : f32
            %70 = arith.cmpf ugt, %69, %result_5 : f32
            %71 = arith.select %70, %69, %result_5 : f32
            %72 = arith.cmpf uno, %result_5, %result_5 : f32
            %73 = arith.select %72, %result_5, %71 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %73, %c16_i32, %c32_i32 : f32
            %74 = arith.cmpf ugt, %73, %result_7 : f32
            %75 = arith.select %74, %73, %result_7 : f32
            %76 = arith.cmpf uno, %result_7, %result_7 : f32
            %77 = arith.select %76, %result_7, %75 : f32
            %78 = arith.cmpi eq, %34, %c0 : index
            scf.if %78 {
              %79 = "disc_shape.linearize"(%33, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %80 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %77, %80[%79] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %38 = memref.alloc() : memref<32xf32, 3>
          %39 = gpu.block_id  x
          %40 = gpu.thread_id  x
          %41 = arith.divui %40, %c32 : index
          %42 = arith.remui %40, %c32 : index
          %43 = arith.muli %39, %c8 : index
          %44 = arith.addi %43, %41 : index
          %45 = arith.cmpi slt, %44, %7 : index
          scf.if %45 {
            %46 = arith.subi %3, %42 : index
            %47 = arith.cmpi eq, %46, %c0 : index
            %48 = arith.subi %46, %c1 : index
            %49 = arith.divui %48, %c32 : index
            %50 = arith.addi %49, %c1 : index
            %51 = arith.select %47, %c0, %50 : index
            %52 = arith.remsi %51, %c4 : index
            %53 = arith.subi %51, %52 : index
            %54 = arith.muli %53, %c32 : index
            %55 = arith.addi %42, %54 : index
            %56 = scf.for %arg3 = %42 to %55 step %c128 iter_args(%arg4 = %cst) -> (f32) {
              %64 = arith.addi %arg3, %c32 : index
              %65 = arith.addi %arg3, %c64 : index
              %66 = arith.addi %arg3, %c96 : index
              %67 = "disc_shape.linearize"(%44, %arg3, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %68 = "disc_shape.linearize"(%44, %64, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %69 = "disc_shape.linearize"(%44, %65, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %70 = "disc_shape.linearize"(%44, %66, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %71:3 = "disc_shape.delinearize"(%67, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %72:3 = "disc_shape.delinearize"(%68, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %73:3 = "disc_shape.delinearize"(%69, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %74:3 = "disc_shape.delinearize"(%70, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %75 = arith.muli %2, %1 : index
              %76 = arith.muli %75, %3 : index
              %77 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%76], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %78 = memref.load %77[%67] : memref<?xf32, "gpu">
              %79 = arith.muli %2, %1 : index
              %80 = arith.muli %79, %3 : index
              %81 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%80], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %82 = memref.load %81[%68] : memref<?xf32, "gpu">
              %83 = arith.muli %2, %1 : index
              %84 = arith.muli %83, %3 : index
              %85 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%84], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %86 = memref.load %85[%69] : memref<?xf32, "gpu">
              %87 = arith.muli %2, %1 : index
              %88 = arith.muli %87, %3 : index
              %89 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%88], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %90 = memref.load %89[%70] : memref<?xf32, "gpu">
              %91 = "disc_shape.linearize"(%71#0, %71#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %92 = "disc_shape.linearize"(%72#0, %72#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %93 = "disc_shape.linearize"(%73#0, %73#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %94 = "disc_shape.linearize"(%74#0, %74#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %95 = arith.remui %91, %c8 : index
              %96 = arith.remui %92, %c8 : index
              %97 = arith.remui %93, %c8 : index
              %98 = arith.remui %94, %c8 : index
              %99 = "disc_shape.linearize"(%95, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %100 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %101 = memref.load %100[%99] : memref<32xf32, 3>
              %102 = "disc_shape.linearize"(%96, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %103 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %104 = memref.load %103[%102] : memref<32xf32, 3>
              %105 = "disc_shape.linearize"(%97, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %106 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %107 = memref.load %106[%105] : memref<32xf32, 3>
              %108 = "disc_shape.linearize"(%98, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %109 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %110 = memref.load %109[%108] : memref<32xf32, 3>
              %111 = arith.subf %78, %101 : f32
              %112 = arith.subf %82, %104 : f32
              %113 = arith.subf %86, %107 : f32
              %114 = arith.subf %90, %110 : f32
              %115 = math.exp %111 : f32
              %116 = math.exp %112 : f32
              %117 = math.exp %113 : f32
              %118 = math.exp %114 : f32
              %119 = arith.addf %arg4, %115 : f32
              %120 = arith.addf %119, %116 : f32
              %121 = arith.addf %120, %117 : f32
              %122 = arith.addf %121, %118 : f32
              scf.yield %122 : f32
            }
            %57 = scf.for %arg3 = %55 to %3 step %c32 iter_args(%arg4 = %56) -> (f32) {
              %64 = "disc_shape.linearize"(%44, %arg3, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %65:3 = "disc_shape.delinearize"(%64, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %66 = arith.muli %2, %1 : index
              %67 = arith.muli %66, %3 : index
              %68 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%67], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %69 = memref.load %68[%64] : memref<?xf32, "gpu">
              %70 = "disc_shape.linearize"(%65#0, %65#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %71 = arith.remui %70, %c8 : index
              %72 = "disc_shape.linearize"(%71, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %73 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %74 = memref.load %73[%72] : memref<32xf32, 3>
              %75 = arith.subf %69, %74 : f32
              %76 = math.exp %75 : f32
              %77 = arith.addf %arg4, %76 : f32
              scf.yield %77 : f32
            }
            %result, %valid = gpu.shuffle  xor %57, %c1_i32, %c32_i32 : f32
            %58 = arith.addf %57, %result : f32
            %result_1, %valid_2 = gpu.shuffle  xor %58, %c2_i32, %c32_i32 : f32
            %59 = arith.addf %58, %result_1 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %59, %c4_i32, %c32_i32 : f32
            %60 = arith.addf %59, %result_3 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %60, %c8_i32, %c32_i32 : f32
            %61 = arith.addf %60, %result_5 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %61, %c16_i32, %c32_i32 : f32
            %62 = arith.addf %61, %result_7 : f32
            %63 = arith.cmpi eq, %42, %c0 : index
            scf.if %63 {
              %64 = "disc_shape.linearize"(%41, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %65 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %62, %65[%64] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          scf.if %29 {
            %46 = arith.subi %3, %26 : index
            %47 = arith.cmpi eq, %46, %c0 : index
            %48 = arith.subi %46, %c1 : index
            %49 = arith.divui %48, %c32 : index
            %50 = arith.addi %49, %c1 : index
            %51 = arith.select %47, %c0, %50 : index
            %52 = arith.remsi %51, %c4 : index
            %53 = arith.subi %51, %52 : index
            %54 = arith.muli %53, %c32 : index
            %55 = arith.addi %26, %54 : index
            scf.for %arg3 = %26 to %55 step %c128 {
              %56 = arith.addi %arg3, %c32 : index
              %57 = arith.addi %arg3, %c64 : index
              %58 = arith.addi %arg3, %c96 : index
              %59 = arith.muli %28, %3 : index
              %60 = arith.muli %28, %3 : index
              %61 = arith.muli %28, %3 : index
              %62 = arith.muli %28, %3 : index
              %63 = arith.addi %59, %arg3 : index
              %64 = arith.addi %60, %56 : index
              %65 = arith.addi %61, %57 : index
              %66 = arith.addi %62, %58 : index
              %67 = arith.muli %2, %1 : index
              %68 = arith.muli %2, %1 : index
              %69 = arith.muli %2, %1 : index
              %70 = arith.muli %2, %1 : index
              %71 = arith.muli %67, %3 : index
              %72 = arith.muli %68, %3 : index
              %73 = arith.muli %69, %3 : index
              %74 = arith.muli %70, %3 : index
              %75:3 = "disc_shape.delinearize"(%63, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %76:3 = "disc_shape.delinearize"(%64, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %77:3 = "disc_shape.delinearize"(%65, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %78:3 = "disc_shape.delinearize"(%66, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %79 = arith.muli %2, %1 : index
              %80 = arith.muli %79, %3 : index
              %81 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%80], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %82 = memref.load %81[%63] : memref<?xf32, "gpu">
              %83 = arith.muli %2, %1 : index
              %84 = arith.muli %83, %3 : index
              %85 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%84], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %86 = memref.load %85[%64] : memref<?xf32, "gpu">
              %87 = arith.muli %2, %1 : index
              %88 = arith.muli %87, %3 : index
              %89 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%88], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %90 = memref.load %89[%65] : memref<?xf32, "gpu">
              %91 = arith.muli %2, %1 : index
              %92 = arith.muli %91, %3 : index
              %93 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%92], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %94 = memref.load %93[%66] : memref<?xf32, "gpu">
              %95 = "disc_shape.linearize"(%75#0, %75#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %96 = "disc_shape.linearize"(%76#0, %76#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %97 = "disc_shape.linearize"(%77#0, %77#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %98 = "disc_shape.linearize"(%78#0, %78#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %99 = arith.remui %95, %c8 : index
              %100 = arith.remui %96, %c8 : index
              %101 = arith.remui %97, %c8 : index
              %102 = arith.remui %98, %c8 : index
              %103 = "disc_shape.linearize"(%99, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %104 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %105 = memref.load %104[%103] : memref<32xf32, 3>
              %106 = "disc_shape.linearize"(%100, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %107 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %108 = memref.load %107[%106] : memref<32xf32, 3>
              %109 = "disc_shape.linearize"(%101, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %110 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %111 = memref.load %110[%109] : memref<32xf32, 3>
              %112 = "disc_shape.linearize"(%102, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %113 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %114 = memref.load %113[%112] : memref<32xf32, 3>
              %115 = arith.subf %82, %105 : f32
              %116 = arith.subf %86, %108 : f32
              %117 = arith.subf %90, %111 : f32
              %118 = arith.subf %94, %114 : f32
              %119 = math.exp %115 : f32
              %120 = math.exp %116 : f32
              %121 = math.exp %117 : f32
              %122 = math.exp %118 : f32
              %123 = "disc_shape.linearize"(%75#0, %75#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %124 = "disc_shape.linearize"(%76#0, %76#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %125 = "disc_shape.linearize"(%77#0, %77#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %126 = "disc_shape.linearize"(%78#0, %78#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %127 = arith.remui %123, %c8 : index
              %128 = arith.remui %124, %c8 : index
              %129 = arith.remui %125, %c8 : index
              %130 = arith.remui %126, %c8 : index
              %131 = "disc_shape.linearize"(%127, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %132 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %133 = memref.load %132[%131] : memref<32xf32, 3>
              %134 = "disc_shape.linearize"(%128, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %135 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %136 = memref.load %135[%134] : memref<32xf32, 3>
              %137 = "disc_shape.linearize"(%129, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %138 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %139 = memref.load %138[%137] : memref<32xf32, 3>
              %140 = "disc_shape.linearize"(%130, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %141 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %142 = memref.load %141[%140] : memref<32xf32, 3>
              %143 = arith.divf %119, %133 : f32
              %144 = arith.divf %120, %136 : f32
              %145 = arith.divf %121, %139 : f32
              %146 = arith.divf %122, %142 : f32
              %147 = "disc_shape.linearize"(%63, %71) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %148 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%71], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %143, %148[%147] : memref<?xf32, "gpu">
              %149 = "disc_shape.linearize"(%64, %72) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %150 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%72], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %144, %150[%149] : memref<?xf32, "gpu">
              %151 = "disc_shape.linearize"(%65, %73) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %152 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%73], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %145, %152[%151] : memref<?xf32, "gpu">
              %153 = "disc_shape.linearize"(%66, %74) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %154 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%74], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %146, %154[%153] : memref<?xf32, "gpu">
            }
            scf.for %arg3 = %55 to %3 step %c32 {
              %56 = arith.muli %28, %3 : index
              %57 = arith.addi %56, %arg3 : index
              %58 = arith.muli %2, %1 : index
              %59 = arith.muli %58, %3 : index
              %60:3 = "disc_shape.delinearize"(%57, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %61 = arith.muli %2, %1 : index
              %62 = arith.muli %61, %3 : index
              %63 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%62], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %64 = memref.load %63[%57] : memref<?xf32, "gpu">
              %65 = "disc_shape.linearize"(%60#0, %60#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %66 = arith.remui %65, %c8 : index
              %67 = "disc_shape.linearize"(%66, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %68 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %69 = memref.load %68[%67] : memref<32xf32, 3>
              %70 = arith.subf %64, %69 : f32
              %71 = math.exp %70 : f32
              %72 = "disc_shape.linearize"(%60#0, %60#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %73 = arith.remui %72, %c8 : index
              %74 = "disc_shape.linearize"(%73, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %75 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %76 = memref.load %75[%74] : memref<32xf32, 3>
              %77 = arith.divf %71, %76 : f32
              %78 = "disc_shape.linearize"(%57, %59) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %79 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%59], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %77, %79[%78] : memref<?xf32, "gpu">
            }
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  "disc_ral.dispatch"(%arg0, %c0, %8) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c768 = arith.constant 768 : index
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant -0.000000e+00 : f32
  %cst_0 = arith.constant 0xFF800000 : f32
  %c8 = arith.constant 8 : index
  %c16_i32 = arith.constant 16 : i32
  %c8_i32 = arith.constant 8 : i32
  %c4_i32 = arith.constant 4 : i32
  %c2_i32 = arith.constant 2 : i32
  %c1_i32 = arith.constant 1 : i32
  %c32_i32 = arith.constant 32 : i32
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c1024 = arith.constant 1024 : index
  %c6 = arith.constant 6 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
  %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
  %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
  %4 = arith.index_cast %2 : index to i32
  %5 = arith.index_cast %1 : index to i32
  %6 = arith.muli %4, %5 : i32
  %7 = arith.index_cast %6 : i32 to index
  %8 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %9 = arith.cmpi slt, %7, %c3 : index
  %10 = arith.cmpi sge, %3, %c1024 : index
  %11 = arith.cmpi slt, %7, %c6 : index
  %12 = arith.andi %11, %10 : i1
  %13 = arith.cmpi sge, %3, %c512 : index
  %14 = arith.cmpi sge, %7, %c6 : index
  %15 = arith.andi %14, %13 : i1
  %16 = arith.ori %9, %12 : i1
  %17 = arith.ori %16, %15 : i1
  scf.if %17 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %18 = gpu.block_id  x
          %19 = gpu.thread_id  x
          %20 = memref.alloc() : memref<32xf32, 3>
          %21 = arith.divui %19, %c32 : index
          %22 = arith.remui %19, %c32 : index
          %23 = arith.subi %3, %19 : index
          %24 = arith.cmpi eq, %23, %c0 : index
          %25 = arith.subi %23, %c1 : index
          %26 = arith.divui %25, %c256 : index
          %27 = arith.addi %26, %c1 : index
          %28 = arith.select %24, %c0, %27 : index
          %29 = arith.remsi %28, %c4 : index
          %30 = arith.subi %28, %29 : index
          %31 = arith.muli %30, %c256 : index
          %32 = arith.addi %19, %31 : index
          %33 = scf.for %arg3 = %19 to %32 step %c1024 iter_args(%arg4 = %cst_0) -> (f32) {
            %67 = arith.addi %arg3, %c256 : index
            %68 = arith.addi %arg3, %c512 : index
            %69 = arith.addi %arg3, %c768 : index
            %70 = "disc_shape.linearize"(%18, %arg3, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %71 = "disc_shape.linearize"(%18, %67, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %72 = "disc_shape.linearize"(%18, %68, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %73 = "disc_shape.linearize"(%18, %69, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %74 = arith.muli %2, %1 : index
            %75 = arith.muli %74, %3 : index
            %76 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%75], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %77 = memref.load %76[%70] : memref<?xf32, "gpu">
            %78 = memref.load %76[%71] : memref<?xf32, "gpu">
            %79 = memref.load %76[%72] : memref<?xf32, "gpu">
            %80 = memref.load %76[%73] : memref<?xf32, "gpu">
            %81 = arith.cmpf ugt, %arg4, %77 : f32
            %82 = arith.select %81, %arg4, %77 : f32
            %83 = arith.cmpf uno, %77, %77 : f32
            %84 = arith.select %83, %77, %82 : f32
            %85 = arith.cmpf ugt, %84, %78 : f32
            %86 = arith.select %85, %84, %78 : f32
            %87 = arith.cmpf uno, %78, %78 : f32
            %88 = arith.select %87, %78, %86 : f32
            %89 = arith.cmpf ugt, %88, %79 : f32
            %90 = arith.select %89, %88, %79 : f32
            %91 = arith.cmpf uno, %79, %79 : f32
            %92 = arith.select %91, %79, %90 : f32
            %93 = arith.cmpf ugt, %92, %80 : f32
            %94 = arith.select %93, %92, %80 : f32
            %95 = arith.cmpf uno, %80, %80 : f32
            %96 = arith.select %95, %80, %94 : f32
            scf.yield %96 : f32
          }
          %34 = scf.for %arg3 = %32 to %3 step %c256 iter_args(%arg4 = %33) -> (f32) {
            %67 = "disc_shape.linearize"(%18, %arg3, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %68 = arith.muli %2, %1 : index
            %69 = arith.muli %68, %3 : index
            %70 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%69], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %71 = memref.load %70[%67] : memref<?xf32, "gpu">
            %72 = arith.cmpf ugt, %arg4, %71 : f32
            %73 = arith.select %72, %arg4, %71 : f32
            %74 = arith.cmpf uno, %71, %71 : f32
            %75 = arith.select %74, %71, %73 : f32
            scf.yield %75 : f32
          }
          %result, %valid = gpu.shuffle  xor %34, %c1_i32, %c32_i32 : f32
          %35 = arith.cmpf ugt, %34, %result : f32
          %36 = arith.select %35, %34, %result : f32
          %37 = arith.cmpf uno, %result, %result : f32
          %38 = arith.select %37, %result, %36 : f32
          %result_1, %valid_2 = gpu.shuffle  xor %38, %c2_i32, %c32_i32 : f32
          %39 = arith.cmpf ugt, %38, %result_1 : f32
          %40 = arith.select %39, %38, %result_1 : f32
          %41 = arith.cmpf uno, %result_1, %result_1 : f32
          %42 = arith.select %41, %result_1, %40 : f32
          %result_3, %valid_4 = gpu.shuffle  xor %42, %c4_i32, %c32_i32 : f32
          %43 = arith.cmpf ugt, %42, %result_3 : f32
          %44 = arith.select %43, %42, %result_3 : f32
          %45 = arith.cmpf uno, %result_3, %result_3 : f32
          %46 = arith.select %45, %result_3, %44 : f32
          %result_5, %valid_6 = gpu.shuffle  xor %46, %c8_i32, %c32_i32 : f32
          %47 = arith.cmpf ugt, %46, %result_5 : f32
          %48 = arith.select %47, %46, %result_5 : f32
          %49 = arith.cmpf uno, %result_5, %result_5 : f32
          %50 = arith.select %49, %result_5, %48 : f32
          %result_7, %valid_8 = gpu.shuffle  xor %50, %c16_i32, %c32_i32 : f32
          %51 = arith.cmpf ugt, %50, %result_7 : f32
          %52 = arith.select %51, %50, %result_7 : f32
          %53 = arith.cmpf uno, %result_7, %result_7 : f32
          %54 = arith.select %53, %result_7, %52 : f32
          %55 = memref.alloc() : memref<8xf32, 3>
          %56 = arith.cmpi eq, %22, %c0 : index
          scf.if %56 {
            %67 = "disc_shape.linearize"(%21, %c8) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %68 = memref.reinterpret_cast %55 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
            memref.store %54, %68[%67] : memref<8xf32, 3>
          }
          gpu.barrier
          %57 = arith.cmpi slt, %19, %c32 : index
          scf.if %57 {
            %67 = arith.cmpi slt, %22, %c8 : index
            %68 = scf.if %67 -> (f32) {
              %81 = "disc_shape.linearize"(%22, %c8) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %82 = memref.reinterpret_cast %55 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
              %83 = memref.load %82[%81] : memref<8xf32, 3>
              scf.yield %83 : f32
            } else {
              scf.yield %cst_0 : f32
            }
            %result_19, %valid_20 = gpu.shuffle  xor %68, %c1_i32, %c8_i32 : f32
            %69 = arith.cmpf ugt, %68, %result_19 : f32
            %70 = arith.select %69, %68, %result_19 : f32
            %71 = arith.cmpf uno, %result_19, %result_19 : f32
            %72 = arith.select %71, %result_19, %70 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %72, %c2_i32, %c8_i32 : f32
            %73 = arith.cmpf ugt, %72, %result_21 : f32
            %74 = arith.select %73, %72, %result_21 : f32
            %75 = arith.cmpf uno, %result_21, %result_21 : f32
            %76 = arith.select %75, %result_21, %74 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %76, %c4_i32, %c8_i32 : f32
            %77 = arith.cmpf ugt, %76, %result_23 : f32
            %78 = arith.select %77, %76, %result_23 : f32
            %79 = arith.cmpf uno, %result_23, %result_23 : f32
            %80 = arith.select %79, %result_23, %78 : f32
            scf.if %56 {
              %81 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %82 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %80, %82[%81] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %58 = memref.alloc() : memref<32xf32, 3>
          %59 = scf.for %arg3 = %19 to %32 step %c1024 iter_args(%arg4 = %cst) -> (f32) {
            %67 = arith.addi %arg3, %c256 : index
            %68 = arith.addi %arg3, %c512 : index
            %69 = arith.addi %arg3, %c768 : index
            %70 = "disc_shape.linearize"(%18, %arg3, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %71 = "disc_shape.linearize"(%18, %67, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %72 = "disc_shape.linearize"(%18, %68, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %73 = "disc_shape.linearize"(%18, %69, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %74 = arith.muli %2, %1 : index
            %75 = arith.muli %74, %3 : index
            %76 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%75], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %77 = memref.load %76[%70] : memref<?xf32, "gpu">
            %78 = memref.load %76[%71] : memref<?xf32, "gpu">
            %79 = memref.load %76[%72] : memref<?xf32, "gpu">
            %80 = memref.load %76[%73] : memref<?xf32, "gpu">
            %81 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %82 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %83 = memref.load %82[%81] : memref<32xf32, 3>
            %84 = arith.subf %77, %83 : f32
            %85 = arith.subf %78, %83 : f32
            %86 = arith.subf %79, %83 : f32
            %87 = arith.subf %80, %83 : f32
            %88 = math.exp %84 : f32
            %89 = math.exp %85 : f32
            %90 = math.exp %86 : f32
            %91 = math.exp %87 : f32
            %92 = arith.addf %arg4, %88 : f32
            %93 = arith.addf %92, %89 : f32
            %94 = arith.addf %93, %90 : f32
            %95 = arith.addf %94, %91 : f32
            scf.yield %95 : f32
          }
          %60 = scf.for %arg3 = %32 to %3 step %c256 iter_args(%arg4 = %59) -> (f32) {
            %67 = "disc_shape.linearize"(%18, %arg3, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
            %68 = arith.muli %2, %1 : index
            %69 = arith.muli %68, %3 : index
            %70 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%69], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %71 = memref.load %70[%67] : memref<?xf32, "gpu">
            %72 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %73 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %74 = memref.load %73[%72] : memref<32xf32, 3>
            %75 = arith.subf %71, %74 : f32
            %76 = math.exp %75 : f32
            %77 = arith.addf %arg4, %76 : f32
            scf.yield %77 : f32
          }
          %result_9, %valid_10 = gpu.shuffle  xor %60, %c1_i32, %c32_i32 : f32
          %61 = arith.addf %60, %result_9 : f32
          %result_11, %valid_12 = gpu.shuffle  xor %61, %c2_i32, %c32_i32 : f32
          %62 = arith.addf %61, %result_11 : f32
          %result_13, %valid_14 = gpu.shuffle  xor %62, %c4_i32, %c32_i32 : f32
          %63 = arith.addf %62, %result_13 : f32
          %result_15, %valid_16 = gpu.shuffle  xor %63, %c8_i32, %c32_i32 : f32
          %64 = arith.addf %63, %result_15 : f32
          %result_17, %valid_18 = gpu.shuffle  xor %64, %c16_i32, %c32_i32 : f32
          %65 = arith.addf %64, %result_17 : f32
          %66 = memref.alloc() : memref<8xf32, 3>
          scf.if %56 {
            %67 = "disc_shape.linearize"(%21, %c8) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %68 = memref.reinterpret_cast %66 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
            memref.store %65, %68[%67] : memref<8xf32, 3>
          }
          gpu.barrier
          scf.if %57 {
            %67 = arith.cmpi slt, %22, %c8 : index
            %68 = scf.if %67 -> (f32) {
              %72 = "disc_shape.linearize"(%22, %c8) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %73 = memref.reinterpret_cast %66 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
              %74 = memref.load %73[%72] : memref<8xf32, 3>
              scf.yield %74 : f32
            } else {
              scf.yield %cst : f32
            }
            %result_19, %valid_20 = gpu.shuffle  xor %68, %c1_i32, %c8_i32 : f32
            %69 = arith.addf %68, %result_19 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %69, %c2_i32, %c8_i32 : f32
            %70 = arith.addf %69, %result_21 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %70, %c4_i32, %c8_i32 : f32
            %71 = arith.addf %70, %result_23 : f32
            scf.if %56 {
              %72 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %73 = memref.reinterpret_cast %58 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %71, %73[%72] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          scf.for %arg3 = %19 to %32 step %c1024 {
            %67 = arith.addi %arg3, %c256 : index
            %68 = arith.addi %arg3, %c512 : index
            %69 = arith.addi %arg3, %c768 : index
            %70 = arith.muli %18, %3 : index
            %71 = arith.addi %70, %arg3 : index
            %72 = arith.addi %70, %67 : index
            %73 = arith.addi %70, %68 : index
            %74 = arith.addi %70, %69 : index
            %75 = arith.muli %2, %1 : index
            %76 = arith.muli %75, %3 : index
            %77 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%76], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %78 = memref.load %77[%71] : memref<?xf32, "gpu">
            %79 = memref.load %77[%72] : memref<?xf32, "gpu">
            %80 = memref.load %77[%73] : memref<?xf32, "gpu">
            %81 = memref.load %77[%74] : memref<?xf32, "gpu">
            %82 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %83 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %84 = memref.load %83[%82] : memref<32xf32, 3>
            %85 = arith.subf %78, %84 : f32
            %86 = arith.subf %79, %84 : f32
            %87 = arith.subf %80, %84 : f32
            %88 = arith.subf %81, %84 : f32
            %89 = math.exp %85 : f32
            %90 = math.exp %86 : f32
            %91 = math.exp %87 : f32
            %92 = math.exp %88 : f32
            %93 = memref.reinterpret_cast %58 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %94 = memref.load %93[%82] : memref<32xf32, 3>
            %95 = arith.divf %89, %94 : f32
            %96 = arith.divf %90, %94 : f32
            %97 = arith.divf %91, %94 : f32
            %98 = arith.divf %92, %94 : f32
            %99 = "disc_shape.linearize"(%71, %76) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %100 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%76], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %95, %100[%99] : memref<?xf32, "gpu">
            %101 = "disc_shape.linearize"(%72, %76) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            memref.store %96, %100[%101] : memref<?xf32, "gpu">
            %102 = "disc_shape.linearize"(%73, %76) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            memref.store %97, %100[%102] : memref<?xf32, "gpu">
            %103 = "disc_shape.linearize"(%74, %76) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            memref.store %98, %100[%103] : memref<?xf32, "gpu">
          }
          scf.for %arg3 = %32 to %3 step %c256 {
            %67 = arith.muli %18, %3 : index
            %68 = arith.addi %67, %arg3 : index
            %69 = arith.muli %2, %1 : index
            %70 = arith.muli %69, %3 : index
            %71 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %72 = memref.load %71[%68] : memref<?xf32, "gpu">
            %73 = "disc_shape.linearize"(%c0, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %74 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %75 = memref.load %74[%73] : memref<32xf32, 3>
            %76 = arith.subf %72, %75 : f32
            %77 = math.exp %76 : f32
            %78 = memref.reinterpret_cast %58 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %79 = memref.load %78[%73] : memref<32xf32, 3>
            %80 = arith.divf %77, %79 : f32
            %81 = "disc_shape.linearize"(%68, %70) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
            %82 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %80, %82[%81] : memref<?xf32, "gpu">
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      %18 = arith.cmpi eq, %7, %c0 : index
      %19 = arith.subi %7, %c1 : index
      %20 = arith.divui %19, %c8 : index
      %21 = arith.addi %20, %c1 : index
      %22 = arith.select %18, %c0, %21 : index
      scf.parallel (%arg1) = (%c0) to (%22) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %23 = gpu.block_id  x
          %24 = gpu.thread_id  x
          %25 = arith.divui %24, %c32 : index
          %26 = arith.remui %24, %c32 : index
          %27 = arith.muli %23, %c8 : index
          %28 = arith.addi %27, %25 : index
          %29 = arith.cmpi slt, %28, %7 : index
          %30 = memref.alloc() : memref<32xf32, 3>
          scf.if %29 {
            %32 = arith.subi %3, %26 : index
            %33 = arith.cmpi eq, %32, %c0 : index
            %34 = arith.subi %32, %c1 : index
            %35 = arith.divui %34, %c32 : index
            %36 = arith.addi %35, %c1 : index
            %37 = arith.select %33, %c0, %36 : index
            %38 = arith.remsi %37, %c4 : index
            %39 = arith.subi %37, %38 : index
            %40 = arith.muli %39, %c32 : index
            %41 = arith.addi %26, %40 : index
            %42 = scf.for %arg3 = %26 to %41 step %c128 iter_args(%arg4 = %cst_0) -> (f32) {
              %65 = arith.addi %arg3, %c32 : index
              %66 = arith.addi %arg3, %c64 : index
              %67 = arith.addi %arg3, %c96 : index
              %68 = "disc_shape.linearize"(%28, %arg3, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %69 = "disc_shape.linearize"(%28, %65, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %70 = "disc_shape.linearize"(%28, %66, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %71 = "disc_shape.linearize"(%28, %67, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %72 = arith.muli %2, %1 : index
              %73 = arith.muli %72, %3 : index
              %74 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%73], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %75 = memref.load %74[%68] : memref<?xf32, "gpu">
              %76 = memref.load %74[%69] : memref<?xf32, "gpu">
              %77 = memref.load %74[%70] : memref<?xf32, "gpu">
              %78 = memref.load %74[%71] : memref<?xf32, "gpu">
              %79 = arith.cmpf ugt, %arg4, %75 : f32
              %80 = arith.select %79, %arg4, %75 : f32
              %81 = arith.cmpf uno, %75, %75 : f32
              %82 = arith.select %81, %75, %80 : f32
              %83 = arith.cmpf ugt, %82, %76 : f32
              %84 = arith.select %83, %82, %76 : f32
              %85 = arith.cmpf uno, %76, %76 : f32
              %86 = arith.select %85, %76, %84 : f32
              %87 = arith.cmpf ugt, %86, %77 : f32
              %88 = arith.select %87, %86, %77 : f32
              %89 = arith.cmpf uno, %77, %77 : f32
              %90 = arith.select %89, %77, %88 : f32
              %91 = arith.cmpf ugt, %90, %78 : f32
              %92 = arith.select %91, %90, %78 : f32
              %93 = arith.cmpf uno, %78, %78 : f32
              %94 = arith.select %93, %78, %92 : f32
              scf.yield %94 : f32
            }
            %43 = scf.for %arg3 = %41 to %3 step %c32 iter_args(%arg4 = %42) -> (f32) {
              %65 = "disc_shape.linearize"(%28, %arg3, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %66 = arith.muli %2, %1 : index
              %67 = arith.muli %66, %3 : index
              %68 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%67], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %69 = memref.load %68[%65] : memref<?xf32, "gpu">
              %70 = arith.cmpf ugt, %arg4, %69 : f32
              %71 = arith.select %70, %arg4, %69 : f32
              %72 = arith.cmpf uno, %69, %69 : f32
              %73 = arith.select %72, %69, %71 : f32
              scf.yield %73 : f32
            }
            %result, %valid = gpu.shuffle  xor %43, %c1_i32, %c32_i32 : f32
            %44 = arith.cmpf ugt, %43, %result : f32
            %45 = arith.select %44, %43, %result : f32
            %46 = arith.cmpf uno, %result, %result : f32
            %47 = arith.select %46, %result, %45 : f32
            %result_1, %valid_2 = gpu.shuffle  xor %47, %c2_i32, %c32_i32 : f32
            %48 = arith.cmpf ugt, %47, %result_1 : f32
            %49 = arith.select %48, %47, %result_1 : f32
            %50 = arith.cmpf uno, %result_1, %result_1 : f32
            %51 = arith.select %50, %result_1, %49 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %51, %c4_i32, %c32_i32 : f32
            %52 = arith.cmpf ugt, %51, %result_3 : f32
            %53 = arith.select %52, %51, %result_3 : f32
            %54 = arith.cmpf uno, %result_3, %result_3 : f32
            %55 = arith.select %54, %result_3, %53 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %55, %c8_i32, %c32_i32 : f32
            %56 = arith.cmpf ugt, %55, %result_5 : f32
            %57 = arith.select %56, %55, %result_5 : f32
            %58 = arith.cmpf uno, %result_5, %result_5 : f32
            %59 = arith.select %58, %result_5, %57 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %59, %c16_i32, %c32_i32 : f32
            %60 = arith.cmpf ugt, %59, %result_7 : f32
            %61 = arith.select %60, %59, %result_7 : f32
            %62 = arith.cmpf uno, %result_7, %result_7 : f32
            %63 = arith.select %62, %result_7, %61 : f32
            %64 = arith.cmpi eq, %26, %c0 : index
            scf.if %64 {
              %65 = "disc_shape.linearize"(%25, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %66 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %63, %66[%65] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %31 = memref.alloc() : memref<32xf32, 3>
          scf.if %29 {
            %32 = arith.subi %3, %26 : index
            %33 = arith.cmpi eq, %32, %c0 : index
            %34 = arith.subi %32, %c1 : index
            %35 = arith.divui %34, %c32 : index
            %36 = arith.addi %35, %c1 : index
            %37 = arith.select %33, %c0, %36 : index
            %38 = arith.remsi %37, %c4 : index
            %39 = arith.subi %37, %38 : index
            %40 = arith.muli %39, %c32 : index
            %41 = arith.addi %26, %40 : index
            %42 = scf.for %arg3 = %26 to %41 step %c128 iter_args(%arg4 = %cst) -> (f32) {
              %50 = arith.addi %arg3, %c32 : index
              %51 = arith.addi %arg3, %c64 : index
              %52 = arith.addi %arg3, %c96 : index
              %53 = "disc_shape.linearize"(%28, %arg3, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %54 = "disc_shape.linearize"(%28, %50, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %55 = "disc_shape.linearize"(%28, %51, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %56 = "disc_shape.linearize"(%28, %52, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %57:3 = "disc_shape.delinearize"(%53, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %58:3 = "disc_shape.delinearize"(%54, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %59:3 = "disc_shape.delinearize"(%55, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %60:3 = "disc_shape.delinearize"(%56, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %61 = arith.muli %2, %1 : index
              %62 = arith.muli %61, %3 : index
              %63 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%62], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %64 = memref.load %63[%53] : memref<?xf32, "gpu">
              %65 = memref.load %63[%54] : memref<?xf32, "gpu">
              %66 = memref.load %63[%55] : memref<?xf32, "gpu">
              %67 = memref.load %63[%56] : memref<?xf32, "gpu">
              %68 = "disc_shape.linearize"(%57#0, %57#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %69 = "disc_shape.linearize"(%58#0, %58#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %70 = "disc_shape.linearize"(%59#0, %59#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %71 = "disc_shape.linearize"(%60#0, %60#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %72 = arith.remui %68, %c8 : index
              %73 = arith.remui %69, %c8 : index
              %74 = arith.remui %70, %c8 : index
              %75 = arith.remui %71, %c8 : index
              %76 = "disc_shape.linearize"(%72, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %77 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %78 = memref.load %77[%76] : memref<32xf32, 3>
              %79 = "disc_shape.linearize"(%73, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %80 = memref.load %77[%79] : memref<32xf32, 3>
              %81 = "disc_shape.linearize"(%74, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %82 = memref.load %77[%81] : memref<32xf32, 3>
              %83 = "disc_shape.linearize"(%75, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %84 = memref.load %77[%83] : memref<32xf32, 3>
              %85 = arith.subf %64, %78 : f32
              %86 = arith.subf %65, %80 : f32
              %87 = arith.subf %66, %82 : f32
              %88 = arith.subf %67, %84 : f32
              %89 = math.exp %85 : f32
              %90 = math.exp %86 : f32
              %91 = math.exp %87 : f32
              %92 = math.exp %88 : f32
              %93 = arith.addf %arg4, %89 : f32
              %94 = arith.addf %93, %90 : f32
              %95 = arith.addf %94, %91 : f32
              %96 = arith.addf %95, %92 : f32
              scf.yield %96 : f32
            }
            %43 = scf.for %arg3 = %41 to %3 step %c32 iter_args(%arg4 = %42) -> (f32) {
              %50 = "disc_shape.linearize"(%28, %arg3, %7, %3) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %51:3 = "disc_shape.delinearize"(%50, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %52 = arith.muli %2, %1 : index
              %53 = arith.muli %52, %3 : index
              %54 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%53], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %55 = memref.load %54[%50] : memref<?xf32, "gpu">
              %56 = "disc_shape.linearize"(%51#0, %51#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %57 = arith.remui %56, %c8 : index
              %58 = "disc_shape.linearize"(%57, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %59 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %60 = memref.load %59[%58] : memref<32xf32, 3>
              %61 = arith.subf %55, %60 : f32
              %62 = math.exp %61 : f32
              %63 = arith.addf %arg4, %62 : f32
              scf.yield %63 : f32
            }
            %result, %valid = gpu.shuffle  xor %43, %c1_i32, %c32_i32 : f32
            %44 = arith.addf %43, %result : f32
            %result_1, %valid_2 = gpu.shuffle  xor %44, %c2_i32, %c32_i32 : f32
            %45 = arith.addf %44, %result_1 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %45, %c4_i32, %c32_i32 : f32
            %46 = arith.addf %45, %result_3 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %46, %c8_i32, %c32_i32 : f32
            %47 = arith.addf %46, %result_5 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %47, %c16_i32, %c32_i32 : f32
            %48 = arith.addf %47, %result_7 : f32
            %49 = arith.cmpi eq, %26, %c0 : index
            scf.if %49 {
              %50 = "disc_shape.linearize"(%25, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %51 = memref.reinterpret_cast %31 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %48, %51[%50] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          scf.if %29 {
            %32 = arith.subi %3, %26 : index
            %33 = arith.cmpi eq, %32, %c0 : index
            %34 = arith.subi %32, %c1 : index
            %35 = arith.divui %34, %c32 : index
            %36 = arith.addi %35, %c1 : index
            %37 = arith.select %33, %c0, %36 : index
            %38 = arith.remsi %37, %c4 : index
            %39 = arith.subi %37, %38 : index
            %40 = arith.muli %39, %c32 : index
            %41 = arith.addi %26, %40 : index
            scf.for %arg3 = %26 to %41 step %c128 {
              %42 = arith.addi %arg3, %c32 : index
              %43 = arith.addi %arg3, %c64 : index
              %44 = arith.addi %arg3, %c96 : index
              %45 = arith.muli %28, %3 : index
              %46 = arith.addi %45, %arg3 : index
              %47 = arith.addi %45, %42 : index
              %48 = arith.addi %45, %43 : index
              %49 = arith.addi %45, %44 : index
              %50 = arith.muli %2, %1 : index
              %51 = arith.muli %50, %3 : index
              %52:3 = "disc_shape.delinearize"(%46, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %53:3 = "disc_shape.delinearize"(%47, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %54:3 = "disc_shape.delinearize"(%48, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %55:3 = "disc_shape.delinearize"(%49, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %56 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%51], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %57 = memref.load %56[%46] : memref<?xf32, "gpu">
              %58 = memref.load %56[%47] : memref<?xf32, "gpu">
              %59 = memref.load %56[%48] : memref<?xf32, "gpu">
              %60 = memref.load %56[%49] : memref<?xf32, "gpu">
              %61 = "disc_shape.linearize"(%52#0, %52#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %62 = "disc_shape.linearize"(%53#0, %53#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %63 = "disc_shape.linearize"(%54#0, %54#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %64 = "disc_shape.linearize"(%55#0, %55#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %65 = arith.remui %61, %c8 : index
              %66 = arith.remui %62, %c8 : index
              %67 = arith.remui %63, %c8 : index
              %68 = arith.remui %64, %c8 : index
              %69 = "disc_shape.linearize"(%65, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %70 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %71 = memref.load %70[%69] : memref<32xf32, 3>
              %72 = "disc_shape.linearize"(%66, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %73 = memref.load %70[%72] : memref<32xf32, 3>
              %74 = "disc_shape.linearize"(%67, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %75 = memref.load %70[%74] : memref<32xf32, 3>
              %76 = "disc_shape.linearize"(%68, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %77 = memref.load %70[%76] : memref<32xf32, 3>
              %78 = arith.subf %57, %71 : f32
              %79 = arith.subf %58, %73 : f32
              %80 = arith.subf %59, %75 : f32
              %81 = arith.subf %60, %77 : f32
              %82 = math.exp %78 : f32
              %83 = math.exp %79 : f32
              %84 = math.exp %80 : f32
              %85 = math.exp %81 : f32
              %86 = memref.reinterpret_cast %31 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %87 = memref.load %86[%69] : memref<32xf32, 3>
              %88 = memref.load %86[%72] : memref<32xf32, 3>
              %89 = memref.load %86[%74] : memref<32xf32, 3>
              %90 = memref.load %86[%76] : memref<32xf32, 3>
              %91 = arith.divf %82, %87 : f32
              %92 = arith.divf %83, %88 : f32
              %93 = arith.divf %84, %89 : f32
              %94 = arith.divf %85, %90 : f32
              %95 = "disc_shape.linearize"(%46, %51) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %96 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%51], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %91, %96[%95] : memref<?xf32, "gpu">
              %97 = "disc_shape.linearize"(%47, %51) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              memref.store %92, %96[%97] : memref<?xf32, "gpu">
              %98 = "disc_shape.linearize"(%48, %51) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              memref.store %93, %96[%98] : memref<?xf32, "gpu">
              %99 = "disc_shape.linearize"(%49, %51) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              memref.store %94, %96[%99] : memref<?xf32, "gpu">
            }
            scf.for %arg3 = %41 to %3 step %c32 {
              %42 = arith.muli %28, %3 : index
              %43 = arith.addi %42, %arg3 : index
              %44 = arith.muli %2, %1 : index
              %45 = arith.muli %44, %3 : index
              %46:3 = "disc_shape.delinearize"(%43, %2, %1, %3) : (index, index, index, index) -> (index, index, index)
              %47 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%45], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %48 = memref.load %47[%43] : memref<?xf32, "gpu">
              %49 = "disc_shape.linearize"(%46#0, %46#1, %2, %1) {operand_segment_sizes = dense<2> : vector<2xi32>} : (index, index, index, index) -> index
              %50 = arith.remui %49, %c8 : index
              %51 = "disc_shape.linearize"(%50, %c32) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %52 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %53 = memref.load %52[%51] : memref<32xf32, 3>
              %54 = arith.subf %48, %53 : f32
              %55 = math.exp %54 : f32
              %56 = memref.reinterpret_cast %31 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %57 = memref.load %56[%51] : memref<32xf32, 3>
              %58 = arith.divf %55, %57 : f32
              %59 = "disc_shape.linearize"(%43, %45) {operand_segment_sizes = dense<1> : vector<2xi32>} : (index, index) -> index
              %60 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%45], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %58, %60[%59] : memref<?xf32, "gpu">
            }
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  "disc_ral.dispatch"(%arg0, %c0, %8) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ConvertShapeToStandardPass (disc-convert-shape-to-std) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c768 = arith.constant 768 : index
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant -0.000000e+00 : f32
  %cst_0 = arith.constant 0xFF800000 : f32
  %c8 = arith.constant 8 : index
  %c16_i32 = arith.constant 16 : i32
  %c8_i32 = arith.constant 8 : i32
  %c4_i32 = arith.constant 4 : i32
  %c2_i32 = arith.constant 2 : i32
  %c1_i32 = arith.constant 1 : i32
  %c32_i32 = arith.constant 32 : i32
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c1024 = arith.constant 1024 : index
  %c6 = arith.constant 6 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
  %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
  %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
  %4 = arith.index_cast %2 : index to i32
  %5 = arith.index_cast %1 : index to i32
  %6 = arith.muli %4, %5 : i32
  %7 = arith.index_cast %6 : i32 to index
  %8 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %9 = arith.cmpi slt, %7, %c3 : index
  %10 = arith.cmpi sge, %3, %c1024 : index
  %11 = arith.cmpi slt, %7, %c6 : index
  %12 = arith.andi %11, %10 : i1
  %13 = arith.cmpi sge, %3, %c512 : index
  %14 = arith.cmpi sge, %7, %c6 : index
  %15 = arith.andi %14, %13 : i1
  %16 = arith.ori %9, %12 : i1
  %17 = arith.ori %16, %15 : i1
  scf.if %17 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %18 = gpu.block_id  x
          %19 = gpu.thread_id  x
          %20 = memref.alloc() : memref<32xf32, 3>
          %21 = arith.divui %19, %c32 : index
          %22 = arith.remui %19, %c32 : index
          %23 = arith.subi %3, %19 : index
          %24 = arith.cmpi eq, %23, %c0 : index
          %25 = arith.subi %23, %c1 : index
          %26 = arith.divui %25, %c256 : index
          %27 = arith.addi %26, %c1 : index
          %28 = arith.select %24, %c0, %27 : index
          %29 = arith.remsi %28, %c4 : index
          %30 = arith.subi %28, %29 : index
          %31 = arith.muli %30, %c256 : index
          %32 = arith.addi %19, %31 : index
          %33 = scf.for %arg3 = %19 to %32 step %c1024 iter_args(%arg4 = %cst_0) -> (f32) {
            %67 = arith.addi %arg3, %c256 : index
            %68 = arith.addi %arg3, %c512 : index
            %69 = arith.addi %arg3, %c768 : index
            %c0_19 = arith.constant 0 : index
            %70 = arith.muli %18, %3 : index
            %71 = arith.addi %70, %arg3 : index
            %c0_20 = arith.constant 0 : index
            %72 = arith.muli %18, %3 : index
            %73 = arith.addi %72, %67 : index
            %c0_21 = arith.constant 0 : index
            %74 = arith.muli %18, %3 : index
            %75 = arith.addi %74, %68 : index
            %c0_22 = arith.constant 0 : index
            %76 = arith.muli %18, %3 : index
            %77 = arith.addi %76, %69 : index
            %78 = arith.muli %2, %1 : index
            %79 = arith.muli %78, %3 : index
            %80 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%79], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %81 = memref.load %80[%71] : memref<?xf32, "gpu">
            %82 = memref.load %80[%73] : memref<?xf32, "gpu">
            %83 = memref.load %80[%75] : memref<?xf32, "gpu">
            %84 = memref.load %80[%77] : memref<?xf32, "gpu">
            %85 = arith.cmpf ugt, %arg4, %81 : f32
            %86 = arith.select %85, %arg4, %81 : f32
            %87 = arith.cmpf uno, %81, %81 : f32
            %88 = arith.select %87, %81, %86 : f32
            %89 = arith.cmpf ugt, %88, %82 : f32
            %90 = arith.select %89, %88, %82 : f32
            %91 = arith.cmpf uno, %82, %82 : f32
            %92 = arith.select %91, %82, %90 : f32
            %93 = arith.cmpf ugt, %92, %83 : f32
            %94 = arith.select %93, %92, %83 : f32
            %95 = arith.cmpf uno, %83, %83 : f32
            %96 = arith.select %95, %83, %94 : f32
            %97 = arith.cmpf ugt, %96, %84 : f32
            %98 = arith.select %97, %96, %84 : f32
            %99 = arith.cmpf uno, %84, %84 : f32
            %100 = arith.select %99, %84, %98 : f32
            scf.yield %100 : f32
          }
          %34 = scf.for %arg3 = %32 to %3 step %c256 iter_args(%arg4 = %33) -> (f32) {
            %c0_19 = arith.constant 0 : index
            %67 = arith.muli %18, %3 : index
            %68 = arith.addi %67, %arg3 : index
            %69 = arith.muli %2, %1 : index
            %70 = arith.muli %69, %3 : index
            %71 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %72 = memref.load %71[%68] : memref<?xf32, "gpu">
            %73 = arith.cmpf ugt, %arg4, %72 : f32
            %74 = arith.select %73, %arg4, %72 : f32
            %75 = arith.cmpf uno, %72, %72 : f32
            %76 = arith.select %75, %72, %74 : f32
            scf.yield %76 : f32
          }
          %result, %valid = gpu.shuffle  xor %34, %c1_i32, %c32_i32 : f32
          %35 = arith.cmpf ugt, %34, %result : f32
          %36 = arith.select %35, %34, %result : f32
          %37 = arith.cmpf uno, %result, %result : f32
          %38 = arith.select %37, %result, %36 : f32
          %result_1, %valid_2 = gpu.shuffle  xor %38, %c2_i32, %c32_i32 : f32
          %39 = arith.cmpf ugt, %38, %result_1 : f32
          %40 = arith.select %39, %38, %result_1 : f32
          %41 = arith.cmpf uno, %result_1, %result_1 : f32
          %42 = arith.select %41, %result_1, %40 : f32
          %result_3, %valid_4 = gpu.shuffle  xor %42, %c4_i32, %c32_i32 : f32
          %43 = arith.cmpf ugt, %42, %result_3 : f32
          %44 = arith.select %43, %42, %result_3 : f32
          %45 = arith.cmpf uno, %result_3, %result_3 : f32
          %46 = arith.select %45, %result_3, %44 : f32
          %result_5, %valid_6 = gpu.shuffle  xor %46, %c8_i32, %c32_i32 : f32
          %47 = arith.cmpf ugt, %46, %result_5 : f32
          %48 = arith.select %47, %46, %result_5 : f32
          %49 = arith.cmpf uno, %result_5, %result_5 : f32
          %50 = arith.select %49, %result_5, %48 : f32
          %result_7, %valid_8 = gpu.shuffle  xor %50, %c16_i32, %c32_i32 : f32
          %51 = arith.cmpf ugt, %50, %result_7 : f32
          %52 = arith.select %51, %50, %result_7 : f32
          %53 = arith.cmpf uno, %result_7, %result_7 : f32
          %54 = arith.select %53, %result_7, %52 : f32
          %55 = memref.alloc() : memref<8xf32, 3>
          %56 = arith.cmpi eq, %22, %c0 : index
          scf.if %56 {
            %c0_19 = arith.constant 0 : index
            %67 = memref.reinterpret_cast %55 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
            memref.store %54, %67[%21] : memref<8xf32, 3>
          }
          gpu.barrier
          %57 = arith.cmpi slt, %19, %c32 : index
          scf.if %57 {
            %67 = arith.cmpi slt, %22, %c8 : index
            %68 = scf.if %67 -> (f32) {
              %c0_25 = arith.constant 0 : index
              %81 = memref.reinterpret_cast %55 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
              %82 = memref.load %81[%22] : memref<8xf32, 3>
              scf.yield %82 : f32
            } else {
              scf.yield %cst_0 : f32
            }
            %result_19, %valid_20 = gpu.shuffle  xor %68, %c1_i32, %c8_i32 : f32
            %69 = arith.cmpf ugt, %68, %result_19 : f32
            %70 = arith.select %69, %68, %result_19 : f32
            %71 = arith.cmpf uno, %result_19, %result_19 : f32
            %72 = arith.select %71, %result_19, %70 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %72, %c2_i32, %c8_i32 : f32
            %73 = arith.cmpf ugt, %72, %result_21 : f32
            %74 = arith.select %73, %72, %result_21 : f32
            %75 = arith.cmpf uno, %result_21, %result_21 : f32
            %76 = arith.select %75, %result_21, %74 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %76, %c4_i32, %c8_i32 : f32
            %77 = arith.cmpf ugt, %76, %result_23 : f32
            %78 = arith.select %77, %76, %result_23 : f32
            %79 = arith.cmpf uno, %result_23, %result_23 : f32
            %80 = arith.select %79, %result_23, %78 : f32
            scf.if %56 {
              %c0_25 = arith.constant 0 : index
              %81 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %80, %81[%c0] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %58 = memref.alloc() : memref<32xf32, 3>
          %59 = scf.for %arg3 = %19 to %32 step %c1024 iter_args(%arg4 = %cst) -> (f32) {
            %67 = arith.addi %arg3, %c256 : index
            %68 = arith.addi %arg3, %c512 : index
            %69 = arith.addi %arg3, %c768 : index
            %c0_19 = arith.constant 0 : index
            %70 = arith.muli %18, %3 : index
            %71 = arith.addi %70, %arg3 : index
            %c0_20 = arith.constant 0 : index
            %72 = arith.muli %18, %3 : index
            %73 = arith.addi %72, %67 : index
            %c0_21 = arith.constant 0 : index
            %74 = arith.muli %18, %3 : index
            %75 = arith.addi %74, %68 : index
            %c0_22 = arith.constant 0 : index
            %76 = arith.muli %18, %3 : index
            %77 = arith.addi %76, %69 : index
            %78 = arith.muli %2, %1 : index
            %79 = arith.muli %78, %3 : index
            %80 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%79], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %81 = memref.load %80[%71] : memref<?xf32, "gpu">
            %82 = memref.load %80[%73] : memref<?xf32, "gpu">
            %83 = memref.load %80[%75] : memref<?xf32, "gpu">
            %84 = memref.load %80[%77] : memref<?xf32, "gpu">
            %c0_23 = arith.constant 0 : index
            %85 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %86 = memref.load %85[%c0] : memref<32xf32, 3>
            %87 = arith.subf %81, %86 : f32
            %88 = arith.subf %82, %86 : f32
            %89 = arith.subf %83, %86 : f32
            %90 = arith.subf %84, %86 : f32
            %91 = math.exp %87 : f32
            %92 = math.exp %88 : f32
            %93 = math.exp %89 : f32
            %94 = math.exp %90 : f32
            %95 = arith.addf %arg4, %91 : f32
            %96 = arith.addf %95, %92 : f32
            %97 = arith.addf %96, %93 : f32
            %98 = arith.addf %97, %94 : f32
            scf.yield %98 : f32
          }
          %60 = scf.for %arg3 = %32 to %3 step %c256 iter_args(%arg4 = %59) -> (f32) {
            %c0_19 = arith.constant 0 : index
            %67 = arith.muli %18, %3 : index
            %68 = arith.addi %67, %arg3 : index
            %69 = arith.muli %2, %1 : index
            %70 = arith.muli %69, %3 : index
            %71 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %72 = memref.load %71[%68] : memref<?xf32, "gpu">
            %c0_20 = arith.constant 0 : index
            %73 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %74 = memref.load %73[%c0] : memref<32xf32, 3>
            %75 = arith.subf %72, %74 : f32
            %76 = math.exp %75 : f32
            %77 = arith.addf %arg4, %76 : f32
            scf.yield %77 : f32
          }
          %result_9, %valid_10 = gpu.shuffle  xor %60, %c1_i32, %c32_i32 : f32
          %61 = arith.addf %60, %result_9 : f32
          %result_11, %valid_12 = gpu.shuffle  xor %61, %c2_i32, %c32_i32 : f32
          %62 = arith.addf %61, %result_11 : f32
          %result_13, %valid_14 = gpu.shuffle  xor %62, %c4_i32, %c32_i32 : f32
          %63 = arith.addf %62, %result_13 : f32
          %result_15, %valid_16 = gpu.shuffle  xor %63, %c8_i32, %c32_i32 : f32
          %64 = arith.addf %63, %result_15 : f32
          %result_17, %valid_18 = gpu.shuffle  xor %64, %c16_i32, %c32_i32 : f32
          %65 = arith.addf %64, %result_17 : f32
          %66 = memref.alloc() : memref<8xf32, 3>
          scf.if %56 {
            %c0_19 = arith.constant 0 : index
            %67 = memref.reinterpret_cast %66 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
            memref.store %65, %67[%21] : memref<8xf32, 3>
          }
          gpu.barrier
          scf.if %57 {
            %67 = arith.cmpi slt, %22, %c8 : index
            %68 = scf.if %67 -> (f32) {
              %c0_25 = arith.constant 0 : index
              %72 = memref.reinterpret_cast %66 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
              %73 = memref.load %72[%22] : memref<8xf32, 3>
              scf.yield %73 : f32
            } else {
              scf.yield %cst : f32
            }
            %result_19, %valid_20 = gpu.shuffle  xor %68, %c1_i32, %c8_i32 : f32
            %69 = arith.addf %68, %result_19 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %69, %c2_i32, %c8_i32 : f32
            %70 = arith.addf %69, %result_21 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %70, %c4_i32, %c8_i32 : f32
            %71 = arith.addf %70, %result_23 : f32
            scf.if %56 {
              %c0_25 = arith.constant 0 : index
              %72 = memref.reinterpret_cast %58 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %71, %72[%c0] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          scf.for %arg3 = %19 to %32 step %c1024 {
            %67 = arith.addi %arg3, %c256 : index
            %68 = arith.addi %arg3, %c512 : index
            %69 = arith.addi %arg3, %c768 : index
            %70 = arith.muli %18, %3 : index
            %71 = arith.addi %70, %arg3 : index
            %72 = arith.addi %70, %67 : index
            %73 = arith.addi %70, %68 : index
            %74 = arith.addi %70, %69 : index
            %75 = arith.muli %2, %1 : index
            %76 = arith.muli %75, %3 : index
            %77 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%76], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %78 = memref.load %77[%71] : memref<?xf32, "gpu">
            %79 = memref.load %77[%72] : memref<?xf32, "gpu">
            %80 = memref.load %77[%73] : memref<?xf32, "gpu">
            %81 = memref.load %77[%74] : memref<?xf32, "gpu">
            %c0_19 = arith.constant 0 : index
            %82 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %83 = memref.load %82[%c0] : memref<32xf32, 3>
            %84 = arith.subf %78, %83 : f32
            %85 = arith.subf %79, %83 : f32
            %86 = arith.subf %80, %83 : f32
            %87 = arith.subf %81, %83 : f32
            %88 = math.exp %84 : f32
            %89 = math.exp %85 : f32
            %90 = math.exp %86 : f32
            %91 = math.exp %87 : f32
            %92 = memref.reinterpret_cast %58 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %93 = memref.load %92[%c0] : memref<32xf32, 3>
            %94 = arith.divf %88, %93 : f32
            %95 = arith.divf %89, %93 : f32
            %96 = arith.divf %90, %93 : f32
            %97 = arith.divf %91, %93 : f32
            %c0_20 = arith.constant 0 : index
            %98 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%76], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %94, %98[%71] : memref<?xf32, "gpu">
            %c0_21 = arith.constant 0 : index
            memref.store %95, %98[%72] : memref<?xf32, "gpu">
            %c0_22 = arith.constant 0 : index
            memref.store %96, %98[%73] : memref<?xf32, "gpu">
            %c0_23 = arith.constant 0 : index
            memref.store %97, %98[%74] : memref<?xf32, "gpu">
          }
          scf.for %arg3 = %32 to %3 step %c256 {
            %67 = arith.muli %18, %3 : index
            %68 = arith.addi %67, %arg3 : index
            %69 = arith.muli %2, %1 : index
            %70 = arith.muli %69, %3 : index
            %71 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %72 = memref.load %71[%68] : memref<?xf32, "gpu">
            %c0_19 = arith.constant 0 : index
            %73 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %74 = memref.load %73[%c0] : memref<32xf32, 3>
            %75 = arith.subf %72, %74 : f32
            %76 = math.exp %75 : f32
            %77 = memref.reinterpret_cast %58 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %78 = memref.load %77[%c0] : memref<32xf32, 3>
            %79 = arith.divf %76, %78 : f32
            %c0_20 = arith.constant 0 : index
            %80 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %79, %80[%68] : memref<?xf32, "gpu">
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      %18 = arith.cmpi eq, %7, %c0 : index
      %19 = arith.subi %7, %c1 : index
      %20 = arith.divui %19, %c8 : index
      %21 = arith.addi %20, %c1 : index
      %22 = arith.select %18, %c0, %21 : index
      scf.parallel (%arg1) = (%c0) to (%22) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %23 = gpu.block_id  x
          %24 = gpu.thread_id  x
          %25 = arith.divui %24, %c32 : index
          %26 = arith.remui %24, %c32 : index
          %27 = arith.muli %23, %c8 : index
          %28 = arith.addi %27, %25 : index
          %29 = arith.cmpi slt, %28, %7 : index
          %30 = memref.alloc() : memref<32xf32, 3>
          scf.if %29 {
            %32 = arith.subi %3, %26 : index
            %33 = arith.cmpi eq, %32, %c0 : index
            %34 = arith.subi %32, %c1 : index
            %35 = arith.divui %34, %c32 : index
            %36 = arith.addi %35, %c1 : index
            %37 = arith.select %33, %c0, %36 : index
            %38 = arith.remsi %37, %c4 : index
            %39 = arith.subi %37, %38 : index
            %40 = arith.muli %39, %c32 : index
            %41 = arith.addi %26, %40 : index
            %42 = scf.for %arg3 = %26 to %41 step %c128 iter_args(%arg4 = %cst_0) -> (f32) {
              %65 = arith.addi %arg3, %c32 : index
              %66 = arith.addi %arg3, %c64 : index
              %67 = arith.addi %arg3, %c96 : index
              %c0_9 = arith.constant 0 : index
              %68 = arith.muli %28, %3 : index
              %69 = arith.addi %68, %arg3 : index
              %c0_10 = arith.constant 0 : index
              %70 = arith.muli %28, %3 : index
              %71 = arith.addi %70, %65 : index
              %c0_11 = arith.constant 0 : index
              %72 = arith.muli %28, %3 : index
              %73 = arith.addi %72, %66 : index
              %c0_12 = arith.constant 0 : index
              %74 = arith.muli %28, %3 : index
              %75 = arith.addi %74, %67 : index
              %76 = arith.muli %2, %1 : index
              %77 = arith.muli %76, %3 : index
              %78 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%77], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %79 = memref.load %78[%69] : memref<?xf32, "gpu">
              %80 = memref.load %78[%71] : memref<?xf32, "gpu">
              %81 = memref.load %78[%73] : memref<?xf32, "gpu">
              %82 = memref.load %78[%75] : memref<?xf32, "gpu">
              %83 = arith.cmpf ugt, %arg4, %79 : f32
              %84 = arith.select %83, %arg4, %79 : f32
              %85 = arith.cmpf uno, %79, %79 : f32
              %86 = arith.select %85, %79, %84 : f32
              %87 = arith.cmpf ugt, %86, %80 : f32
              %88 = arith.select %87, %86, %80 : f32
              %89 = arith.cmpf uno, %80, %80 : f32
              %90 = arith.select %89, %80, %88 : f32
              %91 = arith.cmpf ugt, %90, %81 : f32
              %92 = arith.select %91, %90, %81 : f32
              %93 = arith.cmpf uno, %81, %81 : f32
              %94 = arith.select %93, %81, %92 : f32
              %95 = arith.cmpf ugt, %94, %82 : f32
              %96 = arith.select %95, %94, %82 : f32
              %97 = arith.cmpf uno, %82, %82 : f32
              %98 = arith.select %97, %82, %96 : f32
              scf.yield %98 : f32
            }
            %43 = scf.for %arg3 = %41 to %3 step %c32 iter_args(%arg4 = %42) -> (f32) {
              %c0_9 = arith.constant 0 : index
              %65 = arith.muli %28, %3 : index
              %66 = arith.addi %65, %arg3 : index
              %67 = arith.muli %2, %1 : index
              %68 = arith.muli %67, %3 : index
              %69 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%68], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %70 = memref.load %69[%66] : memref<?xf32, "gpu">
              %71 = arith.cmpf ugt, %arg4, %70 : f32
              %72 = arith.select %71, %arg4, %70 : f32
              %73 = arith.cmpf uno, %70, %70 : f32
              %74 = arith.select %73, %70, %72 : f32
              scf.yield %74 : f32
            }
            %result, %valid = gpu.shuffle  xor %43, %c1_i32, %c32_i32 : f32
            %44 = arith.cmpf ugt, %43, %result : f32
            %45 = arith.select %44, %43, %result : f32
            %46 = arith.cmpf uno, %result, %result : f32
            %47 = arith.select %46, %result, %45 : f32
            %result_1, %valid_2 = gpu.shuffle  xor %47, %c2_i32, %c32_i32 : f32
            %48 = arith.cmpf ugt, %47, %result_1 : f32
            %49 = arith.select %48, %47, %result_1 : f32
            %50 = arith.cmpf uno, %result_1, %result_1 : f32
            %51 = arith.select %50, %result_1, %49 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %51, %c4_i32, %c32_i32 : f32
            %52 = arith.cmpf ugt, %51, %result_3 : f32
            %53 = arith.select %52, %51, %result_3 : f32
            %54 = arith.cmpf uno, %result_3, %result_3 : f32
            %55 = arith.select %54, %result_3, %53 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %55, %c8_i32, %c32_i32 : f32
            %56 = arith.cmpf ugt, %55, %result_5 : f32
            %57 = arith.select %56, %55, %result_5 : f32
            %58 = arith.cmpf uno, %result_5, %result_5 : f32
            %59 = arith.select %58, %result_5, %57 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %59, %c16_i32, %c32_i32 : f32
            %60 = arith.cmpf ugt, %59, %result_7 : f32
            %61 = arith.select %60, %59, %result_7 : f32
            %62 = arith.cmpf uno, %result_7, %result_7 : f32
            %63 = arith.select %62, %result_7, %61 : f32
            %64 = arith.cmpi eq, %26, %c0 : index
            scf.if %64 {
              %c0_9 = arith.constant 0 : index
              %65 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %63, %65[%25] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %31 = memref.alloc() : memref<32xf32, 3>
          scf.if %29 {
            %32 = arith.subi %3, %26 : index
            %33 = arith.cmpi eq, %32, %c0 : index
            %34 = arith.subi %32, %c1 : index
            %35 = arith.divui %34, %c32 : index
            %36 = arith.addi %35, %c1 : index
            %37 = arith.select %33, %c0, %36 : index
            %38 = arith.remsi %37, %c4 : index
            %39 = arith.subi %37, %38 : index
            %40 = arith.muli %39, %c32 : index
            %41 = arith.addi %26, %40 : index
            %42 = scf.for %arg3 = %26 to %41 step %c128 iter_args(%arg4 = %cst) -> (f32) {
              %50 = arith.addi %arg3, %c32 : index
              %51 = arith.addi %arg3, %c64 : index
              %52 = arith.addi %arg3, %c96 : index
              %c0_9 = arith.constant 0 : index
              %53 = arith.muli %28, %3 : index
              %54 = arith.addi %53, %arg3 : index
              %c0_10 = arith.constant 0 : index
              %55 = arith.muli %28, %3 : index
              %56 = arith.addi %55, %50 : index
              %c0_11 = arith.constant 0 : index
              %57 = arith.muli %28, %3 : index
              %58 = arith.addi %57, %51 : index
              %c0_12 = arith.constant 0 : index
              %59 = arith.muli %28, %3 : index
              %60 = arith.addi %59, %52 : index
              %61 = arith.remui %54, %3 : index
              %62 = arith.divui %54, %3 : index
              %63 = arith.remui %62, %1 : index
              %64 = arith.divui %62, %1 : index
              %65 = arith.remui %56, %3 : index
              %66 = arith.divui %56, %3 : index
              %67 = arith.remui %66, %1 : index
              %68 = arith.divui %66, %1 : index
              %69 = arith.remui %58, %3 : index
              %70 = arith.divui %58, %3 : index
              %71 = arith.remui %70, %1 : index
              %72 = arith.divui %70, %1 : index
              %73 = arith.remui %60, %3 : index
              %74 = arith.divui %60, %3 : index
              %75 = arith.remui %74, %1 : index
              %76 = arith.divui %74, %1 : index
              %77 = arith.muli %2, %1 : index
              %78 = arith.muli %77, %3 : index
              %79 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%78], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %80 = memref.load %79[%54] : memref<?xf32, "gpu">
              %81 = memref.load %79[%56] : memref<?xf32, "gpu">
              %82 = memref.load %79[%58] : memref<?xf32, "gpu">
              %83 = memref.load %79[%60] : memref<?xf32, "gpu">
              %c0_13 = arith.constant 0 : index
              %84 = arith.muli %64, %1 : index
              %85 = arith.addi %84, %63 : index
              %c0_14 = arith.constant 0 : index
              %86 = arith.muli %68, %1 : index
              %87 = arith.addi %86, %67 : index
              %c0_15 = arith.constant 0 : index
              %88 = arith.muli %72, %1 : index
              %89 = arith.addi %88, %71 : index
              %c0_16 = arith.constant 0 : index
              %90 = arith.muli %76, %1 : index
              %91 = arith.addi %90, %75 : index
              %92 = arith.remui %85, %c8 : index
              %93 = arith.remui %87, %c8 : index
              %94 = arith.remui %89, %c8 : index
              %95 = arith.remui %91, %c8 : index
              %c0_17 = arith.constant 0 : index
              %96 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %97 = memref.load %96[%92] : memref<32xf32, 3>
              %c0_18 = arith.constant 0 : index
              %98 = memref.load %96[%93] : memref<32xf32, 3>
              %c0_19 = arith.constant 0 : index
              %99 = memref.load %96[%94] : memref<32xf32, 3>
              %c0_20 = arith.constant 0 : index
              %100 = memref.load %96[%95] : memref<32xf32, 3>
              %101 = arith.subf %80, %97 : f32
              %102 = arith.subf %81, %98 : f32
              %103 = arith.subf %82, %99 : f32
              %104 = arith.subf %83, %100 : f32
              %105 = math.exp %101 : f32
              %106 = math.exp %102 : f32
              %107 = math.exp %103 : f32
              %108 = math.exp %104 : f32
              %109 = arith.addf %arg4, %105 : f32
              %110 = arith.addf %109, %106 : f32
              %111 = arith.addf %110, %107 : f32
              %112 = arith.addf %111, %108 : f32
              scf.yield %112 : f32
            }
            %43 = scf.for %arg3 = %41 to %3 step %c32 iter_args(%arg4 = %42) -> (f32) {
              %c0_9 = arith.constant 0 : index
              %50 = arith.muli %28, %3 : index
              %51 = arith.addi %50, %arg3 : index
              %52 = arith.remui %51, %3 : index
              %53 = arith.divui %51, %3 : index
              %54 = arith.remui %53, %1 : index
              %55 = arith.divui %53, %1 : index
              %56 = arith.muli %2, %1 : index
              %57 = arith.muli %56, %3 : index
              %58 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%57], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %59 = memref.load %58[%51] : memref<?xf32, "gpu">
              %c0_10 = arith.constant 0 : index
              %60 = arith.muli %55, %1 : index
              %61 = arith.addi %60, %54 : index
              %62 = arith.remui %61, %c8 : index
              %c0_11 = arith.constant 0 : index
              %63 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %64 = memref.load %63[%62] : memref<32xf32, 3>
              %65 = arith.subf %59, %64 : f32
              %66 = math.exp %65 : f32
              %67 = arith.addf %arg4, %66 : f32
              scf.yield %67 : f32
            }
            %result, %valid = gpu.shuffle  xor %43, %c1_i32, %c32_i32 : f32
            %44 = arith.addf %43, %result : f32
            %result_1, %valid_2 = gpu.shuffle  xor %44, %c2_i32, %c32_i32 : f32
            %45 = arith.addf %44, %result_1 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %45, %c4_i32, %c32_i32 : f32
            %46 = arith.addf %45, %result_3 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %46, %c8_i32, %c32_i32 : f32
            %47 = arith.addf %46, %result_5 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %47, %c16_i32, %c32_i32 : f32
            %48 = arith.addf %47, %result_7 : f32
            %49 = arith.cmpi eq, %26, %c0 : index
            scf.if %49 {
              %c0_9 = arith.constant 0 : index
              %50 = memref.reinterpret_cast %31 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %48, %50[%25] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          scf.if %29 {
            %32 = arith.subi %3, %26 : index
            %33 = arith.cmpi eq, %32, %c0 : index
            %34 = arith.subi %32, %c1 : index
            %35 = arith.divui %34, %c32 : index
            %36 = arith.addi %35, %c1 : index
            %37 = arith.select %33, %c0, %36 : index
            %38 = arith.remsi %37, %c4 : index
            %39 = arith.subi %37, %38 : index
            %40 = arith.muli %39, %c32 : index
            %41 = arith.addi %26, %40 : index
            scf.for %arg3 = %26 to %41 step %c128 {
              %42 = arith.addi %arg3, %c32 : index
              %43 = arith.addi %arg3, %c64 : index
              %44 = arith.addi %arg3, %c96 : index
              %45 = arith.muli %28, %3 : index
              %46 = arith.addi %45, %arg3 : index
              %47 = arith.addi %45, %42 : index
              %48 = arith.addi %45, %43 : index
              %49 = arith.addi %45, %44 : index
              %50 = arith.muli %2, %1 : index
              %51 = arith.muli %50, %3 : index
              %52 = arith.remui %46, %3 : index
              %53 = arith.divui %46, %3 : index
              %54 = arith.remui %53, %1 : index
              %55 = arith.divui %53, %1 : index
              %56 = arith.remui %47, %3 : index
              %57 = arith.divui %47, %3 : index
              %58 = arith.remui %57, %1 : index
              %59 = arith.divui %57, %1 : index
              %60 = arith.remui %48, %3 : index
              %61 = arith.divui %48, %3 : index
              %62 = arith.remui %61, %1 : index
              %63 = arith.divui %61, %1 : index
              %64 = arith.remui %49, %3 : index
              %65 = arith.divui %49, %3 : index
              %66 = arith.remui %65, %1 : index
              %67 = arith.divui %65, %1 : index
              %68 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%51], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %69 = memref.load %68[%46] : memref<?xf32, "gpu">
              %70 = memref.load %68[%47] : memref<?xf32, "gpu">
              %71 = memref.load %68[%48] : memref<?xf32, "gpu">
              %72 = memref.load %68[%49] : memref<?xf32, "gpu">
              %c0_1 = arith.constant 0 : index
              %73 = arith.muli %55, %1 : index
              %74 = arith.addi %73, %54 : index
              %c0_2 = arith.constant 0 : index
              %75 = arith.muli %59, %1 : index
              %76 = arith.addi %75, %58 : index
              %c0_3 = arith.constant 0 : index
              %77 = arith.muli %63, %1 : index
              %78 = arith.addi %77, %62 : index
              %c0_4 = arith.constant 0 : index
              %79 = arith.muli %67, %1 : index
              %80 = arith.addi %79, %66 : index
              %81 = arith.remui %74, %c8 : index
              %82 = arith.remui %76, %c8 : index
              %83 = arith.remui %78, %c8 : index
              %84 = arith.remui %80, %c8 : index
              %c0_5 = arith.constant 0 : index
              %85 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %86 = memref.load %85[%81] : memref<32xf32, 3>
              %c0_6 = arith.constant 0 : index
              %87 = memref.load %85[%82] : memref<32xf32, 3>
              %c0_7 = arith.constant 0 : index
              %88 = memref.load %85[%83] : memref<32xf32, 3>
              %c0_8 = arith.constant 0 : index
              %89 = memref.load %85[%84] : memref<32xf32, 3>
              %90 = arith.subf %69, %86 : f32
              %91 = arith.subf %70, %87 : f32
              %92 = arith.subf %71, %88 : f32
              %93 = arith.subf %72, %89 : f32
              %94 = math.exp %90 : f32
              %95 = math.exp %91 : f32
              %96 = math.exp %92 : f32
              %97 = math.exp %93 : f32
              %98 = memref.reinterpret_cast %31 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %99 = memref.load %98[%81] : memref<32xf32, 3>
              %100 = memref.load %98[%82] : memref<32xf32, 3>
              %101 = memref.load %98[%83] : memref<32xf32, 3>
              %102 = memref.load %98[%84] : memref<32xf32, 3>
              %103 = arith.divf %94, %99 : f32
              %104 = arith.divf %95, %100 : f32
              %105 = arith.divf %96, %101 : f32
              %106 = arith.divf %97, %102 : f32
              %c0_9 = arith.constant 0 : index
              %107 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%51], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %103, %107[%46] : memref<?xf32, "gpu">
              %c0_10 = arith.constant 0 : index
              memref.store %104, %107[%47] : memref<?xf32, "gpu">
              %c0_11 = arith.constant 0 : index
              memref.store %105, %107[%48] : memref<?xf32, "gpu">
              %c0_12 = arith.constant 0 : index
              memref.store %106, %107[%49] : memref<?xf32, "gpu">
            }
            scf.for %arg3 = %41 to %3 step %c32 {
              %42 = arith.muli %28, %3 : index
              %43 = arith.addi %42, %arg3 : index
              %44 = arith.muli %2, %1 : index
              %45 = arith.muli %44, %3 : index
              %46 = arith.remui %43, %3 : index
              %47 = arith.divui %43, %3 : index
              %48 = arith.remui %47, %1 : index
              %49 = arith.divui %47, %1 : index
              %50 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%45], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %51 = memref.load %50[%43] : memref<?xf32, "gpu">
              %c0_1 = arith.constant 0 : index
              %52 = arith.muli %49, %1 : index
              %53 = arith.addi %52, %48 : index
              %54 = arith.remui %53, %c8 : index
              %c0_2 = arith.constant 0 : index
              %55 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %56 = memref.load %55[%54] : memref<32xf32, 3>
              %57 = arith.subf %51, %56 : f32
              %58 = math.exp %57 : f32
              %59 = memref.reinterpret_cast %31 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %60 = memref.load %59[%54] : memref<32xf32, 3>
              %61 = arith.divf %58, %60 : f32
              %c0_3 = arith.constant 0 : index
              %62 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%45], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %61, %62[%43] : memref<?xf32, "gpu">
            }
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  "disc_ral.dispatch"(%arg0, %c0, %8) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After Canonicalizer (disc-canonicalize) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c768 = arith.constant 768 : index
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant -0.000000e+00 : f32
  %cst_0 = arith.constant 0xFF800000 : f32
  %c8 = arith.constant 8 : index
  %c16_i32 = arith.constant 16 : i32
  %c8_i32 = arith.constant 8 : i32
  %c4_i32 = arith.constant 4 : i32
  %c2_i32 = arith.constant 2 : i32
  %c1_i32 = arith.constant 1 : i32
  %c32_i32 = arith.constant 32 : i32
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c1024 = arith.constant 1024 : index
  %c6 = arith.constant 6 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
  %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
  %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
  %4 = arith.index_cast %2 : index to i32
  %5 = arith.index_cast %1 : index to i32
  %6 = arith.muli %4, %5 : i32
  %7 = arith.index_cast %6 : i32 to index
  %8 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %9 = arith.cmpi slt, %7, %c3 : index
  %10 = arith.cmpi sge, %3, %c1024 : index
  %11 = arith.cmpi slt, %7, %c6 : index
  %12 = arith.andi %11, %10 : i1
  %13 = arith.cmpi sge, %3, %c512 : index
  %14 = arith.cmpi sge, %7, %c6 : index
  %15 = arith.andi %14, %13 : i1
  %16 = arith.ori %9, %12 : i1
  %17 = arith.ori %16, %15 : i1
  scf.if %17 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %18 = gpu.block_id  x
          %19 = gpu.thread_id  x
          %20 = memref.alloc() : memref<32xf32, 3>
          %21 = arith.divui %19, %c32 : index
          %22 = arith.remui %19, %c32 : index
          %23 = arith.subi %3, %19 : index
          %24 = arith.cmpi eq, %23, %c0 : index
          %25 = arith.subi %23, %c1 : index
          %26 = arith.divui %25, %c256 : index
          %27 = arith.addi %26, %c1 : index
          %28 = arith.select %24, %c0, %27 : index
          %29 = arith.remsi %28, %c4 : index
          %30 = arith.subi %28, %29 : index
          %31 = arith.muli %30, %c256 : index
          %32 = arith.addi %19, %31 : index
          %33 = scf.for %arg3 = %19 to %32 step %c1024 iter_args(%arg4 = %cst_0) -> (f32) {
            %67 = arith.addi %arg3, %c256 : index
            %68 = arith.addi %arg3, %c512 : index
            %69 = arith.addi %arg3, %c768 : index
            %70 = arith.muli %18, %3 : index
            %71 = arith.addi %70, %arg3 : index
            %72 = arith.muli %18, %3 : index
            %73 = arith.addi %72, %67 : index
            %74 = arith.muli %18, %3 : index
            %75 = arith.addi %74, %68 : index
            %76 = arith.muli %18, %3 : index
            %77 = arith.addi %76, %69 : index
            %78 = arith.muli %2, %1 : index
            %79 = arith.muli %78, %3 : index
            %80 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%79], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %81 = memref.load %80[%71] : memref<?xf32, "gpu">
            %82 = memref.load %80[%73] : memref<?xf32, "gpu">
            %83 = memref.load %80[%75] : memref<?xf32, "gpu">
            %84 = memref.load %80[%77] : memref<?xf32, "gpu">
            %85 = arith.cmpf ugt, %arg4, %81 : f32
            %86 = arith.select %85, %arg4, %81 : f32
            %87 = arith.cmpf uno, %81, %81 : f32
            %88 = arith.select %87, %81, %86 : f32
            %89 = arith.cmpf ugt, %88, %82 : f32
            %90 = arith.select %89, %88, %82 : f32
            %91 = arith.cmpf uno, %82, %82 : f32
            %92 = arith.select %91, %82, %90 : f32
            %93 = arith.cmpf ugt, %92, %83 : f32
            %94 = arith.select %93, %92, %83 : f32
            %95 = arith.cmpf uno, %83, %83 : f32
            %96 = arith.select %95, %83, %94 : f32
            %97 = arith.cmpf ugt, %96, %84 : f32
            %98 = arith.select %97, %96, %84 : f32
            %99 = arith.cmpf uno, %84, %84 : f32
            %100 = arith.select %99, %84, %98 : f32
            scf.yield %100 : f32
          }
          %34 = scf.for %arg3 = %32 to %3 step %c256 iter_args(%arg4 = %33) -> (f32) {
            %67 = arith.muli %18, %3 : index
            %68 = arith.addi %67, %arg3 : index
            %69 = arith.muli %2, %1 : index
            %70 = arith.muli %69, %3 : index
            %71 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %72 = memref.load %71[%68] : memref<?xf32, "gpu">
            %73 = arith.cmpf ugt, %arg4, %72 : f32
            %74 = arith.select %73, %arg4, %72 : f32
            %75 = arith.cmpf uno, %72, %72 : f32
            %76 = arith.select %75, %72, %74 : f32
            scf.yield %76 : f32
          }
          %result, %valid = gpu.shuffle  xor %34, %c1_i32, %c32_i32 : f32
          %35 = arith.cmpf ugt, %34, %result : f32
          %36 = arith.select %35, %34, %result : f32
          %37 = arith.cmpf uno, %result, %result : f32
          %38 = arith.select %37, %result, %36 : f32
          %result_1, %valid_2 = gpu.shuffle  xor %38, %c2_i32, %c32_i32 : f32
          %39 = arith.cmpf ugt, %38, %result_1 : f32
          %40 = arith.select %39, %38, %result_1 : f32
          %41 = arith.cmpf uno, %result_1, %result_1 : f32
          %42 = arith.select %41, %result_1, %40 : f32
          %result_3, %valid_4 = gpu.shuffle  xor %42, %c4_i32, %c32_i32 : f32
          %43 = arith.cmpf ugt, %42, %result_3 : f32
          %44 = arith.select %43, %42, %result_3 : f32
          %45 = arith.cmpf uno, %result_3, %result_3 : f32
          %46 = arith.select %45, %result_3, %44 : f32
          %result_5, %valid_6 = gpu.shuffle  xor %46, %c8_i32, %c32_i32 : f32
          %47 = arith.cmpf ugt, %46, %result_5 : f32
          %48 = arith.select %47, %46, %result_5 : f32
          %49 = arith.cmpf uno, %result_5, %result_5 : f32
          %50 = arith.select %49, %result_5, %48 : f32
          %result_7, %valid_8 = gpu.shuffle  xor %50, %c16_i32, %c32_i32 : f32
          %51 = arith.cmpf ugt, %50, %result_7 : f32
          %52 = arith.select %51, %50, %result_7 : f32
          %53 = arith.cmpf uno, %result_7, %result_7 : f32
          %54 = arith.select %53, %result_7, %52 : f32
          %55 = memref.alloc() : memref<8xf32, 3>
          %56 = arith.cmpi eq, %22, %c0 : index
          scf.if %56 {
            %67 = memref.reinterpret_cast %55 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
            memref.store %54, %67[%21] : memref<8xf32, 3>
          }
          gpu.barrier
          %57 = arith.cmpi slt, %19, %c32 : index
          scf.if %57 {
            %67 = arith.cmpi slt, %22, %c8 : index
            %68 = scf.if %67 -> (f32) {
              %81 = memref.reinterpret_cast %55 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
              %82 = memref.load %81[%22] : memref<8xf32, 3>
              scf.yield %82 : f32
            } else {
              scf.yield %cst_0 : f32
            }
            %result_19, %valid_20 = gpu.shuffle  xor %68, %c1_i32, %c8_i32 : f32
            %69 = arith.cmpf ugt, %68, %result_19 : f32
            %70 = arith.select %69, %68, %result_19 : f32
            %71 = arith.cmpf uno, %result_19, %result_19 : f32
            %72 = arith.select %71, %result_19, %70 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %72, %c2_i32, %c8_i32 : f32
            %73 = arith.cmpf ugt, %72, %result_21 : f32
            %74 = arith.select %73, %72, %result_21 : f32
            %75 = arith.cmpf uno, %result_21, %result_21 : f32
            %76 = arith.select %75, %result_21, %74 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %76, %c4_i32, %c8_i32 : f32
            %77 = arith.cmpf ugt, %76, %result_23 : f32
            %78 = arith.select %77, %76, %result_23 : f32
            %79 = arith.cmpf uno, %result_23, %result_23 : f32
            %80 = arith.select %79, %result_23, %78 : f32
            scf.if %56 {
              %81 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %80, %81[%c0] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %58 = memref.alloc() : memref<32xf32, 3>
          %59 = scf.for %arg3 = %19 to %32 step %c1024 iter_args(%arg4 = %cst) -> (f32) {
            %67 = arith.addi %arg3, %c256 : index
            %68 = arith.addi %arg3, %c512 : index
            %69 = arith.addi %arg3, %c768 : index
            %70 = arith.muli %18, %3 : index
            %71 = arith.addi %70, %arg3 : index
            %72 = arith.muli %18, %3 : index
            %73 = arith.addi %72, %67 : index
            %74 = arith.muli %18, %3 : index
            %75 = arith.addi %74, %68 : index
            %76 = arith.muli %18, %3 : index
            %77 = arith.addi %76, %69 : index
            %78 = arith.muli %2, %1 : index
            %79 = arith.muli %78, %3 : index
            %80 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%79], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %81 = memref.load %80[%71] : memref<?xf32, "gpu">
            %82 = memref.load %80[%73] : memref<?xf32, "gpu">
            %83 = memref.load %80[%75] : memref<?xf32, "gpu">
            %84 = memref.load %80[%77] : memref<?xf32, "gpu">
            %85 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %86 = memref.load %85[%c0] : memref<32xf32, 3>
            %87 = arith.subf %81, %86 : f32
            %88 = arith.subf %82, %86 : f32
            %89 = arith.subf %83, %86 : f32
            %90 = arith.subf %84, %86 : f32
            %91 = math.exp %87 : f32
            %92 = math.exp %88 : f32
            %93 = math.exp %89 : f32
            %94 = math.exp %90 : f32
            %95 = arith.addf %arg4, %91 : f32
            %96 = arith.addf %95, %92 : f32
            %97 = arith.addf %96, %93 : f32
            %98 = arith.addf %97, %94 : f32
            scf.yield %98 : f32
          }
          %60 = scf.for %arg3 = %32 to %3 step %c256 iter_args(%arg4 = %59) -> (f32) {
            %67 = arith.muli %18, %3 : index
            %68 = arith.addi %67, %arg3 : index
            %69 = arith.muli %2, %1 : index
            %70 = arith.muli %69, %3 : index
            %71 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %72 = memref.load %71[%68] : memref<?xf32, "gpu">
            %73 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %74 = memref.load %73[%c0] : memref<32xf32, 3>
            %75 = arith.subf %72, %74 : f32
            %76 = math.exp %75 : f32
            %77 = arith.addf %arg4, %76 : f32
            scf.yield %77 : f32
          }
          %result_9, %valid_10 = gpu.shuffle  xor %60, %c1_i32, %c32_i32 : f32
          %61 = arith.addf %60, %result_9 : f32
          %result_11, %valid_12 = gpu.shuffle  xor %61, %c2_i32, %c32_i32 : f32
          %62 = arith.addf %61, %result_11 : f32
          %result_13, %valid_14 = gpu.shuffle  xor %62, %c4_i32, %c32_i32 : f32
          %63 = arith.addf %62, %result_13 : f32
          %result_15, %valid_16 = gpu.shuffle  xor %63, %c8_i32, %c32_i32 : f32
          %64 = arith.addf %63, %result_15 : f32
          %result_17, %valid_18 = gpu.shuffle  xor %64, %c16_i32, %c32_i32 : f32
          %65 = arith.addf %64, %result_17 : f32
          %66 = memref.alloc() : memref<8xf32, 3>
          scf.if %56 {
            %67 = memref.reinterpret_cast %66 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
            memref.store %65, %67[%21] : memref<8xf32, 3>
          }
          gpu.barrier
          scf.if %57 {
            %67 = arith.cmpi slt, %22, %c8 : index
            %68 = scf.if %67 -> (f32) {
              %72 = memref.reinterpret_cast %66 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
              %73 = memref.load %72[%22] : memref<8xf32, 3>
              scf.yield %73 : f32
            } else {
              scf.yield %cst : f32
            }
            %result_19, %valid_20 = gpu.shuffle  xor %68, %c1_i32, %c8_i32 : f32
            %69 = arith.addf %68, %result_19 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %69, %c2_i32, %c8_i32 : f32
            %70 = arith.addf %69, %result_21 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %70, %c4_i32, %c8_i32 : f32
            %71 = arith.addf %70, %result_23 : f32
            scf.if %56 {
              %72 = memref.reinterpret_cast %58 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %71, %72[%c0] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          scf.for %arg3 = %19 to %32 step %c1024 {
            %67 = arith.addi %arg3, %c256 : index
            %68 = arith.addi %arg3, %c512 : index
            %69 = arith.addi %arg3, %c768 : index
            %70 = arith.muli %18, %3 : index
            %71 = arith.addi %70, %arg3 : index
            %72 = arith.addi %70, %67 : index
            %73 = arith.addi %70, %68 : index
            %74 = arith.addi %70, %69 : index
            %75 = arith.muli %2, %1 : index
            %76 = arith.muli %75, %3 : index
            %77 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%76], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %78 = memref.load %77[%71] : memref<?xf32, "gpu">
            %79 = memref.load %77[%72] : memref<?xf32, "gpu">
            %80 = memref.load %77[%73] : memref<?xf32, "gpu">
            %81 = memref.load %77[%74] : memref<?xf32, "gpu">
            %82 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %83 = memref.load %82[%c0] : memref<32xf32, 3>
            %84 = arith.subf %78, %83 : f32
            %85 = arith.subf %79, %83 : f32
            %86 = arith.subf %80, %83 : f32
            %87 = arith.subf %81, %83 : f32
            %88 = math.exp %84 : f32
            %89 = math.exp %85 : f32
            %90 = math.exp %86 : f32
            %91 = math.exp %87 : f32
            %92 = memref.reinterpret_cast %58 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %93 = memref.load %92[%c0] : memref<32xf32, 3>
            %94 = arith.divf %88, %93 : f32
            %95 = arith.divf %89, %93 : f32
            %96 = arith.divf %90, %93 : f32
            %97 = arith.divf %91, %93 : f32
            %98 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%76], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %94, %98[%71] : memref<?xf32, "gpu">
            memref.store %95, %98[%72] : memref<?xf32, "gpu">
            memref.store %96, %98[%73] : memref<?xf32, "gpu">
            memref.store %97, %98[%74] : memref<?xf32, "gpu">
          }
          scf.for %arg3 = %32 to %3 step %c256 {
            %67 = arith.muli %18, %3 : index
            %68 = arith.addi %67, %arg3 : index
            %69 = arith.muli %2, %1 : index
            %70 = arith.muli %69, %3 : index
            %71 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %72 = memref.load %71[%68] : memref<?xf32, "gpu">
            %73 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %74 = memref.load %73[%c0] : memref<32xf32, 3>
            %75 = arith.subf %72, %74 : f32
            %76 = math.exp %75 : f32
            %77 = memref.reinterpret_cast %58 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %78 = memref.load %77[%c0] : memref<32xf32, 3>
            %79 = arith.divf %76, %78 : f32
            %80 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %79, %80[%68] : memref<?xf32, "gpu">
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      %18 = arith.cmpi eq, %7, %c0 : index
      %19 = arith.subi %7, %c1 : index
      %20 = arith.divui %19, %c8 : index
      %21 = arith.addi %20, %c1 : index
      %22 = arith.select %18, %c0, %21 : index
      scf.parallel (%arg1) = (%c0) to (%22) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %23 = gpu.block_id  x
          %24 = gpu.thread_id  x
          %25 = arith.divui %24, %c32 : index
          %26 = arith.remui %24, %c32 : index
          %27 = arith.muli %23, %c8 : index
          %28 = arith.addi %27, %25 : index
          %29 = arith.cmpi slt, %28, %7 : index
          %30 = memref.alloc() : memref<32xf32, 3>
          scf.if %29 {
            %32 = arith.subi %3, %26 : index
            %33 = arith.cmpi eq, %32, %c0 : index
            %34 = arith.subi %32, %c1 : index
            %35 = arith.divui %34, %c32 : index
            %36 = arith.addi %35, %c1 : index
            %37 = arith.select %33, %c0, %36 : index
            %38 = arith.remsi %37, %c4 : index
            %39 = arith.subi %37, %38 : index
            %40 = arith.muli %39, %c32 : index
            %41 = arith.addi %26, %40 : index
            %42 = scf.for %arg3 = %26 to %41 step %c128 iter_args(%arg4 = %cst_0) -> (f32) {
              %65 = arith.addi %arg3, %c32 : index
              %66 = arith.addi %arg3, %c64 : index
              %67 = arith.addi %arg3, %c96 : index
              %68 = arith.muli %28, %3 : index
              %69 = arith.addi %68, %arg3 : index
              %70 = arith.muli %28, %3 : index
              %71 = arith.addi %70, %65 : index
              %72 = arith.muli %28, %3 : index
              %73 = arith.addi %72, %66 : index
              %74 = arith.muli %28, %3 : index
              %75 = arith.addi %74, %67 : index
              %76 = arith.muli %2, %1 : index
              %77 = arith.muli %76, %3 : index
              %78 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%77], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %79 = memref.load %78[%69] : memref<?xf32, "gpu">
              %80 = memref.load %78[%71] : memref<?xf32, "gpu">
              %81 = memref.load %78[%73] : memref<?xf32, "gpu">
              %82 = memref.load %78[%75] : memref<?xf32, "gpu">
              %83 = arith.cmpf ugt, %arg4, %79 : f32
              %84 = arith.select %83, %arg4, %79 : f32
              %85 = arith.cmpf uno, %79, %79 : f32
              %86 = arith.select %85, %79, %84 : f32
              %87 = arith.cmpf ugt, %86, %80 : f32
              %88 = arith.select %87, %86, %80 : f32
              %89 = arith.cmpf uno, %80, %80 : f32
              %90 = arith.select %89, %80, %88 : f32
              %91 = arith.cmpf ugt, %90, %81 : f32
              %92 = arith.select %91, %90, %81 : f32
              %93 = arith.cmpf uno, %81, %81 : f32
              %94 = arith.select %93, %81, %92 : f32
              %95 = arith.cmpf ugt, %94, %82 : f32
              %96 = arith.select %95, %94, %82 : f32
              %97 = arith.cmpf uno, %82, %82 : f32
              %98 = arith.select %97, %82, %96 : f32
              scf.yield %98 : f32
            }
            %43 = scf.for %arg3 = %41 to %3 step %c32 iter_args(%arg4 = %42) -> (f32) {
              %65 = arith.muli %28, %3 : index
              %66 = arith.addi %65, %arg3 : index
              %67 = arith.muli %2, %1 : index
              %68 = arith.muli %67, %3 : index
              %69 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%68], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %70 = memref.load %69[%66] : memref<?xf32, "gpu">
              %71 = arith.cmpf ugt, %arg4, %70 : f32
              %72 = arith.select %71, %arg4, %70 : f32
              %73 = arith.cmpf uno, %70, %70 : f32
              %74 = arith.select %73, %70, %72 : f32
              scf.yield %74 : f32
            }
            %result, %valid = gpu.shuffle  xor %43, %c1_i32, %c32_i32 : f32
            %44 = arith.cmpf ugt, %43, %result : f32
            %45 = arith.select %44, %43, %result : f32
            %46 = arith.cmpf uno, %result, %result : f32
            %47 = arith.select %46, %result, %45 : f32
            %result_1, %valid_2 = gpu.shuffle  xor %47, %c2_i32, %c32_i32 : f32
            %48 = arith.cmpf ugt, %47, %result_1 : f32
            %49 = arith.select %48, %47, %result_1 : f32
            %50 = arith.cmpf uno, %result_1, %result_1 : f32
            %51 = arith.select %50, %result_1, %49 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %51, %c4_i32, %c32_i32 : f32
            %52 = arith.cmpf ugt, %51, %result_3 : f32
            %53 = arith.select %52, %51, %result_3 : f32
            %54 = arith.cmpf uno, %result_3, %result_3 : f32
            %55 = arith.select %54, %result_3, %53 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %55, %c8_i32, %c32_i32 : f32
            %56 = arith.cmpf ugt, %55, %result_5 : f32
            %57 = arith.select %56, %55, %result_5 : f32
            %58 = arith.cmpf uno, %result_5, %result_5 : f32
            %59 = arith.select %58, %result_5, %57 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %59, %c16_i32, %c32_i32 : f32
            %60 = arith.cmpf ugt, %59, %result_7 : f32
            %61 = arith.select %60, %59, %result_7 : f32
            %62 = arith.cmpf uno, %result_7, %result_7 : f32
            %63 = arith.select %62, %result_7, %61 : f32
            %64 = arith.cmpi eq, %26, %c0 : index
            scf.if %64 {
              %65 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %63, %65[%25] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %31 = memref.alloc() : memref<32xf32, 3>
          scf.if %29 {
            %32 = arith.subi %3, %26 : index
            %33 = arith.cmpi eq, %32, %c0 : index
            %34 = arith.subi %32, %c1 : index
            %35 = arith.divui %34, %c32 : index
            %36 = arith.addi %35, %c1 : index
            %37 = arith.select %33, %c0, %36 : index
            %38 = arith.remsi %37, %c4 : index
            %39 = arith.subi %37, %38 : index
            %40 = arith.muli %39, %c32 : index
            %41 = arith.addi %26, %40 : index
            %42 = scf.for %arg3 = %26 to %41 step %c128 iter_args(%arg4 = %cst) -> (f32) {
              %50 = arith.addi %arg3, %c32 : index
              %51 = arith.addi %arg3, %c64 : index
              %52 = arith.addi %arg3, %c96 : index
              %53 = arith.muli %28, %3 : index
              %54 = arith.addi %53, %arg3 : index
              %55 = arith.muli %28, %3 : index
              %56 = arith.addi %55, %50 : index
              %57 = arith.muli %28, %3 : index
              %58 = arith.addi %57, %51 : index
              %59 = arith.muli %28, %3 : index
              %60 = arith.addi %59, %52 : index
              %61 = arith.divui %54, %3 : index
              %62 = arith.remui %61, %1 : index
              %63 = arith.divui %61, %1 : index
              %64 = arith.divui %56, %3 : index
              %65 = arith.remui %64, %1 : index
              %66 = arith.divui %64, %1 : index
              %67 = arith.divui %58, %3 : index
              %68 = arith.remui %67, %1 : index
              %69 = arith.divui %67, %1 : index
              %70 = arith.divui %60, %3 : index
              %71 = arith.remui %70, %1 : index
              %72 = arith.divui %70, %1 : index
              %73 = arith.muli %2, %1 : index
              %74 = arith.muli %73, %3 : index
              %75 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%74], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %76 = memref.load %75[%54] : memref<?xf32, "gpu">
              %77 = memref.load %75[%56] : memref<?xf32, "gpu">
              %78 = memref.load %75[%58] : memref<?xf32, "gpu">
              %79 = memref.load %75[%60] : memref<?xf32, "gpu">
              %80 = arith.muli %63, %1 : index
              %81 = arith.addi %80, %62 : index
              %82 = arith.muli %66, %1 : index
              %83 = arith.addi %82, %65 : index
              %84 = arith.muli %69, %1 : index
              %85 = arith.addi %84, %68 : index
              %86 = arith.muli %72, %1 : index
              %87 = arith.addi %86, %71 : index
              %88 = arith.remui %81, %c8 : index
              %89 = arith.remui %83, %c8 : index
              %90 = arith.remui %85, %c8 : index
              %91 = arith.remui %87, %c8 : index
              %92 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %93 = memref.load %92[%88] : memref<32xf32, 3>
              %94 = memref.load %92[%89] : memref<32xf32, 3>
              %95 = memref.load %92[%90] : memref<32xf32, 3>
              %96 = memref.load %92[%91] : memref<32xf32, 3>
              %97 = arith.subf %76, %93 : f32
              %98 = arith.subf %77, %94 : f32
              %99 = arith.subf %78, %95 : f32
              %100 = arith.subf %79, %96 : f32
              %101 = math.exp %97 : f32
              %102 = math.exp %98 : f32
              %103 = math.exp %99 : f32
              %104 = math.exp %100 : f32
              %105 = arith.addf %arg4, %101 : f32
              %106 = arith.addf %105, %102 : f32
              %107 = arith.addf %106, %103 : f32
              %108 = arith.addf %107, %104 : f32
              scf.yield %108 : f32
            }
            %43 = scf.for %arg3 = %41 to %3 step %c32 iter_args(%arg4 = %42) -> (f32) {
              %50 = arith.muli %28, %3 : index
              %51 = arith.addi %50, %arg3 : index
              %52 = arith.divui %51, %3 : index
              %53 = arith.remui %52, %1 : index
              %54 = arith.divui %52, %1 : index
              %55 = arith.muli %2, %1 : index
              %56 = arith.muli %55, %3 : index
              %57 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%56], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %58 = memref.load %57[%51] : memref<?xf32, "gpu">
              %59 = arith.muli %54, %1 : index
              %60 = arith.addi %59, %53 : index
              %61 = arith.remui %60, %c8 : index
              %62 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %63 = memref.load %62[%61] : memref<32xf32, 3>
              %64 = arith.subf %58, %63 : f32
              %65 = math.exp %64 : f32
              %66 = arith.addf %arg4, %65 : f32
              scf.yield %66 : f32
            }
            %result, %valid = gpu.shuffle  xor %43, %c1_i32, %c32_i32 : f32
            %44 = arith.addf %43, %result : f32
            %result_1, %valid_2 = gpu.shuffle  xor %44, %c2_i32, %c32_i32 : f32
            %45 = arith.addf %44, %result_1 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %45, %c4_i32, %c32_i32 : f32
            %46 = arith.addf %45, %result_3 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %46, %c8_i32, %c32_i32 : f32
            %47 = arith.addf %46, %result_5 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %47, %c16_i32, %c32_i32 : f32
            %48 = arith.addf %47, %result_7 : f32
            %49 = arith.cmpi eq, %26, %c0 : index
            scf.if %49 {
              %50 = memref.reinterpret_cast %31 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %48, %50[%25] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          scf.if %29 {
            %32 = arith.subi %3, %26 : index
            %33 = arith.cmpi eq, %32, %c0 : index
            %34 = arith.subi %32, %c1 : index
            %35 = arith.divui %34, %c32 : index
            %36 = arith.addi %35, %c1 : index
            %37 = arith.select %33, %c0, %36 : index
            %38 = arith.remsi %37, %c4 : index
            %39 = arith.subi %37, %38 : index
            %40 = arith.muli %39, %c32 : index
            %41 = arith.addi %26, %40 : index
            scf.for %arg3 = %26 to %41 step %c128 {
              %42 = arith.addi %arg3, %c32 : index
              %43 = arith.addi %arg3, %c64 : index
              %44 = arith.addi %arg3, %c96 : index
              %45 = arith.muli %28, %3 : index
              %46 = arith.addi %45, %arg3 : index
              %47 = arith.addi %45, %42 : index
              %48 = arith.addi %45, %43 : index
              %49 = arith.addi %45, %44 : index
              %50 = arith.muli %2, %1 : index
              %51 = arith.muli %50, %3 : index
              %52 = arith.divui %46, %3 : index
              %53 = arith.remui %52, %1 : index
              %54 = arith.divui %52, %1 : index
              %55 = arith.divui %47, %3 : index
              %56 = arith.remui %55, %1 : index
              %57 = arith.divui %55, %1 : index
              %58 = arith.divui %48, %3 : index
              %59 = arith.remui %58, %1 : index
              %60 = arith.divui %58, %1 : index
              %61 = arith.divui %49, %3 : index
              %62 = arith.remui %61, %1 : index
              %63 = arith.divui %61, %1 : index
              %64 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%51], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %65 = memref.load %64[%46] : memref<?xf32, "gpu">
              %66 = memref.load %64[%47] : memref<?xf32, "gpu">
              %67 = memref.load %64[%48] : memref<?xf32, "gpu">
              %68 = memref.load %64[%49] : memref<?xf32, "gpu">
              %69 = arith.muli %54, %1 : index
              %70 = arith.addi %69, %53 : index
              %71 = arith.muli %57, %1 : index
              %72 = arith.addi %71, %56 : index
              %73 = arith.muli %60, %1 : index
              %74 = arith.addi %73, %59 : index
              %75 = arith.muli %63, %1 : index
              %76 = arith.addi %75, %62 : index
              %77 = arith.remui %70, %c8 : index
              %78 = arith.remui %72, %c8 : index
              %79 = arith.remui %74, %c8 : index
              %80 = arith.remui %76, %c8 : index
              %81 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %82 = memref.load %81[%77] : memref<32xf32, 3>
              %83 = memref.load %81[%78] : memref<32xf32, 3>
              %84 = memref.load %81[%79] : memref<32xf32, 3>
              %85 = memref.load %81[%80] : memref<32xf32, 3>
              %86 = arith.subf %65, %82 : f32
              %87 = arith.subf %66, %83 : f32
              %88 = arith.subf %67, %84 : f32
              %89 = arith.subf %68, %85 : f32
              %90 = math.exp %86 : f32
              %91 = math.exp %87 : f32
              %92 = math.exp %88 : f32
              %93 = math.exp %89 : f32
              %94 = memref.reinterpret_cast %31 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %95 = memref.load %94[%77] : memref<32xf32, 3>
              %96 = memref.load %94[%78] : memref<32xf32, 3>
              %97 = memref.load %94[%79] : memref<32xf32, 3>
              %98 = memref.load %94[%80] : memref<32xf32, 3>
              %99 = arith.divf %90, %95 : f32
              %100 = arith.divf %91, %96 : f32
              %101 = arith.divf %92, %97 : f32
              %102 = arith.divf %93, %98 : f32
              %103 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%51], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %99, %103[%46] : memref<?xf32, "gpu">
              memref.store %100, %103[%47] : memref<?xf32, "gpu">
              memref.store %101, %103[%48] : memref<?xf32, "gpu">
              memref.store %102, %103[%49] : memref<?xf32, "gpu">
            }
            scf.for %arg3 = %41 to %3 step %c32 {
              %42 = arith.muli %28, %3 : index
              %43 = arith.addi %42, %arg3 : index
              %44 = arith.muli %2, %1 : index
              %45 = arith.muli %44, %3 : index
              %46 = arith.divui %43, %3 : index
              %47 = arith.remui %46, %1 : index
              %48 = arith.divui %46, %1 : index
              %49 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%45], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %50 = memref.load %49[%43] : memref<?xf32, "gpu">
              %51 = arith.muli %48, %1 : index
              %52 = arith.addi %51, %47 : index
              %53 = arith.remui %52, %c8 : index
              %54 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %55 = memref.load %54[%53] : memref<32xf32, 3>
              %56 = arith.subf %50, %55 : f32
              %57 = math.exp %56 : f32
              %58 = memref.reinterpret_cast %31 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %59 = memref.load %58[%53] : memref<32xf32, 3>
              %60 = arith.divf %57, %59 : f32
              %61 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%45], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %60, %61[%43] : memref<?xf32, "gpu">
            }
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  "disc_ral.dispatch"(%arg0, %c0, %8) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c768 = arith.constant 768 : index
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant -0.000000e+00 : f32
  %cst_0 = arith.constant 0xFF800000 : f32
  %c8 = arith.constant 8 : index
  %c16_i32 = arith.constant 16 : i32
  %c8_i32 = arith.constant 8 : i32
  %c4_i32 = arith.constant 4 : i32
  %c2_i32 = arith.constant 2 : i32
  %c1_i32 = arith.constant 1 : i32
  %c32_i32 = arith.constant 32 : i32
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c1024 = arith.constant 1024 : index
  %c6 = arith.constant 6 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
  %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
  %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
  %4 = arith.index_cast %2 : index to i32
  %5 = arith.index_cast %1 : index to i32
  %6 = arith.muli %4, %5 : i32
  %7 = arith.index_cast %6 : i32 to index
  %8 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %9 = arith.cmpi slt, %7, %c3 : index
  %10 = arith.cmpi sge, %3, %c1024 : index
  %11 = arith.cmpi slt, %7, %c6 : index
  %12 = arith.andi %11, %10 : i1
  %13 = arith.cmpi sge, %3, %c512 : index
  %14 = arith.cmpi sge, %7, %c6 : index
  %15 = arith.andi %14, %13 : i1
  %16 = arith.ori %9, %12 : i1
  %17 = arith.ori %16, %15 : i1
  scf.if %17 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %18 = gpu.block_id  x
          %19 = gpu.thread_id  x
          %20 = memref.alloc() : memref<32xf32, 3>
          %21 = arith.divui %19, %c32 : index
          %22 = arith.remui %19, %c32 : index
          %23 = arith.subi %3, %19 : index
          %24 = arith.cmpi eq, %23, %c0 : index
          %25 = arith.subi %23, %c1 : index
          %26 = arith.divui %25, %c256 : index
          %27 = arith.addi %26, %c1 : index
          %28 = arith.select %24, %c0, %27 : index
          %29 = arith.remsi %28, %c4 : index
          %30 = arith.subi %28, %29 : index
          %31 = arith.muli %30, %c256 : index
          %32 = arith.addi %19, %31 : index
          %33 = scf.for %arg3 = %19 to %32 step %c1024 iter_args(%arg4 = %cst_0) -> (f32) {
            %67 = arith.addi %arg3, %c256 : index
            %68 = arith.addi %arg3, %c512 : index
            %69 = arith.addi %arg3, %c768 : index
            %70 = arith.muli %18, %3 : index
            %71 = arith.addi %70, %arg3 : index
            %72 = arith.addi %70, %67 : index
            %73 = arith.addi %70, %68 : index
            %74 = arith.addi %70, %69 : index
            %75 = arith.muli %2, %1 : index
            %76 = arith.muli %75, %3 : index
            %77 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%76], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %78 = memref.load %77[%71] : memref<?xf32, "gpu">
            %79 = memref.load %77[%72] : memref<?xf32, "gpu">
            %80 = memref.load %77[%73] : memref<?xf32, "gpu">
            %81 = memref.load %77[%74] : memref<?xf32, "gpu">
            %82 = arith.cmpf ugt, %arg4, %78 : f32
            %83 = arith.select %82, %arg4, %78 : f32
            %84 = arith.cmpf uno, %78, %78 : f32
            %85 = arith.select %84, %78, %83 : f32
            %86 = arith.cmpf ugt, %85, %79 : f32
            %87 = arith.select %86, %85, %79 : f32
            %88 = arith.cmpf uno, %79, %79 : f32
            %89 = arith.select %88, %79, %87 : f32
            %90 = arith.cmpf ugt, %89, %80 : f32
            %91 = arith.select %90, %89, %80 : f32
            %92 = arith.cmpf uno, %80, %80 : f32
            %93 = arith.select %92, %80, %91 : f32
            %94 = arith.cmpf ugt, %93, %81 : f32
            %95 = arith.select %94, %93, %81 : f32
            %96 = arith.cmpf uno, %81, %81 : f32
            %97 = arith.select %96, %81, %95 : f32
            scf.yield %97 : f32
          }
          %34 = scf.for %arg3 = %32 to %3 step %c256 iter_args(%arg4 = %33) -> (f32) {
            %67 = arith.muli %18, %3 : index
            %68 = arith.addi %67, %arg3 : index
            %69 = arith.muli %2, %1 : index
            %70 = arith.muli %69, %3 : index
            %71 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %72 = memref.load %71[%68] : memref<?xf32, "gpu">
            %73 = arith.cmpf ugt, %arg4, %72 : f32
            %74 = arith.select %73, %arg4, %72 : f32
            %75 = arith.cmpf uno, %72, %72 : f32
            %76 = arith.select %75, %72, %74 : f32
            scf.yield %76 : f32
          }
          %result, %valid = gpu.shuffle  xor %34, %c1_i32, %c32_i32 : f32
          %35 = arith.cmpf ugt, %34, %result : f32
          %36 = arith.select %35, %34, %result : f32
          %37 = arith.cmpf uno, %result, %result : f32
          %38 = arith.select %37, %result, %36 : f32
          %result_1, %valid_2 = gpu.shuffle  xor %38, %c2_i32, %c32_i32 : f32
          %39 = arith.cmpf ugt, %38, %result_1 : f32
          %40 = arith.select %39, %38, %result_1 : f32
          %41 = arith.cmpf uno, %result_1, %result_1 : f32
          %42 = arith.select %41, %result_1, %40 : f32
          %result_3, %valid_4 = gpu.shuffle  xor %42, %c4_i32, %c32_i32 : f32
          %43 = arith.cmpf ugt, %42, %result_3 : f32
          %44 = arith.select %43, %42, %result_3 : f32
          %45 = arith.cmpf uno, %result_3, %result_3 : f32
          %46 = arith.select %45, %result_3, %44 : f32
          %result_5, %valid_6 = gpu.shuffle  xor %46, %c8_i32, %c32_i32 : f32
          %47 = arith.cmpf ugt, %46, %result_5 : f32
          %48 = arith.select %47, %46, %result_5 : f32
          %49 = arith.cmpf uno, %result_5, %result_5 : f32
          %50 = arith.select %49, %result_5, %48 : f32
          %result_7, %valid_8 = gpu.shuffle  xor %50, %c16_i32, %c32_i32 : f32
          %51 = arith.cmpf ugt, %50, %result_7 : f32
          %52 = arith.select %51, %50, %result_7 : f32
          %53 = arith.cmpf uno, %result_7, %result_7 : f32
          %54 = arith.select %53, %result_7, %52 : f32
          %55 = memref.alloc() : memref<8xf32, 3>
          %56 = arith.cmpi eq, %22, %c0 : index
          scf.if %56 {
            %67 = memref.reinterpret_cast %55 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
            memref.store %54, %67[%21] : memref<8xf32, 3>
          }
          gpu.barrier
          %57 = arith.cmpi slt, %19, %c32 : index
          scf.if %57 {
            %67 = arith.cmpi slt, %22, %c8 : index
            %68 = scf.if %67 -> (f32) {
              %81 = memref.reinterpret_cast %55 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
              %82 = memref.load %81[%22] : memref<8xf32, 3>
              scf.yield %82 : f32
            } else {
              scf.yield %cst_0 : f32
            }
            %result_19, %valid_20 = gpu.shuffle  xor %68, %c1_i32, %c8_i32 : f32
            %69 = arith.cmpf ugt, %68, %result_19 : f32
            %70 = arith.select %69, %68, %result_19 : f32
            %71 = arith.cmpf uno, %result_19, %result_19 : f32
            %72 = arith.select %71, %result_19, %70 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %72, %c2_i32, %c8_i32 : f32
            %73 = arith.cmpf ugt, %72, %result_21 : f32
            %74 = arith.select %73, %72, %result_21 : f32
            %75 = arith.cmpf uno, %result_21, %result_21 : f32
            %76 = arith.select %75, %result_21, %74 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %76, %c4_i32, %c8_i32 : f32
            %77 = arith.cmpf ugt, %76, %result_23 : f32
            %78 = arith.select %77, %76, %result_23 : f32
            %79 = arith.cmpf uno, %result_23, %result_23 : f32
            %80 = arith.select %79, %result_23, %78 : f32
            scf.if %56 {
              %81 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %80, %81[%c0] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %58 = memref.alloc() : memref<32xf32, 3>
          %59 = scf.for %arg3 = %19 to %32 step %c1024 iter_args(%arg4 = %cst) -> (f32) {
            %67 = arith.addi %arg3, %c256 : index
            %68 = arith.addi %arg3, %c512 : index
            %69 = arith.addi %arg3, %c768 : index
            %70 = arith.muli %18, %3 : index
            %71 = arith.addi %70, %arg3 : index
            %72 = arith.addi %70, %67 : index
            %73 = arith.addi %70, %68 : index
            %74 = arith.addi %70, %69 : index
            %75 = arith.muli %2, %1 : index
            %76 = arith.muli %75, %3 : index
            %77 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%76], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %78 = memref.load %77[%71] : memref<?xf32, "gpu">
            %79 = memref.load %77[%72] : memref<?xf32, "gpu">
            %80 = memref.load %77[%73] : memref<?xf32, "gpu">
            %81 = memref.load %77[%74] : memref<?xf32, "gpu">
            %82 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %83 = memref.load %82[%c0] : memref<32xf32, 3>
            %84 = arith.subf %78, %83 : f32
            %85 = arith.subf %79, %83 : f32
            %86 = arith.subf %80, %83 : f32
            %87 = arith.subf %81, %83 : f32
            %88 = math.exp %84 : f32
            %89 = math.exp %85 : f32
            %90 = math.exp %86 : f32
            %91 = math.exp %87 : f32
            %92 = arith.addf %arg4, %88 : f32
            %93 = arith.addf %92, %89 : f32
            %94 = arith.addf %93, %90 : f32
            %95 = arith.addf %94, %91 : f32
            scf.yield %95 : f32
          }
          %60 = scf.for %arg3 = %32 to %3 step %c256 iter_args(%arg4 = %59) -> (f32) {
            %67 = arith.muli %18, %3 : index
            %68 = arith.addi %67, %arg3 : index
            %69 = arith.muli %2, %1 : index
            %70 = arith.muli %69, %3 : index
            %71 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %72 = memref.load %71[%68] : memref<?xf32, "gpu">
            %73 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %74 = memref.load %73[%c0] : memref<32xf32, 3>
            %75 = arith.subf %72, %74 : f32
            %76 = math.exp %75 : f32
            %77 = arith.addf %arg4, %76 : f32
            scf.yield %77 : f32
          }
          %result_9, %valid_10 = gpu.shuffle  xor %60, %c1_i32, %c32_i32 : f32
          %61 = arith.addf %60, %result_9 : f32
          %result_11, %valid_12 = gpu.shuffle  xor %61, %c2_i32, %c32_i32 : f32
          %62 = arith.addf %61, %result_11 : f32
          %result_13, %valid_14 = gpu.shuffle  xor %62, %c4_i32, %c32_i32 : f32
          %63 = arith.addf %62, %result_13 : f32
          %result_15, %valid_16 = gpu.shuffle  xor %63, %c8_i32, %c32_i32 : f32
          %64 = arith.addf %63, %result_15 : f32
          %result_17, %valid_18 = gpu.shuffle  xor %64, %c16_i32, %c32_i32 : f32
          %65 = arith.addf %64, %result_17 : f32
          %66 = memref.alloc() : memref<8xf32, 3>
          scf.if %56 {
            %67 = memref.reinterpret_cast %66 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
            memref.store %65, %67[%21] : memref<8xf32, 3>
          }
          gpu.barrier
          scf.if %57 {
            %67 = arith.cmpi slt, %22, %c8 : index
            %68 = scf.if %67 -> (f32) {
              %72 = memref.reinterpret_cast %66 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
              %73 = memref.load %72[%22] : memref<8xf32, 3>
              scf.yield %73 : f32
            } else {
              scf.yield %cst : f32
            }
            %result_19, %valid_20 = gpu.shuffle  xor %68, %c1_i32, %c8_i32 : f32
            %69 = arith.addf %68, %result_19 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %69, %c2_i32, %c8_i32 : f32
            %70 = arith.addf %69, %result_21 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %70, %c4_i32, %c8_i32 : f32
            %71 = arith.addf %70, %result_23 : f32
            scf.if %56 {
              %72 = memref.reinterpret_cast %58 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %71, %72[%c0] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          scf.for %arg3 = %19 to %32 step %c1024 {
            %67 = arith.addi %arg3, %c256 : index
            %68 = arith.addi %arg3, %c512 : index
            %69 = arith.addi %arg3, %c768 : index
            %70 = arith.muli %18, %3 : index
            %71 = arith.addi %70, %arg3 : index
            %72 = arith.addi %70, %67 : index
            %73 = arith.addi %70, %68 : index
            %74 = arith.addi %70, %69 : index
            %75 = arith.muli %2, %1 : index
            %76 = arith.muli %75, %3 : index
            %77 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%76], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %78 = memref.load %77[%71] : memref<?xf32, "gpu">
            %79 = memref.load %77[%72] : memref<?xf32, "gpu">
            %80 = memref.load %77[%73] : memref<?xf32, "gpu">
            %81 = memref.load %77[%74] : memref<?xf32, "gpu">
            %82 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %83 = memref.load %82[%c0] : memref<32xf32, 3>
            %84 = arith.subf %78, %83 : f32
            %85 = arith.subf %79, %83 : f32
            %86 = arith.subf %80, %83 : f32
            %87 = arith.subf %81, %83 : f32
            %88 = math.exp %84 : f32
            %89 = math.exp %85 : f32
            %90 = math.exp %86 : f32
            %91 = math.exp %87 : f32
            %92 = memref.reinterpret_cast %58 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %93 = memref.load %92[%c0] : memref<32xf32, 3>
            %94 = arith.divf %88, %93 : f32
            %95 = arith.divf %89, %93 : f32
            %96 = arith.divf %90, %93 : f32
            %97 = arith.divf %91, %93 : f32
            %98 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%76], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %94, %98[%71] : memref<?xf32, "gpu">
            memref.store %95, %98[%72] : memref<?xf32, "gpu">
            memref.store %96, %98[%73] : memref<?xf32, "gpu">
            memref.store %97, %98[%74] : memref<?xf32, "gpu">
          }
          scf.for %arg3 = %32 to %3 step %c256 {
            %67 = arith.muli %18, %3 : index
            %68 = arith.addi %67, %arg3 : index
            %69 = arith.muli %2, %1 : index
            %70 = arith.muli %69, %3 : index
            %71 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %72 = memref.load %71[%68] : memref<?xf32, "gpu">
            %73 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %74 = memref.load %73[%c0] : memref<32xf32, 3>
            %75 = arith.subf %72, %74 : f32
            %76 = math.exp %75 : f32
            %77 = memref.reinterpret_cast %58 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %78 = memref.load %77[%c0] : memref<32xf32, 3>
            %79 = arith.divf %76, %78 : f32
            %80 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %79, %80[%68] : memref<?xf32, "gpu">
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      %18 = arith.cmpi eq, %7, %c0 : index
      %19 = arith.subi %7, %c1 : index
      %20 = arith.divui %19, %c8 : index
      %21 = arith.addi %20, %c1 : index
      %22 = arith.select %18, %c0, %21 : index
      scf.parallel (%arg1) = (%c0) to (%22) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %23 = gpu.block_id  x
          %24 = gpu.thread_id  x
          %25 = arith.divui %24, %c32 : index
          %26 = arith.remui %24, %c32 : index
          %27 = arith.muli %23, %c8 : index
          %28 = arith.addi %27, %25 : index
          %29 = arith.cmpi slt, %28, %7 : index
          %30 = memref.alloc() : memref<32xf32, 3>
          scf.if %29 {
            %32 = arith.subi %3, %26 : index
            %33 = arith.cmpi eq, %32, %c0 : index
            %34 = arith.subi %32, %c1 : index
            %35 = arith.divui %34, %c32 : index
            %36 = arith.addi %35, %c1 : index
            %37 = arith.select %33, %c0, %36 : index
            %38 = arith.remsi %37, %c4 : index
            %39 = arith.subi %37, %38 : index
            %40 = arith.muli %39, %c32 : index
            %41 = arith.addi %26, %40 : index
            %42 = scf.for %arg3 = %26 to %41 step %c128 iter_args(%arg4 = %cst_0) -> (f32) {
              %65 = arith.addi %arg3, %c32 : index
              %66 = arith.addi %arg3, %c64 : index
              %67 = arith.addi %arg3, %c96 : index
              %68 = arith.muli %28, %3 : index
              %69 = arith.addi %68, %arg3 : index
              %70 = arith.addi %68, %65 : index
              %71 = arith.addi %68, %66 : index
              %72 = arith.addi %68, %67 : index
              %73 = arith.muli %2, %1 : index
              %74 = arith.muli %73, %3 : index
              %75 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%74], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %76 = memref.load %75[%69] : memref<?xf32, "gpu">
              %77 = memref.load %75[%70] : memref<?xf32, "gpu">
              %78 = memref.load %75[%71] : memref<?xf32, "gpu">
              %79 = memref.load %75[%72] : memref<?xf32, "gpu">
              %80 = arith.cmpf ugt, %arg4, %76 : f32
              %81 = arith.select %80, %arg4, %76 : f32
              %82 = arith.cmpf uno, %76, %76 : f32
              %83 = arith.select %82, %76, %81 : f32
              %84 = arith.cmpf ugt, %83, %77 : f32
              %85 = arith.select %84, %83, %77 : f32
              %86 = arith.cmpf uno, %77, %77 : f32
              %87 = arith.select %86, %77, %85 : f32
              %88 = arith.cmpf ugt, %87, %78 : f32
              %89 = arith.select %88, %87, %78 : f32
              %90 = arith.cmpf uno, %78, %78 : f32
              %91 = arith.select %90, %78, %89 : f32
              %92 = arith.cmpf ugt, %91, %79 : f32
              %93 = arith.select %92, %91, %79 : f32
              %94 = arith.cmpf uno, %79, %79 : f32
              %95 = arith.select %94, %79, %93 : f32
              scf.yield %95 : f32
            }
            %43 = scf.for %arg3 = %41 to %3 step %c32 iter_args(%arg4 = %42) -> (f32) {
              %65 = arith.muli %28, %3 : index
              %66 = arith.addi %65, %arg3 : index
              %67 = arith.muli %2, %1 : index
              %68 = arith.muli %67, %3 : index
              %69 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%68], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %70 = memref.load %69[%66] : memref<?xf32, "gpu">
              %71 = arith.cmpf ugt, %arg4, %70 : f32
              %72 = arith.select %71, %arg4, %70 : f32
              %73 = arith.cmpf uno, %70, %70 : f32
              %74 = arith.select %73, %70, %72 : f32
              scf.yield %74 : f32
            }
            %result, %valid = gpu.shuffle  xor %43, %c1_i32, %c32_i32 : f32
            %44 = arith.cmpf ugt, %43, %result : f32
            %45 = arith.select %44, %43, %result : f32
            %46 = arith.cmpf uno, %result, %result : f32
            %47 = arith.select %46, %result, %45 : f32
            %result_1, %valid_2 = gpu.shuffle  xor %47, %c2_i32, %c32_i32 : f32
            %48 = arith.cmpf ugt, %47, %result_1 : f32
            %49 = arith.select %48, %47, %result_1 : f32
            %50 = arith.cmpf uno, %result_1, %result_1 : f32
            %51 = arith.select %50, %result_1, %49 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %51, %c4_i32, %c32_i32 : f32
            %52 = arith.cmpf ugt, %51, %result_3 : f32
            %53 = arith.select %52, %51, %result_3 : f32
            %54 = arith.cmpf uno, %result_3, %result_3 : f32
            %55 = arith.select %54, %result_3, %53 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %55, %c8_i32, %c32_i32 : f32
            %56 = arith.cmpf ugt, %55, %result_5 : f32
            %57 = arith.select %56, %55, %result_5 : f32
            %58 = arith.cmpf uno, %result_5, %result_5 : f32
            %59 = arith.select %58, %result_5, %57 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %59, %c16_i32, %c32_i32 : f32
            %60 = arith.cmpf ugt, %59, %result_7 : f32
            %61 = arith.select %60, %59, %result_7 : f32
            %62 = arith.cmpf uno, %result_7, %result_7 : f32
            %63 = arith.select %62, %result_7, %61 : f32
            %64 = arith.cmpi eq, %26, %c0 : index
            scf.if %64 {
              %65 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %63, %65[%25] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %31 = memref.alloc() : memref<32xf32, 3>
          scf.if %29 {
            %32 = arith.subi %3, %26 : index
            %33 = arith.cmpi eq, %32, %c0 : index
            %34 = arith.subi %32, %c1 : index
            %35 = arith.divui %34, %c32 : index
            %36 = arith.addi %35, %c1 : index
            %37 = arith.select %33, %c0, %36 : index
            %38 = arith.remsi %37, %c4 : index
            %39 = arith.subi %37, %38 : index
            %40 = arith.muli %39, %c32 : index
            %41 = arith.addi %26, %40 : index
            %42 = scf.for %arg3 = %26 to %41 step %c128 iter_args(%arg4 = %cst) -> (f32) {
              %50 = arith.addi %arg3, %c32 : index
              %51 = arith.addi %arg3, %c64 : index
              %52 = arith.addi %arg3, %c96 : index
              %53 = arith.muli %28, %3 : index
              %54 = arith.addi %53, %arg3 : index
              %55 = arith.addi %53, %50 : index
              %56 = arith.addi %53, %51 : index
              %57 = arith.addi %53, %52 : index
              %58 = arith.divui %54, %3 : index
              %59 = arith.remui %58, %1 : index
              %60 = arith.divui %58, %1 : index
              %61 = arith.divui %55, %3 : index
              %62 = arith.remui %61, %1 : index
              %63 = arith.divui %61, %1 : index
              %64 = arith.divui %56, %3 : index
              %65 = arith.remui %64, %1 : index
              %66 = arith.divui %64, %1 : index
              %67 = arith.divui %57, %3 : index
              %68 = arith.remui %67, %1 : index
              %69 = arith.divui %67, %1 : index
              %70 = arith.muli %2, %1 : index
              %71 = arith.muli %70, %3 : index
              %72 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%71], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %73 = memref.load %72[%54] : memref<?xf32, "gpu">
              %74 = memref.load %72[%55] : memref<?xf32, "gpu">
              %75 = memref.load %72[%56] : memref<?xf32, "gpu">
              %76 = memref.load %72[%57] : memref<?xf32, "gpu">
              %77 = arith.muli %60, %1 : index
              %78 = arith.addi %77, %59 : index
              %79 = arith.muli %63, %1 : index
              %80 = arith.addi %79, %62 : index
              %81 = arith.muli %66, %1 : index
              %82 = arith.addi %81, %65 : index
              %83 = arith.muli %69, %1 : index
              %84 = arith.addi %83, %68 : index
              %85 = arith.remui %78, %c8 : index
              %86 = arith.remui %80, %c8 : index
              %87 = arith.remui %82, %c8 : index
              %88 = arith.remui %84, %c8 : index
              %89 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %90 = memref.load %89[%85] : memref<32xf32, 3>
              %91 = memref.load %89[%86] : memref<32xf32, 3>
              %92 = memref.load %89[%87] : memref<32xf32, 3>
              %93 = memref.load %89[%88] : memref<32xf32, 3>
              %94 = arith.subf %73, %90 : f32
              %95 = arith.subf %74, %91 : f32
              %96 = arith.subf %75, %92 : f32
              %97 = arith.subf %76, %93 : f32
              %98 = math.exp %94 : f32
              %99 = math.exp %95 : f32
              %100 = math.exp %96 : f32
              %101 = math.exp %97 : f32
              %102 = arith.addf %arg4, %98 : f32
              %103 = arith.addf %102, %99 : f32
              %104 = arith.addf %103, %100 : f32
              %105 = arith.addf %104, %101 : f32
              scf.yield %105 : f32
            }
            %43 = scf.for %arg3 = %41 to %3 step %c32 iter_args(%arg4 = %42) -> (f32) {
              %50 = arith.muli %28, %3 : index
              %51 = arith.addi %50, %arg3 : index
              %52 = arith.divui %51, %3 : index
              %53 = arith.remui %52, %1 : index
              %54 = arith.divui %52, %1 : index
              %55 = arith.muli %2, %1 : index
              %56 = arith.muli %55, %3 : index
              %57 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%56], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %58 = memref.load %57[%51] : memref<?xf32, "gpu">
              %59 = arith.muli %54, %1 : index
              %60 = arith.addi %59, %53 : index
              %61 = arith.remui %60, %c8 : index
              %62 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %63 = memref.load %62[%61] : memref<32xf32, 3>
              %64 = arith.subf %58, %63 : f32
              %65 = math.exp %64 : f32
              %66 = arith.addf %arg4, %65 : f32
              scf.yield %66 : f32
            }
            %result, %valid = gpu.shuffle  xor %43, %c1_i32, %c32_i32 : f32
            %44 = arith.addf %43, %result : f32
            %result_1, %valid_2 = gpu.shuffle  xor %44, %c2_i32, %c32_i32 : f32
            %45 = arith.addf %44, %result_1 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %45, %c4_i32, %c32_i32 : f32
            %46 = arith.addf %45, %result_3 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %46, %c8_i32, %c32_i32 : f32
            %47 = arith.addf %46, %result_5 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %47, %c16_i32, %c32_i32 : f32
            %48 = arith.addf %47, %result_7 : f32
            %49 = arith.cmpi eq, %26, %c0 : index
            scf.if %49 {
              %50 = memref.reinterpret_cast %31 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %48, %50[%25] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          scf.if %29 {
            %32 = arith.subi %3, %26 : index
            %33 = arith.cmpi eq, %32, %c0 : index
            %34 = arith.subi %32, %c1 : index
            %35 = arith.divui %34, %c32 : index
            %36 = arith.addi %35, %c1 : index
            %37 = arith.select %33, %c0, %36 : index
            %38 = arith.remsi %37, %c4 : index
            %39 = arith.subi %37, %38 : index
            %40 = arith.muli %39, %c32 : index
            %41 = arith.addi %26, %40 : index
            scf.for %arg3 = %26 to %41 step %c128 {
              %42 = arith.addi %arg3, %c32 : index
              %43 = arith.addi %arg3, %c64 : index
              %44 = arith.addi %arg3, %c96 : index
              %45 = arith.muli %28, %3 : index
              %46 = arith.addi %45, %arg3 : index
              %47 = arith.addi %45, %42 : index
              %48 = arith.addi %45, %43 : index
              %49 = arith.addi %45, %44 : index
              %50 = arith.muli %2, %1 : index
              %51 = arith.muli %50, %3 : index
              %52 = arith.divui %46, %3 : index
              %53 = arith.remui %52, %1 : index
              %54 = arith.divui %52, %1 : index
              %55 = arith.divui %47, %3 : index
              %56 = arith.remui %55, %1 : index
              %57 = arith.divui %55, %1 : index
              %58 = arith.divui %48, %3 : index
              %59 = arith.remui %58, %1 : index
              %60 = arith.divui %58, %1 : index
              %61 = arith.divui %49, %3 : index
              %62 = arith.remui %61, %1 : index
              %63 = arith.divui %61, %1 : index
              %64 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%51], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %65 = memref.load %64[%46] : memref<?xf32, "gpu">
              %66 = memref.load %64[%47] : memref<?xf32, "gpu">
              %67 = memref.load %64[%48] : memref<?xf32, "gpu">
              %68 = memref.load %64[%49] : memref<?xf32, "gpu">
              %69 = arith.muli %54, %1 : index
              %70 = arith.addi %69, %53 : index
              %71 = arith.muli %57, %1 : index
              %72 = arith.addi %71, %56 : index
              %73 = arith.muli %60, %1 : index
              %74 = arith.addi %73, %59 : index
              %75 = arith.muli %63, %1 : index
              %76 = arith.addi %75, %62 : index
              %77 = arith.remui %70, %c8 : index
              %78 = arith.remui %72, %c8 : index
              %79 = arith.remui %74, %c8 : index
              %80 = arith.remui %76, %c8 : index
              %81 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %82 = memref.load %81[%77] : memref<32xf32, 3>
              %83 = memref.load %81[%78] : memref<32xf32, 3>
              %84 = memref.load %81[%79] : memref<32xf32, 3>
              %85 = memref.load %81[%80] : memref<32xf32, 3>
              %86 = arith.subf %65, %82 : f32
              %87 = arith.subf %66, %83 : f32
              %88 = arith.subf %67, %84 : f32
              %89 = arith.subf %68, %85 : f32
              %90 = math.exp %86 : f32
              %91 = math.exp %87 : f32
              %92 = math.exp %88 : f32
              %93 = math.exp %89 : f32
              %94 = memref.reinterpret_cast %31 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %95 = memref.load %94[%77] : memref<32xf32, 3>
              %96 = memref.load %94[%78] : memref<32xf32, 3>
              %97 = memref.load %94[%79] : memref<32xf32, 3>
              %98 = memref.load %94[%80] : memref<32xf32, 3>
              %99 = arith.divf %90, %95 : f32
              %100 = arith.divf %91, %96 : f32
              %101 = arith.divf %92, %97 : f32
              %102 = arith.divf %93, %98 : f32
              %103 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%51], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %99, %103[%46] : memref<?xf32, "gpu">
              memref.store %100, %103[%47] : memref<?xf32, "gpu">
              memref.store %101, %103[%48] : memref<?xf32, "gpu">
              memref.store %102, %103[%49] : memref<?xf32, "gpu">
            }
            scf.for %arg3 = %41 to %3 step %c32 {
              %42 = arith.muli %28, %3 : index
              %43 = arith.addi %42, %arg3 : index
              %44 = arith.muli %2, %1 : index
              %45 = arith.muli %44, %3 : index
              %46 = arith.divui %43, %3 : index
              %47 = arith.remui %46, %1 : index
              %48 = arith.divui %46, %1 : index
              %49 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%45], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %50 = memref.load %49[%43] : memref<?xf32, "gpu">
              %51 = arith.muli %48, %1 : index
              %52 = arith.addi %51, %47 : index
              %53 = arith.remui %52, %c8 : index
              %54 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %55 = memref.load %54[%53] : memref<32xf32, 3>
              %56 = arith.subf %50, %55 : f32
              %57 = math.exp %56 : f32
              %58 = memref.reinterpret_cast %31 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %59 = memref.load %58[%53] : memref<32xf32, 3>
              %60 = arith.divf %57, %59 : f32
              %61 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%45], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %60, %61[%43] : memref<?xf32, "gpu">
            }
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  "disc_ral.dispatch"(%arg0, %c0, %8) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After GpuMapParallelLoopsPass (gpu-map-parallel-loops) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c768 = arith.constant 768 : index
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant -0.000000e+00 : f32
  %cst_0 = arith.constant 0xFF800000 : f32
  %c8 = arith.constant 8 : index
  %c16_i32 = arith.constant 16 : i32
  %c8_i32 = arith.constant 8 : i32
  %c4_i32 = arith.constant 4 : i32
  %c2_i32 = arith.constant 2 : i32
  %c1_i32 = arith.constant 1 : i32
  %c32_i32 = arith.constant 32 : i32
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c1024 = arith.constant 1024 : index
  %c6 = arith.constant 6 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
  %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
  %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
  %4 = arith.index_cast %2 : index to i32
  %5 = arith.index_cast %1 : index to i32
  %6 = arith.muli %4, %5 : i32
  %7 = arith.index_cast %6 : i32 to index
  %8 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %9 = arith.cmpi slt, %7, %c3 : index
  %10 = arith.cmpi sge, %3, %c1024 : index
  %11 = arith.cmpi slt, %7, %c6 : index
  %12 = arith.andi %11, %10 : i1
  %13 = arith.cmpi sge, %3, %c512 : index
  %14 = arith.cmpi sge, %7, %c6 : index
  %15 = arith.andi %14, %13 : i1
  %16 = arith.ori %9, %12 : i1
  %17 = arith.ori %16, %15 : i1
  scf.if %17 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %18 = gpu.block_id  x
          %19 = gpu.thread_id  x
          %20 = memref.alloc() : memref<32xf32, 3>
          %21 = arith.divui %19, %c32 : index
          %22 = arith.remui %19, %c32 : index
          %23 = arith.subi %3, %19 : index
          %24 = arith.cmpi eq, %23, %c0 : index
          %25 = arith.subi %23, %c1 : index
          %26 = arith.divui %25, %c256 : index
          %27 = arith.addi %26, %c1 : index
          %28 = arith.select %24, %c0, %27 : index
          %29 = arith.remsi %28, %c4 : index
          %30 = arith.subi %28, %29 : index
          %31 = arith.muli %30, %c256 : index
          %32 = arith.addi %19, %31 : index
          %33 = scf.for %arg3 = %19 to %32 step %c1024 iter_args(%arg4 = %cst_0) -> (f32) {
            %67 = arith.addi %arg3, %c256 : index
            %68 = arith.addi %arg3, %c512 : index
            %69 = arith.addi %arg3, %c768 : index
            %70 = arith.muli %18, %3 : index
            %71 = arith.addi %70, %arg3 : index
            %72 = arith.addi %70, %67 : index
            %73 = arith.addi %70, %68 : index
            %74 = arith.addi %70, %69 : index
            %75 = arith.muli %2, %1 : index
            %76 = arith.muli %75, %3 : index
            %77 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%76], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %78 = memref.load %77[%71] : memref<?xf32, "gpu">
            %79 = memref.load %77[%72] : memref<?xf32, "gpu">
            %80 = memref.load %77[%73] : memref<?xf32, "gpu">
            %81 = memref.load %77[%74] : memref<?xf32, "gpu">
            %82 = arith.cmpf ugt, %arg4, %78 : f32
            %83 = arith.select %82, %arg4, %78 : f32
            %84 = arith.cmpf uno, %78, %78 : f32
            %85 = arith.select %84, %78, %83 : f32
            %86 = arith.cmpf ugt, %85, %79 : f32
            %87 = arith.select %86, %85, %79 : f32
            %88 = arith.cmpf uno, %79, %79 : f32
            %89 = arith.select %88, %79, %87 : f32
            %90 = arith.cmpf ugt, %89, %80 : f32
            %91 = arith.select %90, %89, %80 : f32
            %92 = arith.cmpf uno, %80, %80 : f32
            %93 = arith.select %92, %80, %91 : f32
            %94 = arith.cmpf ugt, %93, %81 : f32
            %95 = arith.select %94, %93, %81 : f32
            %96 = arith.cmpf uno, %81, %81 : f32
            %97 = arith.select %96, %81, %95 : f32
            scf.yield %97 : f32
          }
          %34 = scf.for %arg3 = %32 to %3 step %c256 iter_args(%arg4 = %33) -> (f32) {
            %67 = arith.muli %18, %3 : index
            %68 = arith.addi %67, %arg3 : index
            %69 = arith.muli %2, %1 : index
            %70 = arith.muli %69, %3 : index
            %71 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %72 = memref.load %71[%68] : memref<?xf32, "gpu">
            %73 = arith.cmpf ugt, %arg4, %72 : f32
            %74 = arith.select %73, %arg4, %72 : f32
            %75 = arith.cmpf uno, %72, %72 : f32
            %76 = arith.select %75, %72, %74 : f32
            scf.yield %76 : f32
          }
          %result, %valid = gpu.shuffle  xor %34, %c1_i32, %c32_i32 : f32
          %35 = arith.cmpf ugt, %34, %result : f32
          %36 = arith.select %35, %34, %result : f32
          %37 = arith.cmpf uno, %result, %result : f32
          %38 = arith.select %37, %result, %36 : f32
          %result_1, %valid_2 = gpu.shuffle  xor %38, %c2_i32, %c32_i32 : f32
          %39 = arith.cmpf ugt, %38, %result_1 : f32
          %40 = arith.select %39, %38, %result_1 : f32
          %41 = arith.cmpf uno, %result_1, %result_1 : f32
          %42 = arith.select %41, %result_1, %40 : f32
          %result_3, %valid_4 = gpu.shuffle  xor %42, %c4_i32, %c32_i32 : f32
          %43 = arith.cmpf ugt, %42, %result_3 : f32
          %44 = arith.select %43, %42, %result_3 : f32
          %45 = arith.cmpf uno, %result_3, %result_3 : f32
          %46 = arith.select %45, %result_3, %44 : f32
          %result_5, %valid_6 = gpu.shuffle  xor %46, %c8_i32, %c32_i32 : f32
          %47 = arith.cmpf ugt, %46, %result_5 : f32
          %48 = arith.select %47, %46, %result_5 : f32
          %49 = arith.cmpf uno, %result_5, %result_5 : f32
          %50 = arith.select %49, %result_5, %48 : f32
          %result_7, %valid_8 = gpu.shuffle  xor %50, %c16_i32, %c32_i32 : f32
          %51 = arith.cmpf ugt, %50, %result_7 : f32
          %52 = arith.select %51, %50, %result_7 : f32
          %53 = arith.cmpf uno, %result_7, %result_7 : f32
          %54 = arith.select %53, %result_7, %52 : f32
          %55 = memref.alloc() : memref<8xf32, 3>
          %56 = arith.cmpi eq, %22, %c0 : index
          scf.if %56 {
            %67 = memref.reinterpret_cast %55 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
            memref.store %54, %67[%21] : memref<8xf32, 3>
          }
          gpu.barrier
          %57 = arith.cmpi slt, %19, %c32 : index
          scf.if %57 {
            %67 = arith.cmpi slt, %22, %c8 : index
            %68 = scf.if %67 -> (f32) {
              %81 = memref.reinterpret_cast %55 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
              %82 = memref.load %81[%22] : memref<8xf32, 3>
              scf.yield %82 : f32
            } else {
              scf.yield %cst_0 : f32
            }
            %result_19, %valid_20 = gpu.shuffle  xor %68, %c1_i32, %c8_i32 : f32
            %69 = arith.cmpf ugt, %68, %result_19 : f32
            %70 = arith.select %69, %68, %result_19 : f32
            %71 = arith.cmpf uno, %result_19, %result_19 : f32
            %72 = arith.select %71, %result_19, %70 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %72, %c2_i32, %c8_i32 : f32
            %73 = arith.cmpf ugt, %72, %result_21 : f32
            %74 = arith.select %73, %72, %result_21 : f32
            %75 = arith.cmpf uno, %result_21, %result_21 : f32
            %76 = arith.select %75, %result_21, %74 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %76, %c4_i32, %c8_i32 : f32
            %77 = arith.cmpf ugt, %76, %result_23 : f32
            %78 = arith.select %77, %76, %result_23 : f32
            %79 = arith.cmpf uno, %result_23, %result_23 : f32
            %80 = arith.select %79, %result_23, %78 : f32
            scf.if %56 {
              %81 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %80, %81[%c0] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %58 = memref.alloc() : memref<32xf32, 3>
          %59 = scf.for %arg3 = %19 to %32 step %c1024 iter_args(%arg4 = %cst) -> (f32) {
            %67 = arith.addi %arg3, %c256 : index
            %68 = arith.addi %arg3, %c512 : index
            %69 = arith.addi %arg3, %c768 : index
            %70 = arith.muli %18, %3 : index
            %71 = arith.addi %70, %arg3 : index
            %72 = arith.addi %70, %67 : index
            %73 = arith.addi %70, %68 : index
            %74 = arith.addi %70, %69 : index
            %75 = arith.muli %2, %1 : index
            %76 = arith.muli %75, %3 : index
            %77 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%76], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %78 = memref.load %77[%71] : memref<?xf32, "gpu">
            %79 = memref.load %77[%72] : memref<?xf32, "gpu">
            %80 = memref.load %77[%73] : memref<?xf32, "gpu">
            %81 = memref.load %77[%74] : memref<?xf32, "gpu">
            %82 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %83 = memref.load %82[%c0] : memref<32xf32, 3>
            %84 = arith.subf %78, %83 : f32
            %85 = arith.subf %79, %83 : f32
            %86 = arith.subf %80, %83 : f32
            %87 = arith.subf %81, %83 : f32
            %88 = math.exp %84 : f32
            %89 = math.exp %85 : f32
            %90 = math.exp %86 : f32
            %91 = math.exp %87 : f32
            %92 = arith.addf %arg4, %88 : f32
            %93 = arith.addf %92, %89 : f32
            %94 = arith.addf %93, %90 : f32
            %95 = arith.addf %94, %91 : f32
            scf.yield %95 : f32
          }
          %60 = scf.for %arg3 = %32 to %3 step %c256 iter_args(%arg4 = %59) -> (f32) {
            %67 = arith.muli %18, %3 : index
            %68 = arith.addi %67, %arg3 : index
            %69 = arith.muli %2, %1 : index
            %70 = arith.muli %69, %3 : index
            %71 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %72 = memref.load %71[%68] : memref<?xf32, "gpu">
            %73 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %74 = memref.load %73[%c0] : memref<32xf32, 3>
            %75 = arith.subf %72, %74 : f32
            %76 = math.exp %75 : f32
            %77 = arith.addf %arg4, %76 : f32
            scf.yield %77 : f32
          }
          %result_9, %valid_10 = gpu.shuffle  xor %60, %c1_i32, %c32_i32 : f32
          %61 = arith.addf %60, %result_9 : f32
          %result_11, %valid_12 = gpu.shuffle  xor %61, %c2_i32, %c32_i32 : f32
          %62 = arith.addf %61, %result_11 : f32
          %result_13, %valid_14 = gpu.shuffle  xor %62, %c4_i32, %c32_i32 : f32
          %63 = arith.addf %62, %result_13 : f32
          %result_15, %valid_16 = gpu.shuffle  xor %63, %c8_i32, %c32_i32 : f32
          %64 = arith.addf %63, %result_15 : f32
          %result_17, %valid_18 = gpu.shuffle  xor %64, %c16_i32, %c32_i32 : f32
          %65 = arith.addf %64, %result_17 : f32
          %66 = memref.alloc() : memref<8xf32, 3>
          scf.if %56 {
            %67 = memref.reinterpret_cast %66 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
            memref.store %65, %67[%21] : memref<8xf32, 3>
          }
          gpu.barrier
          scf.if %57 {
            %67 = arith.cmpi slt, %22, %c8 : index
            %68 = scf.if %67 -> (f32) {
              %72 = memref.reinterpret_cast %66 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
              %73 = memref.load %72[%22] : memref<8xf32, 3>
              scf.yield %73 : f32
            } else {
              scf.yield %cst : f32
            }
            %result_19, %valid_20 = gpu.shuffle  xor %68, %c1_i32, %c8_i32 : f32
            %69 = arith.addf %68, %result_19 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %69, %c2_i32, %c8_i32 : f32
            %70 = arith.addf %69, %result_21 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %70, %c4_i32, %c8_i32 : f32
            %71 = arith.addf %70, %result_23 : f32
            scf.if %56 {
              %72 = memref.reinterpret_cast %58 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %71, %72[%c0] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          scf.for %arg3 = %19 to %32 step %c1024 {
            %67 = arith.addi %arg3, %c256 : index
            %68 = arith.addi %arg3, %c512 : index
            %69 = arith.addi %arg3, %c768 : index
            %70 = arith.muli %18, %3 : index
            %71 = arith.addi %70, %arg3 : index
            %72 = arith.addi %70, %67 : index
            %73 = arith.addi %70, %68 : index
            %74 = arith.addi %70, %69 : index
            %75 = arith.muli %2, %1 : index
            %76 = arith.muli %75, %3 : index
            %77 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%76], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %78 = memref.load %77[%71] : memref<?xf32, "gpu">
            %79 = memref.load %77[%72] : memref<?xf32, "gpu">
            %80 = memref.load %77[%73] : memref<?xf32, "gpu">
            %81 = memref.load %77[%74] : memref<?xf32, "gpu">
            %82 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %83 = memref.load %82[%c0] : memref<32xf32, 3>
            %84 = arith.subf %78, %83 : f32
            %85 = arith.subf %79, %83 : f32
            %86 = arith.subf %80, %83 : f32
            %87 = arith.subf %81, %83 : f32
            %88 = math.exp %84 : f32
            %89 = math.exp %85 : f32
            %90 = math.exp %86 : f32
            %91 = math.exp %87 : f32
            %92 = memref.reinterpret_cast %58 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %93 = memref.load %92[%c0] : memref<32xf32, 3>
            %94 = arith.divf %88, %93 : f32
            %95 = arith.divf %89, %93 : f32
            %96 = arith.divf %90, %93 : f32
            %97 = arith.divf %91, %93 : f32
            %98 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%76], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %94, %98[%71] : memref<?xf32, "gpu">
            memref.store %95, %98[%72] : memref<?xf32, "gpu">
            memref.store %96, %98[%73] : memref<?xf32, "gpu">
            memref.store %97, %98[%74] : memref<?xf32, "gpu">
          }
          scf.for %arg3 = %32 to %3 step %c256 {
            %67 = arith.muli %18, %3 : index
            %68 = arith.addi %67, %arg3 : index
            %69 = arith.muli %2, %1 : index
            %70 = arith.muli %69, %3 : index
            %71 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %72 = memref.load %71[%68] : memref<?xf32, "gpu">
            %73 = memref.reinterpret_cast %20 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %74 = memref.load %73[%c0] : memref<32xf32, 3>
            %75 = arith.subf %72, %74 : f32
            %76 = math.exp %75 : f32
            %77 = memref.reinterpret_cast %58 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %78 = memref.load %77[%c0] : memref<32xf32, 3>
            %79 = arith.divf %76, %78 : f32
            %80 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %79, %80[%68] : memref<?xf32, "gpu">
          }
          scf.yield
        } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
        scf.yield
      } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      %18 = arith.cmpi eq, %7, %c0 : index
      %19 = arith.subi %7, %c1 : index
      %20 = arith.divui %19, %c8 : index
      %21 = arith.addi %20, %c1 : index
      %22 = arith.select %18, %c0, %21 : index
      scf.parallel (%arg1) = (%c0) to (%22) step (%c1) {
        scf.parallel (%arg2) = (%c0) to (%c256) step (%c1) {
          %23 = gpu.block_id  x
          %24 = gpu.thread_id  x
          %25 = arith.divui %24, %c32 : index
          %26 = arith.remui %24, %c32 : index
          %27 = arith.muli %23, %c8 : index
          %28 = arith.addi %27, %25 : index
          %29 = arith.cmpi slt, %28, %7 : index
          %30 = memref.alloc() : memref<32xf32, 3>
          scf.if %29 {
            %32 = arith.subi %3, %26 : index
            %33 = arith.cmpi eq, %32, %c0 : index
            %34 = arith.subi %32, %c1 : index
            %35 = arith.divui %34, %c32 : index
            %36 = arith.addi %35, %c1 : index
            %37 = arith.select %33, %c0, %36 : index
            %38 = arith.remsi %37, %c4 : index
            %39 = arith.subi %37, %38 : index
            %40 = arith.muli %39, %c32 : index
            %41 = arith.addi %26, %40 : index
            %42 = scf.for %arg3 = %26 to %41 step %c128 iter_args(%arg4 = %cst_0) -> (f32) {
              %65 = arith.addi %arg3, %c32 : index
              %66 = arith.addi %arg3, %c64 : index
              %67 = arith.addi %arg3, %c96 : index
              %68 = arith.muli %28, %3 : index
              %69 = arith.addi %68, %arg3 : index
              %70 = arith.addi %68, %65 : index
              %71 = arith.addi %68, %66 : index
              %72 = arith.addi %68, %67 : index
              %73 = arith.muli %2, %1 : index
              %74 = arith.muli %73, %3 : index
              %75 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%74], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %76 = memref.load %75[%69] : memref<?xf32, "gpu">
              %77 = memref.load %75[%70] : memref<?xf32, "gpu">
              %78 = memref.load %75[%71] : memref<?xf32, "gpu">
              %79 = memref.load %75[%72] : memref<?xf32, "gpu">
              %80 = arith.cmpf ugt, %arg4, %76 : f32
              %81 = arith.select %80, %arg4, %76 : f32
              %82 = arith.cmpf uno, %76, %76 : f32
              %83 = arith.select %82, %76, %81 : f32
              %84 = arith.cmpf ugt, %83, %77 : f32
              %85 = arith.select %84, %83, %77 : f32
              %86 = arith.cmpf uno, %77, %77 : f32
              %87 = arith.select %86, %77, %85 : f32
              %88 = arith.cmpf ugt, %87, %78 : f32
              %89 = arith.select %88, %87, %78 : f32
              %90 = arith.cmpf uno, %78, %78 : f32
              %91 = arith.select %90, %78, %89 : f32
              %92 = arith.cmpf ugt, %91, %79 : f32
              %93 = arith.select %92, %91, %79 : f32
              %94 = arith.cmpf uno, %79, %79 : f32
              %95 = arith.select %94, %79, %93 : f32
              scf.yield %95 : f32
            }
            %43 = scf.for %arg3 = %41 to %3 step %c32 iter_args(%arg4 = %42) -> (f32) {
              %65 = arith.muli %28, %3 : index
              %66 = arith.addi %65, %arg3 : index
              %67 = arith.muli %2, %1 : index
              %68 = arith.muli %67, %3 : index
              %69 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%68], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %70 = memref.load %69[%66] : memref<?xf32, "gpu">
              %71 = arith.cmpf ugt, %arg4, %70 : f32
              %72 = arith.select %71, %arg4, %70 : f32
              %73 = arith.cmpf uno, %70, %70 : f32
              %74 = arith.select %73, %70, %72 : f32
              scf.yield %74 : f32
            }
            %result, %valid = gpu.shuffle  xor %43, %c1_i32, %c32_i32 : f32
            %44 = arith.cmpf ugt, %43, %result : f32
            %45 = arith.select %44, %43, %result : f32
            %46 = arith.cmpf uno, %result, %result : f32
            %47 = arith.select %46, %result, %45 : f32
            %result_1, %valid_2 = gpu.shuffle  xor %47, %c2_i32, %c32_i32 : f32
            %48 = arith.cmpf ugt, %47, %result_1 : f32
            %49 = arith.select %48, %47, %result_1 : f32
            %50 = arith.cmpf uno, %result_1, %result_1 : f32
            %51 = arith.select %50, %result_1, %49 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %51, %c4_i32, %c32_i32 : f32
            %52 = arith.cmpf ugt, %51, %result_3 : f32
            %53 = arith.select %52, %51, %result_3 : f32
            %54 = arith.cmpf uno, %result_3, %result_3 : f32
            %55 = arith.select %54, %result_3, %53 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %55, %c8_i32, %c32_i32 : f32
            %56 = arith.cmpf ugt, %55, %result_5 : f32
            %57 = arith.select %56, %55, %result_5 : f32
            %58 = arith.cmpf uno, %result_5, %result_5 : f32
            %59 = arith.select %58, %result_5, %57 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %59, %c16_i32, %c32_i32 : f32
            %60 = arith.cmpf ugt, %59, %result_7 : f32
            %61 = arith.select %60, %59, %result_7 : f32
            %62 = arith.cmpf uno, %result_7, %result_7 : f32
            %63 = arith.select %62, %result_7, %61 : f32
            %64 = arith.cmpi eq, %26, %c0 : index
            scf.if %64 {
              %65 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %63, %65[%25] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %31 = memref.alloc() : memref<32xf32, 3>
          scf.if %29 {
            %32 = arith.subi %3, %26 : index
            %33 = arith.cmpi eq, %32, %c0 : index
            %34 = arith.subi %32, %c1 : index
            %35 = arith.divui %34, %c32 : index
            %36 = arith.addi %35, %c1 : index
            %37 = arith.select %33, %c0, %36 : index
            %38 = arith.remsi %37, %c4 : index
            %39 = arith.subi %37, %38 : index
            %40 = arith.muli %39, %c32 : index
            %41 = arith.addi %26, %40 : index
            %42 = scf.for %arg3 = %26 to %41 step %c128 iter_args(%arg4 = %cst) -> (f32) {
              %50 = arith.addi %arg3, %c32 : index
              %51 = arith.addi %arg3, %c64 : index
              %52 = arith.addi %arg3, %c96 : index
              %53 = arith.muli %28, %3 : index
              %54 = arith.addi %53, %arg3 : index
              %55 = arith.addi %53, %50 : index
              %56 = arith.addi %53, %51 : index
              %57 = arith.addi %53, %52 : index
              %58 = arith.divui %54, %3 : index
              %59 = arith.remui %58, %1 : index
              %60 = arith.divui %58, %1 : index
              %61 = arith.divui %55, %3 : index
              %62 = arith.remui %61, %1 : index
              %63 = arith.divui %61, %1 : index
              %64 = arith.divui %56, %3 : index
              %65 = arith.remui %64, %1 : index
              %66 = arith.divui %64, %1 : index
              %67 = arith.divui %57, %3 : index
              %68 = arith.remui %67, %1 : index
              %69 = arith.divui %67, %1 : index
              %70 = arith.muli %2, %1 : index
              %71 = arith.muli %70, %3 : index
              %72 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%71], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %73 = memref.load %72[%54] : memref<?xf32, "gpu">
              %74 = memref.load %72[%55] : memref<?xf32, "gpu">
              %75 = memref.load %72[%56] : memref<?xf32, "gpu">
              %76 = memref.load %72[%57] : memref<?xf32, "gpu">
              %77 = arith.muli %60, %1 : index
              %78 = arith.addi %77, %59 : index
              %79 = arith.muli %63, %1 : index
              %80 = arith.addi %79, %62 : index
              %81 = arith.muli %66, %1 : index
              %82 = arith.addi %81, %65 : index
              %83 = arith.muli %69, %1 : index
              %84 = arith.addi %83, %68 : index
              %85 = arith.remui %78, %c8 : index
              %86 = arith.remui %80, %c8 : index
              %87 = arith.remui %82, %c8 : index
              %88 = arith.remui %84, %c8 : index
              %89 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %90 = memref.load %89[%85] : memref<32xf32, 3>
              %91 = memref.load %89[%86] : memref<32xf32, 3>
              %92 = memref.load %89[%87] : memref<32xf32, 3>
              %93 = memref.load %89[%88] : memref<32xf32, 3>
              %94 = arith.subf %73, %90 : f32
              %95 = arith.subf %74, %91 : f32
              %96 = arith.subf %75, %92 : f32
              %97 = arith.subf %76, %93 : f32
              %98 = math.exp %94 : f32
              %99 = math.exp %95 : f32
              %100 = math.exp %96 : f32
              %101 = math.exp %97 : f32
              %102 = arith.addf %arg4, %98 : f32
              %103 = arith.addf %102, %99 : f32
              %104 = arith.addf %103, %100 : f32
              %105 = arith.addf %104, %101 : f32
              scf.yield %105 : f32
            }
            %43 = scf.for %arg3 = %41 to %3 step %c32 iter_args(%arg4 = %42) -> (f32) {
              %50 = arith.muli %28, %3 : index
              %51 = arith.addi %50, %arg3 : index
              %52 = arith.divui %51, %3 : index
              %53 = arith.remui %52, %1 : index
              %54 = arith.divui %52, %1 : index
              %55 = arith.muli %2, %1 : index
              %56 = arith.muli %55, %3 : index
              %57 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%56], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %58 = memref.load %57[%51] : memref<?xf32, "gpu">
              %59 = arith.muli %54, %1 : index
              %60 = arith.addi %59, %53 : index
              %61 = arith.remui %60, %c8 : index
              %62 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %63 = memref.load %62[%61] : memref<32xf32, 3>
              %64 = arith.subf %58, %63 : f32
              %65 = math.exp %64 : f32
              %66 = arith.addf %arg4, %65 : f32
              scf.yield %66 : f32
            }
            %result, %valid = gpu.shuffle  xor %43, %c1_i32, %c32_i32 : f32
            %44 = arith.addf %43, %result : f32
            %result_1, %valid_2 = gpu.shuffle  xor %44, %c2_i32, %c32_i32 : f32
            %45 = arith.addf %44, %result_1 : f32
            %result_3, %valid_4 = gpu.shuffle  xor %45, %c4_i32, %c32_i32 : f32
            %46 = arith.addf %45, %result_3 : f32
            %result_5, %valid_6 = gpu.shuffle  xor %46, %c8_i32, %c32_i32 : f32
            %47 = arith.addf %46, %result_5 : f32
            %result_7, %valid_8 = gpu.shuffle  xor %47, %c16_i32, %c32_i32 : f32
            %48 = arith.addf %47, %result_7 : f32
            %49 = arith.cmpi eq, %26, %c0 : index
            scf.if %49 {
              %50 = memref.reinterpret_cast %31 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %48, %50[%25] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          scf.if %29 {
            %32 = arith.subi %3, %26 : index
            %33 = arith.cmpi eq, %32, %c0 : index
            %34 = arith.subi %32, %c1 : index
            %35 = arith.divui %34, %c32 : index
            %36 = arith.addi %35, %c1 : index
            %37 = arith.select %33, %c0, %36 : index
            %38 = arith.remsi %37, %c4 : index
            %39 = arith.subi %37, %38 : index
            %40 = arith.muli %39, %c32 : index
            %41 = arith.addi %26, %40 : index
            scf.for %arg3 = %26 to %41 step %c128 {
              %42 = arith.addi %arg3, %c32 : index
              %43 = arith.addi %arg3, %c64 : index
              %44 = arith.addi %arg3, %c96 : index
              %45 = arith.muli %28, %3 : index
              %46 = arith.addi %45, %arg3 : index
              %47 = arith.addi %45, %42 : index
              %48 = arith.addi %45, %43 : index
              %49 = arith.addi %45, %44 : index
              %50 = arith.muli %2, %1 : index
              %51 = arith.muli %50, %3 : index
              %52 = arith.divui %46, %3 : index
              %53 = arith.remui %52, %1 : index
              %54 = arith.divui %52, %1 : index
              %55 = arith.divui %47, %3 : index
              %56 = arith.remui %55, %1 : index
              %57 = arith.divui %55, %1 : index
              %58 = arith.divui %48, %3 : index
              %59 = arith.remui %58, %1 : index
              %60 = arith.divui %58, %1 : index
              %61 = arith.divui %49, %3 : index
              %62 = arith.remui %61, %1 : index
              %63 = arith.divui %61, %1 : index
              %64 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%51], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %65 = memref.load %64[%46] : memref<?xf32, "gpu">
              %66 = memref.load %64[%47] : memref<?xf32, "gpu">
              %67 = memref.load %64[%48] : memref<?xf32, "gpu">
              %68 = memref.load %64[%49] : memref<?xf32, "gpu">
              %69 = arith.muli %54, %1 : index
              %70 = arith.addi %69, %53 : index
              %71 = arith.muli %57, %1 : index
              %72 = arith.addi %71, %56 : index
              %73 = arith.muli %60, %1 : index
              %74 = arith.addi %73, %59 : index
              %75 = arith.muli %63, %1 : index
              %76 = arith.addi %75, %62 : index
              %77 = arith.remui %70, %c8 : index
              %78 = arith.remui %72, %c8 : index
              %79 = arith.remui %74, %c8 : index
              %80 = arith.remui %76, %c8 : index
              %81 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %82 = memref.load %81[%77] : memref<32xf32, 3>
              %83 = memref.load %81[%78] : memref<32xf32, 3>
              %84 = memref.load %81[%79] : memref<32xf32, 3>
              %85 = memref.load %81[%80] : memref<32xf32, 3>
              %86 = arith.subf %65, %82 : f32
              %87 = arith.subf %66, %83 : f32
              %88 = arith.subf %67, %84 : f32
              %89 = arith.subf %68, %85 : f32
              %90 = math.exp %86 : f32
              %91 = math.exp %87 : f32
              %92 = math.exp %88 : f32
              %93 = math.exp %89 : f32
              %94 = memref.reinterpret_cast %31 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %95 = memref.load %94[%77] : memref<32xf32, 3>
              %96 = memref.load %94[%78] : memref<32xf32, 3>
              %97 = memref.load %94[%79] : memref<32xf32, 3>
              %98 = memref.load %94[%80] : memref<32xf32, 3>
              %99 = arith.divf %90, %95 : f32
              %100 = arith.divf %91, %96 : f32
              %101 = arith.divf %92, %97 : f32
              %102 = arith.divf %93, %98 : f32
              %103 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%51], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %99, %103[%46] : memref<?xf32, "gpu">
              memref.store %100, %103[%47] : memref<?xf32, "gpu">
              memref.store %101, %103[%48] : memref<?xf32, "gpu">
              memref.store %102, %103[%49] : memref<?xf32, "gpu">
            }
            scf.for %arg3 = %41 to %3 step %c32 {
              %42 = arith.muli %28, %3 : index
              %43 = arith.addi %42, %arg3 : index
              %44 = arith.muli %2, %1 : index
              %45 = arith.muli %44, %3 : index
              %46 = arith.divui %43, %3 : index
              %47 = arith.remui %46, %1 : index
              %48 = arith.divui %46, %1 : index
              %49 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%45], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %50 = memref.load %49[%43] : memref<?xf32, "gpu">
              %51 = arith.muli %48, %1 : index
              %52 = arith.addi %51, %47 : index
              %53 = arith.remui %52, %c8 : index
              %54 = memref.reinterpret_cast %30 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %55 = memref.load %54[%53] : memref<32xf32, 3>
              %56 = arith.subf %50, %55 : f32
              %57 = math.exp %56 : f32
              %58 = memref.reinterpret_cast %31 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %59 = memref.load %58[%53] : memref<32xf32, 3>
              %60 = arith.divf %57, %59 : f32
              %61 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%45], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %60, %61[%43] : memref<?xf32, "gpu">
            }
          }
          scf.yield
        } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
        scf.yield
      } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  "disc_ral.dispatch"(%arg0, %c0, %8) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ConvertParallelLoopToGpu (convert-parallel-loops-to-gpu) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c768 = arith.constant 768 : index
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant -0.000000e+00 : f32
  %cst_0 = arith.constant 0xFF800000 : f32
  %c8 = arith.constant 8 : index
  %c16_i32 = arith.constant 16 : i32
  %c8_i32 = arith.constant 8 : i32
  %c4_i32 = arith.constant 4 : i32
  %c2_i32 = arith.constant 2 : i32
  %c1_i32 = arith.constant 1 : i32
  %c32_i32 = arith.constant 32 : i32
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c1024 = arith.constant 1024 : index
  %c6 = arith.constant 6 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
  %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
  %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
  %4 = arith.index_cast %2 : index to i32
  %5 = arith.index_cast %1 : index to i32
  %6 = arith.muli %4, %5 : i32
  %7 = arith.index_cast %6 : i32 to index
  %8 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %9 = arith.cmpi slt, %7, %c3 : index
  %10 = arith.cmpi sge, %3, %c1024 : index
  %11 = arith.cmpi slt, %7, %c6 : index
  %12 = arith.andi %11, %10 : i1
  %13 = arith.cmpi sge, %3, %c512 : index
  %14 = arith.cmpi sge, %7, %c6 : index
  %15 = arith.andi %14, %13 : i1
  %16 = arith.ori %9, %12 : i1
  %17 = arith.ori %16, %15 : i1
  scf.if %17 {
    "lmhlo.fusion"() ({
      %c1_1 = arith.constant 1 : index
      %18 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%7)[%c0, %c1]
      %19 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%c256)[%c0, %c1]
      gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %18, %arg8 = %c1_1, %arg9 = %c1_1) threads(%arg4, %arg5, %arg6) in (%arg10 = %19, %arg11 = %c1_1, %arg12 = %c1_1) {
        %20 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%c1, %c0]
        %21 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1, %c0]
        %22 = gpu.block_id  x
        %23 = gpu.thread_id  x
        %24 = memref.alloc() : memref<32xf32, 3>
        %25 = arith.divui %23, %c32 : index
        %26 = arith.remui %23, %c32 : index
        %27 = arith.subi %3, %23 : index
        %28 = arith.cmpi eq, %27, %c0 : index
        %29 = arith.subi %27, %c1 : index
        %30 = arith.divui %29, %c256 : index
        %31 = arith.addi %30, %c1 : index
        %32 = arith.select %28, %c0, %31 : index
        %33 = arith.remsi %32, %c4 : index
        %34 = arith.subi %32, %33 : index
        %35 = arith.muli %34, %c256 : index
        %36 = arith.addi %23, %35 : index
        %37 = scf.for %arg13 = %23 to %36 step %c1024 iter_args(%arg14 = %cst_0) -> (f32) {
          %71 = arith.addi %arg13, %c256 : index
          %72 = arith.addi %arg13, %c512 : index
          %73 = arith.addi %arg13, %c768 : index
          %74 = arith.muli %22, %3 : index
          %75 = arith.addi %74, %arg13 : index
          %76 = arith.addi %74, %71 : index
          %77 = arith.addi %74, %72 : index
          %78 = arith.addi %74, %73 : index
          %79 = arith.muli %2, %1 : index
          %80 = arith.muli %79, %3 : index
          %81 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%80], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %82 = memref.load %81[%75] : memref<?xf32, "gpu">
          %83 = memref.load %81[%76] : memref<?xf32, "gpu">
          %84 = memref.load %81[%77] : memref<?xf32, "gpu">
          %85 = memref.load %81[%78] : memref<?xf32, "gpu">
          %86 = arith.cmpf ugt, %arg14, %82 : f32
          %87 = arith.select %86, %arg14, %82 : f32
          %88 = arith.cmpf uno, %82, %82 : f32
          %89 = arith.select %88, %82, %87 : f32
          %90 = arith.cmpf ugt, %89, %83 : f32
          %91 = arith.select %90, %89, %83 : f32
          %92 = arith.cmpf uno, %83, %83 : f32
          %93 = arith.select %92, %83, %91 : f32
          %94 = arith.cmpf ugt, %93, %84 : f32
          %95 = arith.select %94, %93, %84 : f32
          %96 = arith.cmpf uno, %84, %84 : f32
          %97 = arith.select %96, %84, %95 : f32
          %98 = arith.cmpf ugt, %97, %85 : f32
          %99 = arith.select %98, %97, %85 : f32
          %100 = arith.cmpf uno, %85, %85 : f32
          %101 = arith.select %100, %85, %99 : f32
          scf.yield %101 : f32
        }
        %38 = scf.for %arg13 = %36 to %3 step %c256 iter_args(%arg14 = %37) -> (f32) {
          %71 = arith.muli %22, %3 : index
          %72 = arith.addi %71, %arg13 : index
          %73 = arith.muli %2, %1 : index
          %74 = arith.muli %73, %3 : index
          %75 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%74], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %76 = memref.load %75[%72] : memref<?xf32, "gpu">
          %77 = arith.cmpf ugt, %arg14, %76 : f32
          %78 = arith.select %77, %arg14, %76 : f32
          %79 = arith.cmpf uno, %76, %76 : f32
          %80 = arith.select %79, %76, %78 : f32
          scf.yield %80 : f32
        }
        %result, %valid = gpu.shuffle  xor %38, %c1_i32, %c32_i32 : f32
        %39 = arith.cmpf ugt, %38, %result : f32
        %40 = arith.select %39, %38, %result : f32
        %41 = arith.cmpf uno, %result, %result : f32
        %42 = arith.select %41, %result, %40 : f32
        %result_2, %valid_3 = gpu.shuffle  xor %42, %c2_i32, %c32_i32 : f32
        %43 = arith.cmpf ugt, %42, %result_2 : f32
        %44 = arith.select %43, %42, %result_2 : f32
        %45 = arith.cmpf uno, %result_2, %result_2 : f32
        %46 = arith.select %45, %result_2, %44 : f32
        %result_4, %valid_5 = gpu.shuffle  xor %46, %c4_i32, %c32_i32 : f32
        %47 = arith.cmpf ugt, %46, %result_4 : f32
        %48 = arith.select %47, %46, %result_4 : f32
        %49 = arith.cmpf uno, %result_4, %result_4 : f32
        %50 = arith.select %49, %result_4, %48 : f32
        %result_6, %valid_7 = gpu.shuffle  xor %50, %c8_i32, %c32_i32 : f32
        %51 = arith.cmpf ugt, %50, %result_6 : f32
        %52 = arith.select %51, %50, %result_6 : f32
        %53 = arith.cmpf uno, %result_6, %result_6 : f32
        %54 = arith.select %53, %result_6, %52 : f32
        %result_8, %valid_9 = gpu.shuffle  xor %54, %c16_i32, %c32_i32 : f32
        %55 = arith.cmpf ugt, %54, %result_8 : f32
        %56 = arith.select %55, %54, %result_8 : f32
        %57 = arith.cmpf uno, %result_8, %result_8 : f32
        %58 = arith.select %57, %result_8, %56 : f32
        %59 = memref.alloc() : memref<8xf32, 3>
        %60 = arith.cmpi eq, %26, %c0 : index
        scf.if %60 {
          %71 = memref.reinterpret_cast %59 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
          memref.store %58, %71[%25] : memref<8xf32, 3>
        }
        gpu.barrier
        %61 = arith.cmpi slt, %23, %c32 : index
        scf.if %61 {
          %71 = arith.cmpi slt, %26, %c8 : index
          %72 = scf.if %71 -> (f32) {
            %85 = memref.reinterpret_cast %59 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
            %86 = memref.load %85[%26] : memref<8xf32, 3>
            scf.yield %86 : f32
          } else {
            scf.yield %cst_0 : f32
          }
          %result_20, %valid_21 = gpu.shuffle  xor %72, %c1_i32, %c8_i32 : f32
          %73 = arith.cmpf ugt, %72, %result_20 : f32
          %74 = arith.select %73, %72, %result_20 : f32
          %75 = arith.cmpf uno, %result_20, %result_20 : f32
          %76 = arith.select %75, %result_20, %74 : f32
          %result_22, %valid_23 = gpu.shuffle  xor %76, %c2_i32, %c8_i32 : f32
          %77 = arith.cmpf ugt, %76, %result_22 : f32
          %78 = arith.select %77, %76, %result_22 : f32
          %79 = arith.cmpf uno, %result_22, %result_22 : f32
          %80 = arith.select %79, %result_22, %78 : f32
          %result_24, %valid_25 = gpu.shuffle  xor %80, %c4_i32, %c8_i32 : f32
          %81 = arith.cmpf ugt, %80, %result_24 : f32
          %82 = arith.select %81, %80, %result_24 : f32
          %83 = arith.cmpf uno, %result_24, %result_24 : f32
          %84 = arith.select %83, %result_24, %82 : f32
          scf.if %60 {
            %85 = memref.reinterpret_cast %24 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            memref.store %84, %85[%c0] : memref<32xf32, 3>
          }
        }
        gpu.barrier
        %62 = memref.alloc() : memref<32xf32, 3>
        %63 = scf.for %arg13 = %23 to %36 step %c1024 iter_args(%arg14 = %cst) -> (f32) {
          %71 = arith.addi %arg13, %c256 : index
          %72 = arith.addi %arg13, %c512 : index
          %73 = arith.addi %arg13, %c768 : index
          %74 = arith.muli %22, %3 : index
          %75 = arith.addi %74, %arg13 : index
          %76 = arith.addi %74, %71 : index
          %77 = arith.addi %74, %72 : index
          %78 = arith.addi %74, %73 : index
          %79 = arith.muli %2, %1 : index
          %80 = arith.muli %79, %3 : index
          %81 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%80], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %82 = memref.load %81[%75] : memref<?xf32, "gpu">
          %83 = memref.load %81[%76] : memref<?xf32, "gpu">
          %84 = memref.load %81[%77] : memref<?xf32, "gpu">
          %85 = memref.load %81[%78] : memref<?xf32, "gpu">
          %86 = memref.reinterpret_cast %24 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %87 = memref.load %86[%c0] : memref<32xf32, 3>
          %88 = arith.subf %82, %87 : f32
          %89 = arith.subf %83, %87 : f32
          %90 = arith.subf %84, %87 : f32
          %91 = arith.subf %85, %87 : f32
          %92 = math.exp %88 : f32
          %93 = math.exp %89 : f32
          %94 = math.exp %90 : f32
          %95 = math.exp %91 : f32
          %96 = arith.addf %arg14, %92 : f32
          %97 = arith.addf %96, %93 : f32
          %98 = arith.addf %97, %94 : f32
          %99 = arith.addf %98, %95 : f32
          scf.yield %99 : f32
        }
        %64 = scf.for %arg13 = %36 to %3 step %c256 iter_args(%arg14 = %63) -> (f32) {
          %71 = arith.muli %22, %3 : index
          %72 = arith.addi %71, %arg13 : index
          %73 = arith.muli %2, %1 : index
          %74 = arith.muli %73, %3 : index
          %75 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%74], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %76 = memref.load %75[%72] : memref<?xf32, "gpu">
          %77 = memref.reinterpret_cast %24 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %78 = memref.load %77[%c0] : memref<32xf32, 3>
          %79 = arith.subf %76, %78 : f32
          %80 = math.exp %79 : f32
          %81 = arith.addf %arg14, %80 : f32
          scf.yield %81 : f32
        }
        %result_10, %valid_11 = gpu.shuffle  xor %64, %c1_i32, %c32_i32 : f32
        %65 = arith.addf %64, %result_10 : f32
        %result_12, %valid_13 = gpu.shuffle  xor %65, %c2_i32, %c32_i32 : f32
        %66 = arith.addf %65, %result_12 : f32
        %result_14, %valid_15 = gpu.shuffle  xor %66, %c4_i32, %c32_i32 : f32
        %67 = arith.addf %66, %result_14 : f32
        %result_16, %valid_17 = gpu.shuffle  xor %67, %c8_i32, %c32_i32 : f32
        %68 = arith.addf %67, %result_16 : f32
        %result_18, %valid_19 = gpu.shuffle  xor %68, %c16_i32, %c32_i32 : f32
        %69 = arith.addf %68, %result_18 : f32
        %70 = memref.alloc() : memref<8xf32, 3>
        scf.if %60 {
          %71 = memref.reinterpret_cast %70 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
          memref.store %69, %71[%25] : memref<8xf32, 3>
        }
        gpu.barrier
        scf.if %61 {
          %71 = arith.cmpi slt, %26, %c8 : index
          %72 = scf.if %71 -> (f32) {
            %76 = memref.reinterpret_cast %70 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
            %77 = memref.load %76[%26] : memref<8xf32, 3>
            scf.yield %77 : f32
          } else {
            scf.yield %cst : f32
          }
          %result_20, %valid_21 = gpu.shuffle  xor %72, %c1_i32, %c8_i32 : f32
          %73 = arith.addf %72, %result_20 : f32
          %result_22, %valid_23 = gpu.shuffle  xor %73, %c2_i32, %c8_i32 : f32
          %74 = arith.addf %73, %result_22 : f32
          %result_24, %valid_25 = gpu.shuffle  xor %74, %c4_i32, %c8_i32 : f32
          %75 = arith.addf %74, %result_24 : f32
          scf.if %60 {
            %76 = memref.reinterpret_cast %62 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            memref.store %75, %76[%c0] : memref<32xf32, 3>
          }
        }
        gpu.barrier
        scf.for %arg13 = %23 to %36 step %c1024 {
          %71 = arith.addi %arg13, %c256 : index
          %72 = arith.addi %arg13, %c512 : index
          %73 = arith.addi %arg13, %c768 : index
          %74 = arith.muli %22, %3 : index
          %75 = arith.addi %74, %arg13 : index
          %76 = arith.addi %74, %71 : index
          %77 = arith.addi %74, %72 : index
          %78 = arith.addi %74, %73 : index
          %79 = arith.muli %2, %1 : index
          %80 = arith.muli %79, %3 : index
          %81 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%80], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %82 = memref.load %81[%75] : memref<?xf32, "gpu">
          %83 = memref.load %81[%76] : memref<?xf32, "gpu">
          %84 = memref.load %81[%77] : memref<?xf32, "gpu">
          %85 = memref.load %81[%78] : memref<?xf32, "gpu">
          %86 = memref.reinterpret_cast %24 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %87 = memref.load %86[%c0] : memref<32xf32, 3>
          %88 = arith.subf %82, %87 : f32
          %89 = arith.subf %83, %87 : f32
          %90 = arith.subf %84, %87 : f32
          %91 = arith.subf %85, %87 : f32
          %92 = math.exp %88 : f32
          %93 = math.exp %89 : f32
          %94 = math.exp %90 : f32
          %95 = math.exp %91 : f32
          %96 = memref.reinterpret_cast %62 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %97 = memref.load %96[%c0] : memref<32xf32, 3>
          %98 = arith.divf %92, %97 : f32
          %99 = arith.divf %93, %97 : f32
          %100 = arith.divf %94, %97 : f32
          %101 = arith.divf %95, %97 : f32
          %102 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%80], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          memref.store %98, %102[%75] : memref<?xf32, "gpu">
          memref.store %99, %102[%76] : memref<?xf32, "gpu">
          memref.store %100, %102[%77] : memref<?xf32, "gpu">
          memref.store %101, %102[%78] : memref<?xf32, "gpu">
        }
        scf.for %arg13 = %36 to %3 step %c256 {
          %71 = arith.muli %22, %3 : index
          %72 = arith.addi %71, %arg13 : index
          %73 = arith.muli %2, %1 : index
          %74 = arith.muli %73, %3 : index
          %75 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%74], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %76 = memref.load %75[%72] : memref<?xf32, "gpu">
          %77 = memref.reinterpret_cast %24 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %78 = memref.load %77[%c0] : memref<32xf32, 3>
          %79 = arith.subf %76, %78 : f32
          %80 = math.exp %79 : f32
          %81 = memref.reinterpret_cast %62 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %82 = memref.load %81[%c0] : memref<32xf32, 3>
          %83 = arith.divf %80, %82 : f32
          %84 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%74], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          memref.store %83, %84[%72] : memref<?xf32, "gpu">
        }
        gpu.terminator
      } {SCFToGPU_visited}
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      %18 = arith.cmpi eq, %7, %c0 : index
      %19 = arith.subi %7, %c1 : index
      %20 = arith.divui %19, %c8 : index
      %21 = arith.addi %20, %c1 : index
      %22 = arith.select %18, %c0, %21 : index
      %c1_1 = arith.constant 1 : index
      %23 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%22)[%c0, %c1]
      %24 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%c256)[%c0, %c1]
      gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %23, %arg8 = %c1_1, %arg9 = %c1_1) threads(%arg4, %arg5, %arg6) in (%arg10 = %24, %arg11 = %c1_1, %arg12 = %c1_1) {
        %25 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%c1, %c0]
        %26 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1, %c0]
        %27 = gpu.block_id  x
        %28 = gpu.thread_id  x
        %29 = arith.divui %28, %c32 : index
        %30 = arith.remui %28, %c32 : index
        %31 = arith.muli %27, %c8 : index
        %32 = arith.addi %31, %29 : index
        %33 = arith.cmpi slt, %32, %7 : index
        %34 = memref.alloc() : memref<32xf32, 3>
        scf.if %33 {
          %36 = arith.subi %3, %30 : index
          %37 = arith.cmpi eq, %36, %c0 : index
          %38 = arith.subi %36, %c1 : index
          %39 = arith.divui %38, %c32 : index
          %40 = arith.addi %39, %c1 : index
          %41 = arith.select %37, %c0, %40 : index
          %42 = arith.remsi %41, %c4 : index
          %43 = arith.subi %41, %42 : index
          %44 = arith.muli %43, %c32 : index
          %45 = arith.addi %30, %44 : index
          %46 = scf.for %arg13 = %30 to %45 step %c128 iter_args(%arg14 = %cst_0) -> (f32) {
            %69 = arith.addi %arg13, %c32 : index
            %70 = arith.addi %arg13, %c64 : index
            %71 = arith.addi %arg13, %c96 : index
            %72 = arith.muli %32, %3 : index
            %73 = arith.addi %72, %arg13 : index
            %74 = arith.addi %72, %69 : index
            %75 = arith.addi %72, %70 : index
            %76 = arith.addi %72, %71 : index
            %77 = arith.muli %2, %1 : index
            %78 = arith.muli %77, %3 : index
            %79 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%78], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %80 = memref.load %79[%73] : memref<?xf32, "gpu">
            %81 = memref.load %79[%74] : memref<?xf32, "gpu">
            %82 = memref.load %79[%75] : memref<?xf32, "gpu">
            %83 = memref.load %79[%76] : memref<?xf32, "gpu">
            %84 = arith.cmpf ugt, %arg14, %80 : f32
            %85 = arith.select %84, %arg14, %80 : f32
            %86 = arith.cmpf uno, %80, %80 : f32
            %87 = arith.select %86, %80, %85 : f32
            %88 = arith.cmpf ugt, %87, %81 : f32
            %89 = arith.select %88, %87, %81 : f32
            %90 = arith.cmpf uno, %81, %81 : f32
            %91 = arith.select %90, %81, %89 : f32
            %92 = arith.cmpf ugt, %91, %82 : f32
            %93 = arith.select %92, %91, %82 : f32
            %94 = arith.cmpf uno, %82, %82 : f32
            %95 = arith.select %94, %82, %93 : f32
            %96 = arith.cmpf ugt, %95, %83 : f32
            %97 = arith.select %96, %95, %83 : f32
            %98 = arith.cmpf uno, %83, %83 : f32
            %99 = arith.select %98, %83, %97 : f32
            scf.yield %99 : f32
          }
          %47 = scf.for %arg13 = %45 to %3 step %c32 iter_args(%arg14 = %46) -> (f32) {
            %69 = arith.muli %32, %3 : index
            %70 = arith.addi %69, %arg13 : index
            %71 = arith.muli %2, %1 : index
            %72 = arith.muli %71, %3 : index
            %73 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%72], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %74 = memref.load %73[%70] : memref<?xf32, "gpu">
            %75 = arith.cmpf ugt, %arg14, %74 : f32
            %76 = arith.select %75, %arg14, %74 : f32
            %77 = arith.cmpf uno, %74, %74 : f32
            %78 = arith.select %77, %74, %76 : f32
            scf.yield %78 : f32
          }
          %result, %valid = gpu.shuffle  xor %47, %c1_i32, %c32_i32 : f32
          %48 = arith.cmpf ugt, %47, %result : f32
          %49 = arith.select %48, %47, %result : f32
          %50 = arith.cmpf uno, %result, %result : f32
          %51 = arith.select %50, %result, %49 : f32
          %result_2, %valid_3 = gpu.shuffle  xor %51, %c2_i32, %c32_i32 : f32
          %52 = arith.cmpf ugt, %51, %result_2 : f32
          %53 = arith.select %52, %51, %result_2 : f32
          %54 = arith.cmpf uno, %result_2, %result_2 : f32
          %55 = arith.select %54, %result_2, %53 : f32
          %result_4, %valid_5 = gpu.shuffle  xor %55, %c4_i32, %c32_i32 : f32
          %56 = arith.cmpf ugt, %55, %result_4 : f32
          %57 = arith.select %56, %55, %result_4 : f32
          %58 = arith.cmpf uno, %result_4, %result_4 : f32
          %59 = arith.select %58, %result_4, %57 : f32
          %result_6, %valid_7 = gpu.shuffle  xor %59, %c8_i32, %c32_i32 : f32
          %60 = arith.cmpf ugt, %59, %result_6 : f32
          %61 = arith.select %60, %59, %result_6 : f32
          %62 = arith.cmpf uno, %result_6, %result_6 : f32
          %63 = arith.select %62, %result_6, %61 : f32
          %result_8, %valid_9 = gpu.shuffle  xor %63, %c16_i32, %c32_i32 : f32
          %64 = arith.cmpf ugt, %63, %result_8 : f32
          %65 = arith.select %64, %63, %result_8 : f32
          %66 = arith.cmpf uno, %result_8, %result_8 : f32
          %67 = arith.select %66, %result_8, %65 : f32
          %68 = arith.cmpi eq, %30, %c0 : index
          scf.if %68 {
            %69 = memref.reinterpret_cast %34 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            memref.store %67, %69[%29] : memref<32xf32, 3>
          }
        }
        gpu.barrier
        %35 = memref.alloc() : memref<32xf32, 3>
        scf.if %33 {
          %36 = arith.subi %3, %30 : index
          %37 = arith.cmpi eq, %36, %c0 : index
          %38 = arith.subi %36, %c1 : index
          %39 = arith.divui %38, %c32 : index
          %40 = arith.addi %39, %c1 : index
          %41 = arith.select %37, %c0, %40 : index
          %42 = arith.remsi %41, %c4 : index
          %43 = arith.subi %41, %42 : index
          %44 = arith.muli %43, %c32 : index
          %45 = arith.addi %30, %44 : index
          %46 = scf.for %arg13 = %30 to %45 step %c128 iter_args(%arg14 = %cst) -> (f32) {
            %54 = arith.addi %arg13, %c32 : index
            %55 = arith.addi %arg13, %c64 : index
            %56 = arith.addi %arg13, %c96 : index
            %57 = arith.muli %32, %3 : index
            %58 = arith.addi %57, %arg13 : index
            %59 = arith.addi %57, %54 : index
            %60 = arith.addi %57, %55 : index
            %61 = arith.addi %57, %56 : index
            %62 = arith.divui %58, %3 : index
            %63 = arith.remui %62, %1 : index
            %64 = arith.divui %62, %1 : index
            %65 = arith.divui %59, %3 : index
            %66 = arith.remui %65, %1 : index
            %67 = arith.divui %65, %1 : index
            %68 = arith.divui %60, %3 : index
            %69 = arith.remui %68, %1 : index
            %70 = arith.divui %68, %1 : index
            %71 = arith.divui %61, %3 : index
            %72 = arith.remui %71, %1 : index
            %73 = arith.divui %71, %1 : index
            %74 = arith.muli %2, %1 : index
            %75 = arith.muli %74, %3 : index
            %76 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%75], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %77 = memref.load %76[%58] : memref<?xf32, "gpu">
            %78 = memref.load %76[%59] : memref<?xf32, "gpu">
            %79 = memref.load %76[%60] : memref<?xf32, "gpu">
            %80 = memref.load %76[%61] : memref<?xf32, "gpu">
            %81 = arith.muli %64, %1 : index
            %82 = arith.addi %81, %63 : index
            %83 = arith.muli %67, %1 : index
            %84 = arith.addi %83, %66 : index
            %85 = arith.muli %70, %1 : index
            %86 = arith.addi %85, %69 : index
            %87 = arith.muli %73, %1 : index
            %88 = arith.addi %87, %72 : index
            %89 = arith.remui %82, %c8 : index
            %90 = arith.remui %84, %c8 : index
            %91 = arith.remui %86, %c8 : index
            %92 = arith.remui %88, %c8 : index
            %93 = memref.reinterpret_cast %34 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %94 = memref.load %93[%89] : memref<32xf32, 3>
            %95 = memref.load %93[%90] : memref<32xf32, 3>
            %96 = memref.load %93[%91] : memref<32xf32, 3>
            %97 = memref.load %93[%92] : memref<32xf32, 3>
            %98 = arith.subf %77, %94 : f32
            %99 = arith.subf %78, %95 : f32
            %100 = arith.subf %79, %96 : f32
            %101 = arith.subf %80, %97 : f32
            %102 = math.exp %98 : f32
            %103 = math.exp %99 : f32
            %104 = math.exp %100 : f32
            %105 = math.exp %101 : f32
            %106 = arith.addf %arg14, %102 : f32
            %107 = arith.addf %106, %103 : f32
            %108 = arith.addf %107, %104 : f32
            %109 = arith.addf %108, %105 : f32
            scf.yield %109 : f32
          }
          %47 = scf.for %arg13 = %45 to %3 step %c32 iter_args(%arg14 = %46) -> (f32) {
            %54 = arith.muli %32, %3 : index
            %55 = arith.addi %54, %arg13 : index
            %56 = arith.divui %55, %3 : index
            %57 = arith.remui %56, %1 : index
            %58 = arith.divui %56, %1 : index
            %59 = arith.muli %2, %1 : index
            %60 = arith.muli %59, %3 : index
            %61 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%60], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %62 = memref.load %61[%55] : memref<?xf32, "gpu">
            %63 = arith.muli %58, %1 : index
            %64 = arith.addi %63, %57 : index
            %65 = arith.remui %64, %c8 : index
            %66 = memref.reinterpret_cast %34 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %67 = memref.load %66[%65] : memref<32xf32, 3>
            %68 = arith.subf %62, %67 : f32
            %69 = math.exp %68 : f32
            %70 = arith.addf %arg14, %69 : f32
            scf.yield %70 : f32
          }
          %result, %valid = gpu.shuffle  xor %47, %c1_i32, %c32_i32 : f32
          %48 = arith.addf %47, %result : f32
          %result_2, %valid_3 = gpu.shuffle  xor %48, %c2_i32, %c32_i32 : f32
          %49 = arith.addf %48, %result_2 : f32
          %result_4, %valid_5 = gpu.shuffle  xor %49, %c4_i32, %c32_i32 : f32
          %50 = arith.addf %49, %result_4 : f32
          %result_6, %valid_7 = gpu.shuffle  xor %50, %c8_i32, %c32_i32 : f32
          %51 = arith.addf %50, %result_6 : f32
          %result_8, %valid_9 = gpu.shuffle  xor %51, %c16_i32, %c32_i32 : f32
          %52 = arith.addf %51, %result_8 : f32
          %53 = arith.cmpi eq, %30, %c0 : index
          scf.if %53 {
            %54 = memref.reinterpret_cast %35 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            memref.store %52, %54[%29] : memref<32xf32, 3>
          }
        }
        gpu.barrier
        scf.if %33 {
          %36 = arith.subi %3, %30 : index
          %37 = arith.cmpi eq, %36, %c0 : index
          %38 = arith.subi %36, %c1 : index
          %39 = arith.divui %38, %c32 : index
          %40 = arith.addi %39, %c1 : index
          %41 = arith.select %37, %c0, %40 : index
          %42 = arith.remsi %41, %c4 : index
          %43 = arith.subi %41, %42 : index
          %44 = arith.muli %43, %c32 : index
          %45 = arith.addi %30, %44 : index
          scf.for %arg13 = %30 to %45 step %c128 {
            %46 = arith.addi %arg13, %c32 : index
            %47 = arith.addi %arg13, %c64 : index
            %48 = arith.addi %arg13, %c96 : index
            %49 = arith.muli %32, %3 : index
            %50 = arith.addi %49, %arg13 : index
            %51 = arith.addi %49, %46 : index
            %52 = arith.addi %49, %47 : index
            %53 = arith.addi %49, %48 : index
            %54 = arith.muli %2, %1 : index
            %55 = arith.muli %54, %3 : index
            %56 = arith.divui %50, %3 : index
            %57 = arith.remui %56, %1 : index
            %58 = arith.divui %56, %1 : index
            %59 = arith.divui %51, %3 : index
            %60 = arith.remui %59, %1 : index
            %61 = arith.divui %59, %1 : index
            %62 = arith.divui %52, %3 : index
            %63 = arith.remui %62, %1 : index
            %64 = arith.divui %62, %1 : index
            %65 = arith.divui %53, %3 : index
            %66 = arith.remui %65, %1 : index
            %67 = arith.divui %65, %1 : index
            %68 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%55], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %69 = memref.load %68[%50] : memref<?xf32, "gpu">
            %70 = memref.load %68[%51] : memref<?xf32, "gpu">
            %71 = memref.load %68[%52] : memref<?xf32, "gpu">
            %72 = memref.load %68[%53] : memref<?xf32, "gpu">
            %73 = arith.muli %58, %1 : index
            %74 = arith.addi %73, %57 : index
            %75 = arith.muli %61, %1 : index
            %76 = arith.addi %75, %60 : index
            %77 = arith.muli %64, %1 : index
            %78 = arith.addi %77, %63 : index
            %79 = arith.muli %67, %1 : index
            %80 = arith.addi %79, %66 : index
            %81 = arith.remui %74, %c8 : index
            %82 = arith.remui %76, %c8 : index
            %83 = arith.remui %78, %c8 : index
            %84 = arith.remui %80, %c8 : index
            %85 = memref.reinterpret_cast %34 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %86 = memref.load %85[%81] : memref<32xf32, 3>
            %87 = memref.load %85[%82] : memref<32xf32, 3>
            %88 = memref.load %85[%83] : memref<32xf32, 3>
            %89 = memref.load %85[%84] : memref<32xf32, 3>
            %90 = arith.subf %69, %86 : f32
            %91 = arith.subf %70, %87 : f32
            %92 = arith.subf %71, %88 : f32
            %93 = arith.subf %72, %89 : f32
            %94 = math.exp %90 : f32
            %95 = math.exp %91 : f32
            %96 = math.exp %92 : f32
            %97 = math.exp %93 : f32
            %98 = memref.reinterpret_cast %35 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %99 = memref.load %98[%81] : memref<32xf32, 3>
            %100 = memref.load %98[%82] : memref<32xf32, 3>
            %101 = memref.load %98[%83] : memref<32xf32, 3>
            %102 = memref.load %98[%84] : memref<32xf32, 3>
            %103 = arith.divf %94, %99 : f32
            %104 = arith.divf %95, %100 : f32
            %105 = arith.divf %96, %101 : f32
            %106 = arith.divf %97, %102 : f32
            %107 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%55], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %103, %107[%50] : memref<?xf32, "gpu">
            memref.store %104, %107[%51] : memref<?xf32, "gpu">
            memref.store %105, %107[%52] : memref<?xf32, "gpu">
            memref.store %106, %107[%53] : memref<?xf32, "gpu">
          }
          scf.for %arg13 = %45 to %3 step %c32 {
            %46 = arith.muli %32, %3 : index
            %47 = arith.addi %46, %arg13 : index
            %48 = arith.muli %2, %1 : index
            %49 = arith.muli %48, %3 : index
            %50 = arith.divui %47, %3 : index
            %51 = arith.remui %50, %1 : index
            %52 = arith.divui %50, %1 : index
            %53 = memref.reinterpret_cast %0 to offset: [%c0], sizes: [%49], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %54 = memref.load %53[%47] : memref<?xf32, "gpu">
            %55 = arith.muli %52, %1 : index
            %56 = arith.addi %55, %51 : index
            %57 = arith.remui %56, %c8 : index
            %58 = memref.reinterpret_cast %34 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %59 = memref.load %58[%57] : memref<32xf32, 3>
            %60 = arith.subf %54, %59 : f32
            %61 = math.exp %60 : f32
            %62 = memref.reinterpret_cast %35 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %63 = memref.load %62[%57] : memref<32xf32, 3>
            %64 = arith.divf %61, %63 : f32
            %65 = memref.reinterpret_cast %8 to offset: [%c0], sizes: [%49], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %64, %65[%47] : memref<?xf32, "gpu">
          }
        }
        gpu.terminator
      } {SCFToGPU_visited}
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  "disc_ral.dispatch"(%arg0, %c0, %8) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After GpuLaunchSinkIndexComputations (gpu-launch-sink-index-computations) //----- //
#map0 = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c768 = arith.constant 768 : index
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c96 = arith.constant 96 : index
    %c4 = arith.constant 4 : index
    %cst = arith.constant -0.000000e+00 : f32
    %cst_0 = arith.constant 0xFF800000 : f32
    %c8 = arith.constant 8 : index
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c4_i32 = arith.constant 4 : i32
    %c2_i32 = arith.constant 2 : i32
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c1024 = arith.constant 1024 : index
    %c6 = arith.constant 6 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
    %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
    %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
    %4 = arith.index_cast %2 : index to i32
    %5 = arith.index_cast %1 : index to i32
    %6 = arith.muli %4, %5 : i32
    %7 = arith.index_cast %6 : i32 to index
    %8 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    %9 = arith.cmpi slt, %7, %c3 : index
    %10 = arith.cmpi sge, %3, %c1024 : index
    %11 = arith.cmpi slt, %7, %c6 : index
    %12 = arith.andi %11, %10 : i1
    %13 = arith.cmpi sge, %3, %c512 : index
    %14 = arith.cmpi sge, %7, %c6 : index
    %15 = arith.andi %14, %13 : i1
    %16 = arith.ori %9, %12 : i1
    %17 = arith.ori %16, %15 : i1
    scf.if %17 {
      "lmhlo.fusion"() ({
        %c1_1 = arith.constant 1 : index
        %18 = affine.apply #map0(%7)[%c0, %c1]
        %19 = affine.apply #map0(%c256)[%c0, %c1]
        gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %18, %arg8 = %c1_1, %arg9 = %c1_1) threads(%arg4, %arg5, %arg6) in (%arg10 = %19, %arg11 = %c1_1, %arg12 = %c1_1) {
          %c1_2 = arith.constant 1 : index
          %c0_3 = arith.constant 0 : index
          %c32_4 = arith.constant 32 : index
          %c2_5 = arith.constant 2 : index
          %20 = memref.dim %0, %c2_5 : memref<?x?x?xf32, "gpu">
          %c256_6 = arith.constant 256 : index
          %c4_7 = arith.constant 4 : index
          %c512_8 = arith.constant 512 : index
          %c768_9 = arith.constant 768 : index
          %21 = memref.dim %0, %c0_3 : memref<?x?x?xf32, "gpu">
          %22 = memref.dim %0, %c1_2 : memref<?x?x?xf32, "gpu">
          %c1024_10 = arith.constant 1024 : index
          %cst_11 = arith.constant 0xFF800000 : f32
          %c1_i32_12 = arith.constant 1 : i32
          %c32_i32_13 = arith.constant 32 : i32
          %c2_i32_14 = arith.constant 2 : i32
          %c4_i32_15 = arith.constant 4 : i32
          %c8_i32_16 = arith.constant 8 : i32
          %c16_i32_17 = arith.constant 16 : i32
          %c8_18 = arith.constant 8 : index
          %cst_19 = arith.constant -0.000000e+00 : f32
          %23 = affine.apply #map1(%arg1)[%c1_2, %c0_3]
          %24 = affine.apply #map1(%arg4)[%c1_2, %c0_3]
          %25 = gpu.block_id  x
          %26 = gpu.thread_id  x
          %27 = memref.alloc() : memref<32xf32, 3>
          %28 = arith.divui %26, %c32_4 : index
          %29 = arith.remui %26, %c32_4 : index
          %30 = arith.subi %20, %26 : index
          %31 = arith.cmpi eq, %30, %c0_3 : index
          %32 = arith.subi %30, %c1_2 : index
          %33 = arith.divui %32, %c256_6 : index
          %34 = arith.addi %33, %c1_2 : index
          %35 = arith.select %31, %c0_3, %34 : index
          %36 = arith.remsi %35, %c4_7 : index
          %37 = arith.subi %35, %36 : index
          %38 = arith.muli %37, %c256_6 : index
          %39 = arith.addi %26, %38 : index
          %40 = scf.for %arg13 = %26 to %39 step %c1024_10 iter_args(%arg14 = %cst_11) -> (f32) {
            %74 = arith.addi %arg13, %c256_6 : index
            %75 = arith.addi %arg13, %c512_8 : index
            %76 = arith.addi %arg13, %c768_9 : index
            %77 = arith.muli %25, %20 : index
            %78 = arith.addi %77, %arg13 : index
            %79 = arith.addi %77, %74 : index
            %80 = arith.addi %77, %75 : index
            %81 = arith.addi %77, %76 : index
            %82 = arith.muli %21, %22 : index
            %83 = arith.muli %82, %20 : index
            %84 = memref.reinterpret_cast %0 to offset: [%c0_3], sizes: [%83], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %85 = memref.load %84[%78] : memref<?xf32, "gpu">
            %86 = memref.load %84[%79] : memref<?xf32, "gpu">
            %87 = memref.load %84[%80] : memref<?xf32, "gpu">
            %88 = memref.load %84[%81] : memref<?xf32, "gpu">
            %89 = arith.cmpf ugt, %arg14, %85 : f32
            %90 = arith.select %89, %arg14, %85 : f32
            %91 = arith.cmpf uno, %85, %85 : f32
            %92 = arith.select %91, %85, %90 : f32
            %93 = arith.cmpf ugt, %92, %86 : f32
            %94 = arith.select %93, %92, %86 : f32
            %95 = arith.cmpf uno, %86, %86 : f32
            %96 = arith.select %95, %86, %94 : f32
            %97 = arith.cmpf ugt, %96, %87 : f32
            %98 = arith.select %97, %96, %87 : f32
            %99 = arith.cmpf uno, %87, %87 : f32
            %100 = arith.select %99, %87, %98 : f32
            %101 = arith.cmpf ugt, %100, %88 : f32
            %102 = arith.select %101, %100, %88 : f32
            %103 = arith.cmpf uno, %88, %88 : f32
            %104 = arith.select %103, %88, %102 : f32
            scf.yield %104 : f32
          }
          %41 = scf.for %arg13 = %39 to %20 step %c256_6 iter_args(%arg14 = %40) -> (f32) {
            %74 = arith.muli %25, %20 : index
            %75 = arith.addi %74, %arg13 : index
            %76 = arith.muli %21, %22 : index
            %77 = arith.muli %76, %20 : index
            %78 = memref.reinterpret_cast %0 to offset: [%c0_3], sizes: [%77], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %79 = memref.load %78[%75] : memref<?xf32, "gpu">
            %80 = arith.cmpf ugt, %arg14, %79 : f32
            %81 = arith.select %80, %arg14, %79 : f32
            %82 = arith.cmpf uno, %79, %79 : f32
            %83 = arith.select %82, %79, %81 : f32
            scf.yield %83 : f32
          }
          %result, %valid = gpu.shuffle  xor %41, %c1_i32_12, %c32_i32_13 : f32
          %42 = arith.cmpf ugt, %41, %result : f32
          %43 = arith.select %42, %41, %result : f32
          %44 = arith.cmpf uno, %result, %result : f32
          %45 = arith.select %44, %result, %43 : f32
          %result_20, %valid_21 = gpu.shuffle  xor %45, %c2_i32_14, %c32_i32_13 : f32
          %46 = arith.cmpf ugt, %45, %result_20 : f32
          %47 = arith.select %46, %45, %result_20 : f32
          %48 = arith.cmpf uno, %result_20, %result_20 : f32
          %49 = arith.select %48, %result_20, %47 : f32
          %result_22, %valid_23 = gpu.shuffle  xor %49, %c4_i32_15, %c32_i32_13 : f32
          %50 = arith.cmpf ugt, %49, %result_22 : f32
          %51 = arith.select %50, %49, %result_22 : f32
          %52 = arith.cmpf uno, %result_22, %result_22 : f32
          %53 = arith.select %52, %result_22, %51 : f32
          %result_24, %valid_25 = gpu.shuffle  xor %53, %c8_i32_16, %c32_i32_13 : f32
          %54 = arith.cmpf ugt, %53, %result_24 : f32
          %55 = arith.select %54, %53, %result_24 : f32
          %56 = arith.cmpf uno, %result_24, %result_24 : f32
          %57 = arith.select %56, %result_24, %55 : f32
          %result_26, %valid_27 = gpu.shuffle  xor %57, %c16_i32_17, %c32_i32_13 : f32
          %58 = arith.cmpf ugt, %57, %result_26 : f32
          %59 = arith.select %58, %57, %result_26 : f32
          %60 = arith.cmpf uno, %result_26, %result_26 : f32
          %61 = arith.select %60, %result_26, %59 : f32
          %62 = memref.alloc() : memref<8xf32, 3>
          %63 = arith.cmpi eq, %29, %c0_3 : index
          scf.if %63 {
            %74 = memref.reinterpret_cast %62 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
            memref.store %61, %74[%28] : memref<8xf32, 3>
          }
          gpu.barrier
          %64 = arith.cmpi slt, %26, %c32_4 : index
          scf.if %64 {
            %74 = arith.cmpi slt, %29, %c8_18 : index
            %75 = scf.if %74 -> (f32) {
              %88 = memref.reinterpret_cast %62 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
              %89 = memref.load %88[%29] : memref<8xf32, 3>
              scf.yield %89 : f32
            } else {
              scf.yield %cst_11 : f32
            }
            %result_38, %valid_39 = gpu.shuffle  xor %75, %c1_i32_12, %c8_i32_16 : f32
            %76 = arith.cmpf ugt, %75, %result_38 : f32
            %77 = arith.select %76, %75, %result_38 : f32
            %78 = arith.cmpf uno, %result_38, %result_38 : f32
            %79 = arith.select %78, %result_38, %77 : f32
            %result_40, %valid_41 = gpu.shuffle  xor %79, %c2_i32_14, %c8_i32_16 : f32
            %80 = arith.cmpf ugt, %79, %result_40 : f32
            %81 = arith.select %80, %79, %result_40 : f32
            %82 = arith.cmpf uno, %result_40, %result_40 : f32
            %83 = arith.select %82, %result_40, %81 : f32
            %result_42, %valid_43 = gpu.shuffle  xor %83, %c4_i32_15, %c8_i32_16 : f32
            %84 = arith.cmpf ugt, %83, %result_42 : f32
            %85 = arith.select %84, %83, %result_42 : f32
            %86 = arith.cmpf uno, %result_42, %result_42 : f32
            %87 = arith.select %86, %result_42, %85 : f32
            scf.if %63 {
              %88 = memref.reinterpret_cast %27 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %87, %88[%c0_3] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %65 = memref.alloc() : memref<32xf32, 3>
          %66 = scf.for %arg13 = %26 to %39 step %c1024_10 iter_args(%arg14 = %cst_19) -> (f32) {
            %74 = arith.addi %arg13, %c256_6 : index
            %75 = arith.addi %arg13, %c512_8 : index
            %76 = arith.addi %arg13, %c768_9 : index
            %77 = arith.muli %25, %20 : index
            %78 = arith.addi %77, %arg13 : index
            %79 = arith.addi %77, %74 : index
            %80 = arith.addi %77, %75 : index
            %81 = arith.addi %77, %76 : index
            %82 = arith.muli %21, %22 : index
            %83 = arith.muli %82, %20 : index
            %84 = memref.reinterpret_cast %0 to offset: [%c0_3], sizes: [%83], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %85 = memref.load %84[%78] : memref<?xf32, "gpu">
            %86 = memref.load %84[%79] : memref<?xf32, "gpu">
            %87 = memref.load %84[%80] : memref<?xf32, "gpu">
            %88 = memref.load %84[%81] : memref<?xf32, "gpu">
            %89 = memref.reinterpret_cast %27 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %90 = memref.load %89[%c0_3] : memref<32xf32, 3>
            %91 = arith.subf %85, %90 : f32
            %92 = arith.subf %86, %90 : f32
            %93 = arith.subf %87, %90 : f32
            %94 = arith.subf %88, %90 : f32
            %95 = math.exp %91 : f32
            %96 = math.exp %92 : f32
            %97 = math.exp %93 : f32
            %98 = math.exp %94 : f32
            %99 = arith.addf %arg14, %95 : f32
            %100 = arith.addf %99, %96 : f32
            %101 = arith.addf %100, %97 : f32
            %102 = arith.addf %101, %98 : f32
            scf.yield %102 : f32
          }
          %67 = scf.for %arg13 = %39 to %20 step %c256_6 iter_args(%arg14 = %66) -> (f32) {
            %74 = arith.muli %25, %20 : index
            %75 = arith.addi %74, %arg13 : index
            %76 = arith.muli %21, %22 : index
            %77 = arith.muli %76, %20 : index
            %78 = memref.reinterpret_cast %0 to offset: [%c0_3], sizes: [%77], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %79 = memref.load %78[%75] : memref<?xf32, "gpu">
            %80 = memref.reinterpret_cast %27 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %81 = memref.load %80[%c0_3] : memref<32xf32, 3>
            %82 = arith.subf %79, %81 : f32
            %83 = math.exp %82 : f32
            %84 = arith.addf %arg14, %83 : f32
            scf.yield %84 : f32
          }
          %result_28, %valid_29 = gpu.shuffle  xor %67, %c1_i32_12, %c32_i32_13 : f32
          %68 = arith.addf %67, %result_28 : f32
          %result_30, %valid_31 = gpu.shuffle  xor %68, %c2_i32_14, %c32_i32_13 : f32
          %69 = arith.addf %68, %result_30 : f32
          %result_32, %valid_33 = gpu.shuffle  xor %69, %c4_i32_15, %c32_i32_13 : f32
          %70 = arith.addf %69, %result_32 : f32
          %result_34, %valid_35 = gpu.shuffle  xor %70, %c8_i32_16, %c32_i32_13 : f32
          %71 = arith.addf %70, %result_34 : f32
          %result_36, %valid_37 = gpu.shuffle  xor %71, %c16_i32_17, %c32_i32_13 : f32
          %72 = arith.addf %71, %result_36 : f32
          %73 = memref.alloc() : memref<8xf32, 3>
          scf.if %63 {
            %74 = memref.reinterpret_cast %73 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
            memref.store %72, %74[%28] : memref<8xf32, 3>
          }
          gpu.barrier
          scf.if %64 {
            %74 = arith.cmpi slt, %29, %c8_18 : index
            %75 = scf.if %74 -> (f32) {
              %79 = memref.reinterpret_cast %73 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
              %80 = memref.load %79[%29] : memref<8xf32, 3>
              scf.yield %80 : f32
            } else {
              scf.yield %cst_19 : f32
            }
            %result_38, %valid_39 = gpu.shuffle  xor %75, %c1_i32_12, %c8_i32_16 : f32
            %76 = arith.addf %75, %result_38 : f32
            %result_40, %valid_41 = gpu.shuffle  xor %76, %c2_i32_14, %c8_i32_16 : f32
            %77 = arith.addf %76, %result_40 : f32
            %result_42, %valid_43 = gpu.shuffle  xor %77, %c4_i32_15, %c8_i32_16 : f32
            %78 = arith.addf %77, %result_42 : f32
            scf.if %63 {
              %79 = memref.reinterpret_cast %65 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %78, %79[%c0_3] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          scf.for %arg13 = %26 to %39 step %c1024_10 {
            %74 = arith.addi %arg13, %c256_6 : index
            %75 = arith.addi %arg13, %c512_8 : index
            %76 = arith.addi %arg13, %c768_9 : index
            %77 = arith.muli %25, %20 : index
            %78 = arith.addi %77, %arg13 : index
            %79 = arith.addi %77, %74 : index
            %80 = arith.addi %77, %75 : index
            %81 = arith.addi %77, %76 : index
            %82 = arith.muli %21, %22 : index
            %83 = arith.muli %82, %20 : index
            %84 = memref.reinterpret_cast %0 to offset: [%c0_3], sizes: [%83], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %85 = memref.load %84[%78] : memref<?xf32, "gpu">
            %86 = memref.load %84[%79] : memref<?xf32, "gpu">
            %87 = memref.load %84[%80] : memref<?xf32, "gpu">
            %88 = memref.load %84[%81] : memref<?xf32, "gpu">
            %89 = memref.reinterpret_cast %27 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %90 = memref.load %89[%c0_3] : memref<32xf32, 3>
            %91 = arith.subf %85, %90 : f32
            %92 = arith.subf %86, %90 : f32
            %93 = arith.subf %87, %90 : f32
            %94 = arith.subf %88, %90 : f32
            %95 = math.exp %91 : f32
            %96 = math.exp %92 : f32
            %97 = math.exp %93 : f32
            %98 = math.exp %94 : f32
            %99 = memref.reinterpret_cast %65 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %100 = memref.load %99[%c0_3] : memref<32xf32, 3>
            %101 = arith.divf %95, %100 : f32
            %102 = arith.divf %96, %100 : f32
            %103 = arith.divf %97, %100 : f32
            %104 = arith.divf %98, %100 : f32
            %105 = memref.reinterpret_cast %8 to offset: [%c0_3], sizes: [%83], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %101, %105[%78] : memref<?xf32, "gpu">
            memref.store %102, %105[%79] : memref<?xf32, "gpu">
            memref.store %103, %105[%80] : memref<?xf32, "gpu">
            memref.store %104, %105[%81] : memref<?xf32, "gpu">
          }
          scf.for %arg13 = %39 to %20 step %c256_6 {
            %74 = arith.muli %25, %20 : index
            %75 = arith.addi %74, %arg13 : index
            %76 = arith.muli %21, %22 : index
            %77 = arith.muli %76, %20 : index
            %78 = memref.reinterpret_cast %0 to offset: [%c0_3], sizes: [%77], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %79 = memref.load %78[%75] : memref<?xf32, "gpu">
            %80 = memref.reinterpret_cast %27 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %81 = memref.load %80[%c0_3] : memref<32xf32, 3>
            %82 = arith.subf %79, %81 : f32
            %83 = math.exp %82 : f32
            %84 = memref.reinterpret_cast %65 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
            %85 = memref.load %84[%c0_3] : memref<32xf32, 3>
            %86 = arith.divf %83, %85 : f32
            %87 = memref.reinterpret_cast %8 to offset: [%c0_3], sizes: [%77], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %86, %87[%75] : memref<?xf32, "gpu">
          }
          gpu.terminator
        } {SCFToGPU_visited}
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        %18 = arith.cmpi eq, %7, %c0 : index
        %19 = arith.subi %7, %c1 : index
        %20 = arith.divui %19, %c8 : index
        %21 = arith.addi %20, %c1 : index
        %22 = arith.select %18, %c0, %21 : index
        %c1_1 = arith.constant 1 : index
        %23 = affine.apply #map0(%22)[%c0, %c1]
        %24 = affine.apply #map0(%c256)[%c0, %c1]
        gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %23, %arg8 = %c1_1, %arg9 = %c1_1) threads(%arg4, %arg5, %arg6) in (%arg10 = %24, %arg11 = %c1_1, %arg12 = %c1_1) {
          %c1_2 = arith.constant 1 : index
          %c0_3 = arith.constant 0 : index
          %c32_4 = arith.constant 32 : index
          %c8_5 = arith.constant 8 : index
          %c2_6 = arith.constant 2 : index
          %25 = memref.dim %0, %c2_6 : memref<?x?x?xf32, "gpu">
          %c4_7 = arith.constant 4 : index
          %c64_8 = arith.constant 64 : index
          %c96_9 = arith.constant 96 : index
          %26 = memref.dim %0, %c0_3 : memref<?x?x?xf32, "gpu">
          %27 = memref.dim %0, %c1_2 : memref<?x?x?xf32, "gpu">
          %c128_10 = arith.constant 128 : index
          %cst_11 = arith.constant 0xFF800000 : f32
          %c1_i32_12 = arith.constant 1 : i32
          %c32_i32_13 = arith.constant 32 : i32
          %c2_i32_14 = arith.constant 2 : i32
          %c4_i32_15 = arith.constant 4 : i32
          %c8_i32_16 = arith.constant 8 : i32
          %c16_i32_17 = arith.constant 16 : i32
          %cst_18 = arith.constant -0.000000e+00 : f32
          %28 = affine.apply #map1(%arg1)[%c1_2, %c0_3]
          %29 = affine.apply #map1(%arg4)[%c1_2, %c0_3]
          %30 = gpu.block_id  x
          %31 = gpu.thread_id  x
          %32 = arith.divui %31, %c32_4 : index
          %33 = arith.remui %31, %c32_4 : index
          %34 = arith.muli %30, %c8_5 : index
          %35 = arith.addi %34, %32 : index
          %36 = arith.cmpi slt, %35, %7 : index
          %37 = memref.alloc() : memref<32xf32, 3>
          scf.if %36 {
            %39 = arith.subi %25, %33 : index
            %40 = arith.cmpi eq, %39, %c0_3 : index
            %41 = arith.subi %39, %c1_2 : index
            %42 = arith.divui %41, %c32_4 : index
            %43 = arith.addi %42, %c1_2 : index
            %44 = arith.select %40, %c0_3, %43 : index
            %45 = arith.remsi %44, %c4_7 : index
            %46 = arith.subi %44, %45 : index
            %47 = arith.muli %46, %c32_4 : index
            %48 = arith.addi %33, %47 : index
            %49 = scf.for %arg13 = %33 to %48 step %c128_10 iter_args(%arg14 = %cst_11) -> (f32) {
              %72 = arith.addi %arg13, %c32_4 : index
              %73 = arith.addi %arg13, %c64_8 : index
              %74 = arith.addi %arg13, %c96_9 : index
              %75 = arith.muli %35, %25 : index
              %76 = arith.addi %75, %arg13 : index
              %77 = arith.addi %75, %72 : index
              %78 = arith.addi %75, %73 : index
              %79 = arith.addi %75, %74 : index
              %80 = arith.muli %26, %27 : index
              %81 = arith.muli %80, %25 : index
              %82 = memref.reinterpret_cast %0 to offset: [%c0_3], sizes: [%81], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %83 = memref.load %82[%76] : memref<?xf32, "gpu">
              %84 = memref.load %82[%77] : memref<?xf32, "gpu">
              %85 = memref.load %82[%78] : memref<?xf32, "gpu">
              %86 = memref.load %82[%79] : memref<?xf32, "gpu">
              %87 = arith.cmpf ugt, %arg14, %83 : f32
              %88 = arith.select %87, %arg14, %83 : f32
              %89 = arith.cmpf uno, %83, %83 : f32
              %90 = arith.select %89, %83, %88 : f32
              %91 = arith.cmpf ugt, %90, %84 : f32
              %92 = arith.select %91, %90, %84 : f32
              %93 = arith.cmpf uno, %84, %84 : f32
              %94 = arith.select %93, %84, %92 : f32
              %95 = arith.cmpf ugt, %94, %85 : f32
              %96 = arith.select %95, %94, %85 : f32
              %97 = arith.cmpf uno, %85, %85 : f32
              %98 = arith.select %97, %85, %96 : f32
              %99 = arith.cmpf ugt, %98, %86 : f32
              %100 = arith.select %99, %98, %86 : f32
              %101 = arith.cmpf uno, %86, %86 : f32
              %102 = arith.select %101, %86, %100 : f32
              scf.yield %102 : f32
            }
            %50 = scf.for %arg13 = %48 to %25 step %c32_4 iter_args(%arg14 = %49) -> (f32) {
              %72 = arith.muli %35, %25 : index
              %73 = arith.addi %72, %arg13 : index
              %74 = arith.muli %26, %27 : index
              %75 = arith.muli %74, %25 : index
              %76 = memref.reinterpret_cast %0 to offset: [%c0_3], sizes: [%75], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %77 = memref.load %76[%73] : memref<?xf32, "gpu">
              %78 = arith.cmpf ugt, %arg14, %77 : f32
              %79 = arith.select %78, %arg14, %77 : f32
              %80 = arith.cmpf uno, %77, %77 : f32
              %81 = arith.select %80, %77, %79 : f32
              scf.yield %81 : f32
            }
            %result, %valid = gpu.shuffle  xor %50, %c1_i32_12, %c32_i32_13 : f32
            %51 = arith.cmpf ugt, %50, %result : f32
            %52 = arith.select %51, %50, %result : f32
            %53 = arith.cmpf uno, %result, %result : f32
            %54 = arith.select %53, %result, %52 : f32
            %result_19, %valid_20 = gpu.shuffle  xor %54, %c2_i32_14, %c32_i32_13 : f32
            %55 = arith.cmpf ugt, %54, %result_19 : f32
            %56 = arith.select %55, %54, %result_19 : f32
            %57 = arith.cmpf uno, %result_19, %result_19 : f32
            %58 = arith.select %57, %result_19, %56 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %58, %c4_i32_15, %c32_i32_13 : f32
            %59 = arith.cmpf ugt, %58, %result_21 : f32
            %60 = arith.select %59, %58, %result_21 : f32
            %61 = arith.cmpf uno, %result_21, %result_21 : f32
            %62 = arith.select %61, %result_21, %60 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %62, %c8_i32_16, %c32_i32_13 : f32
            %63 = arith.cmpf ugt, %62, %result_23 : f32
            %64 = arith.select %63, %62, %result_23 : f32
            %65 = arith.cmpf uno, %result_23, %result_23 : f32
            %66 = arith.select %65, %result_23, %64 : f32
            %result_25, %valid_26 = gpu.shuffle  xor %66, %c16_i32_17, %c32_i32_13 : f32
            %67 = arith.cmpf ugt, %66, %result_25 : f32
            %68 = arith.select %67, %66, %result_25 : f32
            %69 = arith.cmpf uno, %result_25, %result_25 : f32
            %70 = arith.select %69, %result_25, %68 : f32
            %71 = arith.cmpi eq, %33, %c0_3 : index
            scf.if %71 {
              %72 = memref.reinterpret_cast %37 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %70, %72[%32] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          %38 = memref.alloc() : memref<32xf32, 3>
          scf.if %36 {
            %39 = arith.subi %25, %33 : index
            %40 = arith.cmpi eq, %39, %c0_3 : index
            %41 = arith.subi %39, %c1_2 : index
            %42 = arith.divui %41, %c32_4 : index
            %43 = arith.addi %42, %c1_2 : index
            %44 = arith.select %40, %c0_3, %43 : index
            %45 = arith.remsi %44, %c4_7 : index
            %46 = arith.subi %44, %45 : index
            %47 = arith.muli %46, %c32_4 : index
            %48 = arith.addi %33, %47 : index
            %49 = scf.for %arg13 = %33 to %48 step %c128_10 iter_args(%arg14 = %cst_18) -> (f32) {
              %57 = arith.addi %arg13, %c32_4 : index
              %58 = arith.addi %arg13, %c64_8 : index
              %59 = arith.addi %arg13, %c96_9 : index
              %60 = arith.muli %35, %25 : index
              %61 = arith.addi %60, %arg13 : index
              %62 = arith.addi %60, %57 : index
              %63 = arith.addi %60, %58 : index
              %64 = arith.addi %60, %59 : index
              %65 = arith.divui %61, %25 : index
              %66 = arith.remui %65, %27 : index
              %67 = arith.divui %65, %27 : index
              %68 = arith.divui %62, %25 : index
              %69 = arith.remui %68, %27 : index
              %70 = arith.divui %68, %27 : index
              %71 = arith.divui %63, %25 : index
              %72 = arith.remui %71, %27 : index
              %73 = arith.divui %71, %27 : index
              %74 = arith.divui %64, %25 : index
              %75 = arith.remui %74, %27 : index
              %76 = arith.divui %74, %27 : index
              %77 = arith.muli %26, %27 : index
              %78 = arith.muli %77, %25 : index
              %79 = memref.reinterpret_cast %0 to offset: [%c0_3], sizes: [%78], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %80 = memref.load %79[%61] : memref<?xf32, "gpu">
              %81 = memref.load %79[%62] : memref<?xf32, "gpu">
              %82 = memref.load %79[%63] : memref<?xf32, "gpu">
              %83 = memref.load %79[%64] : memref<?xf32, "gpu">
              %84 = arith.muli %67, %27 : index
              %85 = arith.addi %84, %66 : index
              %86 = arith.muli %70, %27 : index
              %87 = arith.addi %86, %69 : index
              %88 = arith.muli %73, %27 : index
              %89 = arith.addi %88, %72 : index
              %90 = arith.muli %76, %27 : index
              %91 = arith.addi %90, %75 : index
              %92 = arith.remui %85, %c8_5 : index
              %93 = arith.remui %87, %c8_5 : index
              %94 = arith.remui %89, %c8_5 : index
              %95 = arith.remui %91, %c8_5 : index
              %96 = memref.reinterpret_cast %37 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %97 = memref.load %96[%92] : memref<32xf32, 3>
              %98 = memref.load %96[%93] : memref<32xf32, 3>
              %99 = memref.load %96[%94] : memref<32xf32, 3>
              %100 = memref.load %96[%95] : memref<32xf32, 3>
              %101 = arith.subf %80, %97 : f32
              %102 = arith.subf %81, %98 : f32
              %103 = arith.subf %82, %99 : f32
              %104 = arith.subf %83, %100 : f32
              %105 = math.exp %101 : f32
              %106 = math.exp %102 : f32
              %107 = math.exp %103 : f32
              %108 = math.exp %104 : f32
              %109 = arith.addf %arg14, %105 : f32
              %110 = arith.addf %109, %106 : f32
              %111 = arith.addf %110, %107 : f32
              %112 = arith.addf %111, %108 : f32
              scf.yield %112 : f32
            }
            %50 = scf.for %arg13 = %48 to %25 step %c32_4 iter_args(%arg14 = %49) -> (f32) {
              %57 = arith.muli %35, %25 : index
              %58 = arith.addi %57, %arg13 : index
              %59 = arith.divui %58, %25 : index
              %60 = arith.remui %59, %27 : index
              %61 = arith.divui %59, %27 : index
              %62 = arith.muli %26, %27 : index
              %63 = arith.muli %62, %25 : index
              %64 = memref.reinterpret_cast %0 to offset: [%c0_3], sizes: [%63], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %65 = memref.load %64[%58] : memref<?xf32, "gpu">
              %66 = arith.muli %61, %27 : index
              %67 = arith.addi %66, %60 : index
              %68 = arith.remui %67, %c8_5 : index
              %69 = memref.reinterpret_cast %37 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %70 = memref.load %69[%68] : memref<32xf32, 3>
              %71 = arith.subf %65, %70 : f32
              %72 = math.exp %71 : f32
              %73 = arith.addf %arg14, %72 : f32
              scf.yield %73 : f32
            }
            %result, %valid = gpu.shuffle  xor %50, %c1_i32_12, %c32_i32_13 : f32
            %51 = arith.addf %50, %result : f32
            %result_19, %valid_20 = gpu.shuffle  xor %51, %c2_i32_14, %c32_i32_13 : f32
            %52 = arith.addf %51, %result_19 : f32
            %result_21, %valid_22 = gpu.shuffle  xor %52, %c4_i32_15, %c32_i32_13 : f32
            %53 = arith.addf %52, %result_21 : f32
            %result_23, %valid_24 = gpu.shuffle  xor %53, %c8_i32_16, %c32_i32_13 : f32
            %54 = arith.addf %53, %result_23 : f32
            %result_25, %valid_26 = gpu.shuffle  xor %54, %c16_i32_17, %c32_i32_13 : f32
            %55 = arith.addf %54, %result_25 : f32
            %56 = arith.cmpi eq, %33, %c0_3 : index
            scf.if %56 {
              %57 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              memref.store %55, %57[%32] : memref<32xf32, 3>
            }
          }
          gpu.barrier
          scf.if %36 {
            %39 = arith.subi %25, %33 : index
            %40 = arith.cmpi eq, %39, %c0_3 : index
            %41 = arith.subi %39, %c1_2 : index
            %42 = arith.divui %41, %c32_4 : index
            %43 = arith.addi %42, %c1_2 : index
            %44 = arith.select %40, %c0_3, %43 : index
            %45 = arith.remsi %44, %c4_7 : index
            %46 = arith.subi %44, %45 : index
            %47 = arith.muli %46, %c32_4 : index
            %48 = arith.addi %33, %47 : index
            scf.for %arg13 = %33 to %48 step %c128_10 {
              %49 = arith.addi %arg13, %c32_4 : index
              %50 = arith.addi %arg13, %c64_8 : index
              %51 = arith.addi %arg13, %c96_9 : index
              %52 = arith.muli %35, %25 : index
              %53 = arith.addi %52, %arg13 : index
              %54 = arith.addi %52, %49 : index
              %55 = arith.addi %52, %50 : index
              %56 = arith.addi %52, %51 : index
              %57 = arith.muli %26, %27 : index
              %58 = arith.muli %57, %25 : index
              %59 = arith.divui %53, %25 : index
              %60 = arith.remui %59, %27 : index
              %61 = arith.divui %59, %27 : index
              %62 = arith.divui %54, %25 : index
              %63 = arith.remui %62, %27 : index
              %64 = arith.divui %62, %27 : index
              %65 = arith.divui %55, %25 : index
              %66 = arith.remui %65, %27 : index
              %67 = arith.divui %65, %27 : index
              %68 = arith.divui %56, %25 : index
              %69 = arith.remui %68, %27 : index
              %70 = arith.divui %68, %27 : index
              %71 = memref.reinterpret_cast %0 to offset: [%c0_3], sizes: [%58], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %72 = memref.load %71[%53] : memref<?xf32, "gpu">
              %73 = memref.load %71[%54] : memref<?xf32, "gpu">
              %74 = memref.load %71[%55] : memref<?xf32, "gpu">
              %75 = memref.load %71[%56] : memref<?xf32, "gpu">
              %76 = arith.muli %61, %27 : index
              %77 = arith.addi %76, %60 : index
              %78 = arith.muli %64, %27 : index
              %79 = arith.addi %78, %63 : index
              %80 = arith.muli %67, %27 : index
              %81 = arith.addi %80, %66 : index
              %82 = arith.muli %70, %27 : index
              %83 = arith.addi %82, %69 : index
              %84 = arith.remui %77, %c8_5 : index
              %85 = arith.remui %79, %c8_5 : index
              %86 = arith.remui %81, %c8_5 : index
              %87 = arith.remui %83, %c8_5 : index
              %88 = memref.reinterpret_cast %37 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %89 = memref.load %88[%84] : memref<32xf32, 3>
              %90 = memref.load %88[%85] : memref<32xf32, 3>
              %91 = memref.load %88[%86] : memref<32xf32, 3>
              %92 = memref.load %88[%87] : memref<32xf32, 3>
              %93 = arith.subf %72, %89 : f32
              %94 = arith.subf %73, %90 : f32
              %95 = arith.subf %74, %91 : f32
              %96 = arith.subf %75, %92 : f32
              %97 = math.exp %93 : f32
              %98 = math.exp %94 : f32
              %99 = math.exp %95 : f32
              %100 = math.exp %96 : f32
              %101 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %102 = memref.load %101[%84] : memref<32xf32, 3>
              %103 = memref.load %101[%85] : memref<32xf32, 3>
              %104 = memref.load %101[%86] : memref<32xf32, 3>
              %105 = memref.load %101[%87] : memref<32xf32, 3>
              %106 = arith.divf %97, %102 : f32
              %107 = arith.divf %98, %103 : f32
              %108 = arith.divf %99, %104 : f32
              %109 = arith.divf %100, %105 : f32
              %110 = memref.reinterpret_cast %8 to offset: [%c0_3], sizes: [%58], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %106, %110[%53] : memref<?xf32, "gpu">
              memref.store %107, %110[%54] : memref<?xf32, "gpu">
              memref.store %108, %110[%55] : memref<?xf32, "gpu">
              memref.store %109, %110[%56] : memref<?xf32, "gpu">
            }
            scf.for %arg13 = %48 to %25 step %c32_4 {
              %49 = arith.muli %35, %25 : index
              %50 = arith.addi %49, %arg13 : index
              %51 = arith.muli %26, %27 : index
              %52 = arith.muli %51, %25 : index
              %53 = arith.divui %50, %25 : index
              %54 = arith.remui %53, %27 : index
              %55 = arith.divui %53, %27 : index
              %56 = memref.reinterpret_cast %0 to offset: [%c0_3], sizes: [%52], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %57 = memref.load %56[%50] : memref<?xf32, "gpu">
              %58 = arith.muli %55, %27 : index
              %59 = arith.addi %58, %54 : index
              %60 = arith.remui %59, %c8_5 : index
              %61 = memref.reinterpret_cast %37 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %62 = memref.load %61[%60] : memref<32xf32, 3>
              %63 = arith.subf %57, %62 : f32
              %64 = math.exp %63 : f32
              %65 = memref.reinterpret_cast %38 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
              %66 = memref.load %65[%60] : memref<32xf32, 3>
              %67 = arith.divf %64, %66 : f32
              %68 = memref.reinterpret_cast %8 to offset: [%c0_3], sizes: [%52], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              memref.store %67, %68[%50] : memref<?xf32, "gpu">
            }
          }
          gpu.terminator
        } {SCFToGPU_visited}
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    }
    "disc_ral.dispatch"(%arg0, %c0, %8) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
    return
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After GpuKernelOutlining (gpu-kernel-outlining) //----- //
#map0 = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c768 = arith.constant 768 : index
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c96 = arith.constant 96 : index
    %c4 = arith.constant 4 : index
    %cst = arith.constant -0.000000e+00 : f32
    %cst_0 = arith.constant 0xFF800000 : f32
    %c8 = arith.constant 8 : index
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c4_i32 = arith.constant 4 : i32
    %c2_i32 = arith.constant 2 : i32
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c1024 = arith.constant 1024 : index
    %c6 = arith.constant 6 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
    %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
    %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
    %4 = arith.index_cast %2 : index to i32
    %5 = arith.index_cast %1 : index to i32
    %6 = arith.muli %4, %5 : i32
    %7 = arith.index_cast %6 : i32 to index
    %8 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    %9 = arith.cmpi slt, %7, %c3 : index
    %10 = arith.cmpi sge, %3, %c1024 : index
    %11 = arith.cmpi slt, %7, %c6 : index
    %12 = arith.andi %11, %10 : i1
    %13 = arith.cmpi sge, %3, %c512 : index
    %14 = arith.cmpi sge, %7, %c6 : index
    %15 = arith.andi %14, %13 : i1
    %16 = arith.ori %9, %12 : i1
    %17 = arith.ori %16, %15 : i1
    scf.if %17 {
      "lmhlo.fusion"() ({
        %c1_1 = arith.constant 1 : index
        %18 = affine.apply #map0(%7)[%c0, %c1]
        %19 = affine.apply #map0(%c256)[%c0, %c1]
        gpu.launch_func  @main_kernel::@main_kernel blocks in (%18, %c1_1, %c1_1) threads in (%19, %c1_1, %c1_1) args(%0 : memref<?x?x?xf32, "gpu">, %8 : memref<?x?x?xf32, "gpu">)
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        %18 = arith.cmpi eq, %7, %c0 : index
        %19 = arith.subi %7, %c1 : index
        %20 = arith.divui %19, %c8 : index
        %21 = arith.addi %20, %c1 : index
        %22 = arith.select %18, %c0, %21 : index
        %c1_1 = arith.constant 1 : index
        %23 = affine.apply #map0(%22)[%c0, %c1]
        %24 = affine.apply #map0(%c256)[%c0, %c1]
        gpu.launch_func  @main_kernel_0::@main_kernel blocks in (%23, %c1_1, %c1_1) threads in (%24, %c1_1, %c1_1) args(%0 : memref<?x?x?xf32, "gpu">, %7 : index, %8 : memref<?x?x?xf32, "gpu">)
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    }
    "disc_ral.dispatch"(%arg0, %c0, %8) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel {
    gpu.func @main_kernel(%arg0: memref<?x?x?xf32, "gpu">, %arg1: memref<?x?x?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %c32 = arith.constant 32 : index
      %c2 = arith.constant 2 : index
      %12 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
      %c256 = arith.constant 256 : index
      %c4 = arith.constant 4 : index
      %c512 = arith.constant 512 : index
      %c768 = arith.constant 768 : index
      %13 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
      %14 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
      %c1024 = arith.constant 1024 : index
      %cst = arith.constant 0xFF800000 : f32
      %c1_i32 = arith.constant 1 : i32
      %c32_i32 = arith.constant 32 : i32
      %c2_i32 = arith.constant 2 : i32
      %c4_i32 = arith.constant 4 : i32
      %c8_i32 = arith.constant 8 : i32
      %c16_i32 = arith.constant 16 : i32
      %c8 = arith.constant 8 : index
      %cst_0 = arith.constant -0.000000e+00 : f32
      %15 = affine.apply #map1(%0)[%c1, %c0]
      %16 = affine.apply #map1(%3)[%c1, %c0]
      %17 = gpu.block_id  x
      %18 = gpu.thread_id  x
      %19 = memref.alloc() : memref<32xf32, 3>
      %20 = arith.divui %18, %c32 : index
      %21 = arith.remui %18, %c32 : index
      %22 = arith.subi %12, %18 : index
      %23 = arith.cmpi eq, %22, %c0 : index
      %24 = arith.subi %22, %c1 : index
      %25 = arith.divui %24, %c256 : index
      %26 = arith.addi %25, %c1 : index
      %27 = arith.select %23, %c0, %26 : index
      %28 = arith.remsi %27, %c4 : index
      %29 = arith.subi %27, %28 : index
      %30 = arith.muli %29, %c256 : index
      %31 = arith.addi %18, %30 : index
      %32 = scf.for %arg2 = %18 to %31 step %c1024 iter_args(%arg3 = %cst) -> (f32) {
        %66 = arith.addi %arg2, %c256 : index
        %67 = arith.addi %arg2, %c512 : index
        %68 = arith.addi %arg2, %c768 : index
        %69 = arith.muli %17, %12 : index
        %70 = arith.addi %69, %arg2 : index
        %71 = arith.addi %69, %66 : index
        %72 = arith.addi %69, %67 : index
        %73 = arith.addi %69, %68 : index
        %74 = arith.muli %13, %14 : index
        %75 = arith.muli %74, %12 : index
        %76 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%75], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %77 = memref.load %76[%70] : memref<?xf32, "gpu">
        %78 = memref.load %76[%71] : memref<?xf32, "gpu">
        %79 = memref.load %76[%72] : memref<?xf32, "gpu">
        %80 = memref.load %76[%73] : memref<?xf32, "gpu">
        %81 = arith.cmpf ugt, %arg3, %77 : f32
        %82 = arith.select %81, %arg3, %77 : f32
        %83 = arith.cmpf uno, %77, %77 : f32
        %84 = arith.select %83, %77, %82 : f32
        %85 = arith.cmpf ugt, %84, %78 : f32
        %86 = arith.select %85, %84, %78 : f32
        %87 = arith.cmpf uno, %78, %78 : f32
        %88 = arith.select %87, %78, %86 : f32
        %89 = arith.cmpf ugt, %88, %79 : f32
        %90 = arith.select %89, %88, %79 : f32
        %91 = arith.cmpf uno, %79, %79 : f32
        %92 = arith.select %91, %79, %90 : f32
        %93 = arith.cmpf ugt, %92, %80 : f32
        %94 = arith.select %93, %92, %80 : f32
        %95 = arith.cmpf uno, %80, %80 : f32
        %96 = arith.select %95, %80, %94 : f32
        scf.yield %96 : f32
      }
      %33 = scf.for %arg2 = %31 to %12 step %c256 iter_args(%arg3 = %32) -> (f32) {
        %66 = arith.muli %17, %12 : index
        %67 = arith.addi %66, %arg2 : index
        %68 = arith.muli %13, %14 : index
        %69 = arith.muli %68, %12 : index
        %70 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%69], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %71 = memref.load %70[%67] : memref<?xf32, "gpu">
        %72 = arith.cmpf ugt, %arg3, %71 : f32
        %73 = arith.select %72, %arg3, %71 : f32
        %74 = arith.cmpf uno, %71, %71 : f32
        %75 = arith.select %74, %71, %73 : f32
        scf.yield %75 : f32
      }
      %result, %valid = gpu.shuffle  xor %33, %c1_i32, %c32_i32 : f32
      %34 = arith.cmpf ugt, %33, %result : f32
      %35 = arith.select %34, %33, %result : f32
      %36 = arith.cmpf uno, %result, %result : f32
      %37 = arith.select %36, %result, %35 : f32
      %result_1, %valid_2 = gpu.shuffle  xor %37, %c2_i32, %c32_i32 : f32
      %38 = arith.cmpf ugt, %37, %result_1 : f32
      %39 = arith.select %38, %37, %result_1 : f32
      %40 = arith.cmpf uno, %result_1, %result_1 : f32
      %41 = arith.select %40, %result_1, %39 : f32
      %result_3, %valid_4 = gpu.shuffle  xor %41, %c4_i32, %c32_i32 : f32
      %42 = arith.cmpf ugt, %41, %result_3 : f32
      %43 = arith.select %42, %41, %result_3 : f32
      %44 = arith.cmpf uno, %result_3, %result_3 : f32
      %45 = arith.select %44, %result_3, %43 : f32
      %result_5, %valid_6 = gpu.shuffle  xor %45, %c8_i32, %c32_i32 : f32
      %46 = arith.cmpf ugt, %45, %result_5 : f32
      %47 = arith.select %46, %45, %result_5 : f32
      %48 = arith.cmpf uno, %result_5, %result_5 : f32
      %49 = arith.select %48, %result_5, %47 : f32
      %result_7, %valid_8 = gpu.shuffle  xor %49, %c16_i32, %c32_i32 : f32
      %50 = arith.cmpf ugt, %49, %result_7 : f32
      %51 = arith.select %50, %49, %result_7 : f32
      %52 = arith.cmpf uno, %result_7, %result_7 : f32
      %53 = arith.select %52, %result_7, %51 : f32
      %54 = memref.alloc() : memref<8xf32, 3>
      %55 = arith.cmpi eq, %21, %c0 : index
      scf.if %55 {
        %66 = memref.reinterpret_cast %54 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
        memref.store %53, %66[%20] : memref<8xf32, 3>
      }
      gpu.barrier
      %56 = arith.cmpi slt, %18, %c32 : index
      scf.if %56 {
        %66 = arith.cmpi slt, %21, %c8 : index
        %67 = scf.if %66 -> (f32) {
          %80 = memref.reinterpret_cast %54 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
          %81 = memref.load %80[%21] : memref<8xf32, 3>
          scf.yield %81 : f32
        } else {
          scf.yield %cst : f32
        }
        %result_19, %valid_20 = gpu.shuffle  xor %67, %c1_i32, %c8_i32 : f32
        %68 = arith.cmpf ugt, %67, %result_19 : f32
        %69 = arith.select %68, %67, %result_19 : f32
        %70 = arith.cmpf uno, %result_19, %result_19 : f32
        %71 = arith.select %70, %result_19, %69 : f32
        %result_21, %valid_22 = gpu.shuffle  xor %71, %c2_i32, %c8_i32 : f32
        %72 = arith.cmpf ugt, %71, %result_21 : f32
        %73 = arith.select %72, %71, %result_21 : f32
        %74 = arith.cmpf uno, %result_21, %result_21 : f32
        %75 = arith.select %74, %result_21, %73 : f32
        %result_23, %valid_24 = gpu.shuffle  xor %75, %c4_i32, %c8_i32 : f32
        %76 = arith.cmpf ugt, %75, %result_23 : f32
        %77 = arith.select %76, %75, %result_23 : f32
        %78 = arith.cmpf uno, %result_23, %result_23 : f32
        %79 = arith.select %78, %result_23, %77 : f32
        scf.if %55 {
          %80 = memref.reinterpret_cast %19 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          memref.store %79, %80[%c0] : memref<32xf32, 3>
        }
      }
      gpu.barrier
      %57 = memref.alloc() : memref<32xf32, 3>
      %58 = scf.for %arg2 = %18 to %31 step %c1024 iter_args(%arg3 = %cst_0) -> (f32) {
        %66 = arith.addi %arg2, %c256 : index
        %67 = arith.addi %arg2, %c512 : index
        %68 = arith.addi %arg2, %c768 : index
        %69 = arith.muli %17, %12 : index
        %70 = arith.addi %69, %arg2 : index
        %71 = arith.addi %69, %66 : index
        %72 = arith.addi %69, %67 : index
        %73 = arith.addi %69, %68 : index
        %74 = arith.muli %13, %14 : index
        %75 = arith.muli %74, %12 : index
        %76 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%75], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %77 = memref.load %76[%70] : memref<?xf32, "gpu">
        %78 = memref.load %76[%71] : memref<?xf32, "gpu">
        %79 = memref.load %76[%72] : memref<?xf32, "gpu">
        %80 = memref.load %76[%73] : memref<?xf32, "gpu">
        %81 = memref.reinterpret_cast %19 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        %82 = memref.load %81[%c0] : memref<32xf32, 3>
        %83 = arith.subf %77, %82 : f32
        %84 = arith.subf %78, %82 : f32
        %85 = arith.subf %79, %82 : f32
        %86 = arith.subf %80, %82 : f32
        %87 = math.exp %83 : f32
        %88 = math.exp %84 : f32
        %89 = math.exp %85 : f32
        %90 = math.exp %86 : f32
        %91 = arith.addf %arg3, %87 : f32
        %92 = arith.addf %91, %88 : f32
        %93 = arith.addf %92, %89 : f32
        %94 = arith.addf %93, %90 : f32
        scf.yield %94 : f32
      }
      %59 = scf.for %arg2 = %31 to %12 step %c256 iter_args(%arg3 = %58) -> (f32) {
        %66 = arith.muli %17, %12 : index
        %67 = arith.addi %66, %arg2 : index
        %68 = arith.muli %13, %14 : index
        %69 = arith.muli %68, %12 : index
        %70 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%69], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %71 = memref.load %70[%67] : memref<?xf32, "gpu">
        %72 = memref.reinterpret_cast %19 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        %73 = memref.load %72[%c0] : memref<32xf32, 3>
        %74 = arith.subf %71, %73 : f32
        %75 = math.exp %74 : f32
        %76 = arith.addf %arg3, %75 : f32
        scf.yield %76 : f32
      }
      %result_9, %valid_10 = gpu.shuffle  xor %59, %c1_i32, %c32_i32 : f32
      %60 = arith.addf %59, %result_9 : f32
      %result_11, %valid_12 = gpu.shuffle  xor %60, %c2_i32, %c32_i32 : f32
      %61 = arith.addf %60, %result_11 : f32
      %result_13, %valid_14 = gpu.shuffle  xor %61, %c4_i32, %c32_i32 : f32
      %62 = arith.addf %61, %result_13 : f32
      %result_15, %valid_16 = gpu.shuffle  xor %62, %c8_i32, %c32_i32 : f32
      %63 = arith.addf %62, %result_15 : f32
      %result_17, %valid_18 = gpu.shuffle  xor %63, %c16_i32, %c32_i32 : f32
      %64 = arith.addf %63, %result_17 : f32
      %65 = memref.alloc() : memref<8xf32, 3>
      scf.if %55 {
        %66 = memref.reinterpret_cast %65 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
        memref.store %64, %66[%20] : memref<8xf32, 3>
      }
      gpu.barrier
      scf.if %56 {
        %66 = arith.cmpi slt, %21, %c8 : index
        %67 = scf.if %66 -> (f32) {
          %71 = memref.reinterpret_cast %65 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
          %72 = memref.load %71[%21] : memref<8xf32, 3>
          scf.yield %72 : f32
        } else {
          scf.yield %cst_0 : f32
        }
        %result_19, %valid_20 = gpu.shuffle  xor %67, %c1_i32, %c8_i32 : f32
        %68 = arith.addf %67, %result_19 : f32
        %result_21, %valid_22 = gpu.shuffle  xor %68, %c2_i32, %c8_i32 : f32
        %69 = arith.addf %68, %result_21 : f32
        %result_23, %valid_24 = gpu.shuffle  xor %69, %c4_i32, %c8_i32 : f32
        %70 = arith.addf %69, %result_23 : f32
        scf.if %55 {
          %71 = memref.reinterpret_cast %57 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          memref.store %70, %71[%c0] : memref<32xf32, 3>
        }
      }
      gpu.barrier
      scf.for %arg2 = %18 to %31 step %c1024 {
        %66 = arith.addi %arg2, %c256 : index
        %67 = arith.addi %arg2, %c512 : index
        %68 = arith.addi %arg2, %c768 : index
        %69 = arith.muli %17, %12 : index
        %70 = arith.addi %69, %arg2 : index
        %71 = arith.addi %69, %66 : index
        %72 = arith.addi %69, %67 : index
        %73 = arith.addi %69, %68 : index
        %74 = arith.muli %13, %14 : index
        %75 = arith.muli %74, %12 : index
        %76 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%75], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %77 = memref.load %76[%70] : memref<?xf32, "gpu">
        %78 = memref.load %76[%71] : memref<?xf32, "gpu">
        %79 = memref.load %76[%72] : memref<?xf32, "gpu">
        %80 = memref.load %76[%73] : memref<?xf32, "gpu">
        %81 = memref.reinterpret_cast %19 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        %82 = memref.load %81[%c0] : memref<32xf32, 3>
        %83 = arith.subf %77, %82 : f32
        %84 = arith.subf %78, %82 : f32
        %85 = arith.subf %79, %82 : f32
        %86 = arith.subf %80, %82 : f32
        %87 = math.exp %83 : f32
        %88 = math.exp %84 : f32
        %89 = math.exp %85 : f32
        %90 = math.exp %86 : f32
        %91 = memref.reinterpret_cast %57 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        %92 = memref.load %91[%c0] : memref<32xf32, 3>
        %93 = arith.divf %87, %92 : f32
        %94 = arith.divf %88, %92 : f32
        %95 = arith.divf %89, %92 : f32
        %96 = arith.divf %90, %92 : f32
        %97 = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%75], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %93, %97[%70] : memref<?xf32, "gpu">
        memref.store %94, %97[%71] : memref<?xf32, "gpu">
        memref.store %95, %97[%72] : memref<?xf32, "gpu">
        memref.store %96, %97[%73] : memref<?xf32, "gpu">
      }
      scf.for %arg2 = %31 to %12 step %c256 {
        %66 = arith.muli %17, %12 : index
        %67 = arith.addi %66, %arg2 : index
        %68 = arith.muli %13, %14 : index
        %69 = arith.muli %68, %12 : index
        %70 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%69], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %71 = memref.load %70[%67] : memref<?xf32, "gpu">
        %72 = memref.reinterpret_cast %19 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        %73 = memref.load %72[%c0] : memref<32xf32, 3>
        %74 = arith.subf %71, %73 : f32
        %75 = math.exp %74 : f32
        %76 = memref.reinterpret_cast %57 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        %77 = memref.load %76[%c0] : memref<32xf32, 3>
        %78 = arith.divf %75, %77 : f32
        %79 = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%69], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %78, %79[%67] : memref<?xf32, "gpu">
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_0 {
    gpu.func @main_kernel(%arg0: memref<?x?x?xf32, "gpu">, %arg1: index, %arg2: memref<?x?x?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %c32 = arith.constant 32 : index
      %c8 = arith.constant 8 : index
      %c2 = arith.constant 2 : index
      %12 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
      %c4 = arith.constant 4 : index
      %c64 = arith.constant 64 : index
      %c96 = arith.constant 96 : index
      %13 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
      %14 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
      %c128 = arith.constant 128 : index
      %cst = arith.constant 0xFF800000 : f32
      %c1_i32 = arith.constant 1 : i32
      %c32_i32 = arith.constant 32 : i32
      %c2_i32 = arith.constant 2 : i32
      %c4_i32 = arith.constant 4 : i32
      %c8_i32 = arith.constant 8 : i32
      %c16_i32 = arith.constant 16 : i32
      %cst_0 = arith.constant -0.000000e+00 : f32
      %15 = affine.apply #map1(%0)[%c1, %c0]
      %16 = affine.apply #map1(%3)[%c1, %c0]
      %17 = gpu.block_id  x
      %18 = gpu.thread_id  x
      %19 = arith.divui %18, %c32 : index
      %20 = arith.remui %18, %c32 : index
      %21 = arith.muli %17, %c8 : index
      %22 = arith.addi %21, %19 : index
      %23 = arith.cmpi slt, %22, %arg1 : index
      %24 = memref.alloc() : memref<32xf32, 3>
      scf.if %23 {
        %26 = arith.subi %12, %20 : index
        %27 = arith.cmpi eq, %26, %c0 : index
        %28 = arith.subi %26, %c1 : index
        %29 = arith.divui %28, %c32 : index
        %30 = arith.addi %29, %c1 : index
        %31 = arith.select %27, %c0, %30 : index
        %32 = arith.remsi %31, %c4 : index
        %33 = arith.subi %31, %32 : index
        %34 = arith.muli %33, %c32 : index
        %35 = arith.addi %20, %34 : index
        %36 = scf.for %arg3 = %20 to %35 step %c128 iter_args(%arg4 = %cst) -> (f32) {
          %59 = arith.addi %arg3, %c32 : index
          %60 = arith.addi %arg3, %c64 : index
          %61 = arith.addi %arg3, %c96 : index
          %62 = arith.muli %22, %12 : index
          %63 = arith.addi %62, %arg3 : index
          %64 = arith.addi %62, %59 : index
          %65 = arith.addi %62, %60 : index
          %66 = arith.addi %62, %61 : index
          %67 = arith.muli %13, %14 : index
          %68 = arith.muli %67, %12 : index
          %69 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%68], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %70 = memref.load %69[%63] : memref<?xf32, "gpu">
          %71 = memref.load %69[%64] : memref<?xf32, "gpu">
          %72 = memref.load %69[%65] : memref<?xf32, "gpu">
          %73 = memref.load %69[%66] : memref<?xf32, "gpu">
          %74 = arith.cmpf ugt, %arg4, %70 : f32
          %75 = arith.select %74, %arg4, %70 : f32
          %76 = arith.cmpf uno, %70, %70 : f32
          %77 = arith.select %76, %70, %75 : f32
          %78 = arith.cmpf ugt, %77, %71 : f32
          %79 = arith.select %78, %77, %71 : f32
          %80 = arith.cmpf uno, %71, %71 : f32
          %81 = arith.select %80, %71, %79 : f32
          %82 = arith.cmpf ugt, %81, %72 : f32
          %83 = arith.select %82, %81, %72 : f32
          %84 = arith.cmpf uno, %72, %72 : f32
          %85 = arith.select %84, %72, %83 : f32
          %86 = arith.cmpf ugt, %85, %73 : f32
          %87 = arith.select %86, %85, %73 : f32
          %88 = arith.cmpf uno, %73, %73 : f32
          %89 = arith.select %88, %73, %87 : f32
          scf.yield %89 : f32
        }
        %37 = scf.for %arg3 = %35 to %12 step %c32 iter_args(%arg4 = %36) -> (f32) {
          %59 = arith.muli %22, %12 : index
          %60 = arith.addi %59, %arg3 : index
          %61 = arith.muli %13, %14 : index
          %62 = arith.muli %61, %12 : index
          %63 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%62], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %64 = memref.load %63[%60] : memref<?xf32, "gpu">
          %65 = arith.cmpf ugt, %arg4, %64 : f32
          %66 = arith.select %65, %arg4, %64 : f32
          %67 = arith.cmpf uno, %64, %64 : f32
          %68 = arith.select %67, %64, %66 : f32
          scf.yield %68 : f32
        }
        %result, %valid = gpu.shuffle  xor %37, %c1_i32, %c32_i32 : f32
        %38 = arith.cmpf ugt, %37, %result : f32
        %39 = arith.select %38, %37, %result : f32
        %40 = arith.cmpf uno, %result, %result : f32
        %41 = arith.select %40, %result, %39 : f32
        %result_1, %valid_2 = gpu.shuffle  xor %41, %c2_i32, %c32_i32 : f32
        %42 = arith.cmpf ugt, %41, %result_1 : f32
        %43 = arith.select %42, %41, %result_1 : f32
        %44 = arith.cmpf uno, %result_1, %result_1 : f32
        %45 = arith.select %44, %result_1, %43 : f32
        %result_3, %valid_4 = gpu.shuffle  xor %45, %c4_i32, %c32_i32 : f32
        %46 = arith.cmpf ugt, %45, %result_3 : f32
        %47 = arith.select %46, %45, %result_3 : f32
        %48 = arith.cmpf uno, %result_3, %result_3 : f32
        %49 = arith.select %48, %result_3, %47 : f32
        %result_5, %valid_6 = gpu.shuffle  xor %49, %c8_i32, %c32_i32 : f32
        %50 = arith.cmpf ugt, %49, %result_5 : f32
        %51 = arith.select %50, %49, %result_5 : f32
        %52 = arith.cmpf uno, %result_5, %result_5 : f32
        %53 = arith.select %52, %result_5, %51 : f32
        %result_7, %valid_8 = gpu.shuffle  xor %53, %c16_i32, %c32_i32 : f32
        %54 = arith.cmpf ugt, %53, %result_7 : f32
        %55 = arith.select %54, %53, %result_7 : f32
        %56 = arith.cmpf uno, %result_7, %result_7 : f32
        %57 = arith.select %56, %result_7, %55 : f32
        %58 = arith.cmpi eq, %20, %c0 : index
        scf.if %58 {
          %59 = memref.reinterpret_cast %24 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          memref.store %57, %59[%19] : memref<32xf32, 3>
        }
      }
      gpu.barrier
      %25 = memref.alloc() : memref<32xf32, 3>
      scf.if %23 {
        %26 = arith.subi %12, %20 : index
        %27 = arith.cmpi eq, %26, %c0 : index
        %28 = arith.subi %26, %c1 : index
        %29 = arith.divui %28, %c32 : index
        %30 = arith.addi %29, %c1 : index
        %31 = arith.select %27, %c0, %30 : index
        %32 = arith.remsi %31, %c4 : index
        %33 = arith.subi %31, %32 : index
        %34 = arith.muli %33, %c32 : index
        %35 = arith.addi %20, %34 : index
        %36 = scf.for %arg3 = %20 to %35 step %c128 iter_args(%arg4 = %cst_0) -> (f32) {
          %44 = arith.addi %arg3, %c32 : index
          %45 = arith.addi %arg3, %c64 : index
          %46 = arith.addi %arg3, %c96 : index
          %47 = arith.muli %22, %12 : index
          %48 = arith.addi %47, %arg3 : index
          %49 = arith.addi %47, %44 : index
          %50 = arith.addi %47, %45 : index
          %51 = arith.addi %47, %46 : index
          %52 = arith.divui %48, %12 : index
          %53 = arith.remui %52, %14 : index
          %54 = arith.divui %52, %14 : index
          %55 = arith.divui %49, %12 : index
          %56 = arith.remui %55, %14 : index
          %57 = arith.divui %55, %14 : index
          %58 = arith.divui %50, %12 : index
          %59 = arith.remui %58, %14 : index
          %60 = arith.divui %58, %14 : index
          %61 = arith.divui %51, %12 : index
          %62 = arith.remui %61, %14 : index
          %63 = arith.divui %61, %14 : index
          %64 = arith.muli %13, %14 : index
          %65 = arith.muli %64, %12 : index
          %66 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%65], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %67 = memref.load %66[%48] : memref<?xf32, "gpu">
          %68 = memref.load %66[%49] : memref<?xf32, "gpu">
          %69 = memref.load %66[%50] : memref<?xf32, "gpu">
          %70 = memref.load %66[%51] : memref<?xf32, "gpu">
          %71 = arith.muli %54, %14 : index
          %72 = arith.addi %71, %53 : index
          %73 = arith.muli %57, %14 : index
          %74 = arith.addi %73, %56 : index
          %75 = arith.muli %60, %14 : index
          %76 = arith.addi %75, %59 : index
          %77 = arith.muli %63, %14 : index
          %78 = arith.addi %77, %62 : index
          %79 = arith.remui %72, %c8 : index
          %80 = arith.remui %74, %c8 : index
          %81 = arith.remui %76, %c8 : index
          %82 = arith.remui %78, %c8 : index
          %83 = memref.reinterpret_cast %24 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %84 = memref.load %83[%79] : memref<32xf32, 3>
          %85 = memref.load %83[%80] : memref<32xf32, 3>
          %86 = memref.load %83[%81] : memref<32xf32, 3>
          %87 = memref.load %83[%82] : memref<32xf32, 3>
          %88 = arith.subf %67, %84 : f32
          %89 = arith.subf %68, %85 : f32
          %90 = arith.subf %69, %86 : f32
          %91 = arith.subf %70, %87 : f32
          %92 = math.exp %88 : f32
          %93 = math.exp %89 : f32
          %94 = math.exp %90 : f32
          %95 = math.exp %91 : f32
          %96 = arith.addf %arg4, %92 : f32
          %97 = arith.addf %96, %93 : f32
          %98 = arith.addf %97, %94 : f32
          %99 = arith.addf %98, %95 : f32
          scf.yield %99 : f32
        }
        %37 = scf.for %arg3 = %35 to %12 step %c32 iter_args(%arg4 = %36) -> (f32) {
          %44 = arith.muli %22, %12 : index
          %45 = arith.addi %44, %arg3 : index
          %46 = arith.divui %45, %12 : index
          %47 = arith.remui %46, %14 : index
          %48 = arith.divui %46, %14 : index
          %49 = arith.muli %13, %14 : index
          %50 = arith.muli %49, %12 : index
          %51 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%50], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %52 = memref.load %51[%45] : memref<?xf32, "gpu">
          %53 = arith.muli %48, %14 : index
          %54 = arith.addi %53, %47 : index
          %55 = arith.remui %54, %c8 : index
          %56 = memref.reinterpret_cast %24 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %57 = memref.load %56[%55] : memref<32xf32, 3>
          %58 = arith.subf %52, %57 : f32
          %59 = math.exp %58 : f32
          %60 = arith.addf %arg4, %59 : f32
          scf.yield %60 : f32
        }
        %result, %valid = gpu.shuffle  xor %37, %c1_i32, %c32_i32 : f32
        %38 = arith.addf %37, %result : f32
        %result_1, %valid_2 = gpu.shuffle  xor %38, %c2_i32, %c32_i32 : f32
        %39 = arith.addf %38, %result_1 : f32
        %result_3, %valid_4 = gpu.shuffle  xor %39, %c4_i32, %c32_i32 : f32
        %40 = arith.addf %39, %result_3 : f32
        %result_5, %valid_6 = gpu.shuffle  xor %40, %c8_i32, %c32_i32 : f32
        %41 = arith.addf %40, %result_5 : f32
        %result_7, %valid_8 = gpu.shuffle  xor %41, %c16_i32, %c32_i32 : f32
        %42 = arith.addf %41, %result_7 : f32
        %43 = arith.cmpi eq, %20, %c0 : index
        scf.if %43 {
          %44 = memref.reinterpret_cast %25 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          memref.store %42, %44[%19] : memref<32xf32, 3>
        }
      }
      gpu.barrier
      scf.if %23 {
        %26 = arith.subi %12, %20 : index
        %27 = arith.cmpi eq, %26, %c0 : index
        %28 = arith.subi %26, %c1 : index
        %29 = arith.divui %28, %c32 : index
        %30 = arith.addi %29, %c1 : index
        %31 = arith.select %27, %c0, %30 : index
        %32 = arith.remsi %31, %c4 : index
        %33 = arith.subi %31, %32 : index
        %34 = arith.muli %33, %c32 : index
        %35 = arith.addi %20, %34 : index
        scf.for %arg3 = %20 to %35 step %c128 {
          %36 = arith.addi %arg3, %c32 : index
          %37 = arith.addi %arg3, %c64 : index
          %38 = arith.addi %arg3, %c96 : index
          %39 = arith.muli %22, %12 : index
          %40 = arith.addi %39, %arg3 : index
          %41 = arith.addi %39, %36 : index
          %42 = arith.addi %39, %37 : index
          %43 = arith.addi %39, %38 : index
          %44 = arith.muli %13, %14 : index
          %45 = arith.muli %44, %12 : index
          %46 = arith.divui %40, %12 : index
          %47 = arith.remui %46, %14 : index
          %48 = arith.divui %46, %14 : index
          %49 = arith.divui %41, %12 : index
          %50 = arith.remui %49, %14 : index
          %51 = arith.divui %49, %14 : index
          %52 = arith.divui %42, %12 : index
          %53 = arith.remui %52, %14 : index
          %54 = arith.divui %52, %14 : index
          %55 = arith.divui %43, %12 : index
          %56 = arith.remui %55, %14 : index
          %57 = arith.divui %55, %14 : index
          %58 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%45], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %59 = memref.load %58[%40] : memref<?xf32, "gpu">
          %60 = memref.load %58[%41] : memref<?xf32, "gpu">
          %61 = memref.load %58[%42] : memref<?xf32, "gpu">
          %62 = memref.load %58[%43] : memref<?xf32, "gpu">
          %63 = arith.muli %48, %14 : index
          %64 = arith.addi %63, %47 : index
          %65 = arith.muli %51, %14 : index
          %66 = arith.addi %65, %50 : index
          %67 = arith.muli %54, %14 : index
          %68 = arith.addi %67, %53 : index
          %69 = arith.muli %57, %14 : index
          %70 = arith.addi %69, %56 : index
          %71 = arith.remui %64, %c8 : index
          %72 = arith.remui %66, %c8 : index
          %73 = arith.remui %68, %c8 : index
          %74 = arith.remui %70, %c8 : index
          %75 = memref.reinterpret_cast %24 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %76 = memref.load %75[%71] : memref<32xf32, 3>
          %77 = memref.load %75[%72] : memref<32xf32, 3>
          %78 = memref.load %75[%73] : memref<32xf32, 3>
          %79 = memref.load %75[%74] : memref<32xf32, 3>
          %80 = arith.subf %59, %76 : f32
          %81 = arith.subf %60, %77 : f32
          %82 = arith.subf %61, %78 : f32
          %83 = arith.subf %62, %79 : f32
          %84 = math.exp %80 : f32
          %85 = math.exp %81 : f32
          %86 = math.exp %82 : f32
          %87 = math.exp %83 : f32
          %88 = memref.reinterpret_cast %25 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %89 = memref.load %88[%71] : memref<32xf32, 3>
          %90 = memref.load %88[%72] : memref<32xf32, 3>
          %91 = memref.load %88[%73] : memref<32xf32, 3>
          %92 = memref.load %88[%74] : memref<32xf32, 3>
          %93 = arith.divf %84, %89 : f32
          %94 = arith.divf %85, %90 : f32
          %95 = arith.divf %86, %91 : f32
          %96 = arith.divf %87, %92 : f32
          %97 = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%45], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          memref.store %93, %97[%40] : memref<?xf32, "gpu">
          memref.store %94, %97[%41] : memref<?xf32, "gpu">
          memref.store %95, %97[%42] : memref<?xf32, "gpu">
          memref.store %96, %97[%43] : memref<?xf32, "gpu">
        }
        scf.for %arg3 = %35 to %12 step %c32 {
          %36 = arith.muli %22, %12 : index
          %37 = arith.addi %36, %arg3 : index
          %38 = arith.muli %13, %14 : index
          %39 = arith.muli %38, %12 : index
          %40 = arith.divui %37, %12 : index
          %41 = arith.remui %40, %14 : index
          %42 = arith.divui %40, %14 : index
          %43 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%39], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %44 = memref.load %43[%37] : memref<?xf32, "gpu">
          %45 = arith.muli %42, %14 : index
          %46 = arith.addi %45, %41 : index
          %47 = arith.remui %46, %c8 : index
          %48 = memref.reinterpret_cast %24 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %49 = memref.load %48[%47] : memref<32xf32, 3>
          %50 = arith.subf %44, %49 : f32
          %51 = math.exp %50 : f32
          %52 = memref.reinterpret_cast %25 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %53 = memref.load %52[%47] : memref<32xf32, 3>
          %54 = arith.divf %51, %53 : f32
          %55 = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%39], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          memref.store %54, %55[%37] : memref<?xf32, "gpu">
        }
      }
      gpu.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After AssignKernelNamePass (disc-assign-kernel-name) //----- //
#map0 = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c768 = arith.constant 768 : index
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c96 = arith.constant 96 : index
    %c4 = arith.constant 4 : index
    %cst = arith.constant -0.000000e+00 : f32
    %cst_0 = arith.constant 0xFF800000 : f32
    %c8 = arith.constant 8 : index
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c4_i32 = arith.constant 4 : i32
    %c2_i32 = arith.constant 2 : i32
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c1024 = arith.constant 1024 : index
    %c6 = arith.constant 6 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
    %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
    %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
    %4 = arith.index_cast %2 : index to i32
    %5 = arith.index_cast %1 : index to i32
    %6 = arith.muli %4, %5 : i32
    %7 = arith.index_cast %6 : i32 to index
    %8 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    %9 = arith.cmpi slt, %7, %c3 : index
    %10 = arith.cmpi sge, %3, %c1024 : index
    %11 = arith.cmpi slt, %7, %c6 : index
    %12 = arith.andi %11, %10 : i1
    %13 = arith.cmpi sge, %3, %c512 : index
    %14 = arith.cmpi sge, %7, %c6 : index
    %15 = arith.andi %14, %13 : i1
    %16 = arith.ori %9, %12 : i1
    %17 = arith.ori %16, %15 : i1
    scf.if %17 {
      "lmhlo.fusion"() ({
        %c1_1 = arith.constant 1 : index
        %18 = affine.apply #map0(%7)[%c0, %c1]
        %19 = affine.apply #map0(%c256)[%c0, %c1]
        gpu.launch_func  @main_kernel::@main_kStitch_divide__13_1_0___1b1r blocks in (%18, %c1_1, %c1_1) threads in (%19, %c1_1, %c1_1) args(%0 : memref<?x?x?xf32, "gpu">, %8 : memref<?x?x?xf32, "gpu">)
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1b1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        %18 = arith.cmpi eq, %7, %c0 : index
        %19 = arith.subi %7, %c1 : index
        %20 = arith.divui %19, %c8 : index
        %21 = arith.addi %20, %c1 : index
        %22 = arith.select %18, %c0, %21 : index
        %c1_1 = arith.constant 1 : index
        %23 = affine.apply #map0(%22)[%c0, %c1]
        %24 = affine.apply #map0(%c256)[%c0, %c1]
        gpu.launch_func  @main_kernel_0::@main_kStitch_divide__13_1_0___1w1r blocks in (%23, %c1_1, %c1_1) threads in (%24, %c1_1, %c1_1) args(%0 : memref<?x?x?xf32, "gpu">, %7 : index, %8 : memref<?x?x?xf32, "gpu">)
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kStitch_divide__13_1_0", disc.fusion.tag = "1w1r", disc.fusion_type = "kStitch", disc_row_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    }
    "disc_ral.dispatch"(%arg0, %c0, %8) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel {
    gpu.func @main_kStitch_divide__13_1_0___1b1r(%arg0: memref<?x?x?xf32, "gpu">, %arg1: memref<?x?x?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %c32 = arith.constant 32 : index
      %c2 = arith.constant 2 : index
      %12 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
      %c256 = arith.constant 256 : index
      %c4 = arith.constant 4 : index
      %c512 = arith.constant 512 : index
      %c768 = arith.constant 768 : index
      %13 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
      %14 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
      %c1024 = arith.constant 1024 : index
      %cst = arith.constant 0xFF800000 : f32
      %c1_i32 = arith.constant 1 : i32
      %c32_i32 = arith.constant 32 : i32
      %c2_i32 = arith.constant 2 : i32
      %c4_i32 = arith.constant 4 : i32
      %c8_i32 = arith.constant 8 : i32
      %c16_i32 = arith.constant 16 : i32
      %c8 = arith.constant 8 : index
      %cst_0 = arith.constant -0.000000e+00 : f32
      %15 = affine.apply #map1(%0)[%c1, %c0]
      %16 = affine.apply #map1(%3)[%c1, %c0]
      %17 = gpu.block_id  x
      %18 = gpu.thread_id  x
      %19 = memref.alloc() : memref<32xf32, 3>
      %20 = arith.divui %18, %c32 : index
      %21 = arith.remui %18, %c32 : index
      %22 = arith.subi %12, %18 : index
      %23 = arith.cmpi eq, %22, %c0 : index
      %24 = arith.subi %22, %c1 : index
      %25 = arith.divui %24, %c256 : index
      %26 = arith.addi %25, %c1 : index
      %27 = arith.select %23, %c0, %26 : index
      %28 = arith.remsi %27, %c4 : index
      %29 = arith.subi %27, %28 : index
      %30 = arith.muli %29, %c256 : index
      %31 = arith.addi %18, %30 : index
      %32 = scf.for %arg2 = %18 to %31 step %c1024 iter_args(%arg3 = %cst) -> (f32) {
        %66 = arith.addi %arg2, %c256 : index
        %67 = arith.addi %arg2, %c512 : index
        %68 = arith.addi %arg2, %c768 : index
        %69 = arith.muli %17, %12 : index
        %70 = arith.addi %69, %arg2 : index
        %71 = arith.addi %69, %66 : index
        %72 = arith.addi %69, %67 : index
        %73 = arith.addi %69, %68 : index
        %74 = arith.muli %13, %14 : index
        %75 = arith.muli %74, %12 : index
        %76 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%75], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %77 = memref.load %76[%70] : memref<?xf32, "gpu">
        %78 = memref.load %76[%71] : memref<?xf32, "gpu">
        %79 = memref.load %76[%72] : memref<?xf32, "gpu">
        %80 = memref.load %76[%73] : memref<?xf32, "gpu">
        %81 = arith.cmpf ugt, %arg3, %77 : f32
        %82 = arith.select %81, %arg3, %77 : f32
        %83 = arith.cmpf uno, %77, %77 : f32
        %84 = arith.select %83, %77, %82 : f32
        %85 = arith.cmpf ugt, %84, %78 : f32
        %86 = arith.select %85, %84, %78 : f32
        %87 = arith.cmpf uno, %78, %78 : f32
        %88 = arith.select %87, %78, %86 : f32
        %89 = arith.cmpf ugt, %88, %79 : f32
        %90 = arith.select %89, %88, %79 : f32
        %91 = arith.cmpf uno, %79, %79 : f32
        %92 = arith.select %91, %79, %90 : f32
        %93 = arith.cmpf ugt, %92, %80 : f32
        %94 = arith.select %93, %92, %80 : f32
        %95 = arith.cmpf uno, %80, %80 : f32
        %96 = arith.select %95, %80, %94 : f32
        scf.yield %96 : f32
      }
      %33 = scf.for %arg2 = %31 to %12 step %c256 iter_args(%arg3 = %32) -> (f32) {
        %66 = arith.muli %17, %12 : index
        %67 = arith.addi %66, %arg2 : index
        %68 = arith.muli %13, %14 : index
        %69 = arith.muli %68, %12 : index
        %70 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%69], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %71 = memref.load %70[%67] : memref<?xf32, "gpu">
        %72 = arith.cmpf ugt, %arg3, %71 : f32
        %73 = arith.select %72, %arg3, %71 : f32
        %74 = arith.cmpf uno, %71, %71 : f32
        %75 = arith.select %74, %71, %73 : f32
        scf.yield %75 : f32
      }
      %result, %valid = gpu.shuffle  xor %33, %c1_i32, %c32_i32 : f32
      %34 = arith.cmpf ugt, %33, %result : f32
      %35 = arith.select %34, %33, %result : f32
      %36 = arith.cmpf uno, %result, %result : f32
      %37 = arith.select %36, %result, %35 : f32
      %result_1, %valid_2 = gpu.shuffle  xor %37, %c2_i32, %c32_i32 : f32
      %38 = arith.cmpf ugt, %37, %result_1 : f32
      %39 = arith.select %38, %37, %result_1 : f32
      %40 = arith.cmpf uno, %result_1, %result_1 : f32
      %41 = arith.select %40, %result_1, %39 : f32
      %result_3, %valid_4 = gpu.shuffle  xor %41, %c4_i32, %c32_i32 : f32
      %42 = arith.cmpf ugt, %41, %result_3 : f32
      %43 = arith.select %42, %41, %result_3 : f32
      %44 = arith.cmpf uno, %result_3, %result_3 : f32
      %45 = arith.select %44, %result_3, %43 : f32
      %result_5, %valid_6 = gpu.shuffle  xor %45, %c8_i32, %c32_i32 : f32
      %46 = arith.cmpf ugt, %45, %result_5 : f32
      %47 = arith.select %46, %45, %result_5 : f32
      %48 = arith.cmpf uno, %result_5, %result_5 : f32
      %49 = arith.select %48, %result_5, %47 : f32
      %result_7, %valid_8 = gpu.shuffle  xor %49, %c16_i32, %c32_i32 : f32
      %50 = arith.cmpf ugt, %49, %result_7 : f32
      %51 = arith.select %50, %49, %result_7 : f32
      %52 = arith.cmpf uno, %result_7, %result_7 : f32
      %53 = arith.select %52, %result_7, %51 : f32
      %54 = memref.alloc() : memref<8xf32, 3>
      %55 = arith.cmpi eq, %21, %c0 : index
      scf.if %55 {
        %66 = memref.reinterpret_cast %54 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
        memref.store %53, %66[%20] : memref<8xf32, 3>
      }
      gpu.barrier
      %56 = arith.cmpi slt, %18, %c32 : index
      scf.if %56 {
        %66 = arith.cmpi slt, %21, %c8 : index
        %67 = scf.if %66 -> (f32) {
          %80 = memref.reinterpret_cast %54 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
          %81 = memref.load %80[%21] : memref<8xf32, 3>
          scf.yield %81 : f32
        } else {
          scf.yield %cst : f32
        }
        %result_19, %valid_20 = gpu.shuffle  xor %67, %c1_i32, %c8_i32 : f32
        %68 = arith.cmpf ugt, %67, %result_19 : f32
        %69 = arith.select %68, %67, %result_19 : f32
        %70 = arith.cmpf uno, %result_19, %result_19 : f32
        %71 = arith.select %70, %result_19, %69 : f32
        %result_21, %valid_22 = gpu.shuffle  xor %71, %c2_i32, %c8_i32 : f32
        %72 = arith.cmpf ugt, %71, %result_21 : f32
        %73 = arith.select %72, %71, %result_21 : f32
        %74 = arith.cmpf uno, %result_21, %result_21 : f32
        %75 = arith.select %74, %result_21, %73 : f32
        %result_23, %valid_24 = gpu.shuffle  xor %75, %c4_i32, %c8_i32 : f32
        %76 = arith.cmpf ugt, %75, %result_23 : f32
        %77 = arith.select %76, %75, %result_23 : f32
        %78 = arith.cmpf uno, %result_23, %result_23 : f32
        %79 = arith.select %78, %result_23, %77 : f32
        scf.if %55 {
          %80 = memref.reinterpret_cast %19 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          memref.store %79, %80[%c0] : memref<32xf32, 3>
        }
      }
      gpu.barrier
      %57 = memref.alloc() : memref<32xf32, 3>
      %58 = scf.for %arg2 = %18 to %31 step %c1024 iter_args(%arg3 = %cst_0) -> (f32) {
        %66 = arith.addi %arg2, %c256 : index
        %67 = arith.addi %arg2, %c512 : index
        %68 = arith.addi %arg2, %c768 : index
        %69 = arith.muli %17, %12 : index
        %70 = arith.addi %69, %arg2 : index
        %71 = arith.addi %69, %66 : index
        %72 = arith.addi %69, %67 : index
        %73 = arith.addi %69, %68 : index
        %74 = arith.muli %13, %14 : index
        %75 = arith.muli %74, %12 : index
        %76 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%75], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %77 = memref.load %76[%70] : memref<?xf32, "gpu">
        %78 = memref.load %76[%71] : memref<?xf32, "gpu">
        %79 = memref.load %76[%72] : memref<?xf32, "gpu">
        %80 = memref.load %76[%73] : memref<?xf32, "gpu">
        %81 = memref.reinterpret_cast %19 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        %82 = memref.load %81[%c0] : memref<32xf32, 3>
        %83 = arith.subf %77, %82 : f32
        %84 = arith.subf %78, %82 : f32
        %85 = arith.subf %79, %82 : f32
        %86 = arith.subf %80, %82 : f32
        %87 = math.exp %83 : f32
        %88 = math.exp %84 : f32
        %89 = math.exp %85 : f32
        %90 = math.exp %86 : f32
        %91 = arith.addf %arg3, %87 : f32
        %92 = arith.addf %91, %88 : f32
        %93 = arith.addf %92, %89 : f32
        %94 = arith.addf %93, %90 : f32
        scf.yield %94 : f32
      }
      %59 = scf.for %arg2 = %31 to %12 step %c256 iter_args(%arg3 = %58) -> (f32) {
        %66 = arith.muli %17, %12 : index
        %67 = arith.addi %66, %arg2 : index
        %68 = arith.muli %13, %14 : index
        %69 = arith.muli %68, %12 : index
        %70 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%69], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %71 = memref.load %70[%67] : memref<?xf32, "gpu">
        %72 = memref.reinterpret_cast %19 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        %73 = memref.load %72[%c0] : memref<32xf32, 3>
        %74 = arith.subf %71, %73 : f32
        %75 = math.exp %74 : f32
        %76 = arith.addf %arg3, %75 : f32
        scf.yield %76 : f32
      }
      %result_9, %valid_10 = gpu.shuffle  xor %59, %c1_i32, %c32_i32 : f32
      %60 = arith.addf %59, %result_9 : f32
      %result_11, %valid_12 = gpu.shuffle  xor %60, %c2_i32, %c32_i32 : f32
      %61 = arith.addf %60, %result_11 : f32
      %result_13, %valid_14 = gpu.shuffle  xor %61, %c4_i32, %c32_i32 : f32
      %62 = arith.addf %61, %result_13 : f32
      %result_15, %valid_16 = gpu.shuffle  xor %62, %c8_i32, %c32_i32 : f32
      %63 = arith.addf %62, %result_15 : f32
      %result_17, %valid_18 = gpu.shuffle  xor %63, %c16_i32, %c32_i32 : f32
      %64 = arith.addf %63, %result_17 : f32
      %65 = memref.alloc() : memref<8xf32, 3>
      scf.if %55 {
        %66 = memref.reinterpret_cast %65 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
        memref.store %64, %66[%20] : memref<8xf32, 3>
      }
      gpu.barrier
      scf.if %56 {
        %66 = arith.cmpi slt, %21, %c8 : index
        %67 = scf.if %66 -> (f32) {
          %71 = memref.reinterpret_cast %65 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
          %72 = memref.load %71[%21] : memref<8xf32, 3>
          scf.yield %72 : f32
        } else {
          scf.yield %cst_0 : f32
        }
        %result_19, %valid_20 = gpu.shuffle  xor %67, %c1_i32, %c8_i32 : f32
        %68 = arith.addf %67, %result_19 : f32
        %result_21, %valid_22 = gpu.shuffle  xor %68, %c2_i32, %c8_i32 : f32
        %69 = arith.addf %68, %result_21 : f32
        %result_23, %valid_24 = gpu.shuffle  xor %69, %c4_i32, %c8_i32 : f32
        %70 = arith.addf %69, %result_23 : f32
        scf.if %55 {
          %71 = memref.reinterpret_cast %57 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          memref.store %70, %71[%c0] : memref<32xf32, 3>
        }
      }
      gpu.barrier
      scf.for %arg2 = %18 to %31 step %c1024 {
        %66 = arith.addi %arg2, %c256 : index
        %67 = arith.addi %arg2, %c512 : index
        %68 = arith.addi %arg2, %c768 : index
        %69 = arith.muli %17, %12 : index
        %70 = arith.addi %69, %arg2 : index
        %71 = arith.addi %69, %66 : index
        %72 = arith.addi %69, %67 : index
        %73 = arith.addi %69, %68 : index
        %74 = arith.muli %13, %14 : index
        %75 = arith.muli %74, %12 : index
        %76 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%75], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %77 = memref.load %76[%70] : memref<?xf32, "gpu">
        %78 = memref.load %76[%71] : memref<?xf32, "gpu">
        %79 = memref.load %76[%72] : memref<?xf32, "gpu">
        %80 = memref.load %76[%73] : memref<?xf32, "gpu">
        %81 = memref.reinterpret_cast %19 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        %82 = memref.load %81[%c0] : memref<32xf32, 3>
        %83 = arith.subf %77, %82 : f32
        %84 = arith.subf %78, %82 : f32
        %85 = arith.subf %79, %82 : f32
        %86 = arith.subf %80, %82 : f32
        %87 = math.exp %83 : f32
        %88 = math.exp %84 : f32
        %89 = math.exp %85 : f32
        %90 = math.exp %86 : f32
        %91 = memref.reinterpret_cast %57 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        %92 = memref.load %91[%c0] : memref<32xf32, 3>
        %93 = arith.divf %87, %92 : f32
        %94 = arith.divf %88, %92 : f32
        %95 = arith.divf %89, %92 : f32
        %96 = arith.divf %90, %92 : f32
        %97 = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%75], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %93, %97[%70] : memref<?xf32, "gpu">
        memref.store %94, %97[%71] : memref<?xf32, "gpu">
        memref.store %95, %97[%72] : memref<?xf32, "gpu">
        memref.store %96, %97[%73] : memref<?xf32, "gpu">
      }
      scf.for %arg2 = %31 to %12 step %c256 {
        %66 = arith.muli %17, %12 : index
        %67 = arith.addi %66, %arg2 : index
        %68 = arith.muli %13, %14 : index
        %69 = arith.muli %68, %12 : index
        %70 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%69], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %71 = memref.load %70[%67] : memref<?xf32, "gpu">
        %72 = memref.reinterpret_cast %19 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        %73 = memref.load %72[%c0] : memref<32xf32, 3>
        %74 = arith.subf %71, %73 : f32
        %75 = math.exp %74 : f32
        %76 = memref.reinterpret_cast %57 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        %77 = memref.load %76[%c0] : memref<32xf32, 3>
        %78 = arith.divf %75, %77 : f32
        %79 = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%69], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %78, %79[%67] : memref<?xf32, "gpu">
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_0 {
    gpu.func @main_kStitch_divide__13_1_0___1w1r(%arg0: memref<?x?x?xf32, "gpu">, %arg1: index, %arg2: memref<?x?x?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %c32 = arith.constant 32 : index
      %c8 = arith.constant 8 : index
      %c2 = arith.constant 2 : index
      %12 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
      %c4 = arith.constant 4 : index
      %c64 = arith.constant 64 : index
      %c96 = arith.constant 96 : index
      %13 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
      %14 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
      %c128 = arith.constant 128 : index
      %cst = arith.constant 0xFF800000 : f32
      %c1_i32 = arith.constant 1 : i32
      %c32_i32 = arith.constant 32 : i32
      %c2_i32 = arith.constant 2 : i32
      %c4_i32 = arith.constant 4 : i32
      %c8_i32 = arith.constant 8 : i32
      %c16_i32 = arith.constant 16 : i32
      %cst_0 = arith.constant -0.000000e+00 : f32
      %15 = affine.apply #map1(%0)[%c1, %c0]
      %16 = affine.apply #map1(%3)[%c1, %c0]
      %17 = gpu.block_id  x
      %18 = gpu.thread_id  x
      %19 = arith.divui %18, %c32 : index
      %20 = arith.remui %18, %c32 : index
      %21 = arith.muli %17, %c8 : index
      %22 = arith.addi %21, %19 : index
      %23 = arith.cmpi slt, %22, %arg1 : index
      %24 = memref.alloc() : memref<32xf32, 3>
      scf.if %23 {
        %26 = arith.subi %12, %20 : index
        %27 = arith.cmpi eq, %26, %c0 : index
        %28 = arith.subi %26, %c1 : index
        %29 = arith.divui %28, %c32 : index
        %30 = arith.addi %29, %c1 : index
        %31 = arith.select %27, %c0, %30 : index
        %32 = arith.remsi %31, %c4 : index
        %33 = arith.subi %31, %32 : index
        %34 = arith.muli %33, %c32 : index
        %35 = arith.addi %20, %34 : index
        %36 = scf.for %arg3 = %20 to %35 step %c128 iter_args(%arg4 = %cst) -> (f32) {
          %59 = arith.addi %arg3, %c32 : index
          %60 = arith.addi %arg3, %c64 : index
          %61 = arith.addi %arg3, %c96 : index
          %62 = arith.muli %22, %12 : index
          %63 = arith.addi %62, %arg3 : index
          %64 = arith.addi %62, %59 : index
          %65 = arith.addi %62, %60 : index
          %66 = arith.addi %62, %61 : index
          %67 = arith.muli %13, %14 : index
          %68 = arith.muli %67, %12 : index
          %69 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%68], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %70 = memref.load %69[%63] : memref<?xf32, "gpu">
          %71 = memref.load %69[%64] : memref<?xf32, "gpu">
          %72 = memref.load %69[%65] : memref<?xf32, "gpu">
          %73 = memref.load %69[%66] : memref<?xf32, "gpu">
          %74 = arith.cmpf ugt, %arg4, %70 : f32
          %75 = arith.select %74, %arg4, %70 : f32
          %76 = arith.cmpf uno, %70, %70 : f32
          %77 = arith.select %76, %70, %75 : f32
          %78 = arith.cmpf ugt, %77, %71 : f32
          %79 = arith.select %78, %77, %71 : f32
          %80 = arith.cmpf uno, %71, %71 : f32
          %81 = arith.select %80, %71, %79 : f32
          %82 = arith.cmpf ugt, %81, %72 : f32
          %83 = arith.select %82, %81, %72 : f32
          %84 = arith.cmpf uno, %72, %72 : f32
          %85 = arith.select %84, %72, %83 : f32
          %86 = arith.cmpf ugt, %85, %73 : f32
          %87 = arith.select %86, %85, %73 : f32
          %88 = arith.cmpf uno, %73, %73 : f32
          %89 = arith.select %88, %73, %87 : f32
          scf.yield %89 : f32
        }
        %37 = scf.for %arg3 = %35 to %12 step %c32 iter_args(%arg4 = %36) -> (f32) {
          %59 = arith.muli %22, %12 : index
          %60 = arith.addi %59, %arg3 : index
          %61 = arith.muli %13, %14 : index
          %62 = arith.muli %61, %12 : index
          %63 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%62], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %64 = memref.load %63[%60] : memref<?xf32, "gpu">
          %65 = arith.cmpf ugt, %arg4, %64 : f32
          %66 = arith.select %65, %arg4, %64 : f32
          %67 = arith.cmpf uno, %64, %64 : f32
          %68 = arith.select %67, %64, %66 : f32
          scf.yield %68 : f32
        }
        %result, %valid = gpu.shuffle  xor %37, %c1_i32, %c32_i32 : f32
        %38 = arith.cmpf ugt, %37, %result : f32
        %39 = arith.select %38, %37, %result : f32
        %40 = arith.cmpf uno, %result, %result : f32
        %41 = arith.select %40, %result, %39 : f32
        %result_1, %valid_2 = gpu.shuffle  xor %41, %c2_i32, %c32_i32 : f32
        %42 = arith.cmpf ugt, %41, %result_1 : f32
        %43 = arith.select %42, %41, %result_1 : f32
        %44 = arith.cmpf uno, %result_1, %result_1 : f32
        %45 = arith.select %44, %result_1, %43 : f32
        %result_3, %valid_4 = gpu.shuffle  xor %45, %c4_i32, %c32_i32 : f32
        %46 = arith.cmpf ugt, %45, %result_3 : f32
        %47 = arith.select %46, %45, %result_3 : f32
        %48 = arith.cmpf uno, %result_3, %result_3 : f32
        %49 = arith.select %48, %result_3, %47 : f32
        %result_5, %valid_6 = gpu.shuffle  xor %49, %c8_i32, %c32_i32 : f32
        %50 = arith.cmpf ugt, %49, %result_5 : f32
        %51 = arith.select %50, %49, %result_5 : f32
        %52 = arith.cmpf uno, %result_5, %result_5 : f32
        %53 = arith.select %52, %result_5, %51 : f32
        %result_7, %valid_8 = gpu.shuffle  xor %53, %c16_i32, %c32_i32 : f32
        %54 = arith.cmpf ugt, %53, %result_7 : f32
        %55 = arith.select %54, %53, %result_7 : f32
        %56 = arith.cmpf uno, %result_7, %result_7 : f32
        %57 = arith.select %56, %result_7, %55 : f32
        %58 = arith.cmpi eq, %20, %c0 : index
        scf.if %58 {
          %59 = memref.reinterpret_cast %24 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          memref.store %57, %59[%19] : memref<32xf32, 3>
        }
      }
      gpu.barrier
      %25 = memref.alloc() : memref<32xf32, 3>
      scf.if %23 {
        %26 = arith.subi %12, %20 : index
        %27 = arith.cmpi eq, %26, %c0 : index
        %28 = arith.subi %26, %c1 : index
        %29 = arith.divui %28, %c32 : index
        %30 = arith.addi %29, %c1 : index
        %31 = arith.select %27, %c0, %30 : index
        %32 = arith.remsi %31, %c4 : index
        %33 = arith.subi %31, %32 : index
        %34 = arith.muli %33, %c32 : index
        %35 = arith.addi %20, %34 : index
        %36 = scf.for %arg3 = %20 to %35 step %c128 iter_args(%arg4 = %cst_0) -> (f32) {
          %44 = arith.addi %arg3, %c32 : index
          %45 = arith.addi %arg3, %c64 : index
          %46 = arith.addi %arg3, %c96 : index
          %47 = arith.muli %22, %12 : index
          %48 = arith.addi %47, %arg3 : index
          %49 = arith.addi %47, %44 : index
          %50 = arith.addi %47, %45 : index
          %51 = arith.addi %47, %46 : index
          %52 = arith.divui %48, %12 : index
          %53 = arith.remui %52, %14 : index
          %54 = arith.divui %52, %14 : index
          %55 = arith.divui %49, %12 : index
          %56 = arith.remui %55, %14 : index
          %57 = arith.divui %55, %14 : index
          %58 = arith.divui %50, %12 : index
          %59 = arith.remui %58, %14 : index
          %60 = arith.divui %58, %14 : index
          %61 = arith.divui %51, %12 : index
          %62 = arith.remui %61, %14 : index
          %63 = arith.divui %61, %14 : index
          %64 = arith.muli %13, %14 : index
          %65 = arith.muli %64, %12 : index
          %66 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%65], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %67 = memref.load %66[%48] : memref<?xf32, "gpu">
          %68 = memref.load %66[%49] : memref<?xf32, "gpu">
          %69 = memref.load %66[%50] : memref<?xf32, "gpu">
          %70 = memref.load %66[%51] : memref<?xf32, "gpu">
          %71 = arith.muli %54, %14 : index
          %72 = arith.addi %71, %53 : index
          %73 = arith.muli %57, %14 : index
          %74 = arith.addi %73, %56 : index
          %75 = arith.muli %60, %14 : index
          %76 = arith.addi %75, %59 : index
          %77 = arith.muli %63, %14 : index
          %78 = arith.addi %77, %62 : index
          %79 = arith.remui %72, %c8 : index
          %80 = arith.remui %74, %c8 : index
          %81 = arith.remui %76, %c8 : index
          %82 = arith.remui %78, %c8 : index
          %83 = memref.reinterpret_cast %24 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %84 = memref.load %83[%79] : memref<32xf32, 3>
          %85 = memref.load %83[%80] : memref<32xf32, 3>
          %86 = memref.load %83[%81] : memref<32xf32, 3>
          %87 = memref.load %83[%82] : memref<32xf32, 3>
          %88 = arith.subf %67, %84 : f32
          %89 = arith.subf %68, %85 : f32
          %90 = arith.subf %69, %86 : f32
          %91 = arith.subf %70, %87 : f32
          %92 = math.exp %88 : f32
          %93 = math.exp %89 : f32
          %94 = math.exp %90 : f32
          %95 = math.exp %91 : f32
          %96 = arith.addf %arg4, %92 : f32
          %97 = arith.addf %96, %93 : f32
          %98 = arith.addf %97, %94 : f32
          %99 = arith.addf %98, %95 : f32
          scf.yield %99 : f32
        }
        %37 = scf.for %arg3 = %35 to %12 step %c32 iter_args(%arg4 = %36) -> (f32) {
          %44 = arith.muli %22, %12 : index
          %45 = arith.addi %44, %arg3 : index
          %46 = arith.divui %45, %12 : index
          %47 = arith.remui %46, %14 : index
          %48 = arith.divui %46, %14 : index
          %49 = arith.muli %13, %14 : index
          %50 = arith.muli %49, %12 : index
          %51 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%50], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %52 = memref.load %51[%45] : memref<?xf32, "gpu">
          %53 = arith.muli %48, %14 : index
          %54 = arith.addi %53, %47 : index
          %55 = arith.remui %54, %c8 : index
          %56 = memref.reinterpret_cast %24 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %57 = memref.load %56[%55] : memref<32xf32, 3>
          %58 = arith.subf %52, %57 : f32
          %59 = math.exp %58 : f32
          %60 = arith.addf %arg4, %59 : f32
          scf.yield %60 : f32
        }
        %result, %valid = gpu.shuffle  xor %37, %c1_i32, %c32_i32 : f32
        %38 = arith.addf %37, %result : f32
        %result_1, %valid_2 = gpu.shuffle  xor %38, %c2_i32, %c32_i32 : f32
        %39 = arith.addf %38, %result_1 : f32
        %result_3, %valid_4 = gpu.shuffle  xor %39, %c4_i32, %c32_i32 : f32
        %40 = arith.addf %39, %result_3 : f32
        %result_5, %valid_6 = gpu.shuffle  xor %40, %c8_i32, %c32_i32 : f32
        %41 = arith.addf %40, %result_5 : f32
        %result_7, %valid_8 = gpu.shuffle  xor %41, %c16_i32, %c32_i32 : f32
        %42 = arith.addf %41, %result_7 : f32
        %43 = arith.cmpi eq, %20, %c0 : index
        scf.if %43 {
          %44 = memref.reinterpret_cast %25 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          memref.store %42, %44[%19] : memref<32xf32, 3>
        }
      }
      gpu.barrier
      scf.if %23 {
        %26 = arith.subi %12, %20 : index
        %27 = arith.cmpi eq, %26, %c0 : index
        %28 = arith.subi %26, %c1 : index
        %29 = arith.divui %28, %c32 : index
        %30 = arith.addi %29, %c1 : index
        %31 = arith.select %27, %c0, %30 : index
        %32 = arith.remsi %31, %c4 : index
        %33 = arith.subi %31, %32 : index
        %34 = arith.muli %33, %c32 : index
        %35 = arith.addi %20, %34 : index
        scf.for %arg3 = %20 to %35 step %c128 {
          %36 = arith.addi %arg3, %c32 : index
          %37 = arith.addi %arg3, %c64 : index
          %38 = arith.addi %arg3, %c96 : index
          %39 = arith.muli %22, %12 : index
          %40 = arith.addi %39, %arg3 : index
          %41 = arith.addi %39, %36 : index
          %42 = arith.addi %39, %37 : index
          %43 = arith.addi %39, %38 : index
          %44 = arith.muli %13, %14 : index
          %45 = arith.muli %44, %12 : index
          %46 = arith.divui %40, %12 : index
          %47 = arith.remui %46, %14 : index
          %48 = arith.divui %46, %14 : index
          %49 = arith.divui %41, %12 : index
          %50 = arith.remui %49, %14 : index
          %51 = arith.divui %49, %14 : index
          %52 = arith.divui %42, %12 : index
          %53 = arith.remui %52, %14 : index
          %54 = arith.divui %52, %14 : index
          %55 = arith.divui %43, %12 : index
          %56 = arith.remui %55, %14 : index
          %57 = arith.divui %55, %14 : index
          %58 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%45], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %59 = memref.load %58[%40] : memref<?xf32, "gpu">
          %60 = memref.load %58[%41] : memref<?xf32, "gpu">
          %61 = memref.load %58[%42] : memref<?xf32, "gpu">
          %62 = memref.load %58[%43] : memref<?xf32, "gpu">
          %63 = arith.muli %48, %14 : index
          %64 = arith.addi %63, %47 : index
          %65 = arith.muli %51, %14 : index
          %66 = arith.addi %65, %50 : index
          %67 = arith.muli %54, %14 : index
          %68 = arith.addi %67, %53 : index
          %69 = arith.muli %57, %14 : index
          %70 = arith.addi %69, %56 : index
          %71 = arith.remui %64, %c8 : index
          %72 = arith.remui %66, %c8 : index
          %73 = arith.remui %68, %c8 : index
          %74 = arith.remui %70, %c8 : index
          %75 = memref.reinterpret_cast %24 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %76 = memref.load %75[%71] : memref<32xf32, 3>
          %77 = memref.load %75[%72] : memref<32xf32, 3>
          %78 = memref.load %75[%73] : memref<32xf32, 3>
          %79 = memref.load %75[%74] : memref<32xf32, 3>
          %80 = arith.subf %59, %76 : f32
          %81 = arith.subf %60, %77 : f32
          %82 = arith.subf %61, %78 : f32
          %83 = arith.subf %62, %79 : f32
          %84 = math.exp %80 : f32
          %85 = math.exp %81 : f32
          %86 = math.exp %82 : f32
          %87 = math.exp %83 : f32
          %88 = memref.reinterpret_cast %25 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %89 = memref.load %88[%71] : memref<32xf32, 3>
          %90 = memref.load %88[%72] : memref<32xf32, 3>
          %91 = memref.load %88[%73] : memref<32xf32, 3>
          %92 = memref.load %88[%74] : memref<32xf32, 3>
          %93 = arith.divf %84, %89 : f32
          %94 = arith.divf %85, %90 : f32
          %95 = arith.divf %86, %91 : f32
          %96 = arith.divf %87, %92 : f32
          %97 = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%45], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          memref.store %93, %97[%40] : memref<?xf32, "gpu">
          memref.store %94, %97[%41] : memref<?xf32, "gpu">
          memref.store %95, %97[%42] : memref<?xf32, "gpu">
          memref.store %96, %97[%43] : memref<?xf32, "gpu">
        }
        scf.for %arg3 = %35 to %12 step %c32 {
          %36 = arith.muli %22, %12 : index
          %37 = arith.addi %36, %arg3 : index
          %38 = arith.muli %13, %14 : index
          %39 = arith.muli %38, %12 : index
          %40 = arith.divui %37, %12 : index
          %41 = arith.remui %40, %14 : index
          %42 = arith.divui %40, %14 : index
          %43 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%39], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %44 = memref.load %43[%37] : memref<?xf32, "gpu">
          %45 = arith.muli %42, %14 : index
          %46 = arith.addi %45, %41 : index
          %47 = arith.remui %46, %c8 : index
          %48 = memref.reinterpret_cast %24 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %49 = memref.load %48[%47] : memref<32xf32, 3>
          %50 = arith.subf %44, %49 : f32
          %51 = math.exp %50 : f32
          %52 = memref.reinterpret_cast %25 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %53 = memref.load %52[%47] : memref<32xf32, 3>
          %54 = arith.divf %51, %53 : f32
          %55 = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%39], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          memref.store %54, %55[%37] : memref<?xf32, "gpu">
        }
      }
      gpu.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After LhloFusionInlinerPass (lhlo-fusion-inliner) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c768 = arith.constant 768 : index
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant -0.000000e+00 : f32
  %cst_0 = arith.constant 0xFF800000 : f32
  %c8 = arith.constant 8 : index
  %c16_i32 = arith.constant 16 : i32
  %c8_i32 = arith.constant 8 : i32
  %c4_i32 = arith.constant 4 : i32
  %c2_i32 = arith.constant 2 : i32
  %c1_i32 = arith.constant 1 : i32
  %c32_i32 = arith.constant 32 : i32
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c1024 = arith.constant 1024 : index
  %c6 = arith.constant 6 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
  %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
  %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
  %4 = arith.index_cast %2 : index to i32
  %5 = arith.index_cast %1 : index to i32
  %6 = arith.muli %4, %5 : i32
  %7 = arith.index_cast %6 : i32 to index
  %8 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %9 = arith.cmpi slt, %7, %c3 : index
  %10 = arith.cmpi sge, %3, %c1024 : index
  %11 = arith.cmpi slt, %7, %c6 : index
  %12 = arith.andi %11, %10 : i1
  %13 = arith.cmpi sge, %3, %c512 : index
  %14 = arith.cmpi sge, %7, %c6 : index
  %15 = arith.andi %14, %13 : i1
  %16 = arith.ori %9, %12 : i1
  %17 = arith.ori %16, %15 : i1
  scf.if %17 {
    %c1_1 = arith.constant 1 : index
    %18 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%7)[%c0, %c1]
    %19 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%c256)[%c0, %c1]
    gpu.launch_func  @main_kernel::@main_kStitch_divide__13_1_0___1b1r blocks in (%18, %c1_1, %c1_1) threads in (%19, %c1_1, %c1_1) args(%0 : memref<?x?x?xf32, "gpu">, %8 : memref<?x?x?xf32, "gpu">)
  } else {
    %18 = arith.cmpi eq, %7, %c0 : index
    %19 = arith.subi %7, %c1 : index
    %20 = arith.divui %19, %c8 : index
    %21 = arith.addi %20, %c1 : index
    %22 = arith.select %18, %c0, %21 : index
    %c1_1 = arith.constant 1 : index
    %23 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%22)[%c0, %c1]
    %24 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%c256)[%c0, %c1]
    gpu.launch_func  @main_kernel_0::@main_kStitch_divide__13_1_0___1w1r blocks in (%23, %c1_1, %c1_1) threads in (%24, %c1_1, %c1_1) args(%0 : memref<?x?x?xf32, "gpu">, %7 : index, %8 : memref<?x?x?xf32, "gpu">)
  }
  "disc_ral.dispatch"(%arg0, %c0, %8) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ReviseGpuKernelOutliningPass (disc-revise-gpu-kernel-outlining) //----- //
#map0 = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c768 = arith.constant 768 : index
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c96 = arith.constant 96 : index
    %c4 = arith.constant 4 : index
    %cst = arith.constant -0.000000e+00 : f32
    %cst_0 = arith.constant 0xFF800000 : f32
    %c8 = arith.constant 8 : index
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c4_i32 = arith.constant 4 : i32
    %c2_i32 = arith.constant 2 : i32
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c1024 = arith.constant 1024 : index
    %c6 = arith.constant 6 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
    %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
    %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
    %4 = arith.index_cast %2 : index to i32
    %5 = arith.index_cast %1 : index to i32
    %6 = arith.muli %4, %5 : i32
    %7 = arith.index_cast %6 : i32 to index
    %8 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    %9 = arith.cmpi slt, %7, %c3 : index
    %10 = arith.cmpi sge, %3, %c1024 : index
    %11 = arith.cmpi slt, %7, %c6 : index
    %12 = arith.andi %11, %10 : i1
    %13 = arith.cmpi sge, %3, %c512 : index
    %14 = arith.cmpi sge, %7, %c6 : index
    %15 = arith.andi %14, %13 : i1
    %16 = arith.ori %9, %12 : i1
    %17 = arith.ori %16, %15 : i1
    scf.if %17 {
      %c1_1 = arith.constant 1 : index
      %18 = affine.apply #map0(%7)[%c0, %c1]
      %19 = affine.apply #map0(%c256)[%c0, %c1]
      gpu.launch_func  @main_kernel::@main_kStitch_divide__13_1_0___1b1r blocks in (%18, %c1_1, %c1_1) threads in (%19, %c1_1, %c1_1) args(%0 : memref<?x?x?xf32, "gpu">, %8 : memref<?x?x?xf32, "gpu">)
    } else {
      %18 = arith.cmpi eq, %7, %c0 : index
      %19 = arith.subi %7, %c1 : index
      %20 = arith.divui %19, %c8 : index
      %21 = arith.addi %20, %c1 : index
      %22 = arith.select %18, %c0, %21 : index
      %c1_1 = arith.constant 1 : index
      %23 = affine.apply #map0(%22)[%c0, %c1]
      %24 = affine.apply #map0(%c256)[%c0, %c1]
      gpu.launch_func  @main_kernel_0::@main_kStitch_divide__13_1_0___1w1r blocks in (%23, %c1_1, %c1_1) threads in (%24, %c1_1, %c1_1) args(%0 : memref<?x?x?xf32, "gpu">, %7 : index, %8 : memref<?x?x?xf32, "gpu">)
    }
    "disc_ral.dispatch"(%arg0, %c0, %8) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel {
    gpu.func @main_kStitch_divide__13_1_0___1b1r(%arg0: memref<?x?x?xf32, "gpu">, %arg1: memref<?x?x?xf32, "gpu">) workgroup(%arg2 : memref<32xf32, 3>, %arg3 : memref<8xf32, 3>, %arg4 : memref<32xf32, 3>, %arg5 : memref<8xf32, 3>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %c32 = arith.constant 32 : index
      %c2 = arith.constant 2 : index
      %12 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
      %c256 = arith.constant 256 : index
      %c4 = arith.constant 4 : index
      %c512 = arith.constant 512 : index
      %c768 = arith.constant 768 : index
      %13 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
      %14 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
      %c1024 = arith.constant 1024 : index
      %cst = arith.constant 0xFF800000 : f32
      %c1_i32 = arith.constant 1 : i32
      %c32_i32 = arith.constant 32 : i32
      %c2_i32 = arith.constant 2 : i32
      %c4_i32 = arith.constant 4 : i32
      %c8_i32 = arith.constant 8 : i32
      %c16_i32 = arith.constant 16 : i32
      %c8 = arith.constant 8 : index
      %cst_0 = arith.constant -0.000000e+00 : f32
      %15 = affine.apply #map1(%0)[%c1, %c0]
      %16 = affine.apply #map1(%3)[%c1, %c0]
      %17 = gpu.block_id  x
      %18 = gpu.thread_id  x
      %19 = arith.divui %18, %c32 : index
      %20 = arith.remui %18, %c32 : index
      %21 = arith.subi %12, %18 : index
      %22 = arith.cmpi eq, %21, %c0 : index
      %23 = arith.subi %21, %c1 : index
      %24 = arith.divui %23, %c256 : index
      %25 = arith.addi %24, %c1 : index
      %26 = arith.select %22, %c0, %25 : index
      %27 = arith.remsi %26, %c4 : index
      %28 = arith.subi %26, %27 : index
      %29 = arith.muli %28, %c256 : index
      %30 = arith.addi %18, %29 : index
      %31 = scf.for %arg6 = %18 to %30 step %c1024 iter_args(%arg7 = %cst) -> (f32) {
        %62 = arith.addi %arg6, %c256 : index
        %63 = arith.addi %arg6, %c512 : index
        %64 = arith.addi %arg6, %c768 : index
        %65 = arith.muli %17, %12 : index
        %66 = arith.addi %65, %arg6 : index
        %67 = arith.addi %65, %62 : index
        %68 = arith.addi %65, %63 : index
        %69 = arith.addi %65, %64 : index
        %70 = arith.muli %13, %14 : index
        %71 = arith.muli %70, %12 : index
        %72 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%71], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %73 = memref.load %72[%66] : memref<?xf32, "gpu">
        %74 = memref.load %72[%67] : memref<?xf32, "gpu">
        %75 = memref.load %72[%68] : memref<?xf32, "gpu">
        %76 = memref.load %72[%69] : memref<?xf32, "gpu">
        %77 = arith.cmpf ugt, %arg7, %73 : f32
        %78 = arith.select %77, %arg7, %73 : f32
        %79 = arith.cmpf uno, %73, %73 : f32
        %80 = arith.select %79, %73, %78 : f32
        %81 = arith.cmpf ugt, %80, %74 : f32
        %82 = arith.select %81, %80, %74 : f32
        %83 = arith.cmpf uno, %74, %74 : f32
        %84 = arith.select %83, %74, %82 : f32
        %85 = arith.cmpf ugt, %84, %75 : f32
        %86 = arith.select %85, %84, %75 : f32
        %87 = arith.cmpf uno, %75, %75 : f32
        %88 = arith.select %87, %75, %86 : f32
        %89 = arith.cmpf ugt, %88, %76 : f32
        %90 = arith.select %89, %88, %76 : f32
        %91 = arith.cmpf uno, %76, %76 : f32
        %92 = arith.select %91, %76, %90 : f32
        scf.yield %92 : f32
      }
      %32 = scf.for %arg6 = %30 to %12 step %c256 iter_args(%arg7 = %31) -> (f32) {
        %62 = arith.muli %17, %12 : index
        %63 = arith.addi %62, %arg6 : index
        %64 = arith.muli %13, %14 : index
        %65 = arith.muli %64, %12 : index
        %66 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%65], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %67 = memref.load %66[%63] : memref<?xf32, "gpu">
        %68 = arith.cmpf ugt, %arg7, %67 : f32
        %69 = arith.select %68, %arg7, %67 : f32
        %70 = arith.cmpf uno, %67, %67 : f32
        %71 = arith.select %70, %67, %69 : f32
        scf.yield %71 : f32
      }
      %result, %valid = gpu.shuffle  xor %32, %c1_i32, %c32_i32 : f32
      %33 = arith.cmpf ugt, %32, %result : f32
      %34 = arith.select %33, %32, %result : f32
      %35 = arith.cmpf uno, %result, %result : f32
      %36 = arith.select %35, %result, %34 : f32
      %result_1, %valid_2 = gpu.shuffle  xor %36, %c2_i32, %c32_i32 : f32
      %37 = arith.cmpf ugt, %36, %result_1 : f32
      %38 = arith.select %37, %36, %result_1 : f32
      %39 = arith.cmpf uno, %result_1, %result_1 : f32
      %40 = arith.select %39, %result_1, %38 : f32
      %result_3, %valid_4 = gpu.shuffle  xor %40, %c4_i32, %c32_i32 : f32
      %41 = arith.cmpf ugt, %40, %result_3 : f32
      %42 = arith.select %41, %40, %result_3 : f32
      %43 = arith.cmpf uno, %result_3, %result_3 : f32
      %44 = arith.select %43, %result_3, %42 : f32
      %result_5, %valid_6 = gpu.shuffle  xor %44, %c8_i32, %c32_i32 : f32
      %45 = arith.cmpf ugt, %44, %result_5 : f32
      %46 = arith.select %45, %44, %result_5 : f32
      %47 = arith.cmpf uno, %result_5, %result_5 : f32
      %48 = arith.select %47, %result_5, %46 : f32
      %result_7, %valid_8 = gpu.shuffle  xor %48, %c16_i32, %c32_i32 : f32
      %49 = arith.cmpf ugt, %48, %result_7 : f32
      %50 = arith.select %49, %48, %result_7 : f32
      %51 = arith.cmpf uno, %result_7, %result_7 : f32
      %52 = arith.select %51, %result_7, %50 : f32
      %53 = arith.cmpi eq, %20, %c0 : index
      scf.if %53 {
        %62 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
        memref.store %52, %62[%19] : memref<8xf32, 3>
      }
      gpu.barrier
      %54 = arith.cmpi slt, %18, %c32 : index
      scf.if %54 {
        %62 = arith.cmpi slt, %20, %c8 : index
        %63 = scf.if %62 -> (f32) {
          %76 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
          %77 = memref.load %76[%20] : memref<8xf32, 3>
          scf.yield %77 : f32
        } else {
          scf.yield %cst : f32
        }
        %result_19, %valid_20 = gpu.shuffle  xor %63, %c1_i32, %c8_i32 : f32
        %64 = arith.cmpf ugt, %63, %result_19 : f32
        %65 = arith.select %64, %63, %result_19 : f32
        %66 = arith.cmpf uno, %result_19, %result_19 : f32
        %67 = arith.select %66, %result_19, %65 : f32
        %result_21, %valid_22 = gpu.shuffle  xor %67, %c2_i32, %c8_i32 : f32
        %68 = arith.cmpf ugt, %67, %result_21 : f32
        %69 = arith.select %68, %67, %result_21 : f32
        %70 = arith.cmpf uno, %result_21, %result_21 : f32
        %71 = arith.select %70, %result_21, %69 : f32
        %result_23, %valid_24 = gpu.shuffle  xor %71, %c4_i32, %c8_i32 : f32
        %72 = arith.cmpf ugt, %71, %result_23 : f32
        %73 = arith.select %72, %71, %result_23 : f32
        %74 = arith.cmpf uno, %result_23, %result_23 : f32
        %75 = arith.select %74, %result_23, %73 : f32
        scf.if %53 {
          %76 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          memref.store %75, %76[%c0] : memref<32xf32, 3>
        }
      }
      gpu.barrier
      %55 = scf.for %arg6 = %18 to %30 step %c1024 iter_args(%arg7 = %cst_0) -> (f32) {
        %62 = arith.addi %arg6, %c256 : index
        %63 = arith.addi %arg6, %c512 : index
        %64 = arith.addi %arg6, %c768 : index
        %65 = arith.muli %17, %12 : index
        %66 = arith.addi %65, %arg6 : index
        %67 = arith.addi %65, %62 : index
        %68 = arith.addi %65, %63 : index
        %69 = arith.addi %65, %64 : index
        %70 = arith.muli %13, %14 : index
        %71 = arith.muli %70, %12 : index
        %72 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%71], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %73 = memref.load %72[%66] : memref<?xf32, "gpu">
        %74 = memref.load %72[%67] : memref<?xf32, "gpu">
        %75 = memref.load %72[%68] : memref<?xf32, "gpu">
        %76 = memref.load %72[%69] : memref<?xf32, "gpu">
        %77 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        %78 = memref.load %77[%c0] : memref<32xf32, 3>
        %79 = arith.subf %73, %78 : f32
        %80 = arith.subf %74, %78 : f32
        %81 = arith.subf %75, %78 : f32
        %82 = arith.subf %76, %78 : f32
        %83 = math.exp %79 : f32
        %84 = math.exp %80 : f32
        %85 = math.exp %81 : f32
        %86 = math.exp %82 : f32
        %87 = arith.addf %arg7, %83 : f32
        %88 = arith.addf %87, %84 : f32
        %89 = arith.addf %88, %85 : f32
        %90 = arith.addf %89, %86 : f32
        scf.yield %90 : f32
      }
      %56 = scf.for %arg6 = %30 to %12 step %c256 iter_args(%arg7 = %55) -> (f32) {
        %62 = arith.muli %17, %12 : index
        %63 = arith.addi %62, %arg6 : index
        %64 = arith.muli %13, %14 : index
        %65 = arith.muli %64, %12 : index
        %66 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%65], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %67 = memref.load %66[%63] : memref<?xf32, "gpu">
        %68 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        %69 = memref.load %68[%c0] : memref<32xf32, 3>
        %70 = arith.subf %67, %69 : f32
        %71 = math.exp %70 : f32
        %72 = arith.addf %arg7, %71 : f32
        scf.yield %72 : f32
      }
      %result_9, %valid_10 = gpu.shuffle  xor %56, %c1_i32, %c32_i32 : f32
      %57 = arith.addf %56, %result_9 : f32
      %result_11, %valid_12 = gpu.shuffle  xor %57, %c2_i32, %c32_i32 : f32
      %58 = arith.addf %57, %result_11 : f32
      %result_13, %valid_14 = gpu.shuffle  xor %58, %c4_i32, %c32_i32 : f32
      %59 = arith.addf %58, %result_13 : f32
      %result_15, %valid_16 = gpu.shuffle  xor %59, %c8_i32, %c32_i32 : f32
      %60 = arith.addf %59, %result_15 : f32
      %result_17, %valid_18 = gpu.shuffle  xor %60, %c16_i32, %c32_i32 : f32
      %61 = arith.addf %60, %result_17 : f32
      scf.if %53 {
        %62 = memref.reinterpret_cast %arg5 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
        memref.store %61, %62[%19] : memref<8xf32, 3>
      }
      gpu.barrier
      scf.if %54 {
        %62 = arith.cmpi slt, %20, %c8 : index
        %63 = scf.if %62 -> (f32) {
          %67 = memref.reinterpret_cast %arg5 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
          %68 = memref.load %67[%20] : memref<8xf32, 3>
          scf.yield %68 : f32
        } else {
          scf.yield %cst_0 : f32
        }
        %result_19, %valid_20 = gpu.shuffle  xor %63, %c1_i32, %c8_i32 : f32
        %64 = arith.addf %63, %result_19 : f32
        %result_21, %valid_22 = gpu.shuffle  xor %64, %c2_i32, %c8_i32 : f32
        %65 = arith.addf %64, %result_21 : f32
        %result_23, %valid_24 = gpu.shuffle  xor %65, %c4_i32, %c8_i32 : f32
        %66 = arith.addf %65, %result_23 : f32
        scf.if %53 {
          %67 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          memref.store %66, %67[%c0] : memref<32xf32, 3>
        }
      }
      gpu.barrier
      scf.for %arg6 = %18 to %30 step %c1024 {
        %62 = arith.addi %arg6, %c256 : index
        %63 = arith.addi %arg6, %c512 : index
        %64 = arith.addi %arg6, %c768 : index
        %65 = arith.muli %17, %12 : index
        %66 = arith.addi %65, %arg6 : index
        %67 = arith.addi %65, %62 : index
        %68 = arith.addi %65, %63 : index
        %69 = arith.addi %65, %64 : index
        %70 = arith.muli %13, %14 : index
        %71 = arith.muli %70, %12 : index
        %72 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%71], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %73 = memref.load %72[%66] : memref<?xf32, "gpu">
        %74 = memref.load %72[%67] : memref<?xf32, "gpu">
        %75 = memref.load %72[%68] : memref<?xf32, "gpu">
        %76 = memref.load %72[%69] : memref<?xf32, "gpu">
        %77 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        %78 = memref.load %77[%c0] : memref<32xf32, 3>
        %79 = arith.subf %73, %78 : f32
        %80 = arith.subf %74, %78 : f32
        %81 = arith.subf %75, %78 : f32
        %82 = arith.subf %76, %78 : f32
        %83 = math.exp %79 : f32
        %84 = math.exp %80 : f32
        %85 = math.exp %81 : f32
        %86 = math.exp %82 : f32
        %87 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        %88 = memref.load %87[%c0] : memref<32xf32, 3>
        %89 = arith.divf %83, %88 : f32
        %90 = arith.divf %84, %88 : f32
        %91 = arith.divf %85, %88 : f32
        %92 = arith.divf %86, %88 : f32
        %93 = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%71], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %89, %93[%66] : memref<?xf32, "gpu">
        memref.store %90, %93[%67] : memref<?xf32, "gpu">
        memref.store %91, %93[%68] : memref<?xf32, "gpu">
        memref.store %92, %93[%69] : memref<?xf32, "gpu">
      }
      scf.for %arg6 = %30 to %12 step %c256 {
        %62 = arith.muli %17, %12 : index
        %63 = arith.addi %62, %arg6 : index
        %64 = arith.muli %13, %14 : index
        %65 = arith.muli %64, %12 : index
        %66 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%65], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %67 = memref.load %66[%63] : memref<?xf32, "gpu">
        %68 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        %69 = memref.load %68[%c0] : memref<32xf32, 3>
        %70 = arith.subf %67, %69 : f32
        %71 = math.exp %70 : f32
        %72 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        %73 = memref.load %72[%c0] : memref<32xf32, 3>
        %74 = arith.divf %71, %73 : f32
        %75 = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%65], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %74, %75[%63] : memref<?xf32, "gpu">
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_0 {
    gpu.func @main_kStitch_divide__13_1_0___1w1r(%arg0: memref<?x?x?xf32, "gpu">, %arg1: index, %arg2: memref<?x?x?xf32, "gpu">) workgroup(%arg3 : memref<32xf32, 3>, %arg4 : memref<32xf32, 3>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %c32 = arith.constant 32 : index
      %c8 = arith.constant 8 : index
      %c2 = arith.constant 2 : index
      %12 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
      %c4 = arith.constant 4 : index
      %c64 = arith.constant 64 : index
      %c96 = arith.constant 96 : index
      %13 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
      %14 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
      %c128 = arith.constant 128 : index
      %cst = arith.constant 0xFF800000 : f32
      %c1_i32 = arith.constant 1 : i32
      %c32_i32 = arith.constant 32 : i32
      %c2_i32 = arith.constant 2 : i32
      %c4_i32 = arith.constant 4 : i32
      %c8_i32 = arith.constant 8 : i32
      %c16_i32 = arith.constant 16 : i32
      %cst_0 = arith.constant -0.000000e+00 : f32
      %15 = affine.apply #map1(%0)[%c1, %c0]
      %16 = affine.apply #map1(%3)[%c1, %c0]
      %17 = gpu.block_id  x
      %18 = gpu.thread_id  x
      %19 = arith.divui %18, %c32 : index
      %20 = arith.remui %18, %c32 : index
      %21 = arith.muli %17, %c8 : index
      %22 = arith.addi %21, %19 : index
      %23 = arith.cmpi slt, %22, %arg1 : index
      scf.if %23 {
        %24 = arith.subi %12, %20 : index
        %25 = arith.cmpi eq, %24, %c0 : index
        %26 = arith.subi %24, %c1 : index
        %27 = arith.divui %26, %c32 : index
        %28 = arith.addi %27, %c1 : index
        %29 = arith.select %25, %c0, %28 : index
        %30 = arith.remsi %29, %c4 : index
        %31 = arith.subi %29, %30 : index
        %32 = arith.muli %31, %c32 : index
        %33 = arith.addi %20, %32 : index
        %34 = scf.for %arg5 = %20 to %33 step %c128 iter_args(%arg6 = %cst) -> (f32) {
          %57 = arith.addi %arg5, %c32 : index
          %58 = arith.addi %arg5, %c64 : index
          %59 = arith.addi %arg5, %c96 : index
          %60 = arith.muli %22, %12 : index
          %61 = arith.addi %60, %arg5 : index
          %62 = arith.addi %60, %57 : index
          %63 = arith.addi %60, %58 : index
          %64 = arith.addi %60, %59 : index
          %65 = arith.muli %13, %14 : index
          %66 = arith.muli %65, %12 : index
          %67 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%66], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %68 = memref.load %67[%61] : memref<?xf32, "gpu">
          %69 = memref.load %67[%62] : memref<?xf32, "gpu">
          %70 = memref.load %67[%63] : memref<?xf32, "gpu">
          %71 = memref.load %67[%64] : memref<?xf32, "gpu">
          %72 = arith.cmpf ugt, %arg6, %68 : f32
          %73 = arith.select %72, %arg6, %68 : f32
          %74 = arith.cmpf uno, %68, %68 : f32
          %75 = arith.select %74, %68, %73 : f32
          %76 = arith.cmpf ugt, %75, %69 : f32
          %77 = arith.select %76, %75, %69 : f32
          %78 = arith.cmpf uno, %69, %69 : f32
          %79 = arith.select %78, %69, %77 : f32
          %80 = arith.cmpf ugt, %79, %70 : f32
          %81 = arith.select %80, %79, %70 : f32
          %82 = arith.cmpf uno, %70, %70 : f32
          %83 = arith.select %82, %70, %81 : f32
          %84 = arith.cmpf ugt, %83, %71 : f32
          %85 = arith.select %84, %83, %71 : f32
          %86 = arith.cmpf uno, %71, %71 : f32
          %87 = arith.select %86, %71, %85 : f32
          scf.yield %87 : f32
        }
        %35 = scf.for %arg5 = %33 to %12 step %c32 iter_args(%arg6 = %34) -> (f32) {
          %57 = arith.muli %22, %12 : index
          %58 = arith.addi %57, %arg5 : index
          %59 = arith.muli %13, %14 : index
          %60 = arith.muli %59, %12 : index
          %61 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%60], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %62 = memref.load %61[%58] : memref<?xf32, "gpu">
          %63 = arith.cmpf ugt, %arg6, %62 : f32
          %64 = arith.select %63, %arg6, %62 : f32
          %65 = arith.cmpf uno, %62, %62 : f32
          %66 = arith.select %65, %62, %64 : f32
          scf.yield %66 : f32
        }
        %result, %valid = gpu.shuffle  xor %35, %c1_i32, %c32_i32 : f32
        %36 = arith.cmpf ugt, %35, %result : f32
        %37 = arith.select %36, %35, %result : f32
        %38 = arith.cmpf uno, %result, %result : f32
        %39 = arith.select %38, %result, %37 : f32
        %result_1, %valid_2 = gpu.shuffle  xor %39, %c2_i32, %c32_i32 : f32
        %40 = arith.cmpf ugt, %39, %result_1 : f32
        %41 = arith.select %40, %39, %result_1 : f32
        %42 = arith.cmpf uno, %result_1, %result_1 : f32
        %43 = arith.select %42, %result_1, %41 : f32
        %result_3, %valid_4 = gpu.shuffle  xor %43, %c4_i32, %c32_i32 : f32
        %44 = arith.cmpf ugt, %43, %result_3 : f32
        %45 = arith.select %44, %43, %result_3 : f32
        %46 = arith.cmpf uno, %result_3, %result_3 : f32
        %47 = arith.select %46, %result_3, %45 : f32
        %result_5, %valid_6 = gpu.shuffle  xor %47, %c8_i32, %c32_i32 : f32
        %48 = arith.cmpf ugt, %47, %result_5 : f32
        %49 = arith.select %48, %47, %result_5 : f32
        %50 = arith.cmpf uno, %result_5, %result_5 : f32
        %51 = arith.select %50, %result_5, %49 : f32
        %result_7, %valid_8 = gpu.shuffle  xor %51, %c16_i32, %c32_i32 : f32
        %52 = arith.cmpf ugt, %51, %result_7 : f32
        %53 = arith.select %52, %51, %result_7 : f32
        %54 = arith.cmpf uno, %result_7, %result_7 : f32
        %55 = arith.select %54, %result_7, %53 : f32
        %56 = arith.cmpi eq, %20, %c0 : index
        scf.if %56 {
          %57 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          memref.store %55, %57[%19] : memref<32xf32, 3>
        }
      }
      gpu.barrier
      scf.if %23 {
        %24 = arith.subi %12, %20 : index
        %25 = arith.cmpi eq, %24, %c0 : index
        %26 = arith.subi %24, %c1 : index
        %27 = arith.divui %26, %c32 : index
        %28 = arith.addi %27, %c1 : index
        %29 = arith.select %25, %c0, %28 : index
        %30 = arith.remsi %29, %c4 : index
        %31 = arith.subi %29, %30 : index
        %32 = arith.muli %31, %c32 : index
        %33 = arith.addi %20, %32 : index
        %34 = scf.for %arg5 = %20 to %33 step %c128 iter_args(%arg6 = %cst_0) -> (f32) {
          %42 = arith.addi %arg5, %c32 : index
          %43 = arith.addi %arg5, %c64 : index
          %44 = arith.addi %arg5, %c96 : index
          %45 = arith.muli %22, %12 : index
          %46 = arith.addi %45, %arg5 : index
          %47 = arith.addi %45, %42 : index
          %48 = arith.addi %45, %43 : index
          %49 = arith.addi %45, %44 : index
          %50 = arith.divui %46, %12 : index
          %51 = arith.remui %50, %14 : index
          %52 = arith.divui %50, %14 : index
          %53 = arith.divui %47, %12 : index
          %54 = arith.remui %53, %14 : index
          %55 = arith.divui %53, %14 : index
          %56 = arith.divui %48, %12 : index
          %57 = arith.remui %56, %14 : index
          %58 = arith.divui %56, %14 : index
          %59 = arith.divui %49, %12 : index
          %60 = arith.remui %59, %14 : index
          %61 = arith.divui %59, %14 : index
          %62 = arith.muli %13, %14 : index
          %63 = arith.muli %62, %12 : index
          %64 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%63], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %65 = memref.load %64[%46] : memref<?xf32, "gpu">
          %66 = memref.load %64[%47] : memref<?xf32, "gpu">
          %67 = memref.load %64[%48] : memref<?xf32, "gpu">
          %68 = memref.load %64[%49] : memref<?xf32, "gpu">
          %69 = arith.muli %52, %14 : index
          %70 = arith.addi %69, %51 : index
          %71 = arith.muli %55, %14 : index
          %72 = arith.addi %71, %54 : index
          %73 = arith.muli %58, %14 : index
          %74 = arith.addi %73, %57 : index
          %75 = arith.muli %61, %14 : index
          %76 = arith.addi %75, %60 : index
          %77 = arith.remui %70, %c8 : index
          %78 = arith.remui %72, %c8 : index
          %79 = arith.remui %74, %c8 : index
          %80 = arith.remui %76, %c8 : index
          %81 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %82 = memref.load %81[%77] : memref<32xf32, 3>
          %83 = memref.load %81[%78] : memref<32xf32, 3>
          %84 = memref.load %81[%79] : memref<32xf32, 3>
          %85 = memref.load %81[%80] : memref<32xf32, 3>
          %86 = arith.subf %65, %82 : f32
          %87 = arith.subf %66, %83 : f32
          %88 = arith.subf %67, %84 : f32
          %89 = arith.subf %68, %85 : f32
          %90 = math.exp %86 : f32
          %91 = math.exp %87 : f32
          %92 = math.exp %88 : f32
          %93 = math.exp %89 : f32
          %94 = arith.addf %arg6, %90 : f32
          %95 = arith.addf %94, %91 : f32
          %96 = arith.addf %95, %92 : f32
          %97 = arith.addf %96, %93 : f32
          scf.yield %97 : f32
        }
        %35 = scf.for %arg5 = %33 to %12 step %c32 iter_args(%arg6 = %34) -> (f32) {
          %42 = arith.muli %22, %12 : index
          %43 = arith.addi %42, %arg5 : index
          %44 = arith.divui %43, %12 : index
          %45 = arith.remui %44, %14 : index
          %46 = arith.divui %44, %14 : index
          %47 = arith.muli %13, %14 : index
          %48 = arith.muli %47, %12 : index
          %49 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%48], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %50 = memref.load %49[%43] : memref<?xf32, "gpu">
          %51 = arith.muli %46, %14 : index
          %52 = arith.addi %51, %45 : index
          %53 = arith.remui %52, %c8 : index
          %54 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %55 = memref.load %54[%53] : memref<32xf32, 3>
          %56 = arith.subf %50, %55 : f32
          %57 = math.exp %56 : f32
          %58 = arith.addf %arg6, %57 : f32
          scf.yield %58 : f32
        }
        %result, %valid = gpu.shuffle  xor %35, %c1_i32, %c32_i32 : f32
        %36 = arith.addf %35, %result : f32
        %result_1, %valid_2 = gpu.shuffle  xor %36, %c2_i32, %c32_i32 : f32
        %37 = arith.addf %36, %result_1 : f32
        %result_3, %valid_4 = gpu.shuffle  xor %37, %c4_i32, %c32_i32 : f32
        %38 = arith.addf %37, %result_3 : f32
        %result_5, %valid_6 = gpu.shuffle  xor %38, %c8_i32, %c32_i32 : f32
        %39 = arith.addf %38, %result_5 : f32
        %result_7, %valid_8 = gpu.shuffle  xor %39, %c16_i32, %c32_i32 : f32
        %40 = arith.addf %39, %result_7 : f32
        %41 = arith.cmpi eq, %20, %c0 : index
        scf.if %41 {
          %42 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          memref.store %40, %42[%19] : memref<32xf32, 3>
        }
      }
      gpu.barrier
      scf.if %23 {
        %24 = arith.subi %12, %20 : index
        %25 = arith.cmpi eq, %24, %c0 : index
        %26 = arith.subi %24, %c1 : index
        %27 = arith.divui %26, %c32 : index
        %28 = arith.addi %27, %c1 : index
        %29 = arith.select %25, %c0, %28 : index
        %30 = arith.remsi %29, %c4 : index
        %31 = arith.subi %29, %30 : index
        %32 = arith.muli %31, %c32 : index
        %33 = arith.addi %20, %32 : index
        scf.for %arg5 = %20 to %33 step %c128 {
          %34 = arith.addi %arg5, %c32 : index
          %35 = arith.addi %arg5, %c64 : index
          %36 = arith.addi %arg5, %c96 : index
          %37 = arith.muli %22, %12 : index
          %38 = arith.addi %37, %arg5 : index
          %39 = arith.addi %37, %34 : index
          %40 = arith.addi %37, %35 : index
          %41 = arith.addi %37, %36 : index
          %42 = arith.muli %13, %14 : index
          %43 = arith.muli %42, %12 : index
          %44 = arith.divui %38, %12 : index
          %45 = arith.remui %44, %14 : index
          %46 = arith.divui %44, %14 : index
          %47 = arith.divui %39, %12 : index
          %48 = arith.remui %47, %14 : index
          %49 = arith.divui %47, %14 : index
          %50 = arith.divui %40, %12 : index
          %51 = arith.remui %50, %14 : index
          %52 = arith.divui %50, %14 : index
          %53 = arith.divui %41, %12 : index
          %54 = arith.remui %53, %14 : index
          %55 = arith.divui %53, %14 : index
          %56 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%43], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %57 = memref.load %56[%38] : memref<?xf32, "gpu">
          %58 = memref.load %56[%39] : memref<?xf32, "gpu">
          %59 = memref.load %56[%40] : memref<?xf32, "gpu">
          %60 = memref.load %56[%41] : memref<?xf32, "gpu">
          %61 = arith.muli %46, %14 : index
          %62 = arith.addi %61, %45 : index
          %63 = arith.muli %49, %14 : index
          %64 = arith.addi %63, %48 : index
          %65 = arith.muli %52, %14 : index
          %66 = arith.addi %65, %51 : index
          %67 = arith.muli %55, %14 : index
          %68 = arith.addi %67, %54 : index
          %69 = arith.remui %62, %c8 : index
          %70 = arith.remui %64, %c8 : index
          %71 = arith.remui %66, %c8 : index
          %72 = arith.remui %68, %c8 : index
          %73 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %74 = memref.load %73[%69] : memref<32xf32, 3>
          %75 = memref.load %73[%70] : memref<32xf32, 3>
          %76 = memref.load %73[%71] : memref<32xf32, 3>
          %77 = memref.load %73[%72] : memref<32xf32, 3>
          %78 = arith.subf %57, %74 : f32
          %79 = arith.subf %58, %75 : f32
          %80 = arith.subf %59, %76 : f32
          %81 = arith.subf %60, %77 : f32
          %82 = math.exp %78 : f32
          %83 = math.exp %79 : f32
          %84 = math.exp %80 : f32
          %85 = math.exp %81 : f32
          %86 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %87 = memref.load %86[%69] : memref<32xf32, 3>
          %88 = memref.load %86[%70] : memref<32xf32, 3>
          %89 = memref.load %86[%71] : memref<32xf32, 3>
          %90 = memref.load %86[%72] : memref<32xf32, 3>
          %91 = arith.divf %82, %87 : f32
          %92 = arith.divf %83, %88 : f32
          %93 = arith.divf %84, %89 : f32
          %94 = arith.divf %85, %90 : f32
          %95 = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%43], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          memref.store %91, %95[%38] : memref<?xf32, "gpu">
          memref.store %92, %95[%39] : memref<?xf32, "gpu">
          memref.store %93, %95[%40] : memref<?xf32, "gpu">
          memref.store %94, %95[%41] : memref<?xf32, "gpu">
        }
        scf.for %arg5 = %33 to %12 step %c32 {
          %34 = arith.muli %22, %12 : index
          %35 = arith.addi %34, %arg5 : index
          %36 = arith.muli %13, %14 : index
          %37 = arith.muli %36, %12 : index
          %38 = arith.divui %35, %12 : index
          %39 = arith.remui %38, %14 : index
          %40 = arith.divui %38, %14 : index
          %41 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%37], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %42 = memref.load %41[%35] : memref<?xf32, "gpu">
          %43 = arith.muli %40, %14 : index
          %44 = arith.addi %43, %39 : index
          %45 = arith.remui %44, %c8 : index
          %46 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %47 = memref.load %46[%45] : memref<32xf32, 3>
          %48 = arith.subf %42, %47 : f32
          %49 = math.exp %48 : f32
          %50 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
          %51 = memref.load %50[%45] : memref<32xf32, 3>
          %52 = arith.divf %49, %51 : f32
          %53 = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%37], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          memref.store %52, %53[%35] : memref<?xf32, "gpu">
        }
      }
      gpu.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c8 = arith.constant 8 : index
  %c256 = arith.constant 256 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c1024 = arith.constant 1024 : index
  %c6 = arith.constant 6 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
  %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
  %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
  %4 = arith.index_cast %2 : index to i32
  %5 = arith.index_cast %1 : index to i32
  %6 = arith.muli %4, %5 : i32
  %7 = arith.index_cast %6 : i32 to index
  %8 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %9 = arith.cmpi slt, %7, %c3 : index
  %10 = arith.cmpi sge, %3, %c1024 : index
  %11 = arith.cmpi slt, %7, %c6 : index
  %12 = arith.andi %11, %10 : i1
  %13 = arith.cmpi sge, %3, %c512 : index
  %14 = arith.cmpi sge, %7, %c6 : index
  %15 = arith.andi %14, %13 : i1
  %16 = arith.ori %9, %12 : i1
  %17 = arith.ori %16, %15 : i1
  scf.if %17 {
    gpu.launch_func  @main_kernel::@main_kStitch_divide__13_1_0___1b1r blocks in (%7, %c1, %c1) threads in (%c256, %c1, %c1) args(%0 : memref<?x?x?xf32, "gpu">, %8 : memref<?x?x?xf32, "gpu">)
  } else {
    %18 = arith.cmpi eq, %7, %c0 : index
    %19 = arith.subi %7, %c1 : index
    %20 = arith.divui %19, %c8 : index
    %21 = arith.addi %20, %c1 : index
    %22 = arith.select %18, %c0, %21 : index
    gpu.launch_func  @main_kernel_0::@main_kStitch_divide__13_1_0___1w1r blocks in (%22, %c1, %c1) threads in (%c256, %c1, %c1) args(%0 : memref<?x?x?xf32, "gpu">, %7 : index, %8 : memref<?x?x?xf32, "gpu">)
  }
  "disc_ral.dispatch"(%arg0, %c0, %8) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
gpu.module @main_kernel {
  gpu.func @main_kStitch_divide__13_1_0___1b1r(%arg0: memref<?x?x?xf32, "gpu">, %arg1: memref<?x?x?xf32, "gpu">) workgroup(%arg2 : memref<32xf32, 3>, %arg3 : memref<8xf32, 3>, %arg4 : memref<32xf32, 3>, %arg5 : memref<8xf32, 3>) kernel {
    %0 = gpu.block_id  x
    %1 = gpu.block_id  y
    %2 = gpu.block_id  z
    %3 = gpu.thread_id  x
    %4 = gpu.thread_id  y
    %5 = gpu.thread_id  z
    %6 = gpu.grid_dim  x
    %7 = gpu.grid_dim  y
    %8 = gpu.grid_dim  z
    %9 = gpu.block_dim  x
    %10 = gpu.block_dim  y
    %11 = gpu.block_dim  z
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c32 = arith.constant 32 : index
    %c2 = arith.constant 2 : index
    %12 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
    %c256 = arith.constant 256 : index
    %c4 = arith.constant 4 : index
    %c512 = arith.constant 512 : index
    %c768 = arith.constant 768 : index
    %13 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
    %14 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
    %c1024 = arith.constant 1024 : index
    %cst = arith.constant 0xFF800000 : f32
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c2_i32 = arith.constant 2 : i32
    %c4_i32 = arith.constant 4 : i32
    %c8_i32 = arith.constant 8 : i32
    %c16_i32 = arith.constant 16 : i32
    %c8 = arith.constant 8 : index
    %cst_0 = arith.constant -0.000000e+00 : f32
    %15 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%c1, %c0]
    %16 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%3)[%c1, %c0]
    %17 = gpu.block_id  x
    %18 = gpu.thread_id  x
    %19 = arith.divui %18, %c32 : index
    %20 = arith.remui %18, %c32 : index
    %21 = arith.subi %12, %18 : index
    %22 = arith.cmpi eq, %21, %c0 : index
    %23 = arith.subi %21, %c1 : index
    %24 = arith.divui %23, %c256 : index
    %25 = arith.addi %24, %c1 : index
    %26 = arith.select %22, %c0, %25 : index
    %27 = arith.remsi %26, %c4 : index
    %28 = arith.subi %26, %27 : index
    %29 = arith.muli %28, %c256 : index
    %30 = arith.addi %18, %29 : index
    %31 = arith.muli %17, %12 : index
    %32 = arith.muli %13, %14 : index
    %33 = arith.muli %32, %12 : index
    %34 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%33], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %35 = scf.for %arg6 = %18 to %30 step %c1024 iter_args(%arg7 = %cst) -> (f32) {
      %94 = arith.addi %arg6, %c256 : index
      %95 = arith.addi %arg6, %c512 : index
      %96 = arith.addi %arg6, %c768 : index
      %97 = arith.addi %31, %arg6 : index
      %98 = arith.addi %31, %94 : index
      %99 = arith.addi %31, %95 : index
      %100 = arith.addi %31, %96 : index
      %101 = memref.load %34[%97] : memref<?xf32, "gpu">
      %102 = memref.load %34[%98] : memref<?xf32, "gpu">
      %103 = memref.load %34[%99] : memref<?xf32, "gpu">
      %104 = memref.load %34[%100] : memref<?xf32, "gpu">
      %105 = arith.cmpf ugt, %arg7, %101 : f32
      %106 = arith.select %105, %arg7, %101 : f32
      %107 = arith.cmpf uno, %101, %101 : f32
      %108 = arith.select %107, %101, %106 : f32
      %109 = arith.cmpf ugt, %108, %102 : f32
      %110 = arith.select %109, %108, %102 : f32
      %111 = arith.cmpf uno, %102, %102 : f32
      %112 = arith.select %111, %102, %110 : f32
      %113 = arith.cmpf ugt, %112, %103 : f32
      %114 = arith.select %113, %112, %103 : f32
      %115 = arith.cmpf uno, %103, %103 : f32
      %116 = arith.select %115, %103, %114 : f32
      %117 = arith.cmpf ugt, %116, %104 : f32
      %118 = arith.select %117, %116, %104 : f32
      %119 = arith.cmpf uno, %104, %104 : f32
      %120 = arith.select %119, %104, %118 : f32
      scf.yield %120 : f32
    }
    %36 = arith.muli %17, %12 : index
    %37 = arith.muli %13, %14 : index
    %38 = arith.muli %37, %12 : index
    %39 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%38], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %40 = scf.for %arg6 = %30 to %12 step %c256 iter_args(%arg7 = %35) -> (f32) {
      %94 = arith.addi %36, %arg6 : index
      %95 = memref.load %39[%94] : memref<?xf32, "gpu">
      %96 = arith.cmpf ugt, %arg7, %95 : f32
      %97 = arith.select %96, %arg7, %95 : f32
      %98 = arith.cmpf uno, %95, %95 : f32
      %99 = arith.select %98, %95, %97 : f32
      scf.yield %99 : f32
    }
    %result, %valid = gpu.shuffle  xor %40, %c1_i32, %c32_i32 : f32
    %41 = arith.cmpf ugt, %40, %result : f32
    %42 = arith.select %41, %40, %result : f32
    %43 = arith.cmpf uno, %result, %result : f32
    %44 = arith.select %43, %result, %42 : f32
    %result_1, %valid_2 = gpu.shuffle  xor %44, %c2_i32, %c32_i32 : f32
    %45 = arith.cmpf ugt, %44, %result_1 : f32
    %46 = arith.select %45, %44, %result_1 : f32
    %47 = arith.cmpf uno, %result_1, %result_1 : f32
    %48 = arith.select %47, %result_1, %46 : f32
    %result_3, %valid_4 = gpu.shuffle  xor %48, %c4_i32, %c32_i32 : f32
    %49 = arith.cmpf ugt, %48, %result_3 : f32
    %50 = arith.select %49, %48, %result_3 : f32
    %51 = arith.cmpf uno, %result_3, %result_3 : f32
    %52 = arith.select %51, %result_3, %50 : f32
    %result_5, %valid_6 = gpu.shuffle  xor %52, %c8_i32, %c32_i32 : f32
    %53 = arith.cmpf ugt, %52, %result_5 : f32
    %54 = arith.select %53, %52, %result_5 : f32
    %55 = arith.cmpf uno, %result_5, %result_5 : f32
    %56 = arith.select %55, %result_5, %54 : f32
    %result_7, %valid_8 = gpu.shuffle  xor %56, %c16_i32, %c32_i32 : f32
    %57 = arith.cmpf ugt, %56, %result_7 : f32
    %58 = arith.select %57, %56, %result_7 : f32
    %59 = arith.cmpf uno, %result_7, %result_7 : f32
    %60 = arith.select %59, %result_7, %58 : f32
    %61 = arith.cmpi eq, %20, %c0 : index
    scf.if %61 {
      %94 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
      memref.store %60, %94[%19] : memref<8xf32, 3>
    }
    gpu.barrier
    %62 = arith.cmpi slt, %18, %c32 : index
    scf.if %62 {
      %94 = arith.cmpi slt, %20, %c8 : index
      %95 = scf.if %94 -> (f32) {
        %108 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
        %109 = memref.load %108[%20] : memref<8xf32, 3>
        scf.yield %109 : f32
      } else {
        scf.yield %cst : f32
      }
      %result_19, %valid_20 = gpu.shuffle  xor %95, %c1_i32, %c8_i32 : f32
      %96 = arith.cmpf ugt, %95, %result_19 : f32
      %97 = arith.select %96, %95, %result_19 : f32
      %98 = arith.cmpf uno, %result_19, %result_19 : f32
      %99 = arith.select %98, %result_19, %97 : f32
      %result_21, %valid_22 = gpu.shuffle  xor %99, %c2_i32, %c8_i32 : f32
      %100 = arith.cmpf ugt, %99, %result_21 : f32
      %101 = arith.select %100, %99, %result_21 : f32
      %102 = arith.cmpf uno, %result_21, %result_21 : f32
      %103 = arith.select %102, %result_21, %101 : f32
      %result_23, %valid_24 = gpu.shuffle  xor %103, %c4_i32, %c8_i32 : f32
      %104 = arith.cmpf ugt, %103, %result_23 : f32
      %105 = arith.select %104, %103, %result_23 : f32
      %106 = arith.cmpf uno, %result_23, %result_23 : f32
      %107 = arith.select %106, %result_23, %105 : f32
      scf.if %61 {
        %108 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        memref.store %107, %108[%c0] : memref<32xf32, 3>
      }
    }
    gpu.barrier
    %63 = arith.muli %17, %12 : index
    %64 = arith.muli %13, %14 : index
    %65 = arith.muli %64, %12 : index
    %66 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%65], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %67 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %68 = scf.for %arg6 = %18 to %30 step %c1024 iter_args(%arg7 = %cst_0) -> (f32) {
      %94 = arith.addi %arg6, %c256 : index
      %95 = arith.addi %arg6, %c512 : index
      %96 = arith.addi %arg6, %c768 : index
      %97 = arith.addi %63, %arg6 : index
      %98 = arith.addi %63, %94 : index
      %99 = arith.addi %63, %95 : index
      %100 = arith.addi %63, %96 : index
      %101 = memref.load %66[%97] : memref<?xf32, "gpu">
      %102 = memref.load %66[%98] : memref<?xf32, "gpu">
      %103 = memref.load %66[%99] : memref<?xf32, "gpu">
      %104 = memref.load %66[%100] : memref<?xf32, "gpu">
      %105 = memref.load %67[%c0] : memref<32xf32, 3>
      %106 = arith.subf %101, %105 : f32
      %107 = arith.subf %102, %105 : f32
      %108 = arith.subf %103, %105 : f32
      %109 = arith.subf %104, %105 : f32
      %110 = math.exp %106 : f32
      %111 = math.exp %107 : f32
      %112 = math.exp %108 : f32
      %113 = math.exp %109 : f32
      %114 = arith.addf %arg7, %110 : f32
      %115 = arith.addf %114, %111 : f32
      %116 = arith.addf %115, %112 : f32
      %117 = arith.addf %116, %113 : f32
      scf.yield %117 : f32
    }
    %69 = arith.muli %17, %12 : index
    %70 = arith.muli %13, %14 : index
    %71 = arith.muli %70, %12 : index
    %72 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%71], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %73 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %74 = scf.for %arg6 = %30 to %12 step %c256 iter_args(%arg7 = %68) -> (f32) {
      %94 = arith.addi %69, %arg6 : index
      %95 = memref.load %72[%94] : memref<?xf32, "gpu">
      %96 = memref.load %73[%c0] : memref<32xf32, 3>
      %97 = arith.subf %95, %96 : f32
      %98 = math.exp %97 : f32
      %99 = arith.addf %arg7, %98 : f32
      scf.yield %99 : f32
    }
    %result_9, %valid_10 = gpu.shuffle  xor %74, %c1_i32, %c32_i32 : f32
    %75 = arith.addf %74, %result_9 : f32
    %result_11, %valid_12 = gpu.shuffle  xor %75, %c2_i32, %c32_i32 : f32
    %76 = arith.addf %75, %result_11 : f32
    %result_13, %valid_14 = gpu.shuffle  xor %76, %c4_i32, %c32_i32 : f32
    %77 = arith.addf %76, %result_13 : f32
    %result_15, %valid_16 = gpu.shuffle  xor %77, %c8_i32, %c32_i32 : f32
    %78 = arith.addf %77, %result_15 : f32
    %result_17, %valid_18 = gpu.shuffle  xor %78, %c16_i32, %c32_i32 : f32
    %79 = arith.addf %78, %result_17 : f32
    scf.if %61 {
      %94 = memref.reinterpret_cast %arg5 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
      memref.store %79, %94[%19] : memref<8xf32, 3>
    }
    gpu.barrier
    scf.if %62 {
      %94 = arith.cmpi slt, %20, %c8 : index
      %95 = scf.if %94 -> (f32) {
        %99 = memref.reinterpret_cast %arg5 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
        %100 = memref.load %99[%20] : memref<8xf32, 3>
        scf.yield %100 : f32
      } else {
        scf.yield %cst_0 : f32
      }
      %result_19, %valid_20 = gpu.shuffle  xor %95, %c1_i32, %c8_i32 : f32
      %96 = arith.addf %95, %result_19 : f32
      %result_21, %valid_22 = gpu.shuffle  xor %96, %c2_i32, %c8_i32 : f32
      %97 = arith.addf %96, %result_21 : f32
      %result_23, %valid_24 = gpu.shuffle  xor %97, %c4_i32, %c8_i32 : f32
      %98 = arith.addf %97, %result_23 : f32
      scf.if %61 {
        %99 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        memref.store %98, %99[%c0] : memref<32xf32, 3>
      }
    }
    gpu.barrier
    %80 = arith.muli %17, %12 : index
    %81 = arith.muli %13, %14 : index
    %82 = arith.muli %81, %12 : index
    %83 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%82], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %84 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %85 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %86 = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%82], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    scf.for %arg6 = %18 to %30 step %c1024 {
      %94 = arith.addi %arg6, %c256 : index
      %95 = arith.addi %arg6, %c512 : index
      %96 = arith.addi %arg6, %c768 : index
      %97 = arith.addi %80, %arg6 : index
      %98 = arith.addi %80, %94 : index
      %99 = arith.addi %80, %95 : index
      %100 = arith.addi %80, %96 : index
      %101 = memref.load %83[%97] : memref<?xf32, "gpu">
      %102 = memref.load %83[%98] : memref<?xf32, "gpu">
      %103 = memref.load %83[%99] : memref<?xf32, "gpu">
      %104 = memref.load %83[%100] : memref<?xf32, "gpu">
      %105 = memref.load %84[%c0] : memref<32xf32, 3>
      %106 = arith.subf %101, %105 : f32
      %107 = arith.subf %102, %105 : f32
      %108 = arith.subf %103, %105 : f32
      %109 = arith.subf %104, %105 : f32
      %110 = math.exp %106 : f32
      %111 = math.exp %107 : f32
      %112 = math.exp %108 : f32
      %113 = math.exp %109 : f32
      %114 = memref.load %85[%c0] : memref<32xf32, 3>
      %115 = arith.divf %110, %114 : f32
      %116 = arith.divf %111, %114 : f32
      %117 = arith.divf %112, %114 : f32
      %118 = arith.divf %113, %114 : f32
      memref.store %115, %86[%97] : memref<?xf32, "gpu">
      memref.store %116, %86[%98] : memref<?xf32, "gpu">
      memref.store %117, %86[%99] : memref<?xf32, "gpu">
      memref.store %118, %86[%100] : memref<?xf32, "gpu">
    }
    %87 = arith.muli %17, %12 : index
    %88 = arith.muli %13, %14 : index
    %89 = arith.muli %88, %12 : index
    %90 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%89], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %91 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %92 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %93 = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%89], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    scf.for %arg6 = %30 to %12 step %c256 {
      %94 = arith.addi %87, %arg6 : index
      %95 = memref.load %90[%94] : memref<?xf32, "gpu">
      %96 = memref.load %91[%c0] : memref<32xf32, 3>
      %97 = arith.subf %95, %96 : f32
      %98 = math.exp %97 : f32
      %99 = memref.load %92[%c0] : memref<32xf32, 3>
      %100 = arith.divf %98, %99 : f32
      memref.store %100, %93[%94] : memref<?xf32, "gpu">
    }
    gpu.return
  }
}

// -----// IR Dump After SideEffectLoopInvariantCodeMotionPass (disc-side-effect-loop-invariant-code-motion) //----- //
gpu.func @main_kStitch_divide__13_1_0___1b1r(%arg0: memref<?x?x?xf32, "gpu">, %arg1: memref<?x?x?xf32, "gpu">) workgroup(%arg2 : memref<32xf32, 3>, %arg3 : memref<8xf32, 3>, %arg4 : memref<32xf32, 3>, %arg5 : memref<8xf32, 3>) kernel {
  %cst = arith.constant -0.000000e+00 : f32
  %c8 = arith.constant 8 : index
  %c16_i32 = arith.constant 16 : i32
  %c8_i32 = arith.constant 8 : i32
  %c4_i32 = arith.constant 4 : i32
  %c2_i32 = arith.constant 2 : i32
  %c32_i32 = arith.constant 32 : i32
  %c1_i32 = arith.constant 1 : i32
  %cst_0 = arith.constant 0xFF800000 : f32
  %c1024 = arith.constant 1024 : index
  %c768 = arith.constant 768 : index
  %c512 = arith.constant 512 : index
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  cf.br ^bb1
^bb1:  // pred: ^bb0
  %0 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
  %1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
  %2 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
  %3 = gpu.block_id  x
  %4 = gpu.thread_id  x
  %5 = arith.divui %4, %c32 : index
  %6 = arith.remui %4, %c32 : index
  %7 = arith.subi %0, %4 : index
  %8 = arith.cmpi eq, %7, %c0 : index
  %9 = arith.subi %7, %c1 : index
  %10 = arith.divui %9, %c256 : index
  %11 = arith.addi %10, %c1 : index
  %12 = arith.select %8, %c0, %11 : index
  %13 = arith.remsi %12, %c4 : index
  %14 = arith.subi %12, %13 : index
  %15 = arith.muli %14, %c256 : index
  %16 = arith.addi %4, %15 : index
  %17 = arith.muli %3, %0 : index
  %18 = arith.muli %1, %2 : index
  %19 = arith.muli %18, %0 : index
  %20 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%19], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
  %21 = scf.for %arg6 = %4 to %16 step %c1024 iter_args(%arg7 = %cst_0) -> (f32) {
    %86 = arith.addi %arg6, %c256 : index
    %87 = arith.addi %arg6, %c512 : index
    %88 = arith.addi %arg6, %c768 : index
    %89 = arith.addi %17, %arg6 : index
    %90 = arith.addi %17, %86 : index
    %91 = arith.addi %17, %87 : index
    %92 = arith.addi %17, %88 : index
    %93 = memref.load %20[%89] : memref<?xf32, "gpu">
    %94 = memref.load %20[%90] : memref<?xf32, "gpu">
    %95 = memref.load %20[%91] : memref<?xf32, "gpu">
    %96 = memref.load %20[%92] : memref<?xf32, "gpu">
    %97 = arith.cmpf ugt, %arg7, %93 : f32
    %98 = arith.select %97, %arg7, %93 : f32
    %99 = arith.cmpf uno, %93, %93 : f32
    %100 = arith.select %99, %93, %98 : f32
    %101 = arith.cmpf ugt, %100, %94 : f32
    %102 = arith.select %101, %100, %94 : f32
    %103 = arith.cmpf uno, %94, %94 : f32
    %104 = arith.select %103, %94, %102 : f32
    %105 = arith.cmpf ugt, %104, %95 : f32
    %106 = arith.select %105, %104, %95 : f32
    %107 = arith.cmpf uno, %95, %95 : f32
    %108 = arith.select %107, %95, %106 : f32
    %109 = arith.cmpf ugt, %108, %96 : f32
    %110 = arith.select %109, %108, %96 : f32
    %111 = arith.cmpf uno, %96, %96 : f32
    %112 = arith.select %111, %96, %110 : f32
    scf.yield %112 : f32
  }
  %22 = arith.muli %3, %0 : index
  %23 = arith.muli %1, %2 : index
  %24 = arith.muli %23, %0 : index
  %25 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%24], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
  %26 = scf.for %arg6 = %16 to %0 step %c256 iter_args(%arg7 = %21) -> (f32) {
    %86 = arith.addi %22, %arg6 : index
    %87 = memref.load %25[%86] : memref<?xf32, "gpu">
    %88 = arith.cmpf ugt, %arg7, %87 : f32
    %89 = arith.select %88, %arg7, %87 : f32
    %90 = arith.cmpf uno, %87, %87 : f32
    %91 = arith.select %90, %87, %89 : f32
    scf.yield %91 : f32
  }
  %result, %valid = gpu.shuffle  xor %26, %c1_i32, %c32_i32 : f32
  %27 = arith.cmpf ugt, %26, %result : f32
  %28 = arith.select %27, %26, %result : f32
  %29 = arith.cmpf uno, %result, %result : f32
  %30 = arith.select %29, %result, %28 : f32
  %result_1, %valid_2 = gpu.shuffle  xor %30, %c2_i32, %c32_i32 : f32
  %31 = arith.cmpf ugt, %30, %result_1 : f32
  %32 = arith.select %31, %30, %result_1 : f32
  %33 = arith.cmpf uno, %result_1, %result_1 : f32
  %34 = arith.select %33, %result_1, %32 : f32
  %result_3, %valid_4 = gpu.shuffle  xor %34, %c4_i32, %c32_i32 : f32
  %35 = arith.cmpf ugt, %34, %result_3 : f32
  %36 = arith.select %35, %34, %result_3 : f32
  %37 = arith.cmpf uno, %result_3, %result_3 : f32
  %38 = arith.select %37, %result_3, %36 : f32
  %result_5, %valid_6 = gpu.shuffle  xor %38, %c8_i32, %c32_i32 : f32
  %39 = arith.cmpf ugt, %38, %result_5 : f32
  %40 = arith.select %39, %38, %result_5 : f32
  %41 = arith.cmpf uno, %result_5, %result_5 : f32
  %42 = arith.select %41, %result_5, %40 : f32
  %result_7, %valid_8 = gpu.shuffle  xor %42, %c16_i32, %c32_i32 : f32
  %43 = arith.cmpf ugt, %42, %result_7 : f32
  %44 = arith.select %43, %42, %result_7 : f32
  %45 = arith.cmpf uno, %result_7, %result_7 : f32
  %46 = arith.select %45, %result_7, %44 : f32
  %47 = arith.cmpi eq, %6, %c0 : index
  scf.if %47 {
    %86 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
    memref.store %46, %86[%5] : memref<8xf32, 3>
  }
  gpu.barrier
  %48 = arith.cmpi slt, %4, %c32 : index
  scf.if %48 {
    %86 = arith.cmpi slt, %6, %c8 : index
    %87 = scf.if %86 -> (f32) {
      %100 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
      %101 = memref.load %100[%6] : memref<8xf32, 3>
      scf.yield %101 : f32
    } else {
      scf.yield %cst_0 : f32
    }
    %result_19, %valid_20 = gpu.shuffle  xor %87, %c1_i32, %c8_i32 : f32
    %88 = arith.cmpf ugt, %87, %result_19 : f32
    %89 = arith.select %88, %87, %result_19 : f32
    %90 = arith.cmpf uno, %result_19, %result_19 : f32
    %91 = arith.select %90, %result_19, %89 : f32
    %result_21, %valid_22 = gpu.shuffle  xor %91, %c2_i32, %c8_i32 : f32
    %92 = arith.cmpf ugt, %91, %result_21 : f32
    %93 = arith.select %92, %91, %result_21 : f32
    %94 = arith.cmpf uno, %result_21, %result_21 : f32
    %95 = arith.select %94, %result_21, %93 : f32
    %result_23, %valid_24 = gpu.shuffle  xor %95, %c4_i32, %c8_i32 : f32
    %96 = arith.cmpf ugt, %95, %result_23 : f32
    %97 = arith.select %96, %95, %result_23 : f32
    %98 = arith.cmpf uno, %result_23, %result_23 : f32
    %99 = arith.select %98, %result_23, %97 : f32
    scf.if %47 {
      %100 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
      memref.store %99, %100[%c0] : memref<32xf32, 3>
    }
  }
  gpu.barrier
  %49 = arith.muli %3, %0 : index
  %50 = arith.muli %1, %2 : index
  %51 = arith.muli %50, %0 : index
  %52 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%51], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
  %53 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
  %54 = memref.load %53[%c0] : memref<32xf32, 3>
  %55 = scf.for %arg6 = %4 to %16 step %c1024 iter_args(%arg7 = %cst) -> (f32) {
    %86 = arith.addi %arg6, %c256 : index
    %87 = arith.addi %arg6, %c512 : index
    %88 = arith.addi %arg6, %c768 : index
    %89 = arith.addi %49, %arg6 : index
    %90 = arith.addi %49, %86 : index
    %91 = arith.addi %49, %87 : index
    %92 = arith.addi %49, %88 : index
    %93 = memref.load %52[%89] : memref<?xf32, "gpu">
    %94 = memref.load %52[%90] : memref<?xf32, "gpu">
    %95 = memref.load %52[%91] : memref<?xf32, "gpu">
    %96 = memref.load %52[%92] : memref<?xf32, "gpu">
    %97 = arith.subf %93, %54 : f32
    %98 = arith.subf %94, %54 : f32
    %99 = arith.subf %95, %54 : f32
    %100 = arith.subf %96, %54 : f32
    %101 = math.exp %97 : f32
    %102 = math.exp %98 : f32
    %103 = math.exp %99 : f32
    %104 = math.exp %100 : f32
    %105 = arith.addf %arg7, %101 : f32
    %106 = arith.addf %105, %102 : f32
    %107 = arith.addf %106, %103 : f32
    %108 = arith.addf %107, %104 : f32
    scf.yield %108 : f32
  }
  %56 = arith.muli %3, %0 : index
  %57 = arith.muli %1, %2 : index
  %58 = arith.muli %57, %0 : index
  %59 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%58], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
  %60 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
  %61 = memref.load %60[%c0] : memref<32xf32, 3>
  %62 = scf.for %arg6 = %16 to %0 step %c256 iter_args(%arg7 = %55) -> (f32) {
    %86 = arith.addi %56, %arg6 : index
    %87 = memref.load %59[%86] : memref<?xf32, "gpu">
    %88 = arith.subf %87, %61 : f32
    %89 = math.exp %88 : f32
    %90 = arith.addf %arg7, %89 : f32
    scf.yield %90 : f32
  }
  %result_9, %valid_10 = gpu.shuffle  xor %62, %c1_i32, %c32_i32 : f32
  %63 = arith.addf %62, %result_9 : f32
  %result_11, %valid_12 = gpu.shuffle  xor %63, %c2_i32, %c32_i32 : f32
  %64 = arith.addf %63, %result_11 : f32
  %result_13, %valid_14 = gpu.shuffle  xor %64, %c4_i32, %c32_i32 : f32
  %65 = arith.addf %64, %result_13 : f32
  %result_15, %valid_16 = gpu.shuffle  xor %65, %c8_i32, %c32_i32 : f32
  %66 = arith.addf %65, %result_15 : f32
  %result_17, %valid_18 = gpu.shuffle  xor %66, %c16_i32, %c32_i32 : f32
  %67 = arith.addf %66, %result_17 : f32
  scf.if %47 {
    %86 = memref.reinterpret_cast %arg5 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
    memref.store %67, %86[%5] : memref<8xf32, 3>
  }
  gpu.barrier
  scf.if %48 {
    %86 = arith.cmpi slt, %6, %c8 : index
    %87 = scf.if %86 -> (f32) {
      %91 = memref.reinterpret_cast %arg5 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
      %92 = memref.load %91[%6] : memref<8xf32, 3>
      scf.yield %92 : f32
    } else {
      scf.yield %cst : f32
    }
    %result_19, %valid_20 = gpu.shuffle  xor %87, %c1_i32, %c8_i32 : f32
    %88 = arith.addf %87, %result_19 : f32
    %result_21, %valid_22 = gpu.shuffle  xor %88, %c2_i32, %c8_i32 : f32
    %89 = arith.addf %88, %result_21 : f32
    %result_23, %valid_24 = gpu.shuffle  xor %89, %c4_i32, %c8_i32 : f32
    %90 = arith.addf %89, %result_23 : f32
    scf.if %47 {
      %91 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
      memref.store %90, %91[%c0] : memref<32xf32, 3>
    }
  }
  gpu.barrier
  %68 = arith.muli %3, %0 : index
  %69 = arith.muli %1, %2 : index
  %70 = arith.muli %69, %0 : index
  %71 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
  %72 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
  %73 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
  %74 = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%70], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
  %75 = memref.load %72[%c0] : memref<32xf32, 3>
  %76 = memref.load %73[%c0] : memref<32xf32, 3>
  scf.for %arg6 = %4 to %16 step %c1024 {
    %86 = arith.addi %arg6, %c256 : index
    %87 = arith.addi %arg6, %c512 : index
    %88 = arith.addi %arg6, %c768 : index
    %89 = arith.addi %68, %arg6 : index
    %90 = arith.addi %68, %86 : index
    %91 = arith.addi %68, %87 : index
    %92 = arith.addi %68, %88 : index
    %93 = memref.load %71[%89] : memref<?xf32, "gpu">
    %94 = memref.load %71[%90] : memref<?xf32, "gpu">
    %95 = memref.load %71[%91] : memref<?xf32, "gpu">
    %96 = memref.load %71[%92] : memref<?xf32, "gpu">
    %97 = arith.subf %93, %75 : f32
    %98 = arith.subf %94, %75 : f32
    %99 = arith.subf %95, %75 : f32
    %100 = arith.subf %96, %75 : f32
    %101 = math.exp %97 : f32
    %102 = math.exp %98 : f32
    %103 = math.exp %99 : f32
    %104 = math.exp %100 : f32
    %105 = arith.divf %101, %76 : f32
    %106 = arith.divf %102, %76 : f32
    %107 = arith.divf %103, %76 : f32
    %108 = arith.divf %104, %76 : f32
    memref.store %105, %74[%89] : memref<?xf32, "gpu">
    memref.store %106, %74[%90] : memref<?xf32, "gpu">
    memref.store %107, %74[%91] : memref<?xf32, "gpu">
    memref.store %108, %74[%92] : memref<?xf32, "gpu">
  }
  %77 = arith.muli %3, %0 : index
  %78 = arith.muli %1, %2 : index
  %79 = arith.muli %78, %0 : index
  %80 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%79], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
  %81 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
  %82 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
  %83 = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%79], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
  %84 = memref.load %81[%c0] : memref<32xf32, 3>
  %85 = memref.load %82[%c0] : memref<32xf32, 3>
  scf.for %arg6 = %16 to %0 step %c256 {
    %86 = arith.addi %77, %arg6 : index
    %87 = memref.load %80[%86] : memref<?xf32, "gpu">
    %88 = arith.subf %87, %84 : f32
    %89 = math.exp %88 : f32
    %90 = arith.divf %89, %85 : f32
    memref.store %90, %83[%86] : memref<?xf32, "gpu">
  }
  gpu.return
}

// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel {
  gpu.func @main_kStitch_divide__13_1_0___1b1r(%arg0: memref<?x?x?xf32, "gpu">, %arg1: memref<?x?x?xf32, "gpu">) workgroup(%arg2 : memref<32xf32, 3>, %arg3 : memref<8xf32, 3>, %arg4 : memref<32xf32, 3>, %arg5 : memref<8xf32, 3>) kernel {
    %cst = arith.constant -0.000000e+00 : f32
    %c8 = arith.constant 8 : index
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c4_i32 = arith.constant 4 : i32
    %c2_i32 = arith.constant 2 : i32
    %c32_i32 = arith.constant 32 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_0 = arith.constant 0xFF800000 : f32
    %c1024 = arith.constant 1024 : index
    %c768 = arith.constant 768 : index
    %c512 = arith.constant 512 : index
    %c4 = arith.constant 4 : index
    %c256 = arith.constant 256 : index
    %c2 = arith.constant 2 : index
    %c32 = arith.constant 32 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %0 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
    %1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
    %2 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
    %3 = gpu.block_id  x
    %4 = gpu.thread_id  x
    %5 = arith.divui %4, %c32 : index
    %6 = arith.remui %4, %c32 : index
    %7 = arith.subi %0, %4 : index
    %8 = arith.cmpi eq, %7, %c0 : index
    %9 = arith.subi %7, %c1 : index
    %10 = arith.divui %9, %c256 : index
    %11 = arith.addi %10, %c1 : index
    %12 = arith.select %8, %c0, %11 : index
    %13 = arith.remsi %12, %c4 : index
    %14 = arith.subi %12, %13 : index
    %15 = arith.muli %14, %c256 : index
    %16 = arith.addi %4, %15 : index
    %17 = arith.muli %3, %0 : index
    %18 = arith.muli %1, %2 : index
    %19 = arith.muli %18, %0 : index
    %20 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%19], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %21 = scf.for %arg6 = %4 to %16 step %c1024 iter_args(%arg7 = %cst_0) -> (f32) {
      %61 = arith.addi %arg6, %c256 : index
      %62 = arith.addi %arg6, %c512 : index
      %63 = arith.addi %arg6, %c768 : index
      %64 = arith.addi %17, %arg6 : index
      %65 = arith.addi %17, %61 : index
      %66 = arith.addi %17, %62 : index
      %67 = arith.addi %17, %63 : index
      %68 = memref.load %20[%64] : memref<?xf32, "gpu">
      %69 = memref.load %20[%65] : memref<?xf32, "gpu">
      %70 = memref.load %20[%66] : memref<?xf32, "gpu">
      %71 = memref.load %20[%67] : memref<?xf32, "gpu">
      %72 = arith.cmpf ugt, %arg7, %68 : f32
      %73 = arith.select %72, %arg7, %68 : f32
      %74 = arith.cmpf uno, %68, %68 : f32
      %75 = arith.select %74, %68, %73 : f32
      %76 = arith.cmpf ugt, %75, %69 : f32
      %77 = arith.select %76, %75, %69 : f32
      %78 = arith.cmpf uno, %69, %69 : f32
      %79 = arith.select %78, %69, %77 : f32
      %80 = arith.cmpf ugt, %79, %70 : f32
      %81 = arith.select %80, %79, %70 : f32
      %82 = arith.cmpf uno, %70, %70 : f32
      %83 = arith.select %82, %70, %81 : f32
      %84 = arith.cmpf ugt, %83, %71 : f32
      %85 = arith.select %84, %83, %71 : f32
      %86 = arith.cmpf uno, %71, %71 : f32
      %87 = arith.select %86, %71, %85 : f32
      scf.yield %87 : f32
    }
    %22 = scf.for %arg6 = %16 to %0 step %c256 iter_args(%arg7 = %21) -> (f32) {
      %61 = arith.addi %17, %arg6 : index
      %62 = memref.load %20[%61] : memref<?xf32, "gpu">
      %63 = arith.cmpf ugt, %arg7, %62 : f32
      %64 = arith.select %63, %arg7, %62 : f32
      %65 = arith.cmpf uno, %62, %62 : f32
      %66 = arith.select %65, %62, %64 : f32
      scf.yield %66 : f32
    }
    %result, %valid = gpu.shuffle  xor %22, %c1_i32, %c32_i32 : f32
    %23 = arith.cmpf ugt, %22, %result : f32
    %24 = arith.select %23, %22, %result : f32
    %25 = arith.cmpf uno, %result, %result : f32
    %26 = arith.select %25, %result, %24 : f32
    %result_1, %valid_2 = gpu.shuffle  xor %26, %c2_i32, %c32_i32 : f32
    %27 = arith.cmpf ugt, %26, %result_1 : f32
    %28 = arith.select %27, %26, %result_1 : f32
    %29 = arith.cmpf uno, %result_1, %result_1 : f32
    %30 = arith.select %29, %result_1, %28 : f32
    %result_3, %valid_4 = gpu.shuffle  xor %30, %c4_i32, %c32_i32 : f32
    %31 = arith.cmpf ugt, %30, %result_3 : f32
    %32 = arith.select %31, %30, %result_3 : f32
    %33 = arith.cmpf uno, %result_3, %result_3 : f32
    %34 = arith.select %33, %result_3, %32 : f32
    %result_5, %valid_6 = gpu.shuffle  xor %34, %c8_i32, %c32_i32 : f32
    %35 = arith.cmpf ugt, %34, %result_5 : f32
    %36 = arith.select %35, %34, %result_5 : f32
    %37 = arith.cmpf uno, %result_5, %result_5 : f32
    %38 = arith.select %37, %result_5, %36 : f32
    %result_7, %valid_8 = gpu.shuffle  xor %38, %c16_i32, %c32_i32 : f32
    %39 = arith.cmpf ugt, %38, %result_7 : f32
    %40 = arith.select %39, %38, %result_7 : f32
    %41 = arith.cmpf uno, %result_7, %result_7 : f32
    %42 = arith.select %41, %result_7, %40 : f32
    %43 = arith.cmpi eq, %6, %c0 : index
    scf.if %43 {
      %61 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
      memref.store %42, %61[%5] : memref<8xf32, 3>
    }
    gpu.barrier
    %44 = arith.cmpi slt, %4, %c32 : index
    scf.if %44 {
      %61 = arith.cmpi slt, %6, %c8 : index
      %62 = scf.if %61 -> (f32) {
        %75 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
        %76 = memref.load %75[%6] : memref<8xf32, 3>
        scf.yield %76 : f32
      } else {
        scf.yield %cst_0 : f32
      }
      %result_19, %valid_20 = gpu.shuffle  xor %62, %c1_i32, %c8_i32 : f32
      %63 = arith.cmpf ugt, %62, %result_19 : f32
      %64 = arith.select %63, %62, %result_19 : f32
      %65 = arith.cmpf uno, %result_19, %result_19 : f32
      %66 = arith.select %65, %result_19, %64 : f32
      %result_21, %valid_22 = gpu.shuffle  xor %66, %c2_i32, %c8_i32 : f32
      %67 = arith.cmpf ugt, %66, %result_21 : f32
      %68 = arith.select %67, %66, %result_21 : f32
      %69 = arith.cmpf uno, %result_21, %result_21 : f32
      %70 = arith.select %69, %result_21, %68 : f32
      %result_23, %valid_24 = gpu.shuffle  xor %70, %c4_i32, %c8_i32 : f32
      %71 = arith.cmpf ugt, %70, %result_23 : f32
      %72 = arith.select %71, %70, %result_23 : f32
      %73 = arith.cmpf uno, %result_23, %result_23 : f32
      %74 = arith.select %73, %result_23, %72 : f32
      scf.if %43 {
        %75 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        memref.store %74, %75[%c0] : memref<32xf32, 3>
      }
    }
    gpu.barrier
    %45 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %46 = memref.load %45[%c0] : memref<32xf32, 3>
    %47 = scf.for %arg6 = %4 to %16 step %c1024 iter_args(%arg7 = %cst) -> (f32) {
      %61 = arith.addi %arg6, %c256 : index
      %62 = arith.addi %arg6, %c512 : index
      %63 = arith.addi %arg6, %c768 : index
      %64 = arith.addi %17, %arg6 : index
      %65 = arith.addi %17, %61 : index
      %66 = arith.addi %17, %62 : index
      %67 = arith.addi %17, %63 : index
      %68 = memref.load %20[%64] : memref<?xf32, "gpu">
      %69 = memref.load %20[%65] : memref<?xf32, "gpu">
      %70 = memref.load %20[%66] : memref<?xf32, "gpu">
      %71 = memref.load %20[%67] : memref<?xf32, "gpu">
      %72 = arith.subf %68, %46 : f32
      %73 = arith.subf %69, %46 : f32
      %74 = arith.subf %70, %46 : f32
      %75 = arith.subf %71, %46 : f32
      %76 = math.exp %72 : f32
      %77 = math.exp %73 : f32
      %78 = math.exp %74 : f32
      %79 = math.exp %75 : f32
      %80 = arith.addf %arg7, %76 : f32
      %81 = arith.addf %80, %77 : f32
      %82 = arith.addf %81, %78 : f32
      %83 = arith.addf %82, %79 : f32
      scf.yield %83 : f32
    }
    %48 = memref.load %45[%c0] : memref<32xf32, 3>
    %49 = scf.for %arg6 = %16 to %0 step %c256 iter_args(%arg7 = %47) -> (f32) {
      %61 = arith.addi %17, %arg6 : index
      %62 = memref.load %20[%61] : memref<?xf32, "gpu">
      %63 = arith.subf %62, %48 : f32
      %64 = math.exp %63 : f32
      %65 = arith.addf %arg7, %64 : f32
      scf.yield %65 : f32
    }
    %result_9, %valid_10 = gpu.shuffle  xor %49, %c1_i32, %c32_i32 : f32
    %50 = arith.addf %49, %result_9 : f32
    %result_11, %valid_12 = gpu.shuffle  xor %50, %c2_i32, %c32_i32 : f32
    %51 = arith.addf %50, %result_11 : f32
    %result_13, %valid_14 = gpu.shuffle  xor %51, %c4_i32, %c32_i32 : f32
    %52 = arith.addf %51, %result_13 : f32
    %result_15, %valid_16 = gpu.shuffle  xor %52, %c8_i32, %c32_i32 : f32
    %53 = arith.addf %52, %result_15 : f32
    %result_17, %valid_18 = gpu.shuffle  xor %53, %c16_i32, %c32_i32 : f32
    %54 = arith.addf %53, %result_17 : f32
    scf.if %43 {
      %61 = memref.reinterpret_cast %arg5 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
      memref.store %54, %61[%5] : memref<8xf32, 3>
    }
    gpu.barrier
    scf.if %44 {
      %61 = arith.cmpi slt, %6, %c8 : index
      %62 = scf.if %61 -> (f32) {
        %66 = memref.reinterpret_cast %arg5 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
        %67 = memref.load %66[%6] : memref<8xf32, 3>
        scf.yield %67 : f32
      } else {
        scf.yield %cst : f32
      }
      %result_19, %valid_20 = gpu.shuffle  xor %62, %c1_i32, %c8_i32 : f32
      %63 = arith.addf %62, %result_19 : f32
      %result_21, %valid_22 = gpu.shuffle  xor %63, %c2_i32, %c8_i32 : f32
      %64 = arith.addf %63, %result_21 : f32
      %result_23, %valid_24 = gpu.shuffle  xor %64, %c4_i32, %c8_i32 : f32
      %65 = arith.addf %64, %result_23 : f32
      scf.if %43 {
        %66 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        memref.store %65, %66[%c0] : memref<32xf32, 3>
      }
    }
    gpu.barrier
    %55 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %56 = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%19], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %57 = memref.load %45[%c0] : memref<32xf32, 3>
    %58 = memref.load %55[%c0] : memref<32xf32, 3>
    scf.for %arg6 = %4 to %16 step %c1024 {
      %61 = arith.addi %arg6, %c256 : index
      %62 = arith.addi %arg6, %c512 : index
      %63 = arith.addi %arg6, %c768 : index
      %64 = arith.addi %17, %arg6 : index
      %65 = arith.addi %17, %61 : index
      %66 = arith.addi %17, %62 : index
      %67 = arith.addi %17, %63 : index
      %68 = memref.load %20[%64] : memref<?xf32, "gpu">
      %69 = memref.load %20[%65] : memref<?xf32, "gpu">
      %70 = memref.load %20[%66] : memref<?xf32, "gpu">
      %71 = memref.load %20[%67] : memref<?xf32, "gpu">
      %72 = arith.subf %68, %57 : f32
      %73 = arith.subf %69, %57 : f32
      %74 = arith.subf %70, %57 : f32
      %75 = arith.subf %71, %57 : f32
      %76 = math.exp %72 : f32
      %77 = math.exp %73 : f32
      %78 = math.exp %74 : f32
      %79 = math.exp %75 : f32
      %80 = arith.divf %76, %58 : f32
      %81 = arith.divf %77, %58 : f32
      %82 = arith.divf %78, %58 : f32
      %83 = arith.divf %79, %58 : f32
      memref.store %80, %56[%64] : memref<?xf32, "gpu">
      memref.store %81, %56[%65] : memref<?xf32, "gpu">
      memref.store %82, %56[%66] : memref<?xf32, "gpu">
      memref.store %83, %56[%67] : memref<?xf32, "gpu">
    }
    %59 = memref.load %45[%c0] : memref<32xf32, 3>
    %60 = memref.load %55[%c0] : memref<32xf32, 3>
    scf.for %arg6 = %16 to %0 step %c256 {
      %61 = arith.addi %17, %arg6 : index
      %62 = memref.load %20[%61] : memref<?xf32, "gpu">
      %63 = arith.subf %62, %59 : f32
      %64 = math.exp %63 : f32
      %65 = arith.divf %64, %60 : f32
      memref.store %65, %56[%61] : memref<?xf32, "gpu">
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel {
  gpu.func @main_kStitch_divide__13_1_0___1b1r(%arg0: memref<?x?x?xf32, "gpu">, %arg1: memref<?x?x?xf32, "gpu">) workgroup(%arg2 : memref<32xf32, 3>, %arg3 : memref<8xf32, 3>, %arg4 : memref<32xf32, 3>, %arg5 : memref<8xf32, 3>) kernel {
    %cst = arith.constant -0.000000e+00 : f32
    %c8 = arith.constant 8 : index
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c4_i32 = arith.constant 4 : i32
    %c2_i32 = arith.constant 2 : i32
    %c32_i32 = arith.constant 32 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_0 = arith.constant 0xFF800000 : f32
    %c1024 = arith.constant 1024 : index
    %c768 = arith.constant 768 : index
    %c512 = arith.constant 512 : index
    %c4 = arith.constant 4 : index
    %c256 = arith.constant 256 : index
    %c2 = arith.constant 2 : index
    %c32 = arith.constant 32 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %0 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
    %1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
    %2 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
    %3 = gpu.block_id  x
    %4 = gpu.thread_id  x
    %5 = arith.divui %4, %c32 : index
    %6 = arith.remui %4, %c32 : index
    %7 = arith.subi %0, %4 : index
    %8 = arith.cmpi eq, %7, %c0 : index
    %9 = arith.subi %7, %c1 : index
    %10 = arith.divui %9, %c256 : index
    %11 = arith.addi %10, %c1 : index
    %12 = arith.select %8, %c0, %11 : index
    %13 = arith.remsi %12, %c4 : index
    %14 = arith.subi %12, %13 : index
    %15 = arith.muli %14, %c256 : index
    %16 = arith.addi %4, %15 : index
    %17 = arith.muli %3, %0 : index
    %18 = arith.muli %1, %2 : index
    %19 = arith.muli %18, %0 : index
    %20 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%19], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    cf.br ^bb2(%4, %cst_0 : index, f32)
  ^bb2(%21: index, %22: f32):  // 2 preds: ^bb1, ^bb3
    %23 = arith.cmpi slt, %21, %16 : index
    cf.cond_br %23, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %24 = arith.addi %21, %c256 : index
    %25 = arith.addi %21, %c512 : index
    %26 = arith.addi %21, %c768 : index
    %27 = arith.addi %17, %21 : index
    %28 = arith.addi %17, %24 : index
    %29 = arith.addi %17, %25 : index
    %30 = arith.addi %17, %26 : index
    %31 = memref.load %20[%27] : memref<?xf32, "gpu">
    %32 = memref.load %20[%28] : memref<?xf32, "gpu">
    %33 = memref.load %20[%29] : memref<?xf32, "gpu">
    %34 = memref.load %20[%30] : memref<?xf32, "gpu">
    %35 = arith.cmpf ugt, %22, %31 : f32
    %36 = arith.select %35, %22, %31 : f32
    %37 = arith.cmpf uno, %31, %31 : f32
    %38 = arith.select %37, %31, %36 : f32
    %39 = arith.cmpf ugt, %38, %32 : f32
    %40 = arith.select %39, %38, %32 : f32
    %41 = arith.cmpf uno, %32, %32 : f32
    %42 = arith.select %41, %32, %40 : f32
    %43 = arith.cmpf ugt, %42, %33 : f32
    %44 = arith.select %43, %42, %33 : f32
    %45 = arith.cmpf uno, %33, %33 : f32
    %46 = arith.select %45, %33, %44 : f32
    %47 = arith.cmpf ugt, %46, %34 : f32
    %48 = arith.select %47, %46, %34 : f32
    %49 = arith.cmpf uno, %34, %34 : f32
    %50 = arith.select %49, %34, %48 : f32
    %51 = arith.addi %21, %c1024 : index
    cf.br ^bb2(%51, %50 : index, f32)
  ^bb4:  // pred: ^bb2
    cf.br ^bb5(%16, %22 : index, f32)
  ^bb5(%52: index, %53: f32):  // 2 preds: ^bb4, ^bb6
    %54 = arith.cmpi slt, %52, %0 : index
    cf.cond_br %54, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %55 = arith.addi %17, %52 : index
    %56 = memref.load %20[%55] : memref<?xf32, "gpu">
    %57 = arith.cmpf ugt, %53, %56 : f32
    %58 = arith.select %57, %53, %56 : f32
    %59 = arith.cmpf uno, %56, %56 : f32
    %60 = arith.select %59, %56, %58 : f32
    %61 = arith.addi %52, %c256 : index
    cf.br ^bb5(%61, %60 : index, f32)
  ^bb7:  // pred: ^bb5
    %result, %valid = gpu.shuffle  xor %53, %c1_i32, %c32_i32 : f32
    %62 = arith.cmpf ugt, %53, %result : f32
    %63 = arith.select %62, %53, %result : f32
    %64 = arith.cmpf uno, %result, %result : f32
    %65 = arith.select %64, %result, %63 : f32
    %result_1, %valid_2 = gpu.shuffle  xor %65, %c2_i32, %c32_i32 : f32
    %66 = arith.cmpf ugt, %65, %result_1 : f32
    %67 = arith.select %66, %65, %result_1 : f32
    %68 = arith.cmpf uno, %result_1, %result_1 : f32
    %69 = arith.select %68, %result_1, %67 : f32
    %result_3, %valid_4 = gpu.shuffle  xor %69, %c4_i32, %c32_i32 : f32
    %70 = arith.cmpf ugt, %69, %result_3 : f32
    %71 = arith.select %70, %69, %result_3 : f32
    %72 = arith.cmpf uno, %result_3, %result_3 : f32
    %73 = arith.select %72, %result_3, %71 : f32
    %result_5, %valid_6 = gpu.shuffle  xor %73, %c8_i32, %c32_i32 : f32
    %74 = arith.cmpf ugt, %73, %result_5 : f32
    %75 = arith.select %74, %73, %result_5 : f32
    %76 = arith.cmpf uno, %result_5, %result_5 : f32
    %77 = arith.select %76, %result_5, %75 : f32
    %result_7, %valid_8 = gpu.shuffle  xor %77, %c16_i32, %c32_i32 : f32
    %78 = arith.cmpf ugt, %77, %result_7 : f32
    %79 = arith.select %78, %77, %result_7 : f32
    %80 = arith.cmpf uno, %result_7, %result_7 : f32
    %81 = arith.select %80, %result_7, %79 : f32
    %82 = arith.cmpi eq, %6, %c0 : index
    cf.cond_br %82, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %83 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
    memref.store %81, %83[%5] : memref<8xf32, 3>
    cf.br ^bb9
  ^bb9:  // 2 preds: ^bb7, ^bb8
    gpu.barrier
    %84 = arith.cmpi slt, %4, %c32 : index
    cf.cond_br %84, ^bb10, ^bb17
  ^bb10:  // pred: ^bb9
    %85 = arith.cmpi slt, %6, %c8 : index
    cf.cond_br %85, ^bb11, ^bb12
  ^bb11:  // pred: ^bb10
    %86 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
    %87 = memref.load %86[%6] : memref<8xf32, 3>
    cf.br ^bb13(%87 : f32)
  ^bb12:  // pred: ^bb10
    cf.br ^bb13(%cst_0 : f32)
  ^bb13(%88: f32):  // 2 preds: ^bb11, ^bb12
    cf.br ^bb14
  ^bb14:  // pred: ^bb13
    %result_9, %valid_10 = gpu.shuffle  xor %88, %c1_i32, %c8_i32 : f32
    %89 = arith.cmpf ugt, %88, %result_9 : f32
    %90 = arith.select %89, %88, %result_9 : f32
    %91 = arith.cmpf uno, %result_9, %result_9 : f32
    %92 = arith.select %91, %result_9, %90 : f32
    %result_11, %valid_12 = gpu.shuffle  xor %92, %c2_i32, %c8_i32 : f32
    %93 = arith.cmpf ugt, %92, %result_11 : f32
    %94 = arith.select %93, %92, %result_11 : f32
    %95 = arith.cmpf uno, %result_11, %result_11 : f32
    %96 = arith.select %95, %result_11, %94 : f32
    %result_13, %valid_14 = gpu.shuffle  xor %96, %c4_i32, %c8_i32 : f32
    %97 = arith.cmpf ugt, %96, %result_13 : f32
    %98 = arith.select %97, %96, %result_13 : f32
    %99 = arith.cmpf uno, %result_13, %result_13 : f32
    %100 = arith.select %99, %result_13, %98 : f32
    cf.cond_br %82, ^bb15, ^bb16
  ^bb15:  // pred: ^bb14
    %101 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    memref.store %100, %101[%c0] : memref<32xf32, 3>
    cf.br ^bb16
  ^bb16:  // 2 preds: ^bb14, ^bb15
    cf.br ^bb17
  ^bb17:  // 2 preds: ^bb9, ^bb16
    gpu.barrier
    %102 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %103 = memref.load %102[%c0] : memref<32xf32, 3>
    cf.br ^bb18(%4, %cst : index, f32)
  ^bb18(%104: index, %105: f32):  // 2 preds: ^bb17, ^bb19
    %106 = arith.cmpi slt, %104, %16 : index
    cf.cond_br %106, ^bb19, ^bb20
  ^bb19:  // pred: ^bb18
    %107 = arith.addi %104, %c256 : index
    %108 = arith.addi %104, %c512 : index
    %109 = arith.addi %104, %c768 : index
    %110 = arith.addi %17, %104 : index
    %111 = arith.addi %17, %107 : index
    %112 = arith.addi %17, %108 : index
    %113 = arith.addi %17, %109 : index
    %114 = memref.load %20[%110] : memref<?xf32, "gpu">
    %115 = memref.load %20[%111] : memref<?xf32, "gpu">
    %116 = memref.load %20[%112] : memref<?xf32, "gpu">
    %117 = memref.load %20[%113] : memref<?xf32, "gpu">
    %118 = arith.subf %114, %103 : f32
    %119 = arith.subf %115, %103 : f32
    %120 = arith.subf %116, %103 : f32
    %121 = arith.subf %117, %103 : f32
    %122 = math.exp %118 : f32
    %123 = math.exp %119 : f32
    %124 = math.exp %120 : f32
    %125 = math.exp %121 : f32
    %126 = arith.addf %105, %122 : f32
    %127 = arith.addf %126, %123 : f32
    %128 = arith.addf %127, %124 : f32
    %129 = arith.addf %128, %125 : f32
    %130 = arith.addi %104, %c1024 : index
    cf.br ^bb18(%130, %129 : index, f32)
  ^bb20:  // pred: ^bb18
    %131 = memref.load %102[%c0] : memref<32xf32, 3>
    cf.br ^bb21(%16, %105 : index, f32)
  ^bb21(%132: index, %133: f32):  // 2 preds: ^bb20, ^bb22
    %134 = arith.cmpi slt, %132, %0 : index
    cf.cond_br %134, ^bb22, ^bb23
  ^bb22:  // pred: ^bb21
    %135 = arith.addi %17, %132 : index
    %136 = memref.load %20[%135] : memref<?xf32, "gpu">
    %137 = arith.subf %136, %131 : f32
    %138 = math.exp %137 : f32
    %139 = arith.addf %133, %138 : f32
    %140 = arith.addi %132, %c256 : index
    cf.br ^bb21(%140, %139 : index, f32)
  ^bb23:  // pred: ^bb21
    %result_15, %valid_16 = gpu.shuffle  xor %133, %c1_i32, %c32_i32 : f32
    %141 = arith.addf %133, %result_15 : f32
    %result_17, %valid_18 = gpu.shuffle  xor %141, %c2_i32, %c32_i32 : f32
    %142 = arith.addf %141, %result_17 : f32
    %result_19, %valid_20 = gpu.shuffle  xor %142, %c4_i32, %c32_i32 : f32
    %143 = arith.addf %142, %result_19 : f32
    %result_21, %valid_22 = gpu.shuffle  xor %143, %c8_i32, %c32_i32 : f32
    %144 = arith.addf %143, %result_21 : f32
    %result_23, %valid_24 = gpu.shuffle  xor %144, %c16_i32, %c32_i32 : f32
    %145 = arith.addf %144, %result_23 : f32
    cf.cond_br %82, ^bb24, ^bb25
  ^bb24:  // pred: ^bb23
    %146 = memref.reinterpret_cast %arg5 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
    memref.store %145, %146[%5] : memref<8xf32, 3>
    cf.br ^bb25
  ^bb25:  // 2 preds: ^bb23, ^bb24
    gpu.barrier
    cf.cond_br %84, ^bb26, ^bb33
  ^bb26:  // pred: ^bb25
    %147 = arith.cmpi slt, %6, %c8 : index
    cf.cond_br %147, ^bb27, ^bb28
  ^bb27:  // pred: ^bb26
    %148 = memref.reinterpret_cast %arg5 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
    %149 = memref.load %148[%6] : memref<8xf32, 3>
    cf.br ^bb29(%149 : f32)
  ^bb28:  // pred: ^bb26
    cf.br ^bb29(%cst : f32)
  ^bb29(%150: f32):  // 2 preds: ^bb27, ^bb28
    cf.br ^bb30
  ^bb30:  // pred: ^bb29
    %result_25, %valid_26 = gpu.shuffle  xor %150, %c1_i32, %c8_i32 : f32
    %151 = arith.addf %150, %result_25 : f32
    %result_27, %valid_28 = gpu.shuffle  xor %151, %c2_i32, %c8_i32 : f32
    %152 = arith.addf %151, %result_27 : f32
    %result_29, %valid_30 = gpu.shuffle  xor %152, %c4_i32, %c8_i32 : f32
    %153 = arith.addf %152, %result_29 : f32
    cf.cond_br %82, ^bb31, ^bb32
  ^bb31:  // pred: ^bb30
    %154 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    memref.store %153, %154[%c0] : memref<32xf32, 3>
    cf.br ^bb32
  ^bb32:  // 2 preds: ^bb30, ^bb31
    cf.br ^bb33
  ^bb33:  // 2 preds: ^bb25, ^bb32
    gpu.barrier
    %155 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %156 = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%19], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %157 = memref.load %102[%c0] : memref<32xf32, 3>
    %158 = memref.load %155[%c0] : memref<32xf32, 3>
    cf.br ^bb34(%4 : index)
  ^bb34(%159: index):  // 2 preds: ^bb33, ^bb35
    %160 = arith.cmpi slt, %159, %16 : index
    cf.cond_br %160, ^bb35, ^bb36
  ^bb35:  // pred: ^bb34
    %161 = arith.addi %159, %c256 : index
    %162 = arith.addi %159, %c512 : index
    %163 = arith.addi %159, %c768 : index
    %164 = arith.addi %17, %159 : index
    %165 = arith.addi %17, %161 : index
    %166 = arith.addi %17, %162 : index
    %167 = arith.addi %17, %163 : index
    %168 = memref.load %20[%164] : memref<?xf32, "gpu">
    %169 = memref.load %20[%165] : memref<?xf32, "gpu">
    %170 = memref.load %20[%166] : memref<?xf32, "gpu">
    %171 = memref.load %20[%167] : memref<?xf32, "gpu">
    %172 = arith.subf %168, %157 : f32
    %173 = arith.subf %169, %157 : f32
    %174 = arith.subf %170, %157 : f32
    %175 = arith.subf %171, %157 : f32
    %176 = math.exp %172 : f32
    %177 = math.exp %173 : f32
    %178 = math.exp %174 : f32
    %179 = math.exp %175 : f32
    %180 = arith.divf %176, %158 : f32
    %181 = arith.divf %177, %158 : f32
    %182 = arith.divf %178, %158 : f32
    %183 = arith.divf %179, %158 : f32
    memref.store %180, %156[%164] : memref<?xf32, "gpu">
    memref.store %181, %156[%165] : memref<?xf32, "gpu">
    memref.store %182, %156[%166] : memref<?xf32, "gpu">
    memref.store %183, %156[%167] : memref<?xf32, "gpu">
    %184 = arith.addi %159, %c1024 : index
    cf.br ^bb34(%184 : index)
  ^bb36:  // pred: ^bb34
    %185 = memref.load %102[%c0] : memref<32xf32, 3>
    %186 = memref.load %155[%c0] : memref<32xf32, 3>
    cf.br ^bb37(%16 : index)
  ^bb37(%187: index):  // 2 preds: ^bb36, ^bb38
    %188 = arith.cmpi slt, %187, %0 : index
    cf.cond_br %188, ^bb38, ^bb39
  ^bb38:  // pred: ^bb37
    %189 = arith.addi %17, %187 : index
    %190 = memref.load %20[%189] : memref<?xf32, "gpu">
    %191 = arith.subf %190, %185 : f32
    %192 = math.exp %191 : f32
    %193 = arith.divf %192, %186 : f32
    memref.store %193, %156[%189] : memref<?xf32, "gpu">
    %194 = arith.addi %187, %c256 : index
    cf.br ^bb37(%194 : index)
  ^bb39:  // pred: ^bb37
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel {
  gpu.func @main_kStitch_divide__13_1_0___1b1r(%arg0: memref<?x?x?xf32, "gpu">, %arg1: memref<?x?x?xf32, "gpu">) workgroup(%arg2 : memref<32xf32, 3>, %arg3 : memref<8xf32, 3>, %arg4 : memref<32xf32, 3>, %arg5 : memref<8xf32, 3>) kernel {
    %cst = arith.constant -0.000000e+00 : f32
    %c8 = arith.constant 8 : index
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c4_i32 = arith.constant 4 : i32
    %c2_i32 = arith.constant 2 : i32
    %c32_i32 = arith.constant 32 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_0 = arith.constant 0xFF800000 : f32
    %c1024 = arith.constant 1024 : index
    %c768 = arith.constant 768 : index
    %c512 = arith.constant 512 : index
    %c4 = arith.constant 4 : index
    %c256 = arith.constant 256 : index
    %c2 = arith.constant 2 : index
    %c32 = arith.constant 32 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %0 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
    %1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
    %2 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
    %3 = gpu.block_id  x
    %4 = gpu.thread_id  x
    %5 = arith.divui %4, %c32 : index
    %6 = arith.remui %4, %c32 : index
    %7 = arith.subi %0, %4 : index
    %8 = arith.cmpi eq, %7, %c0 : index
    %9 = arith.subi %7, %c1 : index
    %10 = arith.divui %9, %c256 : index
    %11 = arith.addi %10, %c1 : index
    %12 = arith.select %8, %c0, %11 : index
    %13 = arith.remsi %12, %c4 : index
    %14 = arith.subi %12, %13 : index
    %15 = arith.muli %14, %c256 : index
    %16 = arith.addi %4, %15 : index
    %17 = arith.muli %3, %0 : index
    %18 = arith.muli %1, %2 : index
    %19 = arith.muli %18, %0 : index
    %20 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%19], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    cf.br ^bb2(%4, %cst_0 : index, f32)
  ^bb2(%21: index, %22: f32):  // 2 preds: ^bb1, ^bb3
    %23 = arith.cmpi slt, %21, %16 : index
    cf.cond_br %23, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %24 = arith.addi %21, %c256 : index
    %25 = arith.addi %21, %c512 : index
    %26 = arith.addi %21, %c768 : index
    %27 = arith.addi %17, %21 : index
    %28 = arith.addi %17, %24 : index
    %29 = arith.addi %17, %25 : index
    %30 = arith.addi %17, %26 : index
    %31 = memref.load %20[%27] : memref<?xf32, "gpu">
    %32 = memref.load %20[%28] : memref<?xf32, "gpu">
    %33 = memref.load %20[%29] : memref<?xf32, "gpu">
    %34 = memref.load %20[%30] : memref<?xf32, "gpu">
    %35 = arith.cmpf ugt, %22, %31 : f32
    %36 = arith.select %35, %22, %31 : f32
    %37 = arith.cmpf uno, %31, %31 : f32
    %38 = arith.select %37, %31, %36 : f32
    %39 = arith.cmpf ugt, %38, %32 : f32
    %40 = arith.select %39, %38, %32 : f32
    %41 = arith.cmpf uno, %32, %32 : f32
    %42 = arith.select %41, %32, %40 : f32
    %43 = arith.cmpf ugt, %42, %33 : f32
    %44 = arith.select %43, %42, %33 : f32
    %45 = arith.cmpf uno, %33, %33 : f32
    %46 = arith.select %45, %33, %44 : f32
    %47 = arith.cmpf ugt, %46, %34 : f32
    %48 = arith.select %47, %46, %34 : f32
    %49 = arith.cmpf uno, %34, %34 : f32
    %50 = arith.select %49, %34, %48 : f32
    %51 = arith.addi %21, %c1024 : index
    cf.br ^bb2(%51, %50 : index, f32)
  ^bb4:  // pred: ^bb2
    cf.br ^bb5(%16, %22 : index, f32)
  ^bb5(%52: index, %53: f32):  // 2 preds: ^bb4, ^bb6
    %54 = arith.cmpi slt, %52, %0 : index
    cf.cond_br %54, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %55 = arith.addi %17, %52 : index
    %56 = memref.load %20[%55] : memref<?xf32, "gpu">
    %57 = arith.cmpf ugt, %53, %56 : f32
    %58 = arith.select %57, %53, %56 : f32
    %59 = arith.cmpf uno, %56, %56 : f32
    %60 = arith.select %59, %56, %58 : f32
    %61 = arith.addi %52, %c256 : index
    cf.br ^bb5(%61, %60 : index, f32)
  ^bb7:  // pred: ^bb5
    %result, %valid = gpu.shuffle  xor %53, %c1_i32, %c32_i32 : f32
    %62 = arith.cmpf ugt, %53, %result : f32
    %63 = arith.select %62, %53, %result : f32
    %64 = arith.cmpf uno, %result, %result : f32
    %65 = arith.select %64, %result, %63 : f32
    %result_1, %valid_2 = gpu.shuffle  xor %65, %c2_i32, %c32_i32 : f32
    %66 = arith.cmpf ugt, %65, %result_1 : f32
    %67 = arith.select %66, %65, %result_1 : f32
    %68 = arith.cmpf uno, %result_1, %result_1 : f32
    %69 = arith.select %68, %result_1, %67 : f32
    %result_3, %valid_4 = gpu.shuffle  xor %69, %c4_i32, %c32_i32 : f32
    %70 = arith.cmpf ugt, %69, %result_3 : f32
    %71 = arith.select %70, %69, %result_3 : f32
    %72 = arith.cmpf uno, %result_3, %result_3 : f32
    %73 = arith.select %72, %result_3, %71 : f32
    %result_5, %valid_6 = gpu.shuffle  xor %73, %c8_i32, %c32_i32 : f32
    %74 = arith.cmpf ugt, %73, %result_5 : f32
    %75 = arith.select %74, %73, %result_5 : f32
    %76 = arith.cmpf uno, %result_5, %result_5 : f32
    %77 = arith.select %76, %result_5, %75 : f32
    %result_7, %valid_8 = gpu.shuffle  xor %77, %c16_i32, %c32_i32 : f32
    %78 = arith.cmpf ugt, %77, %result_7 : f32
    %79 = arith.select %78, %77, %result_7 : f32
    %80 = arith.cmpf uno, %result_7, %result_7 : f32
    %81 = arith.select %80, %result_7, %79 : f32
    %82 = arith.cmpi eq, %6, %c0 : index
    cf.cond_br %82, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %83 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
    memref.store %81, %83[%5] : memref<8xf32, 3>
    cf.br ^bb9
  ^bb9:  // 2 preds: ^bb7, ^bb8
    gpu.barrier
    %84 = arith.cmpi slt, %4, %c32 : index
    cf.cond_br %84, ^bb10, ^bb17
  ^bb10:  // pred: ^bb9
    %85 = arith.cmpi slt, %6, %c8 : index
    cf.cond_br %85, ^bb11, ^bb12
  ^bb11:  // pred: ^bb10
    %86 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
    %87 = memref.load %86[%6] : memref<8xf32, 3>
    cf.br ^bb13(%87 : f32)
  ^bb12:  // pred: ^bb10
    cf.br ^bb13(%cst_0 : f32)
  ^bb13(%88: f32):  // 2 preds: ^bb11, ^bb12
    cf.br ^bb14
  ^bb14:  // pred: ^bb13
    %result_9, %valid_10 = gpu.shuffle  xor %88, %c1_i32, %c8_i32 : f32
    %89 = arith.cmpf ugt, %88, %result_9 : f32
    %90 = arith.select %89, %88, %result_9 : f32
    %91 = arith.cmpf uno, %result_9, %result_9 : f32
    %92 = arith.select %91, %result_9, %90 : f32
    %result_11, %valid_12 = gpu.shuffle  xor %92, %c2_i32, %c8_i32 : f32
    %93 = arith.cmpf ugt, %92, %result_11 : f32
    %94 = arith.select %93, %92, %result_11 : f32
    %95 = arith.cmpf uno, %result_11, %result_11 : f32
    %96 = arith.select %95, %result_11, %94 : f32
    %result_13, %valid_14 = gpu.shuffle  xor %96, %c4_i32, %c8_i32 : f32
    %97 = arith.cmpf ugt, %96, %result_13 : f32
    %98 = arith.select %97, %96, %result_13 : f32
    %99 = arith.cmpf uno, %result_13, %result_13 : f32
    %100 = arith.select %99, %result_13, %98 : f32
    cf.cond_br %82, ^bb15, ^bb16
  ^bb15:  // pred: ^bb14
    %101 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    memref.store %100, %101[%c0] : memref<32xf32, 3>
    cf.br ^bb16
  ^bb16:  // 2 preds: ^bb14, ^bb15
    cf.br ^bb17
  ^bb17:  // 2 preds: ^bb9, ^bb16
    gpu.barrier
    %102 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %103 = memref.load %102[%c0] : memref<32xf32, 3>
    cf.br ^bb18(%4, %cst : index, f32)
  ^bb18(%104: index, %105: f32):  // 2 preds: ^bb17, ^bb19
    %106 = arith.cmpi slt, %104, %16 : index
    cf.cond_br %106, ^bb19, ^bb20
  ^bb19:  // pred: ^bb18
    %107 = arith.addi %104, %c256 : index
    %108 = arith.addi %104, %c512 : index
    %109 = arith.addi %104, %c768 : index
    %110 = arith.addi %17, %104 : index
    %111 = arith.addi %17, %107 : index
    %112 = arith.addi %17, %108 : index
    %113 = arith.addi %17, %109 : index
    %114 = memref.load %20[%110] : memref<?xf32, "gpu">
    %115 = memref.load %20[%111] : memref<?xf32, "gpu">
    %116 = memref.load %20[%112] : memref<?xf32, "gpu">
    %117 = memref.load %20[%113] : memref<?xf32, "gpu">
    %118 = arith.subf %114, %103 : f32
    %119 = arith.subf %115, %103 : f32
    %120 = arith.subf %116, %103 : f32
    %121 = arith.subf %117, %103 : f32
    %122 = math.exp %118 : f32
    %123 = math.exp %119 : f32
    %124 = math.exp %120 : f32
    %125 = math.exp %121 : f32
    %126 = arith.addf %105, %122 : f32
    %127 = arith.addf %126, %123 : f32
    %128 = arith.addf %127, %124 : f32
    %129 = arith.addf %128, %125 : f32
    %130 = arith.addi %104, %c1024 : index
    cf.br ^bb18(%130, %129 : index, f32)
  ^bb20:  // pred: ^bb18
    %131 = memref.load %102[%c0] : memref<32xf32, 3>
    cf.br ^bb21(%16, %105 : index, f32)
  ^bb21(%132: index, %133: f32):  // 2 preds: ^bb20, ^bb22
    %134 = arith.cmpi slt, %132, %0 : index
    cf.cond_br %134, ^bb22, ^bb23
  ^bb22:  // pred: ^bb21
    %135 = arith.addi %17, %132 : index
    %136 = memref.load %20[%135] : memref<?xf32, "gpu">
    %137 = arith.subf %136, %131 : f32
    %138 = math.exp %137 : f32
    %139 = arith.addf %133, %138 : f32
    %140 = arith.addi %132, %c256 : index
    cf.br ^bb21(%140, %139 : index, f32)
  ^bb23:  // pred: ^bb21
    %result_15, %valid_16 = gpu.shuffle  xor %133, %c1_i32, %c32_i32 : f32
    %141 = arith.addf %133, %result_15 : f32
    %result_17, %valid_18 = gpu.shuffle  xor %141, %c2_i32, %c32_i32 : f32
    %142 = arith.addf %141, %result_17 : f32
    %result_19, %valid_20 = gpu.shuffle  xor %142, %c4_i32, %c32_i32 : f32
    %143 = arith.addf %142, %result_19 : f32
    %result_21, %valid_22 = gpu.shuffle  xor %143, %c8_i32, %c32_i32 : f32
    %144 = arith.addf %143, %result_21 : f32
    %result_23, %valid_24 = gpu.shuffle  xor %144, %c16_i32, %c32_i32 : f32
    %145 = arith.addf %144, %result_23 : f32
    cf.cond_br %82, ^bb24, ^bb25
  ^bb24:  // pred: ^bb23
    %146 = memref.reinterpret_cast %arg5 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
    memref.store %145, %146[%5] : memref<8xf32, 3>
    cf.br ^bb25
  ^bb25:  // 2 preds: ^bb23, ^bb24
    gpu.barrier
    cf.cond_br %84, ^bb26, ^bb33
  ^bb26:  // pred: ^bb25
    %147 = arith.cmpi slt, %6, %c8 : index
    cf.cond_br %147, ^bb27, ^bb28
  ^bb27:  // pred: ^bb26
    %148 = memref.reinterpret_cast %arg5 to offset: [0], sizes: [8], strides: [1] : memref<8xf32, 3> to memref<8xf32, 3>
    %149 = memref.load %148[%6] : memref<8xf32, 3>
    cf.br ^bb29(%149 : f32)
  ^bb28:  // pred: ^bb26
    cf.br ^bb29(%cst : f32)
  ^bb29(%150: f32):  // 2 preds: ^bb27, ^bb28
    cf.br ^bb30
  ^bb30:  // pred: ^bb29
    %result_25, %valid_26 = gpu.shuffle  xor %150, %c1_i32, %c8_i32 : f32
    %151 = arith.addf %150, %result_25 : f32
    %result_27, %valid_28 = gpu.shuffle  xor %151, %c2_i32, %c8_i32 : f32
    %152 = arith.addf %151, %result_27 : f32
    %result_29, %valid_30 = gpu.shuffle  xor %152, %c4_i32, %c8_i32 : f32
    %153 = arith.addf %152, %result_29 : f32
    cf.cond_br %82, ^bb31, ^bb32
  ^bb31:  // pred: ^bb30
    %154 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    memref.store %153, %154[%c0] : memref<32xf32, 3>
    cf.br ^bb32
  ^bb32:  // 2 preds: ^bb30, ^bb31
    cf.br ^bb33
  ^bb33:  // 2 preds: ^bb25, ^bb32
    gpu.barrier
    %155 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %156 = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%19], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %157 = memref.load %102[%c0] : memref<32xf32, 3>
    %158 = memref.load %155[%c0] : memref<32xf32, 3>
    cf.br ^bb34(%4 : index)
  ^bb34(%159: index):  // 2 preds: ^bb33, ^bb35
    %160 = arith.cmpi slt, %159, %16 : index
    cf.cond_br %160, ^bb35, ^bb36
  ^bb35:  // pred: ^bb34
    %161 = arith.addi %159, %c256 : index
    %162 = arith.addi %159, %c512 : index
    %163 = arith.addi %159, %c768 : index
    %164 = arith.addi %17, %159 : index
    %165 = arith.addi %17, %161 : index
    %166 = arith.addi %17, %162 : index
    %167 = arith.addi %17, %163 : index
    %168 = memref.load %20[%164] : memref<?xf32, "gpu">
    %169 = memref.load %20[%165] : memref<?xf32, "gpu">
    %170 = memref.load %20[%166] : memref<?xf32, "gpu">
    %171 = memref.load %20[%167] : memref<?xf32, "gpu">
    %172 = arith.subf %168, %157 : f32
    %173 = arith.subf %169, %157 : f32
    %174 = arith.subf %170, %157 : f32
    %175 = arith.subf %171, %157 : f32
    %176 = math.exp %172 : f32
    %177 = math.exp %173 : f32
    %178 = math.exp %174 : f32
    %179 = math.exp %175 : f32
    %180 = arith.divf %176, %158 : f32
    %181 = arith.divf %177, %158 : f32
    %182 = arith.divf %178, %158 : f32
    %183 = arith.divf %179, %158 : f32
    memref.store %180, %156[%164] : memref<?xf32, "gpu">
    memref.store %181, %156[%165] : memref<?xf32, "gpu">
    memref.store %182, %156[%166] : memref<?xf32, "gpu">
    memref.store %183, %156[%167] : memref<?xf32, "gpu">
    %184 = arith.addi %159, %c1024 : index
    cf.br ^bb34(%184 : index)
  ^bb36:  // pred: ^bb34
    %185 = memref.load %102[%c0] : memref<32xf32, 3>
    %186 = memref.load %155[%c0] : memref<32xf32, 3>
    cf.br ^bb37(%16 : index)
  ^bb37(%187: index):  // 2 preds: ^bb36, ^bb38
    %188 = arith.cmpi slt, %187, %0 : index
    cf.cond_br %188, ^bb38, ^bb39
  ^bb38:  // pred: ^bb37
    %189 = arith.addi %17, %187 : index
    %190 = memref.load %20[%189] : memref<?xf32, "gpu">
    %191 = arith.subf %190, %185 : f32
    %192 = math.exp %191 : f32
    %193 = arith.divf %192, %186 : f32
    memref.store %193, %156[%189] : memref<?xf32, "gpu">
    %194 = arith.addi %187, %c256 : index
    cf.br ^bb37(%194 : index)
  ^bb39:  // pred: ^bb37
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel {
  llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_0() {addr_space = 3 : i32} : !llvm.array<32 x f32>
  llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_1() {addr_space = 3 : i32} : !llvm.array<8 x f32>
  llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_2() {addr_space = 3 : i32} : !llvm.array<32 x f32>
  llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_3() {addr_space = 3 : i32} : !llvm.array<8 x f32>
  llvm.func @__nv_expf(f32) -> f32
  llvm.func @main_kStitch_divide__13_1_0___1b1r(%arg0: !llvm.ptr<f32>, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: !llvm.ptr<f32>, %arg10: !llvm.ptr<f32>, %arg11: i32, %arg12: i32, %arg13: i32, %arg14: i32, %arg15: i32, %arg16: i32, %arg17: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %3 = llvm.insertvalue %arg2, %2[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %4 = llvm.insertvalue %arg3, %3[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %5 = llvm.insertvalue %arg6, %4[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %6 = llvm.insertvalue %arg4, %5[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %7 = llvm.insertvalue %arg7, %6[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %8 = llvm.insertvalue %arg5, %7[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %9 = llvm.insertvalue %arg8, %8[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %10 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)>
    %11 = llvm.insertvalue %arg9, %10[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %12 = llvm.insertvalue %arg10, %11[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %13 = llvm.insertvalue %arg11, %12[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %14 = llvm.insertvalue %arg12, %13[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %15 = llvm.insertvalue %arg15, %14[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %16 = llvm.insertvalue %arg13, %15[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %17 = llvm.insertvalue %arg16, %16[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %18 = llvm.insertvalue %arg14, %17[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %19 = llvm.insertvalue %arg17, %18[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %20 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_0 : !llvm.ptr<array<32 x f32>, 3>
    %21 = llvm.getelementptr %20[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %22 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %23 = llvm.insertvalue %21, %22[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %24 = llvm.insertvalue %21, %23[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %25 = llvm.mlir.constant(0 : index) : i32
    %26 = llvm.insertvalue %25, %24[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %27 = llvm.mlir.constant(32 : index) : i32
    %28 = llvm.insertvalue %27, %26[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %29 = llvm.mlir.constant(1 : index) : i32
    %30 = llvm.insertvalue %29, %28[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %31 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_1 : !llvm.ptr<array<8 x f32>, 3>
    %32 = llvm.getelementptr %31[0, 0] : (!llvm.ptr<array<8 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %33 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %34 = llvm.insertvalue %32, %33[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %35 = llvm.insertvalue %32, %34[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %36 = llvm.mlir.constant(0 : index) : i32
    %37 = llvm.insertvalue %36, %35[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %38 = llvm.mlir.constant(8 : index) : i32
    %39 = llvm.insertvalue %38, %37[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %40 = llvm.mlir.constant(1 : index) : i32
    %41 = llvm.insertvalue %40, %39[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %42 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_2 : !llvm.ptr<array<32 x f32>, 3>
    %43 = llvm.getelementptr %42[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %44 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %45 = llvm.insertvalue %43, %44[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %46 = llvm.insertvalue %43, %45[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %47 = llvm.mlir.constant(0 : index) : i32
    %48 = llvm.insertvalue %47, %46[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %49 = llvm.mlir.constant(32 : index) : i32
    %50 = llvm.insertvalue %49, %48[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %51 = llvm.mlir.constant(1 : index) : i32
    %52 = llvm.insertvalue %51, %50[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %53 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_3 : !llvm.ptr<array<8 x f32>, 3>
    %54 = llvm.getelementptr %53[0, 0] : (!llvm.ptr<array<8 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %55 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %56 = llvm.insertvalue %54, %55[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %57 = llvm.insertvalue %54, %56[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %58 = llvm.mlir.constant(0 : index) : i32
    %59 = llvm.insertvalue %58, %57[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %60 = llvm.mlir.constant(8 : index) : i32
    %61 = llvm.insertvalue %60, %59[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %62 = llvm.mlir.constant(1 : index) : i32
    %63 = llvm.insertvalue %62, %61[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %64 = llvm.mlir.constant(-0.000000e+00 : f32) : f32
    %65 = llvm.mlir.constant(8 : index) : i32
    %66 = llvm.mlir.constant(16 : i32) : i32
    %67 = llvm.mlir.constant(8 : i32) : i32
    %68 = llvm.mlir.constant(4 : i32) : i32
    %69 = llvm.mlir.constant(2 : i32) : i32
    %70 = llvm.mlir.constant(32 : i32) : i32
    %71 = llvm.mlir.constant(1 : i32) : i32
    %72 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %73 = llvm.mlir.constant(1024 : index) : i32
    %74 = llvm.mlir.constant(768 : index) : i32
    %75 = llvm.mlir.constant(512 : index) : i32
    %76 = llvm.mlir.constant(4 : index) : i32
    %77 = llvm.mlir.constant(256 : index) : i32
    %78 = llvm.mlir.constant(2 : index) : i32
    %79 = llvm.mlir.constant(32 : index) : i32
    %80 = llvm.mlir.constant(0 : index) : i32
    %81 = llvm.mlir.constant(1 : index) : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %82 = llvm.extractvalue %9[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %83 = llvm.extractvalue %9[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %84 = llvm.extractvalue %9[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %85 = nvvm.read.ptx.sreg.ctaid.x : i32
    %86 = nvvm.read.ptx.sreg.tid.x : i32
    %87 = llvm.udiv %86, %79  : i32
    %88 = llvm.urem %86, %79  : i32
    %89 = llvm.sub %82, %86  : i32
    %90 = llvm.icmp "eq" %89, %80 : i32
    %91 = llvm.sub %89, %81  : i32
    %92 = llvm.udiv %91, %77  : i32
    %93 = llvm.add %92, %81  : i32
    %94 = llvm.select %90, %80, %93 : i1, i32
    %95 = llvm.srem %94, %76  : i32
    %96 = llvm.sub %94, %95  : i32
    %97 = llvm.mul %96, %77  : i32
    %98 = llvm.add %86, %97  : i32
    %99 = llvm.mul %85, %82  : i32
    %100 = llvm.mul %83, %84  : i32
    %101 = llvm.mul %100, %82  : i32
    %102 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %103 = llvm.extractvalue %9[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %104 = llvm.extractvalue %9[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %105 = llvm.insertvalue %103, %102[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %106 = llvm.insertvalue %104, %105[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %107 = llvm.insertvalue %80, %106[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %108 = llvm.insertvalue %101, %107[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %109 = llvm.insertvalue %81, %108[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    llvm.br ^bb2(%86, %72 : i32, f32)
  ^bb2(%110: i32, %111: f32):  // 2 preds: ^bb1, ^bb3
    %112 = llvm.icmp "slt" %110, %98 : i32
    llvm.cond_br %112, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %113 = llvm.add %110, %77  : i32
    %114 = llvm.add %110, %75  : i32
    %115 = llvm.add %110, %74  : i32
    %116 = llvm.add %99, %110  : i32
    %117 = llvm.add %99, %113  : i32
    %118 = llvm.add %99, %114  : i32
    %119 = llvm.add %99, %115  : i32
    %120 = llvm.extractvalue %109[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %121 = llvm.getelementptr %120[%116] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %122 = llvm.load %121 : !llvm.ptr<f32>
    %123 = llvm.extractvalue %109[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %124 = llvm.getelementptr %123[%117] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %125 = llvm.load %124 : !llvm.ptr<f32>
    %126 = llvm.extractvalue %109[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %127 = llvm.getelementptr %126[%118] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %128 = llvm.load %127 : !llvm.ptr<f32>
    %129 = llvm.extractvalue %109[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %130 = llvm.getelementptr %129[%119] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %131 = llvm.load %130 : !llvm.ptr<f32>
    %132 = llvm.fcmp "ugt" %111, %122 : f32
    %133 = llvm.select %132, %111, %122 : i1, f32
    %134 = llvm.fcmp "uno" %122, %122 : f32
    %135 = llvm.select %134, %122, %133 : i1, f32
    %136 = llvm.fcmp "ugt" %135, %125 : f32
    %137 = llvm.select %136, %135, %125 : i1, f32
    %138 = llvm.fcmp "uno" %125, %125 : f32
    %139 = llvm.select %138, %125, %137 : i1, f32
    %140 = llvm.fcmp "ugt" %139, %128 : f32
    %141 = llvm.select %140, %139, %128 : i1, f32
    %142 = llvm.fcmp "uno" %128, %128 : f32
    %143 = llvm.select %142, %128, %141 : i1, f32
    %144 = llvm.fcmp "ugt" %143, %131 : f32
    %145 = llvm.select %144, %143, %131 : i1, f32
    %146 = llvm.fcmp "uno" %131, %131 : f32
    %147 = llvm.select %146, %131, %145 : i1, f32
    %148 = llvm.add %110, %73  : i32
    llvm.br ^bb2(%148, %147 : i32, f32)
  ^bb4:  // pred: ^bb2
    llvm.br ^bb5(%98, %111 : i32, f32)
  ^bb5(%149: i32, %150: f32):  // 2 preds: ^bb4, ^bb6
    %151 = llvm.icmp "slt" %149, %82 : i32
    llvm.cond_br %151, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %152 = llvm.add %99, %149  : i32
    %153 = llvm.extractvalue %109[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %154 = llvm.getelementptr %153[%152] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %155 = llvm.load %154 : !llvm.ptr<f32>
    %156 = llvm.fcmp "ugt" %150, %155 : f32
    %157 = llvm.select %156, %150, %155 : i1, f32
    %158 = llvm.fcmp "uno" %155, %155 : f32
    %159 = llvm.select %158, %155, %157 : i1, f32
    %160 = llvm.add %149, %77  : i32
    llvm.br ^bb5(%160, %159 : i32, f32)
  ^bb7:  // pred: ^bb5
    %161 = llvm.mlir.constant(1 : i32) : i32
    %162 = llvm.mlir.constant(-1 : i32) : i32
    %163 = llvm.mlir.constant(32 : i32) : i32
    %164 = llvm.sub %163, %70  : i32
    %165 = llvm.lshr %162, %164  : i32
    %166 = llvm.sub %70, %161  : i32
    %167 = nvvm.shfl.sync  bfly %165, %150, %71, %166 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %168 = llvm.extractvalue %167[0] : !llvm.struct<(f32, i1)> 
    %169 = llvm.extractvalue %167[1] : !llvm.struct<(f32, i1)> 
    %170 = llvm.fcmp "ugt" %150, %168 : f32
    %171 = llvm.select %170, %150, %168 : i1, f32
    %172 = llvm.fcmp "uno" %168, %168 : f32
    %173 = llvm.select %172, %168, %171 : i1, f32
    %174 = llvm.mlir.constant(1 : i32) : i32
    %175 = llvm.mlir.constant(-1 : i32) : i32
    %176 = llvm.mlir.constant(32 : i32) : i32
    %177 = llvm.sub %176, %70  : i32
    %178 = llvm.lshr %175, %177  : i32
    %179 = llvm.sub %70, %174  : i32
    %180 = nvvm.shfl.sync  bfly %178, %173, %69, %179 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %181 = llvm.extractvalue %180[0] : !llvm.struct<(f32, i1)> 
    %182 = llvm.extractvalue %180[1] : !llvm.struct<(f32, i1)> 
    %183 = llvm.fcmp "ugt" %173, %181 : f32
    %184 = llvm.select %183, %173, %181 : i1, f32
    %185 = llvm.fcmp "uno" %181, %181 : f32
    %186 = llvm.select %185, %181, %184 : i1, f32
    %187 = llvm.mlir.constant(1 : i32) : i32
    %188 = llvm.mlir.constant(-1 : i32) : i32
    %189 = llvm.mlir.constant(32 : i32) : i32
    %190 = llvm.sub %189, %70  : i32
    %191 = llvm.lshr %188, %190  : i32
    %192 = llvm.sub %70, %187  : i32
    %193 = nvvm.shfl.sync  bfly %191, %186, %68, %192 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %194 = llvm.extractvalue %193[0] : !llvm.struct<(f32, i1)> 
    %195 = llvm.extractvalue %193[1] : !llvm.struct<(f32, i1)> 
    %196 = llvm.fcmp "ugt" %186, %194 : f32
    %197 = llvm.select %196, %186, %194 : i1, f32
    %198 = llvm.fcmp "uno" %194, %194 : f32
    %199 = llvm.select %198, %194, %197 : i1, f32
    %200 = llvm.mlir.constant(1 : i32) : i32
    %201 = llvm.mlir.constant(-1 : i32) : i32
    %202 = llvm.mlir.constant(32 : i32) : i32
    %203 = llvm.sub %202, %70  : i32
    %204 = llvm.lshr %201, %203  : i32
    %205 = llvm.sub %70, %200  : i32
    %206 = nvvm.shfl.sync  bfly %204, %199, %67, %205 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %207 = llvm.extractvalue %206[0] : !llvm.struct<(f32, i1)> 
    %208 = llvm.extractvalue %206[1] : !llvm.struct<(f32, i1)> 
    %209 = llvm.fcmp "ugt" %199, %207 : f32
    %210 = llvm.select %209, %199, %207 : i1, f32
    %211 = llvm.fcmp "uno" %207, %207 : f32
    %212 = llvm.select %211, %207, %210 : i1, f32
    %213 = llvm.mlir.constant(1 : i32) : i32
    %214 = llvm.mlir.constant(-1 : i32) : i32
    %215 = llvm.mlir.constant(32 : i32) : i32
    %216 = llvm.sub %215, %70  : i32
    %217 = llvm.lshr %214, %216  : i32
    %218 = llvm.sub %70, %213  : i32
    %219 = nvvm.shfl.sync  bfly %217, %212, %66, %218 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %220 = llvm.extractvalue %219[0] : !llvm.struct<(f32, i1)> 
    %221 = llvm.extractvalue %219[1] : !llvm.struct<(f32, i1)> 
    %222 = llvm.fcmp "ugt" %212, %220 : f32
    %223 = llvm.select %222, %212, %220 : i1, f32
    %224 = llvm.fcmp "uno" %220, %220 : f32
    %225 = llvm.select %224, %220, %223 : i1, f32
    %226 = llvm.icmp "eq" %88, %80 : i32
    llvm.cond_br %226, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %227 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %228 = llvm.extractvalue %41[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %229 = llvm.extractvalue %41[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %230 = llvm.insertvalue %228, %227[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %231 = llvm.insertvalue %229, %230[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %232 = llvm.mlir.constant(0 : index) : i32
    %233 = llvm.insertvalue %232, %231[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %234 = llvm.mlir.constant(8 : index) : i32
    %235 = llvm.insertvalue %234, %233[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %236 = llvm.mlir.constant(1 : index) : i32
    %237 = llvm.insertvalue %236, %235[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %238 = llvm.extractvalue %237[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %239 = llvm.getelementptr %238[%87] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %225, %239 : !llvm.ptr<f32, 3>
    llvm.br ^bb9
  ^bb9:  // 2 preds: ^bb7, ^bb8
    nvvm.barrier0
    %240 = llvm.icmp "slt" %86, %79 : i32
    llvm.cond_br %240, ^bb10, ^bb17
  ^bb10:  // pred: ^bb9
    %241 = llvm.icmp "slt" %88, %65 : i32
    llvm.cond_br %241, ^bb11, ^bb12
  ^bb11:  // pred: ^bb10
    %242 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %243 = llvm.extractvalue %41[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %244 = llvm.extractvalue %41[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %245 = llvm.insertvalue %243, %242[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %246 = llvm.insertvalue %244, %245[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %247 = llvm.mlir.constant(0 : index) : i32
    %248 = llvm.insertvalue %247, %246[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %249 = llvm.mlir.constant(8 : index) : i32
    %250 = llvm.insertvalue %249, %248[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %251 = llvm.mlir.constant(1 : index) : i32
    %252 = llvm.insertvalue %251, %250[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %253 = llvm.extractvalue %252[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %254 = llvm.getelementptr %253[%88] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %255 = llvm.load %254 : !llvm.ptr<f32, 3>
    llvm.br ^bb13(%255 : f32)
  ^bb12:  // pred: ^bb10
    llvm.br ^bb13(%72 : f32)
  ^bb13(%256: f32):  // 2 preds: ^bb11, ^bb12
    llvm.br ^bb14
  ^bb14:  // pred: ^bb13
    %257 = llvm.mlir.constant(1 : i32) : i32
    %258 = llvm.mlir.constant(-1 : i32) : i32
    %259 = llvm.mlir.constant(32 : i32) : i32
    %260 = llvm.sub %259, %67  : i32
    %261 = llvm.lshr %258, %260  : i32
    %262 = llvm.sub %67, %257  : i32
    %263 = nvvm.shfl.sync  bfly %261, %256, %71, %262 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %264 = llvm.extractvalue %263[0] : !llvm.struct<(f32, i1)> 
    %265 = llvm.extractvalue %263[1] : !llvm.struct<(f32, i1)> 
    %266 = llvm.fcmp "ugt" %256, %264 : f32
    %267 = llvm.select %266, %256, %264 : i1, f32
    %268 = llvm.fcmp "uno" %264, %264 : f32
    %269 = llvm.select %268, %264, %267 : i1, f32
    %270 = llvm.mlir.constant(1 : i32) : i32
    %271 = llvm.mlir.constant(-1 : i32) : i32
    %272 = llvm.mlir.constant(32 : i32) : i32
    %273 = llvm.sub %272, %67  : i32
    %274 = llvm.lshr %271, %273  : i32
    %275 = llvm.sub %67, %270  : i32
    %276 = nvvm.shfl.sync  bfly %274, %269, %69, %275 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %277 = llvm.extractvalue %276[0] : !llvm.struct<(f32, i1)> 
    %278 = llvm.extractvalue %276[1] : !llvm.struct<(f32, i1)> 
    %279 = llvm.fcmp "ugt" %269, %277 : f32
    %280 = llvm.select %279, %269, %277 : i1, f32
    %281 = llvm.fcmp "uno" %277, %277 : f32
    %282 = llvm.select %281, %277, %280 : i1, f32
    %283 = llvm.mlir.constant(1 : i32) : i32
    %284 = llvm.mlir.constant(-1 : i32) : i32
    %285 = llvm.mlir.constant(32 : i32) : i32
    %286 = llvm.sub %285, %67  : i32
    %287 = llvm.lshr %284, %286  : i32
    %288 = llvm.sub %67, %283  : i32
    %289 = nvvm.shfl.sync  bfly %287, %282, %68, %288 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %290 = llvm.extractvalue %289[0] : !llvm.struct<(f32, i1)> 
    %291 = llvm.extractvalue %289[1] : !llvm.struct<(f32, i1)> 
    %292 = llvm.fcmp "ugt" %282, %290 : f32
    %293 = llvm.select %292, %282, %290 : i1, f32
    %294 = llvm.fcmp "uno" %290, %290 : f32
    %295 = llvm.select %294, %290, %293 : i1, f32
    llvm.cond_br %226, ^bb15, ^bb16
  ^bb15:  // pred: ^bb14
    %296 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %297 = llvm.extractvalue %30[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %298 = llvm.extractvalue %30[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %299 = llvm.insertvalue %297, %296[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %300 = llvm.insertvalue %298, %299[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %301 = llvm.mlir.constant(0 : index) : i32
    %302 = llvm.insertvalue %301, %300[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %303 = llvm.mlir.constant(32 : index) : i32
    %304 = llvm.insertvalue %303, %302[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %305 = llvm.mlir.constant(1 : index) : i32
    %306 = llvm.insertvalue %305, %304[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %307 = llvm.extractvalue %306[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %308 = llvm.getelementptr %307[%80] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %295, %308 : !llvm.ptr<f32, 3>
    llvm.br ^bb16
  ^bb16:  // 2 preds: ^bb14, ^bb15
    llvm.br ^bb17
  ^bb17:  // 2 preds: ^bb9, ^bb16
    nvvm.barrier0
    %309 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %310 = llvm.extractvalue %30[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %311 = llvm.extractvalue %30[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %312 = llvm.insertvalue %310, %309[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %313 = llvm.insertvalue %311, %312[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %314 = llvm.mlir.constant(0 : index) : i32
    %315 = llvm.insertvalue %314, %313[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %316 = llvm.mlir.constant(32 : index) : i32
    %317 = llvm.insertvalue %316, %315[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %318 = llvm.mlir.constant(1 : index) : i32
    %319 = llvm.insertvalue %318, %317[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %320 = llvm.extractvalue %319[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %321 = llvm.getelementptr %320[%80] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %322 = llvm.load %321 : !llvm.ptr<f32, 3>
    llvm.br ^bb18(%86, %64 : i32, f32)
  ^bb18(%323: i32, %324: f32):  // 2 preds: ^bb17, ^bb19
    %325 = llvm.icmp "slt" %323, %98 : i32
    llvm.cond_br %325, ^bb19, ^bb20
  ^bb19:  // pred: ^bb18
    %326 = llvm.add %323, %77  : i32
    %327 = llvm.add %323, %75  : i32
    %328 = llvm.add %323, %74  : i32
    %329 = llvm.add %99, %323  : i32
    %330 = llvm.add %99, %326  : i32
    %331 = llvm.add %99, %327  : i32
    %332 = llvm.add %99, %328  : i32
    %333 = llvm.extractvalue %109[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %334 = llvm.getelementptr %333[%329] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %335 = llvm.load %334 : !llvm.ptr<f32>
    %336 = llvm.extractvalue %109[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %337 = llvm.getelementptr %336[%330] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %338 = llvm.load %337 : !llvm.ptr<f32>
    %339 = llvm.extractvalue %109[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %340 = llvm.getelementptr %339[%331] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %341 = llvm.load %340 : !llvm.ptr<f32>
    %342 = llvm.extractvalue %109[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %343 = llvm.getelementptr %342[%332] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %344 = llvm.load %343 : !llvm.ptr<f32>
    %345 = llvm.fsub %335, %322  : f32
    %346 = llvm.fsub %338, %322  : f32
    %347 = llvm.fsub %341, %322  : f32
    %348 = llvm.fsub %344, %322  : f32
    %349 = llvm.call @__nv_expf(%345) : (f32) -> f32
    %350 = llvm.call @__nv_expf(%346) : (f32) -> f32
    %351 = llvm.call @__nv_expf(%347) : (f32) -> f32
    %352 = llvm.call @__nv_expf(%348) : (f32) -> f32
    %353 = llvm.fadd %324, %349  : f32
    %354 = llvm.fadd %353, %350  : f32
    %355 = llvm.fadd %354, %351  : f32
    %356 = llvm.fadd %355, %352  : f32
    %357 = llvm.add %323, %73  : i32
    llvm.br ^bb18(%357, %356 : i32, f32)
  ^bb20:  // pred: ^bb18
    %358 = llvm.extractvalue %319[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %359 = llvm.getelementptr %358[%80] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %360 = llvm.load %359 : !llvm.ptr<f32, 3>
    llvm.br ^bb21(%98, %324 : i32, f32)
  ^bb21(%361: i32, %362: f32):  // 2 preds: ^bb20, ^bb22
    %363 = llvm.icmp "slt" %361, %82 : i32
    llvm.cond_br %363, ^bb22, ^bb23
  ^bb22:  // pred: ^bb21
    %364 = llvm.add %99, %361  : i32
    %365 = llvm.extractvalue %109[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %366 = llvm.getelementptr %365[%364] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %367 = llvm.load %366 : !llvm.ptr<f32>
    %368 = llvm.fsub %367, %360  : f32
    %369 = llvm.call @__nv_expf(%368) : (f32) -> f32
    %370 = llvm.fadd %362, %369  : f32
    %371 = llvm.add %361, %77  : i32
    llvm.br ^bb21(%371, %370 : i32, f32)
  ^bb23:  // pred: ^bb21
    %372 = llvm.mlir.constant(1 : i32) : i32
    %373 = llvm.mlir.constant(-1 : i32) : i32
    %374 = llvm.mlir.constant(32 : i32) : i32
    %375 = llvm.sub %374, %70  : i32
    %376 = llvm.lshr %373, %375  : i32
    %377 = llvm.sub %70, %372  : i32
    %378 = nvvm.shfl.sync  bfly %376, %362, %71, %377 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %379 = llvm.extractvalue %378[0] : !llvm.struct<(f32, i1)> 
    %380 = llvm.extractvalue %378[1] : !llvm.struct<(f32, i1)> 
    %381 = llvm.fadd %362, %379  : f32
    %382 = llvm.mlir.constant(1 : i32) : i32
    %383 = llvm.mlir.constant(-1 : i32) : i32
    %384 = llvm.mlir.constant(32 : i32) : i32
    %385 = llvm.sub %384, %70  : i32
    %386 = llvm.lshr %383, %385  : i32
    %387 = llvm.sub %70, %382  : i32
    %388 = nvvm.shfl.sync  bfly %386, %381, %69, %387 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %389 = llvm.extractvalue %388[0] : !llvm.struct<(f32, i1)> 
    %390 = llvm.extractvalue %388[1] : !llvm.struct<(f32, i1)> 
    %391 = llvm.fadd %381, %389  : f32
    %392 = llvm.mlir.constant(1 : i32) : i32
    %393 = llvm.mlir.constant(-1 : i32) : i32
    %394 = llvm.mlir.constant(32 : i32) : i32
    %395 = llvm.sub %394, %70  : i32
    %396 = llvm.lshr %393, %395  : i32
    %397 = llvm.sub %70, %392  : i32
    %398 = nvvm.shfl.sync  bfly %396, %391, %68, %397 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %399 = llvm.extractvalue %398[0] : !llvm.struct<(f32, i1)> 
    %400 = llvm.extractvalue %398[1] : !llvm.struct<(f32, i1)> 
    %401 = llvm.fadd %391, %399  : f32
    %402 = llvm.mlir.constant(1 : i32) : i32
    %403 = llvm.mlir.constant(-1 : i32) : i32
    %404 = llvm.mlir.constant(32 : i32) : i32
    %405 = llvm.sub %404, %70  : i32
    %406 = llvm.lshr %403, %405  : i32
    %407 = llvm.sub %70, %402  : i32
    %408 = nvvm.shfl.sync  bfly %406, %401, %67, %407 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %409 = llvm.extractvalue %408[0] : !llvm.struct<(f32, i1)> 
    %410 = llvm.extractvalue %408[1] : !llvm.struct<(f32, i1)> 
    %411 = llvm.fadd %401, %409  : f32
    %412 = llvm.mlir.constant(1 : i32) : i32
    %413 = llvm.mlir.constant(-1 : i32) : i32
    %414 = llvm.mlir.constant(32 : i32) : i32
    %415 = llvm.sub %414, %70  : i32
    %416 = llvm.lshr %413, %415  : i32
    %417 = llvm.sub %70, %412  : i32
    %418 = nvvm.shfl.sync  bfly %416, %411, %66, %417 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %419 = llvm.extractvalue %418[0] : !llvm.struct<(f32, i1)> 
    %420 = llvm.extractvalue %418[1] : !llvm.struct<(f32, i1)> 
    %421 = llvm.fadd %411, %419  : f32
    llvm.cond_br %226, ^bb24, ^bb25
  ^bb24:  // pred: ^bb23
    %422 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %423 = llvm.extractvalue %63[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %424 = llvm.extractvalue %63[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %425 = llvm.insertvalue %423, %422[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %426 = llvm.insertvalue %424, %425[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %427 = llvm.mlir.constant(0 : index) : i32
    %428 = llvm.insertvalue %427, %426[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %429 = llvm.mlir.constant(8 : index) : i32
    %430 = llvm.insertvalue %429, %428[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %431 = llvm.mlir.constant(1 : index) : i32
    %432 = llvm.insertvalue %431, %430[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %433 = llvm.extractvalue %432[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %434 = llvm.getelementptr %433[%87] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %421, %434 : !llvm.ptr<f32, 3>
    llvm.br ^bb25
  ^bb25:  // 2 preds: ^bb23, ^bb24
    nvvm.barrier0
    llvm.cond_br %240, ^bb26, ^bb33
  ^bb26:  // pred: ^bb25
    %435 = llvm.icmp "slt" %88, %65 : i32
    llvm.cond_br %435, ^bb27, ^bb28
  ^bb27:  // pred: ^bb26
    %436 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %437 = llvm.extractvalue %63[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %438 = llvm.extractvalue %63[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %439 = llvm.insertvalue %437, %436[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %440 = llvm.insertvalue %438, %439[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %441 = llvm.mlir.constant(0 : index) : i32
    %442 = llvm.insertvalue %441, %440[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %443 = llvm.mlir.constant(8 : index) : i32
    %444 = llvm.insertvalue %443, %442[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %445 = llvm.mlir.constant(1 : index) : i32
    %446 = llvm.insertvalue %445, %444[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %447 = llvm.extractvalue %446[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %448 = llvm.getelementptr %447[%88] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %449 = llvm.load %448 : !llvm.ptr<f32, 3>
    llvm.br ^bb29(%449 : f32)
  ^bb28:  // pred: ^bb26
    llvm.br ^bb29(%64 : f32)
  ^bb29(%450: f32):  // 2 preds: ^bb27, ^bb28
    llvm.br ^bb30
  ^bb30:  // pred: ^bb29
    %451 = llvm.mlir.constant(1 : i32) : i32
    %452 = llvm.mlir.constant(-1 : i32) : i32
    %453 = llvm.mlir.constant(32 : i32) : i32
    %454 = llvm.sub %453, %67  : i32
    %455 = llvm.lshr %452, %454  : i32
    %456 = llvm.sub %67, %451  : i32
    %457 = nvvm.shfl.sync  bfly %455, %450, %71, %456 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %458 = llvm.extractvalue %457[0] : !llvm.struct<(f32, i1)> 
    %459 = llvm.extractvalue %457[1] : !llvm.struct<(f32, i1)> 
    %460 = llvm.fadd %450, %458  : f32
    %461 = llvm.mlir.constant(1 : i32) : i32
    %462 = llvm.mlir.constant(-1 : i32) : i32
    %463 = llvm.mlir.constant(32 : i32) : i32
    %464 = llvm.sub %463, %67  : i32
    %465 = llvm.lshr %462, %464  : i32
    %466 = llvm.sub %67, %461  : i32
    %467 = nvvm.shfl.sync  bfly %465, %460, %69, %466 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %468 = llvm.extractvalue %467[0] : !llvm.struct<(f32, i1)> 
    %469 = llvm.extractvalue %467[1] : !llvm.struct<(f32, i1)> 
    %470 = llvm.fadd %460, %468  : f32
    %471 = llvm.mlir.constant(1 : i32) : i32
    %472 = llvm.mlir.constant(-1 : i32) : i32
    %473 = llvm.mlir.constant(32 : i32) : i32
    %474 = llvm.sub %473, %67  : i32
    %475 = llvm.lshr %472, %474  : i32
    %476 = llvm.sub %67, %471  : i32
    %477 = nvvm.shfl.sync  bfly %475, %470, %68, %476 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %478 = llvm.extractvalue %477[0] : !llvm.struct<(f32, i1)> 
    %479 = llvm.extractvalue %477[1] : !llvm.struct<(f32, i1)> 
    %480 = llvm.fadd %470, %478  : f32
    llvm.cond_br %226, ^bb31, ^bb32
  ^bb31:  // pred: ^bb30
    %481 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %482 = llvm.extractvalue %52[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %483 = llvm.extractvalue %52[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %484 = llvm.insertvalue %482, %481[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %485 = llvm.insertvalue %483, %484[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %486 = llvm.mlir.constant(0 : index) : i32
    %487 = llvm.insertvalue %486, %485[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %488 = llvm.mlir.constant(32 : index) : i32
    %489 = llvm.insertvalue %488, %487[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %490 = llvm.mlir.constant(1 : index) : i32
    %491 = llvm.insertvalue %490, %489[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %492 = llvm.extractvalue %491[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %493 = llvm.getelementptr %492[%80] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %480, %493 : !llvm.ptr<f32, 3>
    llvm.br ^bb32
  ^bb32:  // 2 preds: ^bb30, ^bb31
    llvm.br ^bb33
  ^bb33:  // 2 preds: ^bb25, ^bb32
    nvvm.barrier0
    %494 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %495 = llvm.extractvalue %52[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %496 = llvm.extractvalue %52[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %497 = llvm.insertvalue %495, %494[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %498 = llvm.insertvalue %496, %497[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %499 = llvm.mlir.constant(0 : index) : i32
    %500 = llvm.insertvalue %499, %498[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %501 = llvm.mlir.constant(32 : index) : i32
    %502 = llvm.insertvalue %501, %500[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %503 = llvm.mlir.constant(1 : index) : i32
    %504 = llvm.insertvalue %503, %502[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %505 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %506 = llvm.extractvalue %19[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %507 = llvm.extractvalue %19[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %508 = llvm.insertvalue %506, %505[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %509 = llvm.insertvalue %507, %508[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %510 = llvm.insertvalue %80, %509[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %511 = llvm.insertvalue %101, %510[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %512 = llvm.insertvalue %81, %511[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %513 = llvm.extractvalue %319[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %514 = llvm.getelementptr %513[%80] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %515 = llvm.load %514 : !llvm.ptr<f32, 3>
    %516 = llvm.extractvalue %504[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %517 = llvm.getelementptr %516[%80] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %518 = llvm.load %517 : !llvm.ptr<f32, 3>
    llvm.br ^bb34(%86 : i32)
  ^bb34(%519: i32):  // 2 preds: ^bb33, ^bb35
    %520 = llvm.icmp "slt" %519, %98 : i32
    llvm.cond_br %520, ^bb35, ^bb36
  ^bb35:  // pred: ^bb34
    %521 = llvm.add %519, %77  : i32
    %522 = llvm.add %519, %75  : i32
    %523 = llvm.add %519, %74  : i32
    %524 = llvm.add %99, %519  : i32
    %525 = llvm.add %99, %521  : i32
    %526 = llvm.add %99, %522  : i32
    %527 = llvm.add %99, %523  : i32
    %528 = llvm.extractvalue %109[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %529 = llvm.getelementptr %528[%524] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %530 = llvm.load %529 : !llvm.ptr<f32>
    %531 = llvm.extractvalue %109[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %532 = llvm.getelementptr %531[%525] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %533 = llvm.load %532 : !llvm.ptr<f32>
    %534 = llvm.extractvalue %109[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %535 = llvm.getelementptr %534[%526] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %536 = llvm.load %535 : !llvm.ptr<f32>
    %537 = llvm.extractvalue %109[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %538 = llvm.getelementptr %537[%527] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %539 = llvm.load %538 : !llvm.ptr<f32>
    %540 = llvm.fsub %530, %515  : f32
    %541 = llvm.fsub %533, %515  : f32
    %542 = llvm.fsub %536, %515  : f32
    %543 = llvm.fsub %539, %515  : f32
    %544 = llvm.call @__nv_expf(%540) : (f32) -> f32
    %545 = llvm.call @__nv_expf(%541) : (f32) -> f32
    %546 = llvm.call @__nv_expf(%542) : (f32) -> f32
    %547 = llvm.call @__nv_expf(%543) : (f32) -> f32
    %548 = llvm.fdiv %544, %518  : f32
    %549 = llvm.fdiv %545, %518  : f32
    %550 = llvm.fdiv %546, %518  : f32
    %551 = llvm.fdiv %547, %518  : f32
    %552 = llvm.extractvalue %512[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %553 = llvm.getelementptr %552[%524] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %548, %553 : !llvm.ptr<f32>
    %554 = llvm.extractvalue %512[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %555 = llvm.getelementptr %554[%525] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %549, %555 : !llvm.ptr<f32>
    %556 = llvm.extractvalue %512[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %557 = llvm.getelementptr %556[%526] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %550, %557 : !llvm.ptr<f32>
    %558 = llvm.extractvalue %512[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %559 = llvm.getelementptr %558[%527] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %551, %559 : !llvm.ptr<f32>
    %560 = llvm.add %519, %73  : i32
    llvm.br ^bb34(%560 : i32)
  ^bb36:  // pred: ^bb34
    %561 = llvm.extractvalue %319[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %562 = llvm.getelementptr %561[%80] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %563 = llvm.load %562 : !llvm.ptr<f32, 3>
    %564 = llvm.extractvalue %504[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %565 = llvm.getelementptr %564[%80] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %566 = llvm.load %565 : !llvm.ptr<f32, 3>
    llvm.br ^bb37(%98 : i32)
  ^bb37(%567: i32):  // 2 preds: ^bb36, ^bb38
    %568 = llvm.icmp "slt" %567, %82 : i32
    llvm.cond_br %568, ^bb38, ^bb39
  ^bb38:  // pred: ^bb37
    %569 = llvm.add %99, %567  : i32
    %570 = llvm.extractvalue %109[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %571 = llvm.getelementptr %570[%569] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %572 = llvm.load %571 : !llvm.ptr<f32>
    %573 = llvm.fsub %572, %563  : f32
    %574 = llvm.call @__nv_expf(%573) : (f32) -> f32
    %575 = llvm.fdiv %574, %566  : f32
    %576 = llvm.extractvalue %512[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %577 = llvm.getelementptr %576[%569] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %575, %577 : !llvm.ptr<f32>
    %578 = llvm.add %567, %77  : i32
    llvm.br ^bb37(%578 : i32)
  ^bb39:  // pred: ^bb37
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kStitch_divide__13_1_0___1b1r(%arg0: !llvm.ptr<f32>, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: !llvm.ptr<f32>, %arg10: !llvm.ptr<f32>, %arg11: i32, %arg12: i32, %arg13: i32, %arg14: i32, %arg15: i32, %arg16: i32, %arg17: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(-1 : i32) : i32
  %1 = llvm.mlir.constant(256 : index) : i32
  %2 = llvm.mlir.constant(4 : index) : i32
  %3 = llvm.mlir.constant(512 : index) : i32
  %4 = llvm.mlir.constant(768 : index) : i32
  %5 = llvm.mlir.constant(1024 : index) : i32
  %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
  %7 = llvm.mlir.constant(1 : i32) : i32
  %8 = llvm.mlir.constant(32 : i32) : i32
  %9 = llvm.mlir.constant(2 : i32) : i32
  %10 = llvm.mlir.constant(4 : i32) : i32
  %11 = llvm.mlir.constant(8 : i32) : i32
  %12 = llvm.mlir.constant(16 : i32) : i32
  %13 = llvm.mlir.constant(-0.000000e+00 : f32) : f32
  %14 = llvm.mlir.constant(8 : index) : i32
  %15 = llvm.mlir.constant(1 : index) : i32
  %16 = llvm.mlir.constant(32 : index) : i32
  %17 = llvm.mlir.constant(0 : index) : i32
  %18 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_0 : !llvm.ptr<array<32 x f32>, 3>
  %19 = llvm.getelementptr %18[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
  %20 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_1 : !llvm.ptr<array<8 x f32>, 3>
  %21 = llvm.getelementptr %20[0, 0] : (!llvm.ptr<array<8 x f32>, 3>) -> !llvm.ptr<f32, 3>
  %22 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_2 : !llvm.ptr<array<32 x f32>, 3>
  %23 = llvm.getelementptr %22[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
  %24 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_3 : !llvm.ptr<array<8 x f32>, 3>
  %25 = llvm.getelementptr %24[0, 0] : (!llvm.ptr<array<8 x f32>, 3>) -> !llvm.ptr<f32, 3>
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %26 = nvvm.read.ptx.sreg.ctaid.x : i32
  %27 = nvvm.read.ptx.sreg.tid.x : i32
  %28 = llvm.udiv %27, %16  : i32
  %29 = llvm.urem %27, %16  : i32
  %30 = llvm.sub %arg5, %27  : i32
  %31 = llvm.icmp "eq" %30, %17 : i32
  %32 = llvm.sub %30, %15  : i32
  %33 = llvm.udiv %32, %1  : i32
  %34 = llvm.add %33, %15  : i32
  %35 = llvm.select %31, %17, %34 : i1, i32
  %36 = llvm.srem %35, %2  : i32
  %37 = llvm.sub %35, %36  : i32
  %38 = llvm.mul %37, %1  : i32
  %39 = llvm.add %27, %38  : i32
  %40 = llvm.mul %26, %arg5  : i32
  llvm.br ^bb2(%27, %6 : i32, f32)
^bb2(%41: i32, %42: f32):  // 2 preds: ^bb1, ^bb3
  %43 = llvm.icmp "slt" %41, %39 : i32
  llvm.cond_br %43, ^bb3, ^bb4
^bb3:  // pred: ^bb2
  %44 = llvm.add %41, %1  : i32
  %45 = llvm.add %41, %3  : i32
  %46 = llvm.add %41, %4  : i32
  %47 = llvm.add %40, %41  : i32
  %48 = llvm.add %40, %44  : i32
  %49 = llvm.add %40, %45  : i32
  %50 = llvm.add %40, %46  : i32
  %51 = llvm.getelementptr %arg1[%47] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %52 = llvm.load %51 : !llvm.ptr<f32>
  %53 = llvm.getelementptr %arg1[%48] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %54 = llvm.load %53 : !llvm.ptr<f32>
  %55 = llvm.getelementptr %arg1[%49] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %56 = llvm.load %55 : !llvm.ptr<f32>
  %57 = llvm.getelementptr %arg1[%50] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %58 = llvm.load %57 : !llvm.ptr<f32>
  %59 = llvm.fcmp "ugt" %42, %52 : f32
  %60 = llvm.select %59, %42, %52 : i1, f32
  %61 = llvm.fcmp "uno" %52, %52 : f32
  %62 = llvm.select %61, %52, %60 : i1, f32
  %63 = llvm.fcmp "ugt" %62, %54 : f32
  %64 = llvm.select %63, %62, %54 : i1, f32
  %65 = llvm.fcmp "uno" %54, %54 : f32
  %66 = llvm.select %65, %54, %64 : i1, f32
  %67 = llvm.fcmp "ugt" %66, %56 : f32
  %68 = llvm.select %67, %66, %56 : i1, f32
  %69 = llvm.fcmp "uno" %56, %56 : f32
  %70 = llvm.select %69, %56, %68 : i1, f32
  %71 = llvm.fcmp "ugt" %70, %58 : f32
  %72 = llvm.select %71, %70, %58 : i1, f32
  %73 = llvm.fcmp "uno" %58, %58 : f32
  %74 = llvm.select %73, %58, %72 : i1, f32
  %75 = llvm.add %41, %5  : i32
  llvm.br ^bb2(%75, %74 : i32, f32)
^bb4:  // pred: ^bb2
  llvm.br ^bb5(%39, %42 : i32, f32)
^bb5(%76: i32, %77: f32):  // 2 preds: ^bb4, ^bb6
  %78 = llvm.icmp "slt" %76, %arg5 : i32
  llvm.cond_br %78, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %79 = llvm.add %40, %76  : i32
  %80 = llvm.getelementptr %arg1[%79] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %81 = llvm.load %80 : !llvm.ptr<f32>
  %82 = llvm.fcmp "ugt" %77, %81 : f32
  %83 = llvm.select %82, %77, %81 : i1, f32
  %84 = llvm.fcmp "uno" %81, %81 : f32
  %85 = llvm.select %84, %81, %83 : i1, f32
  %86 = llvm.add %76, %1  : i32
  llvm.br ^bb5(%86, %85 : i32, f32)
^bb7:  // pred: ^bb5
  %87 = llvm.sub %8, %8  : i32
  %88 = llvm.lshr %0, %87  : i32
  %89 = llvm.sub %8, %7  : i32
  %90 = nvvm.shfl.sync  bfly %88, %77, %7, %89 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %91 = llvm.extractvalue %90[0] : !llvm.struct<(f32, i1)> 
  %92 = llvm.fcmp "ugt" %77, %91 : f32
  %93 = llvm.select %92, %77, %91 : i1, f32
  %94 = llvm.fcmp "uno" %91, %91 : f32
  %95 = llvm.select %94, %91, %93 : i1, f32
  %96 = llvm.sub %8, %8  : i32
  %97 = llvm.lshr %0, %96  : i32
  %98 = llvm.sub %8, %7  : i32
  %99 = nvvm.shfl.sync  bfly %97, %95, %9, %98 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %100 = llvm.extractvalue %99[0] : !llvm.struct<(f32, i1)> 
  %101 = llvm.fcmp "ugt" %95, %100 : f32
  %102 = llvm.select %101, %95, %100 : i1, f32
  %103 = llvm.fcmp "uno" %100, %100 : f32
  %104 = llvm.select %103, %100, %102 : i1, f32
  %105 = llvm.sub %8, %8  : i32
  %106 = llvm.lshr %0, %105  : i32
  %107 = llvm.sub %8, %7  : i32
  %108 = nvvm.shfl.sync  bfly %106, %104, %10, %107 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %109 = llvm.extractvalue %108[0] : !llvm.struct<(f32, i1)> 
  %110 = llvm.fcmp "ugt" %104, %109 : f32
  %111 = llvm.select %110, %104, %109 : i1, f32
  %112 = llvm.fcmp "uno" %109, %109 : f32
  %113 = llvm.select %112, %109, %111 : i1, f32
  %114 = llvm.sub %8, %8  : i32
  %115 = llvm.lshr %0, %114  : i32
  %116 = llvm.sub %8, %7  : i32
  %117 = nvvm.shfl.sync  bfly %115, %113, %11, %116 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %118 = llvm.extractvalue %117[0] : !llvm.struct<(f32, i1)> 
  %119 = llvm.fcmp "ugt" %113, %118 : f32
  %120 = llvm.select %119, %113, %118 : i1, f32
  %121 = llvm.fcmp "uno" %118, %118 : f32
  %122 = llvm.select %121, %118, %120 : i1, f32
  %123 = llvm.sub %8, %8  : i32
  %124 = llvm.lshr %0, %123  : i32
  %125 = llvm.sub %8, %7  : i32
  %126 = nvvm.shfl.sync  bfly %124, %122, %12, %125 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %127 = llvm.extractvalue %126[0] : !llvm.struct<(f32, i1)> 
  %128 = llvm.fcmp "ugt" %122, %127 : f32
  %129 = llvm.select %128, %122, %127 : i1, f32
  %130 = llvm.fcmp "uno" %127, %127 : f32
  %131 = llvm.select %130, %127, %129 : i1, f32
  %132 = llvm.icmp "eq" %29, %17 : i32
  llvm.cond_br %132, ^bb8, ^bb9
^bb8:  // pred: ^bb7
  %133 = llvm.getelementptr %21[%28] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %131, %133 : !llvm.ptr<f32, 3>
  llvm.br ^bb9
^bb9:  // 2 preds: ^bb7, ^bb8
  nvvm.barrier0
  %134 = llvm.icmp "slt" %27, %16 : i32
  llvm.cond_br %134, ^bb10, ^bb17
^bb10:  // pred: ^bb9
  %135 = llvm.icmp "slt" %29, %14 : i32
  llvm.cond_br %135, ^bb11, ^bb12
^bb11:  // pred: ^bb10
  %136 = llvm.getelementptr %21[%29] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %137 = llvm.load %136 : !llvm.ptr<f32, 3>
  llvm.br ^bb13(%137 : f32)
^bb12:  // pred: ^bb10
  llvm.br ^bb13(%6 : f32)
^bb13(%138: f32):  // 2 preds: ^bb11, ^bb12
  llvm.br ^bb14
^bb14:  // pred: ^bb13
  %139 = llvm.sub %8, %11  : i32
  %140 = llvm.lshr %0, %139  : i32
  %141 = llvm.sub %11, %7  : i32
  %142 = nvvm.shfl.sync  bfly %140, %138, %7, %141 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %143 = llvm.extractvalue %142[0] : !llvm.struct<(f32, i1)> 
  %144 = llvm.fcmp "ugt" %138, %143 : f32
  %145 = llvm.select %144, %138, %143 : i1, f32
  %146 = llvm.fcmp "uno" %143, %143 : f32
  %147 = llvm.select %146, %143, %145 : i1, f32
  %148 = llvm.sub %8, %11  : i32
  %149 = llvm.lshr %0, %148  : i32
  %150 = llvm.sub %11, %7  : i32
  %151 = nvvm.shfl.sync  bfly %149, %147, %9, %150 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %152 = llvm.extractvalue %151[0] : !llvm.struct<(f32, i1)> 
  %153 = llvm.fcmp "ugt" %147, %152 : f32
  %154 = llvm.select %153, %147, %152 : i1, f32
  %155 = llvm.fcmp "uno" %152, %152 : f32
  %156 = llvm.select %155, %152, %154 : i1, f32
  %157 = llvm.sub %8, %11  : i32
  %158 = llvm.lshr %0, %157  : i32
  %159 = llvm.sub %11, %7  : i32
  %160 = nvvm.shfl.sync  bfly %158, %156, %10, %159 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %161 = llvm.extractvalue %160[0] : !llvm.struct<(f32, i1)> 
  %162 = llvm.fcmp "ugt" %156, %161 : f32
  %163 = llvm.select %162, %156, %161 : i1, f32
  %164 = llvm.fcmp "uno" %161, %161 : f32
  %165 = llvm.select %164, %161, %163 : i1, f32
  llvm.cond_br %132, ^bb15, ^bb16
^bb15:  // pred: ^bb14
  llvm.store %165, %19 : !llvm.ptr<f32, 3>
  llvm.br ^bb16
^bb16:  // 2 preds: ^bb14, ^bb15
  llvm.br ^bb17
^bb17:  // 2 preds: ^bb9, ^bb16
  nvvm.barrier0
  %166 = llvm.load %19 : !llvm.ptr<f32, 3>
  llvm.br ^bb18(%27, %13 : i32, f32)
^bb18(%167: i32, %168: f32):  // 2 preds: ^bb17, ^bb19
  %169 = llvm.icmp "slt" %167, %39 : i32
  llvm.cond_br %169, ^bb19, ^bb20
^bb19:  // pred: ^bb18
  %170 = llvm.add %167, %1  : i32
  %171 = llvm.add %167, %3  : i32
  %172 = llvm.add %167, %4  : i32
  %173 = llvm.add %40, %167  : i32
  %174 = llvm.add %40, %170  : i32
  %175 = llvm.add %40, %171  : i32
  %176 = llvm.add %40, %172  : i32
  %177 = llvm.getelementptr %arg1[%173] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %178 = llvm.load %177 : !llvm.ptr<f32>
  %179 = llvm.getelementptr %arg1[%174] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %180 = llvm.load %179 : !llvm.ptr<f32>
  %181 = llvm.getelementptr %arg1[%175] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %182 = llvm.load %181 : !llvm.ptr<f32>
  %183 = llvm.getelementptr %arg1[%176] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %184 = llvm.load %183 : !llvm.ptr<f32>
  %185 = llvm.fsub %178, %166  : f32
  %186 = llvm.fsub %180, %166  : f32
  %187 = llvm.fsub %182, %166  : f32
  %188 = llvm.fsub %184, %166  : f32
  %189 = llvm.call @__nv_expf(%185) : (f32) -> f32
  %190 = llvm.call @__nv_expf(%186) : (f32) -> f32
  %191 = llvm.call @__nv_expf(%187) : (f32) -> f32
  %192 = llvm.call @__nv_expf(%188) : (f32) -> f32
  %193 = llvm.fadd %168, %189  : f32
  %194 = llvm.fadd %193, %190  : f32
  %195 = llvm.fadd %194, %191  : f32
  %196 = llvm.fadd %195, %192  : f32
  %197 = llvm.add %167, %5  : i32
  llvm.br ^bb18(%197, %196 : i32, f32)
^bb20:  // pred: ^bb18
  %198 = llvm.load %19 : !llvm.ptr<f32, 3>
  llvm.br ^bb21(%39, %168 : i32, f32)
^bb21(%199: i32, %200: f32):  // 2 preds: ^bb20, ^bb22
  %201 = llvm.icmp "slt" %199, %arg5 : i32
  llvm.cond_br %201, ^bb22, ^bb23
^bb22:  // pred: ^bb21
  %202 = llvm.add %40, %199  : i32
  %203 = llvm.getelementptr %arg1[%202] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %204 = llvm.load %203 : !llvm.ptr<f32>
  %205 = llvm.fsub %204, %198  : f32
  %206 = llvm.call @__nv_expf(%205) : (f32) -> f32
  %207 = llvm.fadd %200, %206  : f32
  %208 = llvm.add %199, %1  : i32
  llvm.br ^bb21(%208, %207 : i32, f32)
^bb23:  // pred: ^bb21
  %209 = llvm.sub %8, %8  : i32
  %210 = llvm.lshr %0, %209  : i32
  %211 = llvm.sub %8, %7  : i32
  %212 = nvvm.shfl.sync  bfly %210, %200, %7, %211 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %213 = llvm.extractvalue %212[0] : !llvm.struct<(f32, i1)> 
  %214 = llvm.fadd %200, %213  : f32
  %215 = llvm.sub %8, %8  : i32
  %216 = llvm.lshr %0, %215  : i32
  %217 = llvm.sub %8, %7  : i32
  %218 = nvvm.shfl.sync  bfly %216, %214, %9, %217 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %219 = llvm.extractvalue %218[0] : !llvm.struct<(f32, i1)> 
  %220 = llvm.fadd %214, %219  : f32
  %221 = llvm.sub %8, %8  : i32
  %222 = llvm.lshr %0, %221  : i32
  %223 = llvm.sub %8, %7  : i32
  %224 = nvvm.shfl.sync  bfly %222, %220, %10, %223 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %225 = llvm.extractvalue %224[0] : !llvm.struct<(f32, i1)> 
  %226 = llvm.fadd %220, %225  : f32
  %227 = llvm.sub %8, %8  : i32
  %228 = llvm.lshr %0, %227  : i32
  %229 = llvm.sub %8, %7  : i32
  %230 = nvvm.shfl.sync  bfly %228, %226, %11, %229 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %231 = llvm.extractvalue %230[0] : !llvm.struct<(f32, i1)> 
  %232 = llvm.fadd %226, %231  : f32
  %233 = llvm.sub %8, %8  : i32
  %234 = llvm.lshr %0, %233  : i32
  %235 = llvm.sub %8, %7  : i32
  %236 = nvvm.shfl.sync  bfly %234, %232, %12, %235 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %237 = llvm.extractvalue %236[0] : !llvm.struct<(f32, i1)> 
  %238 = llvm.fadd %232, %237  : f32
  llvm.cond_br %132, ^bb24, ^bb25
^bb24:  // pred: ^bb23
  %239 = llvm.getelementptr %25[%28] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %238, %239 : !llvm.ptr<f32, 3>
  llvm.br ^bb25
^bb25:  // 2 preds: ^bb23, ^bb24
  nvvm.barrier0
  llvm.cond_br %134, ^bb26, ^bb33
^bb26:  // pred: ^bb25
  %240 = llvm.icmp "slt" %29, %14 : i32
  llvm.cond_br %240, ^bb27, ^bb28
^bb27:  // pred: ^bb26
  %241 = llvm.getelementptr %25[%29] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %242 = llvm.load %241 : !llvm.ptr<f32, 3>
  llvm.br ^bb29(%242 : f32)
^bb28:  // pred: ^bb26
  llvm.br ^bb29(%13 : f32)
^bb29(%243: f32):  // 2 preds: ^bb27, ^bb28
  llvm.br ^bb30
^bb30:  // pred: ^bb29
  %244 = llvm.sub %8, %11  : i32
  %245 = llvm.lshr %0, %244  : i32
  %246 = llvm.sub %11, %7  : i32
  %247 = nvvm.shfl.sync  bfly %245, %243, %7, %246 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %248 = llvm.extractvalue %247[0] : !llvm.struct<(f32, i1)> 
  %249 = llvm.fadd %243, %248  : f32
  %250 = llvm.sub %8, %11  : i32
  %251 = llvm.lshr %0, %250  : i32
  %252 = llvm.sub %11, %7  : i32
  %253 = nvvm.shfl.sync  bfly %251, %249, %9, %252 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %254 = llvm.extractvalue %253[0] : !llvm.struct<(f32, i1)> 
  %255 = llvm.fadd %249, %254  : f32
  %256 = llvm.sub %8, %11  : i32
  %257 = llvm.lshr %0, %256  : i32
  %258 = llvm.sub %11, %7  : i32
  %259 = nvvm.shfl.sync  bfly %257, %255, %10, %258 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %260 = llvm.extractvalue %259[0] : !llvm.struct<(f32, i1)> 
  %261 = llvm.fadd %255, %260  : f32
  llvm.cond_br %132, ^bb31, ^bb32
^bb31:  // pred: ^bb30
  llvm.store %261, %23 : !llvm.ptr<f32, 3>
  llvm.br ^bb32
^bb32:  // 2 preds: ^bb30, ^bb31
  llvm.br ^bb33
^bb33:  // 2 preds: ^bb25, ^bb32
  nvvm.barrier0
  %262 = llvm.load %19 : !llvm.ptr<f32, 3>
  %263 = llvm.load %23 : !llvm.ptr<f32, 3>
  llvm.br ^bb34(%27 : i32)
^bb34(%264: i32):  // 2 preds: ^bb33, ^bb35
  %265 = llvm.icmp "slt" %264, %39 : i32
  llvm.cond_br %265, ^bb35, ^bb36
^bb35:  // pred: ^bb34
  %266 = llvm.add %264, %1  : i32
  %267 = llvm.add %264, %3  : i32
  %268 = llvm.add %264, %4  : i32
  %269 = llvm.add %40, %264  : i32
  %270 = llvm.add %40, %266  : i32
  %271 = llvm.add %40, %267  : i32
  %272 = llvm.add %40, %268  : i32
  %273 = llvm.getelementptr %arg1[%269] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %274 = llvm.load %273 : !llvm.ptr<f32>
  %275 = llvm.getelementptr %arg1[%270] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %276 = llvm.load %275 : !llvm.ptr<f32>
  %277 = llvm.getelementptr %arg1[%271] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %278 = llvm.load %277 : !llvm.ptr<f32>
  %279 = llvm.getelementptr %arg1[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %280 = llvm.load %279 : !llvm.ptr<f32>
  %281 = llvm.fsub %274, %262  : f32
  %282 = llvm.fsub %276, %262  : f32
  %283 = llvm.fsub %278, %262  : f32
  %284 = llvm.fsub %280, %262  : f32
  %285 = llvm.call @__nv_expf(%281) : (f32) -> f32
  %286 = llvm.call @__nv_expf(%282) : (f32) -> f32
  %287 = llvm.call @__nv_expf(%283) : (f32) -> f32
  %288 = llvm.call @__nv_expf(%284) : (f32) -> f32
  %289 = llvm.fdiv %285, %263  : f32
  %290 = llvm.fdiv %286, %263  : f32
  %291 = llvm.fdiv %287, %263  : f32
  %292 = llvm.fdiv %288, %263  : f32
  %293 = llvm.getelementptr %arg10[%269] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  llvm.store %289, %293 : !llvm.ptr<f32>
  %294 = llvm.getelementptr %arg10[%270] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  llvm.store %290, %294 : !llvm.ptr<f32>
  %295 = llvm.getelementptr %arg10[%271] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  llvm.store %291, %295 : !llvm.ptr<f32>
  %296 = llvm.getelementptr %arg10[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  llvm.store %292, %296 : !llvm.ptr<f32>
  %297 = llvm.add %264, %5  : i32
  llvm.br ^bb34(%297 : i32)
^bb36:  // pred: ^bb34
  %298 = llvm.load %19 : !llvm.ptr<f32, 3>
  %299 = llvm.load %23 : !llvm.ptr<f32, 3>
  llvm.br ^bb37(%39 : i32)
^bb37(%300: i32):  // 2 preds: ^bb36, ^bb38
  %301 = llvm.icmp "slt" %300, %arg5 : i32
  llvm.cond_br %301, ^bb38, ^bb39
^bb38:  // pred: ^bb37
  %302 = llvm.add %40, %300  : i32
  %303 = llvm.getelementptr %arg1[%302] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %304 = llvm.load %303 : !llvm.ptr<f32>
  %305 = llvm.fsub %304, %298  : f32
  %306 = llvm.call @__nv_expf(%305) : (f32) -> f32
  %307 = llvm.fdiv %306, %299  : f32
  %308 = llvm.getelementptr %arg10[%302] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  llvm.store %307, %308 : !llvm.ptr<f32>
  %309 = llvm.add %300, %1  : i32
  llvm.br ^bb37(%309 : i32)
^bb39:  // pred: ^bb37
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel {
  llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_0() {addr_space = 3 : i32} : !llvm.array<32 x f32>
  llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_1() {addr_space = 3 : i32} : !llvm.array<8 x f32>
  llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_2() {addr_space = 3 : i32} : !llvm.array<32 x f32>
  llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_3() {addr_space = 3 : i32} : !llvm.array<8 x f32>
  llvm.func @__nv_expf(f32) -> f32
  llvm.func @main_kStitch_divide__13_1_0___1b1r(%arg0: !llvm.ptr<f32>, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [0 : index, 2 : index, 3 : index, 4 : index, 6 : index, 7 : index, 8 : index, 9 : index, 11 : index, 12 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(-1 : i32) : i32
    %1 = llvm.mlir.constant(256 : index) : i32
    %2 = llvm.mlir.constant(4 : index) : i32
    %3 = llvm.mlir.constant(512 : index) : i32
    %4 = llvm.mlir.constant(768 : index) : i32
    %5 = llvm.mlir.constant(1024 : index) : i32
    %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %7 = llvm.mlir.constant(1 : i32) : i32
    %8 = llvm.mlir.constant(32 : i32) : i32
    %9 = llvm.mlir.constant(2 : i32) : i32
    %10 = llvm.mlir.constant(4 : i32) : i32
    %11 = llvm.mlir.constant(8 : i32) : i32
    %12 = llvm.mlir.constant(16 : i32) : i32
    %13 = llvm.mlir.constant(-0.000000e+00 : f32) : f32
    %14 = llvm.mlir.constant(8 : index) : i32
    %15 = llvm.mlir.constant(1 : index) : i32
    %16 = llvm.mlir.constant(32 : index) : i32
    %17 = llvm.mlir.constant(0 : index) : i32
    %18 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_0 : !llvm.ptr<array<32 x f32>, 3>
    %19 = llvm.getelementptr %18[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %20 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_1 : !llvm.ptr<array<8 x f32>, 3>
    %21 = llvm.getelementptr %20[0, 0] : (!llvm.ptr<array<8 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %22 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_2 : !llvm.ptr<array<32 x f32>, 3>
    %23 = llvm.getelementptr %22[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %24 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_3 : !llvm.ptr<array<8 x f32>, 3>
    %25 = llvm.getelementptr %24[0, 0] : (!llvm.ptr<array<8 x f32>, 3>) -> !llvm.ptr<f32, 3>
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %26 = nvvm.read.ptx.sreg.ctaid.x : i32
    %27 = nvvm.read.ptx.sreg.tid.x : i32
    %28 = llvm.udiv %27, %16  : i32
    %29 = llvm.urem %27, %16  : i32
    %30 = llvm.sub %arg1, %27  : i32
    %31 = llvm.icmp "eq" %30, %17 : i32
    %32 = llvm.sub %30, %15  : i32
    %33 = llvm.udiv %32, %1  : i32
    %34 = llvm.add %33, %15  : i32
    %35 = llvm.select %31, %17, %34 : i1, i32
    %36 = llvm.srem %35, %2  : i32
    %37 = llvm.sub %35, %36  : i32
    %38 = llvm.mul %37, %1  : i32
    %39 = llvm.add %27, %38  : i32
    %40 = llvm.mul %26, %arg1  : i32
    llvm.br ^bb2(%27, %6 : i32, f32)
  ^bb2(%41: i32, %42: f32):  // 2 preds: ^bb1, ^bb3
    %43 = llvm.icmp "slt" %41, %39 : i32
    llvm.cond_br %43, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %44 = llvm.add %41, %1  : i32
    %45 = llvm.add %41, %3  : i32
    %46 = llvm.add %41, %4  : i32
    %47 = llvm.add %40, %41  : i32
    %48 = llvm.add %40, %44  : i32
    %49 = llvm.add %40, %45  : i32
    %50 = llvm.add %40, %46  : i32
    %51 = llvm.getelementptr %arg0[%47] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %52 = llvm.load %51 : !llvm.ptr<f32>
    %53 = llvm.getelementptr %arg0[%48] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %54 = llvm.load %53 : !llvm.ptr<f32>
    %55 = llvm.getelementptr %arg0[%49] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %56 = llvm.load %55 : !llvm.ptr<f32>
    %57 = llvm.getelementptr %arg0[%50] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %58 = llvm.load %57 : !llvm.ptr<f32>
    %59 = llvm.fcmp "ugt" %42, %52 : f32
    %60 = llvm.select %59, %42, %52 : i1, f32
    %61 = llvm.fcmp "uno" %52, %52 : f32
    %62 = llvm.select %61, %52, %60 : i1, f32
    %63 = llvm.fcmp "ugt" %62, %54 : f32
    %64 = llvm.select %63, %62, %54 : i1, f32
    %65 = llvm.fcmp "uno" %54, %54 : f32
    %66 = llvm.select %65, %54, %64 : i1, f32
    %67 = llvm.fcmp "ugt" %66, %56 : f32
    %68 = llvm.select %67, %66, %56 : i1, f32
    %69 = llvm.fcmp "uno" %56, %56 : f32
    %70 = llvm.select %69, %56, %68 : i1, f32
    %71 = llvm.fcmp "ugt" %70, %58 : f32
    %72 = llvm.select %71, %70, %58 : i1, f32
    %73 = llvm.fcmp "uno" %58, %58 : f32
    %74 = llvm.select %73, %58, %72 : i1, f32
    %75 = llvm.add %41, %5  : i32
    llvm.br ^bb2(%75, %74 : i32, f32)
  ^bb4:  // pred: ^bb2
    llvm.br ^bb5(%39, %42 : i32, f32)
  ^bb5(%76: i32, %77: f32):  // 2 preds: ^bb4, ^bb6
    %78 = llvm.icmp "slt" %76, %arg1 : i32
    llvm.cond_br %78, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %79 = llvm.add %40, %76  : i32
    %80 = llvm.getelementptr %arg0[%79] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %81 = llvm.load %80 : !llvm.ptr<f32>
    %82 = llvm.fcmp "ugt" %77, %81 : f32
    %83 = llvm.select %82, %77, %81 : i1, f32
    %84 = llvm.fcmp "uno" %81, %81 : f32
    %85 = llvm.select %84, %81, %83 : i1, f32
    %86 = llvm.add %76, %1  : i32
    llvm.br ^bb5(%86, %85 : i32, f32)
  ^bb7:  // pred: ^bb5
    %87 = llvm.sub %8, %8  : i32
    %88 = llvm.lshr %0, %87  : i32
    %89 = llvm.sub %8, %7  : i32
    %90 = nvvm.shfl.sync  bfly %88, %77, %7, %89 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %91 = llvm.extractvalue %90[0] : !llvm.struct<(f32, i1)> 
    %92 = llvm.fcmp "ugt" %77, %91 : f32
    %93 = llvm.select %92, %77, %91 : i1, f32
    %94 = llvm.fcmp "uno" %91, %91 : f32
    %95 = llvm.select %94, %91, %93 : i1, f32
    %96 = llvm.sub %8, %8  : i32
    %97 = llvm.lshr %0, %96  : i32
    %98 = llvm.sub %8, %7  : i32
    %99 = nvvm.shfl.sync  bfly %97, %95, %9, %98 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %100 = llvm.extractvalue %99[0] : !llvm.struct<(f32, i1)> 
    %101 = llvm.fcmp "ugt" %95, %100 : f32
    %102 = llvm.select %101, %95, %100 : i1, f32
    %103 = llvm.fcmp "uno" %100, %100 : f32
    %104 = llvm.select %103, %100, %102 : i1, f32
    %105 = llvm.sub %8, %8  : i32
    %106 = llvm.lshr %0, %105  : i32
    %107 = llvm.sub %8, %7  : i32
    %108 = nvvm.shfl.sync  bfly %106, %104, %10, %107 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %109 = llvm.extractvalue %108[0] : !llvm.struct<(f32, i1)> 
    %110 = llvm.fcmp "ugt" %104, %109 : f32
    %111 = llvm.select %110, %104, %109 : i1, f32
    %112 = llvm.fcmp "uno" %109, %109 : f32
    %113 = llvm.select %112, %109, %111 : i1, f32
    %114 = llvm.sub %8, %8  : i32
    %115 = llvm.lshr %0, %114  : i32
    %116 = llvm.sub %8, %7  : i32
    %117 = nvvm.shfl.sync  bfly %115, %113, %11, %116 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %118 = llvm.extractvalue %117[0] : !llvm.struct<(f32, i1)> 
    %119 = llvm.fcmp "ugt" %113, %118 : f32
    %120 = llvm.select %119, %113, %118 : i1, f32
    %121 = llvm.fcmp "uno" %118, %118 : f32
    %122 = llvm.select %121, %118, %120 : i1, f32
    %123 = llvm.sub %8, %8  : i32
    %124 = llvm.lshr %0, %123  : i32
    %125 = llvm.sub %8, %7  : i32
    %126 = nvvm.shfl.sync  bfly %124, %122, %12, %125 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %127 = llvm.extractvalue %126[0] : !llvm.struct<(f32, i1)> 
    %128 = llvm.fcmp "ugt" %122, %127 : f32
    %129 = llvm.select %128, %122, %127 : i1, f32
    %130 = llvm.fcmp "uno" %127, %127 : f32
    %131 = llvm.select %130, %127, %129 : i1, f32
    %132 = llvm.icmp "eq" %29, %17 : i32
    llvm.cond_br %132, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %133 = llvm.getelementptr %21[%28] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %131, %133 : !llvm.ptr<f32, 3>
    llvm.br ^bb9
  ^bb9:  // 2 preds: ^bb7, ^bb8
    nvvm.barrier0
    %134 = llvm.icmp "slt" %27, %16 : i32
    llvm.cond_br %134, ^bb10, ^bb17
  ^bb10:  // pred: ^bb9
    %135 = llvm.icmp "slt" %29, %14 : i32
    llvm.cond_br %135, ^bb11, ^bb12
  ^bb11:  // pred: ^bb10
    %136 = llvm.getelementptr %21[%29] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %137 = llvm.load %136 : !llvm.ptr<f32, 3>
    llvm.br ^bb13(%137 : f32)
  ^bb12:  // pred: ^bb10
    llvm.br ^bb13(%6 : f32)
  ^bb13(%138: f32):  // 2 preds: ^bb11, ^bb12
    llvm.br ^bb14
  ^bb14:  // pred: ^bb13
    %139 = llvm.sub %8, %11  : i32
    %140 = llvm.lshr %0, %139  : i32
    %141 = llvm.sub %11, %7  : i32
    %142 = nvvm.shfl.sync  bfly %140, %138, %7, %141 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %143 = llvm.extractvalue %142[0] : !llvm.struct<(f32, i1)> 
    %144 = llvm.fcmp "ugt" %138, %143 : f32
    %145 = llvm.select %144, %138, %143 : i1, f32
    %146 = llvm.fcmp "uno" %143, %143 : f32
    %147 = llvm.select %146, %143, %145 : i1, f32
    %148 = llvm.sub %8, %11  : i32
    %149 = llvm.lshr %0, %148  : i32
    %150 = llvm.sub %11, %7  : i32
    %151 = nvvm.shfl.sync  bfly %149, %147, %9, %150 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %152 = llvm.extractvalue %151[0] : !llvm.struct<(f32, i1)> 
    %153 = llvm.fcmp "ugt" %147, %152 : f32
    %154 = llvm.select %153, %147, %152 : i1, f32
    %155 = llvm.fcmp "uno" %152, %152 : f32
    %156 = llvm.select %155, %152, %154 : i1, f32
    %157 = llvm.sub %8, %11  : i32
    %158 = llvm.lshr %0, %157  : i32
    %159 = llvm.sub %11, %7  : i32
    %160 = nvvm.shfl.sync  bfly %158, %156, %10, %159 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %161 = llvm.extractvalue %160[0] : !llvm.struct<(f32, i1)> 
    %162 = llvm.fcmp "ugt" %156, %161 : f32
    %163 = llvm.select %162, %156, %161 : i1, f32
    %164 = llvm.fcmp "uno" %161, %161 : f32
    %165 = llvm.select %164, %161, %163 : i1, f32
    llvm.cond_br %132, ^bb15, ^bb16
  ^bb15:  // pred: ^bb14
    llvm.store %165, %19 : !llvm.ptr<f32, 3>
    llvm.br ^bb16
  ^bb16:  // 2 preds: ^bb14, ^bb15
    llvm.br ^bb17
  ^bb17:  // 2 preds: ^bb9, ^bb16
    nvvm.barrier0
    %166 = llvm.load %19 : !llvm.ptr<f32, 3>
    llvm.br ^bb18(%27, %13 : i32, f32)
  ^bb18(%167: i32, %168: f32):  // 2 preds: ^bb17, ^bb19
    %169 = llvm.icmp "slt" %167, %39 : i32
    llvm.cond_br %169, ^bb19, ^bb20
  ^bb19:  // pred: ^bb18
    %170 = llvm.add %167, %1  : i32
    %171 = llvm.add %167, %3  : i32
    %172 = llvm.add %167, %4  : i32
    %173 = llvm.add %40, %167  : i32
    %174 = llvm.add %40, %170  : i32
    %175 = llvm.add %40, %171  : i32
    %176 = llvm.add %40, %172  : i32
    %177 = llvm.getelementptr %arg0[%173] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %178 = llvm.load %177 : !llvm.ptr<f32>
    %179 = llvm.getelementptr %arg0[%174] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %180 = llvm.load %179 : !llvm.ptr<f32>
    %181 = llvm.getelementptr %arg0[%175] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %182 = llvm.load %181 : !llvm.ptr<f32>
    %183 = llvm.getelementptr %arg0[%176] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %184 = llvm.load %183 : !llvm.ptr<f32>
    %185 = llvm.fsub %178, %166  : f32
    %186 = llvm.fsub %180, %166  : f32
    %187 = llvm.fsub %182, %166  : f32
    %188 = llvm.fsub %184, %166  : f32
    %189 = llvm.call @__nv_expf(%185) : (f32) -> f32
    %190 = llvm.call @__nv_expf(%186) : (f32) -> f32
    %191 = llvm.call @__nv_expf(%187) : (f32) -> f32
    %192 = llvm.call @__nv_expf(%188) : (f32) -> f32
    %193 = llvm.fadd %168, %189  : f32
    %194 = llvm.fadd %193, %190  : f32
    %195 = llvm.fadd %194, %191  : f32
    %196 = llvm.fadd %195, %192  : f32
    %197 = llvm.add %167, %5  : i32
    llvm.br ^bb18(%197, %196 : i32, f32)
  ^bb20:  // pred: ^bb18
    %198 = llvm.load %19 : !llvm.ptr<f32, 3>
    llvm.br ^bb21(%39, %168 : i32, f32)
  ^bb21(%199: i32, %200: f32):  // 2 preds: ^bb20, ^bb22
    %201 = llvm.icmp "slt" %199, %arg1 : i32
    llvm.cond_br %201, ^bb22, ^bb23
  ^bb22:  // pred: ^bb21
    %202 = llvm.add %40, %199  : i32
    %203 = llvm.getelementptr %arg0[%202] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %204 = llvm.load %203 : !llvm.ptr<f32>
    %205 = llvm.fsub %204, %198  : f32
    %206 = llvm.call @__nv_expf(%205) : (f32) -> f32
    %207 = llvm.fadd %200, %206  : f32
    %208 = llvm.add %199, %1  : i32
    llvm.br ^bb21(%208, %207 : i32, f32)
  ^bb23:  // pred: ^bb21
    %209 = llvm.sub %8, %8  : i32
    %210 = llvm.lshr %0, %209  : i32
    %211 = llvm.sub %8, %7  : i32
    %212 = nvvm.shfl.sync  bfly %210, %200, %7, %211 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %213 = llvm.extractvalue %212[0] : !llvm.struct<(f32, i1)> 
    %214 = llvm.fadd %200, %213  : f32
    %215 = llvm.sub %8, %8  : i32
    %216 = llvm.lshr %0, %215  : i32
    %217 = llvm.sub %8, %7  : i32
    %218 = nvvm.shfl.sync  bfly %216, %214, %9, %217 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %219 = llvm.extractvalue %218[0] : !llvm.struct<(f32, i1)> 
    %220 = llvm.fadd %214, %219  : f32
    %221 = llvm.sub %8, %8  : i32
    %222 = llvm.lshr %0, %221  : i32
    %223 = llvm.sub %8, %7  : i32
    %224 = nvvm.shfl.sync  bfly %222, %220, %10, %223 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %225 = llvm.extractvalue %224[0] : !llvm.struct<(f32, i1)> 
    %226 = llvm.fadd %220, %225  : f32
    %227 = llvm.sub %8, %8  : i32
    %228 = llvm.lshr %0, %227  : i32
    %229 = llvm.sub %8, %7  : i32
    %230 = nvvm.shfl.sync  bfly %228, %226, %11, %229 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %231 = llvm.extractvalue %230[0] : !llvm.struct<(f32, i1)> 
    %232 = llvm.fadd %226, %231  : f32
    %233 = llvm.sub %8, %8  : i32
    %234 = llvm.lshr %0, %233  : i32
    %235 = llvm.sub %8, %7  : i32
    %236 = nvvm.shfl.sync  bfly %234, %232, %12, %235 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %237 = llvm.extractvalue %236[0] : !llvm.struct<(f32, i1)> 
    %238 = llvm.fadd %232, %237  : f32
    llvm.cond_br %132, ^bb24, ^bb25
  ^bb24:  // pred: ^bb23
    %239 = llvm.getelementptr %25[%28] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %238, %239 : !llvm.ptr<f32, 3>
    llvm.br ^bb25
  ^bb25:  // 2 preds: ^bb23, ^bb24
    nvvm.barrier0
    llvm.cond_br %134, ^bb26, ^bb33
  ^bb26:  // pred: ^bb25
    %240 = llvm.icmp "slt" %29, %14 : i32
    llvm.cond_br %240, ^bb27, ^bb28
  ^bb27:  // pred: ^bb26
    %241 = llvm.getelementptr %25[%29] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %242 = llvm.load %241 : !llvm.ptr<f32, 3>
    llvm.br ^bb29(%242 : f32)
  ^bb28:  // pred: ^bb26
    llvm.br ^bb29(%13 : f32)
  ^bb29(%243: f32):  // 2 preds: ^bb27, ^bb28
    llvm.br ^bb30
  ^bb30:  // pred: ^bb29
    %244 = llvm.sub %8, %11  : i32
    %245 = llvm.lshr %0, %244  : i32
    %246 = llvm.sub %11, %7  : i32
    %247 = nvvm.shfl.sync  bfly %245, %243, %7, %246 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %248 = llvm.extractvalue %247[0] : !llvm.struct<(f32, i1)> 
    %249 = llvm.fadd %243, %248  : f32
    %250 = llvm.sub %8, %11  : i32
    %251 = llvm.lshr %0, %250  : i32
    %252 = llvm.sub %11, %7  : i32
    %253 = nvvm.shfl.sync  bfly %251, %249, %9, %252 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %254 = llvm.extractvalue %253[0] : !llvm.struct<(f32, i1)> 
    %255 = llvm.fadd %249, %254  : f32
    %256 = llvm.sub %8, %11  : i32
    %257 = llvm.lshr %0, %256  : i32
    %258 = llvm.sub %11, %7  : i32
    %259 = nvvm.shfl.sync  bfly %257, %255, %10, %258 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %260 = llvm.extractvalue %259[0] : !llvm.struct<(f32, i1)> 
    %261 = llvm.fadd %255, %260  : f32
    llvm.cond_br %132, ^bb31, ^bb32
  ^bb31:  // pred: ^bb30
    llvm.store %261, %23 : !llvm.ptr<f32, 3>
    llvm.br ^bb32
  ^bb32:  // 2 preds: ^bb30, ^bb31
    llvm.br ^bb33
  ^bb33:  // 2 preds: ^bb25, ^bb32
    nvvm.barrier0
    %262 = llvm.load %19 : !llvm.ptr<f32, 3>
    %263 = llvm.load %23 : !llvm.ptr<f32, 3>
    llvm.br ^bb34(%27 : i32)
  ^bb34(%264: i32):  // 2 preds: ^bb33, ^bb35
    %265 = llvm.icmp "slt" %264, %39 : i32
    llvm.cond_br %265, ^bb35, ^bb36
  ^bb35:  // pred: ^bb34
    %266 = llvm.add %264, %1  : i32
    %267 = llvm.add %264, %3  : i32
    %268 = llvm.add %264, %4  : i32
    %269 = llvm.add %40, %264  : i32
    %270 = llvm.add %40, %266  : i32
    %271 = llvm.add %40, %267  : i32
    %272 = llvm.add %40, %268  : i32
    %273 = llvm.getelementptr %arg0[%269] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %274 = llvm.load %273 : !llvm.ptr<f32>
    %275 = llvm.getelementptr %arg0[%270] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %276 = llvm.load %275 : !llvm.ptr<f32>
    %277 = llvm.getelementptr %arg0[%271] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %278 = llvm.load %277 : !llvm.ptr<f32>
    %279 = llvm.getelementptr %arg0[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %280 = llvm.load %279 : !llvm.ptr<f32>
    %281 = llvm.fsub %274, %262  : f32
    %282 = llvm.fsub %276, %262  : f32
    %283 = llvm.fsub %278, %262  : f32
    %284 = llvm.fsub %280, %262  : f32
    %285 = llvm.call @__nv_expf(%281) : (f32) -> f32
    %286 = llvm.call @__nv_expf(%282) : (f32) -> f32
    %287 = llvm.call @__nv_expf(%283) : (f32) -> f32
    %288 = llvm.call @__nv_expf(%284) : (f32) -> f32
    %289 = llvm.fdiv %285, %263  : f32
    %290 = llvm.fdiv %286, %263  : f32
    %291 = llvm.fdiv %287, %263  : f32
    %292 = llvm.fdiv %288, %263  : f32
    %293 = llvm.getelementptr %arg2[%269] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %289, %293 : !llvm.ptr<f32>
    %294 = llvm.getelementptr %arg2[%270] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %290, %294 : !llvm.ptr<f32>
    %295 = llvm.getelementptr %arg2[%271] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %291, %295 : !llvm.ptr<f32>
    %296 = llvm.getelementptr %arg2[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %292, %296 : !llvm.ptr<f32>
    %297 = llvm.add %264, %5  : i32
    llvm.br ^bb34(%297 : i32)
  ^bb36:  // pred: ^bb34
    %298 = llvm.load %19 : !llvm.ptr<f32, 3>
    %299 = llvm.load %23 : !llvm.ptr<f32, 3>
    llvm.br ^bb37(%39 : i32)
  ^bb37(%300: i32):  // 2 preds: ^bb36, ^bb38
    %301 = llvm.icmp "slt" %300, %arg1 : i32
    llvm.cond_br %301, ^bb38, ^bb39
  ^bb38:  // pred: ^bb37
    %302 = llvm.add %40, %300  : i32
    %303 = llvm.getelementptr %arg0[%302] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %304 = llvm.load %303 : !llvm.ptr<f32>
    %305 = llvm.fsub %304, %298  : f32
    %306 = llvm.call @__nv_expf(%305) : (f32) -> f32
    %307 = llvm.fdiv %306, %299  : f32
    %308 = llvm.getelementptr %arg2[%302] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %307, %308 : !llvm.ptr<f32>
    %309 = llvm.add %300, %1  : i32
    llvm.br ^bb37(%309 : i32)
  ^bb39:  // pred: ^bb37
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\00\E8\19\00\00\00\00\00\00\02\00\01\01@\00\00\00\A8\19\00\00\00\00\00\00\A2\19\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8:\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\002@:\00\01\00\117\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0D\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F5)_shndx\00.nv.info\00.text.main_kStitch_divide__13_1_0___1b1r2\00\0F,\00\15oshared.\00\15\9Fconstant21\00\1D\1F01\00\12\FD\01debug_frame\00.rel\11\00\1Aa\12\00!nv&\00oactionV\01 \0F\94\00\0F\0Fy\01\A2a__ocg_3\00/\00$0\00\0F\FF\16$__cuda_sm70_shflsync_bfly_p\00$____wg_G\00\0F@_0_$\0D\00/265\00\19_1__281\00\19\122f\00/305\00\19_3__32\91\02\1Fo_param\98\02.\0F\01\00\06\8CU\00\00\00\03\00\0B\00\01\00\11\AA\18\00,\0C\00\01\00\11\D8\18\00,\09\00\01\00\A1\15\01\00\00\22\00\0B\00`,\0F\001\00 \01\07\000\00!\020\00,\0A\00\01\00\11Y\18\00,\04\00\01\00\11\89\18\00,\07\00\01\00f2\00\00\00\12\10\A8\00!\80-\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13P\C0\00\10\04\9B\00R\044\00\00\00E\005\04\D4\0A<\00&8\00H\00\04p\00L\88\80\80(o\00\10\08\15\00x\16\89\80\80(\09\05J\01\004\00\22\18\00\01\00\22p\00\01\00\048\01\13\108\01\90\04/\08\00\08\00\00\00$\0C\00%#\08*\01'\04\12\0C\00\17\11\0C\00\12#0\00\04$\00\17\08$\00\05\0C\00!7\04$\06\90\015\00\00\04\0A\08\00\05x\00\A3\01\18\00\03\19\18\00\04\17\0CH\06U\10\00\00\F0!\10\00\10\01>\00%\F0\11\10\00\01\01\00\F09\F0!\00\03\1B\FF\00\04(X\00 \12\00\00p\12\00\00\B0\12\00\00\00\13\00\00`\13\00\00\90\14\00\00\D0\14\00\00\10\15\00\000!\00\00`!\00\00\80!\00\00\A0!\00\00\C0!\00\00P\22\00\00p\22\00\00\90\08\00\F4\16*\00\00\E0*\00\00`+\00\00\B0+\00\00\F0+\00\000,\00\00\04\1C\08\00@(\00\00@*\00\00\04\1E\E6\00C\00\00K\00\01\00\94\02\02\08\10\0A/\22\00\00\DF\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\E0\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00@\00\00\00\14\FB\01\22\C8\00\01\00\13\02\D1\00*H\00\10\00/\04\00\01\00\FF|@$v\01\FF\D7\04\C1\FF\00\8E\07\00\C4\0F\00\19y\02\00\01\00\F1\00!\00\00\00\22\0E\00\B9z\04\00\00F\00\00\FB\04\90\E2\0F\00Ey\00\00\10\0D!\00 \80\03\10\00\81$t\0C\FF\00\00\80\FF@\00\F3\05\E2\0F\00\10z\00\02\00Z\00\00\FF\E1\FF\07\00\E4\1F\04\0C\10\00\A0pR\F0\03\00\E4\0F\00\10xl\00\F0\07\00\00\FF\E0\FF\07\00\C8\0F\00\12x\03\00\00\FC\FF\FF\FF\C0\8E\07 \002\19y\00\01\00\10%\9D\00\83\0E\00\07r\03\03\FF\00\01\00\000\000r\03\03\A2\001\FF\FC\8E@\00P\0Cr\00\02\03\80\04\D3b\F2\03\00\DA\0F\00G\19\00\00`\0C\A0\00a\EA\0F\00\10r\04 \00\01\90\00\01\C0\00F\01\00\F0\06\C0\001\1Cx\00\01\00?p\F0\F0\D0\00\02@\0Cx\00\04?\062pB\F4 \00Ar\0E\FF\FF\D0\02\C6\8E\07\00\D8\0F\00G\A9\00\00\90\06p\00\05P\00\14\E1\F0\00a\0D\03\00\F4\FF\FF\F0\00b\C4\0F\00\02x\10\04\03\12\0FP\01@$z\0B\00 \01 \0E\02\D0\00\B0\1F\00%v\04\0B\00X\00\00\10\10\00a\CA\0F\00\81y\0F\C3\05\E3\00\19\1E\0C\00\A2\00\00\10x\07\0B\00\01@\01\93\E4\0F\04\10x\09\0B\00\02\10\00z\C6\0F\00%v\06\07@\00%\12\06@\00u\E2\02\00%v\08\09 \00\95\E2\0F\08\10x\0B\0B\00\03P\00v\00\10x\13\0E\00\04\10\00E\81y\11\08@\00fb\09\00%v\0A\A0\00\10\C8\C0\00\11\15\C0\00\12\13P\00U\00\81y\14\0A0\00\10f0\00&\04\15p\00e\1F\04\10x\17\15\C0\00\10\C80\00\16\13\E0\00 b\01\C0\00\16\170\00\88/\00\10x\1D\15\00\020\00\16\16\D0\00zb\03\00%v\1C\1D\D0\00*\1F\15\D0\00E\09\0E\00\08\10\00Z\01\81y\15\1C\D0\00%\1E\1F@\00\01\D0\00\11\19\D0\00\15\09\D0\00%\18\1E0\00\10f`\01' \19p\00\00\80\01\1A\19\D0\00&\17 `\00\1C\0Fp\01Z\00\10x\0B\19\D0\00\1F\1A`\01\07\01\D0\00&\05\19\D0\00\10\1F\00\024\0E\00\0C\10\00j/\00\81y\19\0A@\01%\04\05@\00\01\D0\00\11\1B\D0\00\15\07\D0\00\16\1Cp\01z&\03\01%v\06\1B\D0\00%\1D\1B\D0\00\01@\02\16\07p\01\11\22\D0\00\1A\1D\D0\00H\1D\1B\00\020\00*\08\080\00\17\0A0\00\00\C0\00H\1B\1B\00\030\00*\0B\0A0\00(\04\1B\00\02Q\0Br\00\0F\0F_\03\A1\F4\03\00\E4O\08\0Br\00\0C\10\00\B2\C0\F6\03\00\D6\0F\00\08\A2\0F\0F\D2\07\22\80\05\B0\02\16\0C\E0\00 \A2\0E@\00%\12\12@\00\10\8F@\00!\11\11\10\00\10\F6\A0\03\10\02`\00\01 \00P\C0\F8\03\00\D2P\00!\12\12`\000\00\00\06P\00U\0Br\00\14\14@\00\01\10\00$\12\110\00q\C8\0F\00\08\B2\11\11@\00\060\00&\13\13`\00\00\10\00(\11\140\001\A2\14\14@\00\060\00*\16\16`\00)\14\13`\00!\13\13@\00\060\00*\15\15`\00)\13\16`\00!\16\16@\00\060\00*\18\18`\00)\16\15`\00!\15\15@\00\060\00*\17\17`\00)\15\18`\00!\18\18@\00\060\00*\1A\1A`\00)\18\17`\00!\17\17@\00\060\00*\19\19`\00)\17\1A`\00!\1A\1A@\00\060\00&\1C\1C`\00\10\01p\00\19\19`\00!\19\19@\00\060\00*\07\07`\00)\19\1C`\00!\1C\1C@\00\060\00*\08\08\C0\00)\1C\07`\00!\07\07@\00\060\00*\0B\0B`\00)\07\08`\00!\08\08@\00\030\00f\10x\0E\0E\00\10\90\04\00p\00\19\0B`\00!\0B\0B@\00\030\00Q\0Cr\00\0E\0D\D0\06\14\F8`\00*\0C\0C\00\03)\0B\0C\00\03!\0C\0CP\00\00\00\03\90\E2\0F\00G\C9\00\00\90\F9\9A\0B\11\83\90\06#Ay\89\0B\03\A0\06@$x\04\03\12\005\0E\0A\8E\10\07#p\03 \00\13\E8\F0\06\13\04\F0\06\12\DA\E0\06'@\03@\00St\14\FF\04\000\08\10\E4`\04\1C\0A\C0\06\10\0A\C0\033\14\02\8E\E0\03\1B\0D\C0\06%\06\0A`\04\01\C0\06+\08\0A\C0\06\1A\06@\00\1C\10\C0\06\15\08 \00\01 \05&\0A\0A \05\00p\01\1B\11\C0\06\1C\0F`\05\06\A0\00\01 \05\11\11\C0\00\15\11 \05\16\12\90\04\10f`\06&\04\11p\00\00\C0\06*\13\11 \05*\05\04\F0\05&\06\130\00!/\000\00\09 \05+\06\06 \06\070\00j\0F\01\10x\11\11 \05\1C\09P\05\07\90\00\01 \02\12\0D\8F\09\15\F0 \05\01\10\00\22\C0\F4 \021\82\0D\0D0\02$\00\00 \05\1B\0A \05%\10\10@\00*\8F\08p\05\01 \05$\0D\10p\02\00 \05\22\82\10\FF\09\03p\05\06`\05#\F0\03\10\00$\10\0F0\00\12\C8\A0\05\1A\100\00&\05\05`\00\16\00\80\05\030\00\14\82\80\05\050\00*\06\06`\00)\12\05`\00:\05\05\120\00*\09\090\04)\05\06`\004\06\06\050\00\11\C80\00\11\060\00#\C0\F0P\00!\09\090\00\00\AB\0C\1F\E4\D0\09\01\18\0EP\08\00@\00%\0C\0C\10\04\01\80\01(\09\0C\80\011\B2\0C\0C`\00\00\80\01\1E\C4\90\03\00\F0\03\00_\03Ap\16p\00p\03V\89\00\00\A0\01 \000\02x\0B\CF\02\070\0A\0Fp\03\03\15\0B\C0\07\07`\03\0C\A0\02%\A2\0E`\03\030\00\19\00\80\03\0C\A0\02%\E2\0Ep\03\060\00\07p\03\0F\A0\02\03\06\90\00\10\CC \00(\0A\0A\F0\02\06\00\02\06\B0\02\05\D0\01\09\F0\01\BF\F4\03\00\D2\8F\00\08\82\05\05\0C\E0\01\02\1F\F0\E0\01\03\17\A2\E0\01\020\00*\0A\0A\D0\02(\06\090\00\14\82\F0\01\05\10\02)\09\0A\10\02!\0A\0A0\00\00\10\02\10\CA\00\0C\11\0C\00\0C\13\0A0\0D\14A\B0\0C\04\D0\010z\00\03\A0\01Dpb\F4\030\0D#\B0\04 \00\10\E2\F0\0C0\05\02\1F@\05\00\F0\0C\96\D6\0F\00G)\00\00\80\04@\001$r\07`\00'\03\00\A0\05\16\02@\00\09\B0\0C\10\C6p\0D%\04\07p\0D\13\C8\C0\05\10\03\C0\05\12\F6\C0\05V\B9\00\00 \02`\00\10\02\D0\0D\13ZP\02\0F\E0\02\02F\04\04\00\FD\B0\0C;$t\06\F0\05\16\08P\0A%\C8\1F \023\06\02\8E\10\02*\0D\08\F0\05%\0B\07\90\02\01\F0\05(\0F\07\F0\05\04\00\0D\010\02a\E4\0F\04$z\0F\10\00\12\0F\10\00\05\10\0B\13\06\B0\05E%v\0E\0F\10\00\1B\E4`\02 \E2\0E\00\066\07\00\03`\00E\81y\0F\0E \00\16$\C0\00\1F\11\C0\00\01\09P\05\05\D0\03$\0C\0D\B0\07\01\D0\03\05P\05\12\06\80\00+\0C\08P\05\03p\02\14\F8P\05\12\0DP\02\11\FA`\01\06`\05\01\F0\02U\01\10x\07\07\B0\06q\CE\0F\00\08\C2\0A\0Ap\00Q\00\80\06\00\C80\00$\0A\0F@\00\00\90\02D\B2\0F\0F\0A \00\020\08!\07\040\08*\FA\03\90\04\14\F8\C0\00$\0F\0C\F0\02\00\C0\00\22\C2\0C@\0B\040\08_\D9\00\00\10\FE0\08\07\09`\02\03\90\02\140\A0\04\13\E6p\02\1A\01p\02(\00\010\08\1F\0A@\02\12\1F\0A@\02\02\1A\0E@\02\1B\CA0\02\16\C8 \02\1D\0A\C0\01\1B\F0\C0\01\1B\F6\10\07/\80\05\10\07\01\0E\10\03\16\07\B0\02\09`\01\05 \02)\0D\0C \02\12\0Cp\00\00 \02\0F\F0\05\01!z\00P\01\08\F0\05(\80\00\F0\05\1B\09\F0\05\1B\060\01\11\06\C0\05\15\090\01\16\09\B0\05+\A4\0E \05\02\00\01\19\09\00\014\09\09\0C\00\01\07\D0\04\1F\09\D0\04\04\F1\00\89\7F\07\0C\00\1F \0C\00\00\0E\00\00b\0E\D0\04&`\03\90\04\06p\0B\00p\00\11/p\00\19\07p\00(\07\07p\00r\89\7F\04\07\00\1F@P\00\11d\B0\00*\04\04@\00)\07\04@\009\04\04\07@\00X\09\04\00\1F\80@\00*\09\09@\00\1C\04\F0\00\17\04\F0\01@\19x\04\FF \06@\02\16\01\00 \04\87\89\7F\06\09\00\1F\00\0DP\00*\06\06P\00)\09\06P\007\06\06\09P\00@\0Cr\00\05@\01!pR\D0\03\0DP\12\81\89\7F\0B\06\00\1F\00\0E`\00ft\0E\00$\B4\07\10\03\00\80\0A\10\B6\EF\02\12\80\E0\01\00 \03\12t\00\06#\FF\00\D0\02\14\B2p\0C\12\F8\90\00G\B2\00\06\0BP\043\1C\B8\00p\00Ap\02\00\E4\90\03\10\050\01 p@0\00`\0F\04\19x\05\05\FF\05\10\FF\C5\00\01\C0\094\0B\0B\06@\04\12\E2\E0\05\17\07p\001\0Cx\00\A0\061p@\FA`\06#\88\B3\B0\0C\00\A5\0Cf\E8\0F\00\1D{\00\01\00\89\EC\0F\00GY\00\00@\B0\08\00\9E\04$\80\FF\C0\021\82x\06\F0\13\01\01\00\93\CA\0F\00\84\C9\0A\05\00\04P\00\81\A2\02\00Gy\00\06\C2Q\0E\22\80\0Bp\02C\09\0A\00\07p\02/\A4N\D0\02\01\02\90\07%\C0\FC\E0\01\02\10\05\11\00\10\03c\89\7F\08\09\00\07`\02'\A4\0E\E0\0D\05@\00)\09\08@\00!\08\08P\00\05@\00C\0C\08\00\07`\02a\E2\04\00$r\0B\90\011\08\00\8EP\06$\0B\B2\00\04\11\FC\E0\05\09\F0\01\0A\90\01\01\10\00\00\C0\01\14\0Cp\00\10\DAp\00\04\B0\0D\02p\00c\88\B3\00\FF\0C\00\10\01-\E4\07\80\03\22Hyh\19\01\10\00\1E\E4\80\01T\84y\08\FF\00@\00\22\A2Np\08\07\A0\03!\02x\DE\00\14\80\90\01\00 \15&\80\03`\00B$t\09\FF\B1\01\03\80\02&r\0A\E0\14\10\C6@\00\1B\13\90\04\02\80\05\15\0A\90\04\11\0C\80\05\02\00\14[\8F\04\10x\0E\A0\14\19\10\A0\14J\81y\0D\0C`\0A&\0E\0E@\00\00p\05\16\12\90\14\01\10\0E%\10\10 \00<\E4\0F\08\D0\07G%v\12\12 \00\00\B0\14\15\10 \00\10h\10\00%\13\12\10\00\B4\A2N\00$t\16\FF\9D\98\BB;\F0\02\01\D0\0A\04\80\07\01\10\03a\15\FF\00\00|C \00\11\C6\E0\03!\0A\03`\07\02\D0\03c!r\14\08\0D\00\E4\06`\C8\8F\00#t\0B\9F\112?\16  \01T!r\0D\08\0F \00\D2\C6\0F\01#t\0B\0B\01\00@K\15@ \00E#t\0C\0D0\00\D4\C6\0F\00!t\0F\0B\7F\00@\CB\00\00@\004\11\08\11@\00u\E2\0F\02#t\0C\0C@\00\D4\E4\0F\08#x\0F\14;\AA\B8?\0F\08P\00'\10\11\80\00e\08!r\13\08\13@\00hO\00!t\0E\0C`\00\00@\00e`p\A52\0F\00@\00\17\10\A0\00W\08#t\14\13P\00P\00#x\0E\0Dp\00\12\0Ep\000\04$x\FF\02\15\800\04I!t\12\10`\007t\14\14P\00P\00#x\0D\0Dp\00\13\0E \00T\08s\0F\00\0F\B0\02p\22\0E\00#x\12\11`\00#\12\08\00\018t\0E\14P\00E$x\10\10p\00!\C4\0F0\00\00P\00$\12\00`\00%\0E\13\A0\00`\E4\0F\04\08s\0D\CD\05\02`\00\10\A4`\00(\13\13\80\00T\19x\0E\0B\17\10\05\10\CA\90\004\11\00\120\00\81\E2\0E\00#r\0F\0E\0F\10\07\00\10\00e\1F\00\19x\09\140\00\10\CC0\008\16\00\13\C0\00Cr\0D\0C\0D\1F\03q\00\C8O\00#r\10\91\01#\0D\00 \020r\09\09`\13\13\10P\00\01\E0\0D\18\FCp\09\0Ap\07\00\C0\03&\90\07\C0\03\00 \0C&p\07 \00\04\90\04\08 \0C&\10\040\00\0E \0C+\0A\0B \0C\12\0A \0C\02\90\04XG\E9\00\00\C0\10\041v\0A\FF0\00\03\E0\01\09\10\09\13\C80\03\07 \0C;\02x\12 \04\16\0C\A0\09k\E2\9F\00\10x\0F\10\04\16\11\10\04p\E2\0F\04%v\0C\0C\C0\03$\12\02\90\03\19\13\00\04\0B\10\0C\1C\04@\04\09\C0\11\16\E4 \0C\22\12\02`\07E%v\10\11\10\00\0F@\04\001$z\13@\00\0F@\04\02\10d@\00%\12\13@\00\1F\CCP\04\11\11\0B\B1\0F\03\E0\00L\00$t\17P\04!\0B\0AP\04/\FC\03P\04\03\16\0CP\044\C8\0F\00\00\04$\17@ \04\19\15`\04:!t\0F\E0\03:t\0D\15\10\04\0AP\04E\19x\0C\0C\D0\02\01@\03\09P\04W\04#t\0D\0Dp\00U\08#t\0E\11P\00\03\80\04\08@\04\0Ep\04'\10\0D\90\00X\04#t\0E\0EP\00H$x\0D\0D@\04\0Ep\04 \10\15\90\00\01~\1Ex\E2\0F\04!t\12\0E\E0\00\0D@\04\03p\04&\17@@\00\00\A0\00\02p\03\1A\0F`\04\10\C4`\04\18\16`\04F\04\19x\14\E0\03\1E\E2`\04T\08s\10\00\10p\00\10\A2P\04\11\16p\04#\16\08\A0\01!x\16`\04\22\16\00\D0\05;\08s\12P\04(\0C\0CP\04\10$\90\1A\04\00\01\03P\04\19\16P\04\11\0Cp\13\15\0CP\045\09\09\12\10\00\01P\04'\14\16P\00\00\C0\03/p\FC\C0\0D\09\07\00\04\040\04\19\02\C0\0D\010\12\08\10\04'\E0\01@\00\08\C0\03\01\D0\02\1B\0E\D0\0D\07\F0\03/\E4\9F\C0\0F\00\02\E0\03\1B\0E\C0\0F\15\0Ep\03\07\10\08*\E8\0E\B0\0F\10\22p\00\19\13`\03 \02x\1F\03!|C\05\02\1B\E4\A0\04\13\C4\80\03\06`\04G\00!r\0A`\03\10\CF\F0\06\00\7F\034?\13 \C0\02\1B\12\B0\07\02\10\07\14\14P\025t\13\120\00\01\B0\07*\11\10\E0\02(\13\130\00\09\B0\06\10\E4 \02 \11\0A0\02\22\11\08 \00E!t\0D\13@\00\12\C6 \00\00@\02\22\11\00 \00@#x\0D\120\00$\0D\08P\00%\0A\13P\00\12\C6 \00\000\00\13\0D0\00\00\C0\06\14\11@\0290\0E\00\00\07\00\B0\02Dr\09\10\110\02 \C8\1F\10\00$\0A\0D\10\00/\C6O\A0\0E\01\1C\0B\A0\0E\17\F0 \00?\02x\0D\90\14\02\15\0B\10\0A\02\00\14\15\0D0\05\0A\E0\1B6$t\0D\D0\01\00@\02&t\0F \05\01\90\015\08\08\0B\B0\01PO\01#t\0C\CA\005?\0D  \05\01\10\05\22\0F@\80\03J!t\0D\0C\D0\03\07\E0\04\020\01\15\08P\01\12\C8\10\00\05@\01*\CC\0F0\01\11$0\01\15\0C \01-\CC\1F\A0\07\01`\0C\04\10\0F\03p\0B&\80\01\80\07%!rP\0C@\00\00\CAO\00\0E6\08\00\1F\90\0C2!r\0B\E0\00\05 \00%\0A\0B\D0\0E\01 \00\11\0A \06\06 \00%\0D\0A\A0\0E\01 \00\13\0C \12\00 \00\10\CF \00\15\0C`\0E\00 \00\12\B2\90\08\04@\00c\88\B3\00\04\0D(\89#\0F\C0\0D\04\17\B0\E0\006$t\04\10\0C\0F\C0\0D\02C\04\05\00(0\01!\E2\04\C0\0D(2\09\C0\0D%\05\04\C0\0D\93d\EE\00!r\05\05\04\00\01\00 \CA/p\0F\15\05\A0\0D\11d\F0\008\05\06\00 \00%\07\0A\80\0D\8A\A4\02\00!\B2\07\07\0A\D0\003\FF\07$\80\00/\E4\050\0D 4\05\FF\00@\00!b.\C0\01&0\05\10\05!\84y\00\01\05\C0\05\000\0D(\00\05 \01\08\10\15Q\1F\00$z\0D\00\03\15\02\00\03 \0E\0D\00\03\15\06\90\17%\07\0D@\05 \E4O0 \15\0D\A0\04\0C\A0\14\01\A0\17'\10\07@\00\00\D0\04\18\0D\10\09E%v\12\09 \00\01\D0\08*\11\10\10\0D'\14\0B \00\0A\B0\08\01\10\0D%\15\14\10\00p\E2\8E\00\02x\16\00\90\03\05\D0\03\08\B0\08\00\80\10\10x\FE\00\D0\80\00\00\E2\F2\03\00\E2/\04 x\18\10\00A>\00\00@p\03\03 \004~\00B\80\05C\02\02\00\04\B0\00`\E4\0F\00\08r\181\00\02\01\00P\CE\0F\00 \98\8F\1E#\80K@\00\00\D0\02&\04\0F\F0\03X\00#t\08\0C\10\08T!r\0F\04\11 \00\01\A0\05(\08\08\F0\07V!r\13\04\13 \00W\02!t\11\08\10\04W\00#t\0A\0FP\00Z\08#t\12\13\D0\08)\11\0C\A0\05Dr\15\04\15P\00\10\E2p\04%\0A\0Ap\00\010\00)\0E\0C\B0\05:t\0C\120\09\18\11\80\09\00\10\09\18\0A\A0\04\10$@\0B\04\D0\05\01 \00\1B\12\C0\09'\14\11P\00\00\D0\08\17\0F\10\09@\00\08s\0E\1A\03\060\08:x\12\13@\0D\09\E0\08\01@\00\08\10\09\0E\00\0A*\12\13P\0D%\16\15\E0\08\01P\05(\0F\080\00\00p\06\14\10\80\00xb\0E\00$x\14\14\D0\00\00@\00\00P\00\22\16\000\00\01\00#\13\\\90\02\01p\0D7\13\00\12\A0\06R r\0E\0F\0E\EA/\01\A0\06E \08\0E\0E`\02\10\C60\00\19\15 \09T r\0A\0A\110\00u\E2/\00 \98\0E\0EP\02z\C6\0F\00 \08\0A\0A@\00C\17\00\18\00\81\0D\10b@\005\0C\0C\13@\00jO\00 \98\0A\0A@\00%\0C\0C@\00\84\E2\0F\00 r\14\14\150\00u\C6\1F\00 \98\0C\0C0\00\00 \008\08\14\140\00\06\90&\02\80\0DI \98\14\140\00#r\15\82*!@\00P\035r\11\17\00\01\010\01%\0C\0D0\01\010\006\13\17\0A \00d\04 r\17\17\14\10\00\02\F0\0C\17\070\00P\08\86y\00\0C \01a\04\19\10\0C\00\E6P%\18\0BP\00T\86y\00\0E\13 \00 \E8\01\10\00*\08\15\10\00$\06\17\10\00!\E2\01\00\08(\00\FB`\0A\0Ap\059M)\00\10\00\0E0\04/\02\050\04\03\22\F4\03\90\0AF\00\00\B0\01\80\050\08r\021\00\02\82\03\010\04%\02\02@\01\00\00\19 s\0C?\05\02\D0\01/f\100\17\00\11\02\90\05\15\03\90\05%\06\020\17*\CCO\D0$k\A2\0E\00$t\08`\0A\00\9E\07\06`\0Ac\10x\03\03\00\01\D0\04\18\C8\E0\1B\13\F0\F0\075\05\04\070\04\12\CFp\00\13\\p\00\00\80\08 t\08\CF\003?\08 \80\08\03\C0\04\22\0B@\10\00J!t\0A\08 \04%\08\08P\03\01\C0\08 \0A\05\90\03#\0A\08@\000x\0A\05`\03\01\14\02\01\10\017\05\00\0A\C0\081 r\05q\00\05P\03&(\05\80\01\10\C8P\01(\05\05P\01E r\05\0C0\00!\CA/\00\02\1B\05\00\02/\90\FE\00\02\08\0B\10\00\00\E0\09\00\0B\01\06P\01C\08\00\80*\A0\01T\C4\0F\00Dy`\0CP\C0\03\00\EA\7F\A0\15\15\0A \15\00 \0EGt\0D\FF\02@\00\06p\19\01`\15Q\04\08r\0A\0C\F0\00\00@\150\E4\0F\04`\00\14\F0`\00\01p\06\05 \00\05p\00\15pp\00\17\0Fp\00\01P\00\00\10\16\06\D0\15\10\C4@\006\0B\0C\0A\00 \19\04\F0\0A\12\E4 \00\03\C0\09\00@\00\01\E0\00#p+ \00\16\C6@\15\13\0B\B0\12\00\80\00&\F0\00\80\00\10G\F0\02\16\E9P\01\040\00\11\04\F0\00\11\8F`\0B\17\01p\00\00`\00*\C0+@\01&\A0\00@\01c!r\0A\04\0C\00\01\00\01\C0\07\0A@\01\00\80\01*\00,@\00\17`\90\002!r\0A\10\01\07@\00\1B\04@\00\1B@@\00\17 @\002\02r\07\9C\0E\12\0F@\00\00\E0\00&@\F6\E0\00\03\80\16\05 \00AHs\00\000\03\000\02Q\E8\0F\00\89s\D0\0AA\0C\06\00\0E\C0\02`Py\00\08`\D3@\00Q\C3\03\00\EA\1FP\00#\F0\FFP\00f\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-\11\01\89\02\0E\01\00\22@\00\01\00=V\01\000\00\08\01\00\1F\0B@\00\04\13\96)\00?\98\02\00@\00\0A\00L\06.\02\00\01\00\1105\03\02h3\13\00$\00\13\07\A7\04\00\01\00\03\14(>\00\17\01T\00\00\01\00*\08\05@\00\0F\80\00\05\1A)\7F2\05\01\00\13\E0@\00\11T\06\00\06\E4\00*\04\00\01\00\1F[@\00\0414\06\00\01\00&\C0\00@\00\1F\0B@\00\00!G\01\18\00\0D@\00/\F8\06\C0\00\03\1B\08\08\00#$\01l\0E\0B\01\00\12\D0%\03\03l\03\02\80\00\17\048\00\22\10\00\01\00\13\B5\14\01\0C\84\01*\F0\07h\00\1F\00\C0\00\04\1F\E6@\00\04\13\F8@\00?x\01\00@\00\0A\132@\00\15\06\92\00\02\01\00*\80\09\E04\12\03\8C\00:$\80\00\01\00\13\87\A4\00+\03\00\01\00\03\F09/H\00\80\00\0B\13\06\9F\0D\05(:\0B\01\00*\A8\00\08\00\04\10\01\13\018\00\04(\01\0C\01\00*\10/\08\00\088\00\17\06\A0\00\0F\01\00\05\04\B8\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
  llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_0() {addr_space = 3 : i32} : !llvm.array<32 x f32>
  llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_1() {addr_space = 3 : i32} : !llvm.array<8 x f32>
  llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_2() {addr_space = 3 : i32} : !llvm.array<32 x f32>
  llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_3() {addr_space = 3 : i32} : !llvm.array<8 x f32>
  llvm.func @__nv_expf(f32) -> f32
  llvm.func @main_kStitch_divide__13_1_0___1b1r(%arg0: !llvm.ptr<f32>, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [0 : index, 2 : index, 3 : index, 4 : index, 6 : index, 7 : index, 8 : index, 9 : index, 11 : index, 12 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(-1 : i32) : i32
    %1 = llvm.mlir.constant(256 : index) : i32
    %2 = llvm.mlir.constant(4 : index) : i32
    %3 = llvm.mlir.constant(512 : index) : i32
    %4 = llvm.mlir.constant(768 : index) : i32
    %5 = llvm.mlir.constant(1024 : index) : i32
    %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %7 = llvm.mlir.constant(1 : i32) : i32
    %8 = llvm.mlir.constant(32 : i32) : i32
    %9 = llvm.mlir.constant(2 : i32) : i32
    %10 = llvm.mlir.constant(4 : i32) : i32
    %11 = llvm.mlir.constant(8 : i32) : i32
    %12 = llvm.mlir.constant(16 : i32) : i32
    %13 = llvm.mlir.constant(-0.000000e+00 : f32) : f32
    %14 = llvm.mlir.constant(8 : index) : i32
    %15 = llvm.mlir.constant(1 : index) : i32
    %16 = llvm.mlir.constant(32 : index) : i32
    %17 = llvm.mlir.constant(0 : index) : i32
    %18 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_0 : !llvm.ptr<array<32 x f32>, 3>
    %19 = llvm.getelementptr %18[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %20 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_1 : !llvm.ptr<array<8 x f32>, 3>
    %21 = llvm.getelementptr %20[0, 0] : (!llvm.ptr<array<8 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %22 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_2 : !llvm.ptr<array<32 x f32>, 3>
    %23 = llvm.getelementptr %22[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %24 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_3 : !llvm.ptr<array<8 x f32>, 3>
    %25 = llvm.getelementptr %24[0, 0] : (!llvm.ptr<array<8 x f32>, 3>) -> !llvm.ptr<f32, 3>
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %26 = nvvm.read.ptx.sreg.ctaid.x : i32
    %27 = nvvm.read.ptx.sreg.tid.x : i32
    %28 = llvm.udiv %27, %16  : i32
    %29 = llvm.urem %27, %16  : i32
    %30 = llvm.sub %arg1, %27  : i32
    %31 = llvm.icmp "eq" %30, %17 : i32
    %32 = llvm.sub %30, %15  : i32
    %33 = llvm.udiv %32, %1  : i32
    %34 = llvm.add %33, %15  : i32
    %35 = llvm.select %31, %17, %34 : i1, i32
    %36 = llvm.srem %35, %2  : i32
    %37 = llvm.sub %35, %36  : i32
    %38 = llvm.mul %37, %1  : i32
    %39 = llvm.add %27, %38  : i32
    %40 = llvm.mul %26, %arg1  : i32
    llvm.br ^bb2(%27, %6 : i32, f32)
  ^bb2(%41: i32, %42: f32):  // 2 preds: ^bb1, ^bb3
    %43 = llvm.icmp "slt" %41, %39 : i32
    llvm.cond_br %43, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %44 = llvm.add %41, %1  : i32
    %45 = llvm.add %41, %3  : i32
    %46 = llvm.add %41, %4  : i32
    %47 = llvm.add %40, %41  : i32
    %48 = llvm.add %40, %44  : i32
    %49 = llvm.add %40, %45  : i32
    %50 = llvm.add %40, %46  : i32
    %51 = llvm.getelementptr %arg0[%47] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %52 = llvm.load %51 : !llvm.ptr<f32>
    %53 = llvm.getelementptr %arg0[%48] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %54 = llvm.load %53 : !llvm.ptr<f32>
    %55 = llvm.getelementptr %arg0[%49] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %56 = llvm.load %55 : !llvm.ptr<f32>
    %57 = llvm.getelementptr %arg0[%50] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %58 = llvm.load %57 : !llvm.ptr<f32>
    %59 = llvm.fcmp "ugt" %42, %52 : f32
    %60 = llvm.select %59, %42, %52 : i1, f32
    %61 = llvm.fcmp "uno" %52, %52 : f32
    %62 = llvm.select %61, %52, %60 : i1, f32
    %63 = llvm.fcmp "ugt" %62, %54 : f32
    %64 = llvm.select %63, %62, %54 : i1, f32
    %65 = llvm.fcmp "uno" %54, %54 : f32
    %66 = llvm.select %65, %54, %64 : i1, f32
    %67 = llvm.fcmp "ugt" %66, %56 : f32
    %68 = llvm.select %67, %66, %56 : i1, f32
    %69 = llvm.fcmp "uno" %56, %56 : f32
    %70 = llvm.select %69, %56, %68 : i1, f32
    %71 = llvm.fcmp "ugt" %70, %58 : f32
    %72 = llvm.select %71, %70, %58 : i1, f32
    %73 = llvm.fcmp "uno" %58, %58 : f32
    %74 = llvm.select %73, %58, %72 : i1, f32
    %75 = llvm.add %41, %5  : i32
    llvm.br ^bb2(%75, %74 : i32, f32)
  ^bb4:  // pred: ^bb2
    llvm.br ^bb5(%39, %42 : i32, f32)
  ^bb5(%76: i32, %77: f32):  // 2 preds: ^bb4, ^bb6
    %78 = llvm.icmp "slt" %76, %arg1 : i32
    llvm.cond_br %78, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %79 = llvm.add %40, %76  : i32
    %80 = llvm.getelementptr %arg0[%79] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %81 = llvm.load %80 : !llvm.ptr<f32>
    %82 = llvm.fcmp "ugt" %77, %81 : f32
    %83 = llvm.select %82, %77, %81 : i1, f32
    %84 = llvm.fcmp "uno" %81, %81 : f32
    %85 = llvm.select %84, %81, %83 : i1, f32
    %86 = llvm.add %76, %1  : i32
    llvm.br ^bb5(%86, %85 : i32, f32)
  ^bb7:  // pred: ^bb5
    %87 = llvm.sub %8, %8  : i32
    %88 = llvm.lshr %0, %87  : i32
    %89 = llvm.sub %8, %7  : i32
    %90 = nvvm.shfl.sync  bfly %88, %77, %7, %89 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %91 = llvm.extractvalue %90[0] : !llvm.struct<(f32, i1)> 
    %92 = llvm.fcmp "ugt" %77, %91 : f32
    %93 = llvm.select %92, %77, %91 : i1, f32
    %94 = llvm.fcmp "uno" %91, %91 : f32
    %95 = llvm.select %94, %91, %93 : i1, f32
    %96 = llvm.sub %8, %8  : i32
    %97 = llvm.lshr %0, %96  : i32
    %98 = llvm.sub %8, %7  : i32
    %99 = nvvm.shfl.sync  bfly %97, %95, %9, %98 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %100 = llvm.extractvalue %99[0] : !llvm.struct<(f32, i1)> 
    %101 = llvm.fcmp "ugt" %95, %100 : f32
    %102 = llvm.select %101, %95, %100 : i1, f32
    %103 = llvm.fcmp "uno" %100, %100 : f32
    %104 = llvm.select %103, %100, %102 : i1, f32
    %105 = llvm.sub %8, %8  : i32
    %106 = llvm.lshr %0, %105  : i32
    %107 = llvm.sub %8, %7  : i32
    %108 = nvvm.shfl.sync  bfly %106, %104, %10, %107 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %109 = llvm.extractvalue %108[0] : !llvm.struct<(f32, i1)> 
    %110 = llvm.fcmp "ugt" %104, %109 : f32
    %111 = llvm.select %110, %104, %109 : i1, f32
    %112 = llvm.fcmp "uno" %109, %109 : f32
    %113 = llvm.select %112, %109, %111 : i1, f32
    %114 = llvm.sub %8, %8  : i32
    %115 = llvm.lshr %0, %114  : i32
    %116 = llvm.sub %8, %7  : i32
    %117 = nvvm.shfl.sync  bfly %115, %113, %11, %116 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %118 = llvm.extractvalue %117[0] : !llvm.struct<(f32, i1)> 
    %119 = llvm.fcmp "ugt" %113, %118 : f32
    %120 = llvm.select %119, %113, %118 : i1, f32
    %121 = llvm.fcmp "uno" %118, %118 : f32
    %122 = llvm.select %121, %118, %120 : i1, f32
    %123 = llvm.sub %8, %8  : i32
    %124 = llvm.lshr %0, %123  : i32
    %125 = llvm.sub %8, %7  : i32
    %126 = nvvm.shfl.sync  bfly %124, %122, %12, %125 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %127 = llvm.extractvalue %126[0] : !llvm.struct<(f32, i1)> 
    %128 = llvm.fcmp "ugt" %122, %127 : f32
    %129 = llvm.select %128, %122, %127 : i1, f32
    %130 = llvm.fcmp "uno" %127, %127 : f32
    %131 = llvm.select %130, %127, %129 : i1, f32
    %132 = llvm.icmp "eq" %29, %17 : i32
    llvm.cond_br %132, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %133 = llvm.getelementptr %21[%28] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %131, %133 : !llvm.ptr<f32, 3>
    llvm.br ^bb9
  ^bb9:  // 2 preds: ^bb7, ^bb8
    nvvm.barrier0
    %134 = llvm.icmp "slt" %27, %16 : i32
    llvm.cond_br %134, ^bb10, ^bb17
  ^bb10:  // pred: ^bb9
    %135 = llvm.icmp "slt" %29, %14 : i32
    llvm.cond_br %135, ^bb11, ^bb12
  ^bb11:  // pred: ^bb10
    %136 = llvm.getelementptr %21[%29] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %137 = llvm.load %136 : !llvm.ptr<f32, 3>
    llvm.br ^bb13(%137 : f32)
  ^bb12:  // pred: ^bb10
    llvm.br ^bb13(%6 : f32)
  ^bb13(%138: f32):  // 2 preds: ^bb11, ^bb12
    llvm.br ^bb14
  ^bb14:  // pred: ^bb13
    %139 = llvm.sub %8, %11  : i32
    %140 = llvm.lshr %0, %139  : i32
    %141 = llvm.sub %11, %7  : i32
    %142 = nvvm.shfl.sync  bfly %140, %138, %7, %141 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %143 = llvm.extractvalue %142[0] : !llvm.struct<(f32, i1)> 
    %144 = llvm.fcmp "ugt" %138, %143 : f32
    %145 = llvm.select %144, %138, %143 : i1, f32
    %146 = llvm.fcmp "uno" %143, %143 : f32
    %147 = llvm.select %146, %143, %145 : i1, f32
    %148 = llvm.sub %8, %11  : i32
    %149 = llvm.lshr %0, %148  : i32
    %150 = llvm.sub %11, %7  : i32
    %151 = nvvm.shfl.sync  bfly %149, %147, %9, %150 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %152 = llvm.extractvalue %151[0] : !llvm.struct<(f32, i1)> 
    %153 = llvm.fcmp "ugt" %147, %152 : f32
    %154 = llvm.select %153, %147, %152 : i1, f32
    %155 = llvm.fcmp "uno" %152, %152 : f32
    %156 = llvm.select %155, %152, %154 : i1, f32
    %157 = llvm.sub %8, %11  : i32
    %158 = llvm.lshr %0, %157  : i32
    %159 = llvm.sub %11, %7  : i32
    %160 = nvvm.shfl.sync  bfly %158, %156, %10, %159 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %161 = llvm.extractvalue %160[0] : !llvm.struct<(f32, i1)> 
    %162 = llvm.fcmp "ugt" %156, %161 : f32
    %163 = llvm.select %162, %156, %161 : i1, f32
    %164 = llvm.fcmp "uno" %161, %161 : f32
    %165 = llvm.select %164, %161, %163 : i1, f32
    llvm.cond_br %132, ^bb15, ^bb16
  ^bb15:  // pred: ^bb14
    llvm.store %165, %19 : !llvm.ptr<f32, 3>
    llvm.br ^bb16
  ^bb16:  // 2 preds: ^bb14, ^bb15
    llvm.br ^bb17
  ^bb17:  // 2 preds: ^bb9, ^bb16
    nvvm.barrier0
    %166 = llvm.load %19 : !llvm.ptr<f32, 3>
    llvm.br ^bb18(%27, %13 : i32, f32)
  ^bb18(%167: i32, %168: f32):  // 2 preds: ^bb17, ^bb19
    %169 = llvm.icmp "slt" %167, %39 : i32
    llvm.cond_br %169, ^bb19, ^bb20
  ^bb19:  // pred: ^bb18
    %170 = llvm.add %167, %1  : i32
    %171 = llvm.add %167, %3  : i32
    %172 = llvm.add %167, %4  : i32
    %173 = llvm.add %40, %167  : i32
    %174 = llvm.add %40, %170  : i32
    %175 = llvm.add %40, %171  : i32
    %176 = llvm.add %40, %172  : i32
    %177 = llvm.getelementptr %arg0[%173] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %178 = llvm.load %177 : !llvm.ptr<f32>
    %179 = llvm.getelementptr %arg0[%174] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %180 = llvm.load %179 : !llvm.ptr<f32>
    %181 = llvm.getelementptr %arg0[%175] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %182 = llvm.load %181 : !llvm.ptr<f32>
    %183 = llvm.getelementptr %arg0[%176] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %184 = llvm.load %183 : !llvm.ptr<f32>
    %185 = llvm.fsub %178, %166  : f32
    %186 = llvm.fsub %180, %166  : f32
    %187 = llvm.fsub %182, %166  : f32
    %188 = llvm.fsub %184, %166  : f32
    %189 = llvm.call @__nv_expf(%185) : (f32) -> f32
    %190 = llvm.call @__nv_expf(%186) : (f32) -> f32
    %191 = llvm.call @__nv_expf(%187) : (f32) -> f32
    %192 = llvm.call @__nv_expf(%188) : (f32) -> f32
    %193 = llvm.fadd %168, %189  : f32
    %194 = llvm.fadd %193, %190  : f32
    %195 = llvm.fadd %194, %191  : f32
    %196 = llvm.fadd %195, %192  : f32
    %197 = llvm.add %167, %5  : i32
    llvm.br ^bb18(%197, %196 : i32, f32)
  ^bb20:  // pred: ^bb18
    %198 = llvm.load %19 : !llvm.ptr<f32, 3>
    llvm.br ^bb21(%39, %168 : i32, f32)
  ^bb21(%199: i32, %200: f32):  // 2 preds: ^bb20, ^bb22
    %201 = llvm.icmp "slt" %199, %arg1 : i32
    llvm.cond_br %201, ^bb22, ^bb23
  ^bb22:  // pred: ^bb21
    %202 = llvm.add %40, %199  : i32
    %203 = llvm.getelementptr %arg0[%202] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %204 = llvm.load %203 : !llvm.ptr<f32>
    %205 = llvm.fsub %204, %198  : f32
    %206 = llvm.call @__nv_expf(%205) : (f32) -> f32
    %207 = llvm.fadd %200, %206  : f32
    %208 = llvm.add %199, %1  : i32
    llvm.br ^bb21(%208, %207 : i32, f32)
  ^bb23:  // pred: ^bb21
    %209 = llvm.sub %8, %8  : i32
    %210 = llvm.lshr %0, %209  : i32
    %211 = llvm.sub %8, %7  : i32
    %212 = nvvm.shfl.sync  bfly %210, %200, %7, %211 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %213 = llvm.extractvalue %212[0] : !llvm.struct<(f32, i1)> 
    %214 = llvm.fadd %200, %213  : f32
    %215 = llvm.sub %8, %8  : i32
    %216 = llvm.lshr %0, %215  : i32
    %217 = llvm.sub %8, %7  : i32
    %218 = nvvm.shfl.sync  bfly %216, %214, %9, %217 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %219 = llvm.extractvalue %218[0] : !llvm.struct<(f32, i1)> 
    %220 = llvm.fadd %214, %219  : f32
    %221 = llvm.sub %8, %8  : i32
    %222 = llvm.lshr %0, %221  : i32
    %223 = llvm.sub %8, %7  : i32
    %224 = nvvm.shfl.sync  bfly %222, %220, %10, %223 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %225 = llvm.extractvalue %224[0] : !llvm.struct<(f32, i1)> 
    %226 = llvm.fadd %220, %225  : f32
    %227 = llvm.sub %8, %8  : i32
    %228 = llvm.lshr %0, %227  : i32
    %229 = llvm.sub %8, %7  : i32
    %230 = nvvm.shfl.sync  bfly %228, %226, %11, %229 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %231 = llvm.extractvalue %230[0] : !llvm.struct<(f32, i1)> 
    %232 = llvm.fadd %226, %231  : f32
    %233 = llvm.sub %8, %8  : i32
    %234 = llvm.lshr %0, %233  : i32
    %235 = llvm.sub %8, %7  : i32
    %236 = nvvm.shfl.sync  bfly %234, %232, %12, %235 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %237 = llvm.extractvalue %236[0] : !llvm.struct<(f32, i1)> 
    %238 = llvm.fadd %232, %237  : f32
    llvm.cond_br %132, ^bb24, ^bb25
  ^bb24:  // pred: ^bb23
    %239 = llvm.getelementptr %25[%28] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %238, %239 : !llvm.ptr<f32, 3>
    llvm.br ^bb25
  ^bb25:  // 2 preds: ^bb23, ^bb24
    nvvm.barrier0
    llvm.cond_br %134, ^bb26, ^bb33
  ^bb26:  // pred: ^bb25
    %240 = llvm.icmp "slt" %29, %14 : i32
    llvm.cond_br %240, ^bb27, ^bb28
  ^bb27:  // pred: ^bb26
    %241 = llvm.getelementptr %25[%29] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %242 = llvm.load %241 : !llvm.ptr<f32, 3>
    llvm.br ^bb29(%242 : f32)
  ^bb28:  // pred: ^bb26
    llvm.br ^bb29(%13 : f32)
  ^bb29(%243: f32):  // 2 preds: ^bb27, ^bb28
    llvm.br ^bb30
  ^bb30:  // pred: ^bb29
    %244 = llvm.sub %8, %11  : i32
    %245 = llvm.lshr %0, %244  : i32
    %246 = llvm.sub %11, %7  : i32
    %247 = nvvm.shfl.sync  bfly %245, %243, %7, %246 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %248 = llvm.extractvalue %247[0] : !llvm.struct<(f32, i1)> 
    %249 = llvm.fadd %243, %248  : f32
    %250 = llvm.sub %8, %11  : i32
    %251 = llvm.lshr %0, %250  : i32
    %252 = llvm.sub %11, %7  : i32
    %253 = nvvm.shfl.sync  bfly %251, %249, %9, %252 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %254 = llvm.extractvalue %253[0] : !llvm.struct<(f32, i1)> 
    %255 = llvm.fadd %249, %254  : f32
    %256 = llvm.sub %8, %11  : i32
    %257 = llvm.lshr %0, %256  : i32
    %258 = llvm.sub %11, %7  : i32
    %259 = nvvm.shfl.sync  bfly %257, %255, %10, %258 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %260 = llvm.extractvalue %259[0] : !llvm.struct<(f32, i1)> 
    %261 = llvm.fadd %255, %260  : f32
    llvm.cond_br %132, ^bb31, ^bb32
  ^bb31:  // pred: ^bb30
    llvm.store %261, %23 : !llvm.ptr<f32, 3>
    llvm.br ^bb32
  ^bb32:  // 2 preds: ^bb30, ^bb31
    llvm.br ^bb33
  ^bb33:  // 2 preds: ^bb25, ^bb32
    nvvm.barrier0
    %262 = llvm.load %19 : !llvm.ptr<f32, 3>
    %263 = llvm.load %23 : !llvm.ptr<f32, 3>
    llvm.br ^bb34(%27 : i32)
  ^bb34(%264: i32):  // 2 preds: ^bb33, ^bb35
    %265 = llvm.icmp "slt" %264, %39 : i32
    llvm.cond_br %265, ^bb35, ^bb36
  ^bb35:  // pred: ^bb34
    %266 = llvm.add %264, %1  : i32
    %267 = llvm.add %264, %3  : i32
    %268 = llvm.add %264, %4  : i32
    %269 = llvm.add %40, %264  : i32
    %270 = llvm.add %40, %266  : i32
    %271 = llvm.add %40, %267  : i32
    %272 = llvm.add %40, %268  : i32
    %273 = llvm.getelementptr %arg0[%269] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %274 = llvm.load %273 : !llvm.ptr<f32>
    %275 = llvm.getelementptr %arg0[%270] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %276 = llvm.load %275 : !llvm.ptr<f32>
    %277 = llvm.getelementptr %arg0[%271] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %278 = llvm.load %277 : !llvm.ptr<f32>
    %279 = llvm.getelementptr %arg0[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %280 = llvm.load %279 : !llvm.ptr<f32>
    %281 = llvm.fsub %274, %262  : f32
    %282 = llvm.fsub %276, %262  : f32
    %283 = llvm.fsub %278, %262  : f32
    %284 = llvm.fsub %280, %262  : f32
    %285 = llvm.call @__nv_expf(%281) : (f32) -> f32
    %286 = llvm.call @__nv_expf(%282) : (f32) -> f32
    %287 = llvm.call @__nv_expf(%283) : (f32) -> f32
    %288 = llvm.call @__nv_expf(%284) : (f32) -> f32
    %289 = llvm.fdiv %285, %263  : f32
    %290 = llvm.fdiv %286, %263  : f32
    %291 = llvm.fdiv %287, %263  : f32
    %292 = llvm.fdiv %288, %263  : f32
    %293 = llvm.getelementptr %arg2[%269] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %289, %293 : !llvm.ptr<f32>
    %294 = llvm.getelementptr %arg2[%270] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %290, %294 : !llvm.ptr<f32>
    %295 = llvm.getelementptr %arg2[%271] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %291, %295 : !llvm.ptr<f32>
    %296 = llvm.getelementptr %arg2[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %292, %296 : !llvm.ptr<f32>
    %297 = llvm.add %264, %5  : i32
    llvm.br ^bb34(%297 : i32)
  ^bb36:  // pred: ^bb34
    %298 = llvm.load %19 : !llvm.ptr<f32, 3>
    %299 = llvm.load %23 : !llvm.ptr<f32, 3>
    llvm.br ^bb37(%39 : i32)
  ^bb37(%300: i32):  // 2 preds: ^bb36, ^bb38
    %301 = llvm.icmp "slt" %300, %arg1 : i32
    llvm.cond_br %301, ^bb38, ^bb39
  ^bb38:  // pred: ^bb37
    %302 = llvm.add %40, %300  : i32
    %303 = llvm.getelementptr %arg0[%302] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %304 = llvm.load %303 : !llvm.ptr<f32>
    %305 = llvm.fsub %304, %298  : f32
    %306 = llvm.call @__nv_expf(%305) : (f32) -> f32
    %307 = llvm.fdiv %306, %299  : f32
    %308 = llvm.getelementptr %arg2[%302] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %307, %308 : !llvm.ptr<f32>
    %309 = llvm.add %300, %1  : i32
    llvm.br ^bb37(%309 : i32)
  ^bb39:  // pred: ^bb37
    llvm.return
  }
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kStitch_divide__13_1_0___1w1r(%arg0: memref<?x?x?xf32, "gpu">, %arg1: index, %arg2: memref<?x?x?xf32, "gpu">) workgroup(%arg3 : memref<32xf32, 3>, %arg4 : memref<32xf32, 3>) kernel {
    %0 = gpu.block_id  x
    %1 = gpu.block_id  y
    %2 = gpu.block_id  z
    %3 = gpu.thread_id  x
    %4 = gpu.thread_id  y
    %5 = gpu.thread_id  z
    %6 = gpu.grid_dim  x
    %7 = gpu.grid_dim  y
    %8 = gpu.grid_dim  z
    %9 = gpu.block_dim  x
    %10 = gpu.block_dim  y
    %11 = gpu.block_dim  z
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %12 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c96 = arith.constant 96 : index
    %13 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
    %14 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
    %c128 = arith.constant 128 : index
    %cst = arith.constant 0xFF800000 : f32
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c2_i32 = arith.constant 2 : i32
    %c4_i32 = arith.constant 4 : i32
    %c8_i32 = arith.constant 8 : i32
    %c16_i32 = arith.constant 16 : i32
    %cst_0 = arith.constant -0.000000e+00 : f32
    %15 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%c1, %c0]
    %16 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%3)[%c1, %c0]
    %17 = gpu.block_id  x
    %18 = gpu.thread_id  x
    %19 = arith.divui %18, %c32 : index
    %20 = arith.remui %18, %c32 : index
    %21 = arith.muli %17, %c8 : index
    %22 = arith.addi %21, %19 : index
    %23 = arith.cmpi slt, %22, %arg1 : index
    scf.if %23 {
      %24 = arith.subi %12, %20 : index
      %25 = arith.cmpi eq, %24, %c0 : index
      %26 = arith.subi %24, %c1 : index
      %27 = arith.divui %26, %c32 : index
      %28 = arith.addi %27, %c1 : index
      %29 = arith.select %25, %c0, %28 : index
      %30 = arith.remsi %29, %c4 : index
      %31 = arith.subi %29, %30 : index
      %32 = arith.muli %31, %c32 : index
      %33 = arith.addi %20, %32 : index
      %34 = arith.muli %22, %12 : index
      %35 = arith.muli %13, %14 : index
      %36 = arith.muli %35, %12 : index
      %37 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%36], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
      %38 = scf.for %arg5 = %20 to %33 step %c128 iter_args(%arg6 = %cst) -> (f32) {
        %65 = arith.addi %arg5, %c32 : index
        %66 = arith.addi %arg5, %c64 : index
        %67 = arith.addi %arg5, %c96 : index
        %68 = arith.addi %34, %arg5 : index
        %69 = arith.addi %34, %65 : index
        %70 = arith.addi %34, %66 : index
        %71 = arith.addi %34, %67 : index
        %72 = memref.load %37[%68] : memref<?xf32, "gpu">
        %73 = memref.load %37[%69] : memref<?xf32, "gpu">
        %74 = memref.load %37[%70] : memref<?xf32, "gpu">
        %75 = memref.load %37[%71] : memref<?xf32, "gpu">
        %76 = arith.cmpf ugt, %arg6, %72 : f32
        %77 = arith.select %76, %arg6, %72 : f32
        %78 = arith.cmpf uno, %72, %72 : f32
        %79 = arith.select %78, %72, %77 : f32
        %80 = arith.cmpf ugt, %79, %73 : f32
        %81 = arith.select %80, %79, %73 : f32
        %82 = arith.cmpf uno, %73, %73 : f32
        %83 = arith.select %82, %73, %81 : f32
        %84 = arith.cmpf ugt, %83, %74 : f32
        %85 = arith.select %84, %83, %74 : f32
        %86 = arith.cmpf uno, %74, %74 : f32
        %87 = arith.select %86, %74, %85 : f32
        %88 = arith.cmpf ugt, %87, %75 : f32
        %89 = arith.select %88, %87, %75 : f32
        %90 = arith.cmpf uno, %75, %75 : f32
        %91 = arith.select %90, %75, %89 : f32
        scf.yield %91 : f32
      }
      %39 = arith.muli %22, %12 : index
      %40 = arith.muli %13, %14 : index
      %41 = arith.muli %40, %12 : index
      %42 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%41], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
      %43 = scf.for %arg5 = %33 to %12 step %c32 iter_args(%arg6 = %38) -> (f32) {
        %65 = arith.addi %39, %arg5 : index
        %66 = memref.load %42[%65] : memref<?xf32, "gpu">
        %67 = arith.cmpf ugt, %arg6, %66 : f32
        %68 = arith.select %67, %arg6, %66 : f32
        %69 = arith.cmpf uno, %66, %66 : f32
        %70 = arith.select %69, %66, %68 : f32
        scf.yield %70 : f32
      }
      %result, %valid = gpu.shuffle  xor %43, %c1_i32, %c32_i32 : f32
      %44 = arith.cmpf ugt, %43, %result : f32
      %45 = arith.select %44, %43, %result : f32
      %46 = arith.cmpf uno, %result, %result : f32
      %47 = arith.select %46, %result, %45 : f32
      %result_1, %valid_2 = gpu.shuffle  xor %47, %c2_i32, %c32_i32 : f32
      %48 = arith.cmpf ugt, %47, %result_1 : f32
      %49 = arith.select %48, %47, %result_1 : f32
      %50 = arith.cmpf uno, %result_1, %result_1 : f32
      %51 = arith.select %50, %result_1, %49 : f32
      %result_3, %valid_4 = gpu.shuffle  xor %51, %c4_i32, %c32_i32 : f32
      %52 = arith.cmpf ugt, %51, %result_3 : f32
      %53 = arith.select %52, %51, %result_3 : f32
      %54 = arith.cmpf uno, %result_3, %result_3 : f32
      %55 = arith.select %54, %result_3, %53 : f32
      %result_5, %valid_6 = gpu.shuffle  xor %55, %c8_i32, %c32_i32 : f32
      %56 = arith.cmpf ugt, %55, %result_5 : f32
      %57 = arith.select %56, %55, %result_5 : f32
      %58 = arith.cmpf uno, %result_5, %result_5 : f32
      %59 = arith.select %58, %result_5, %57 : f32
      %result_7, %valid_8 = gpu.shuffle  xor %59, %c16_i32, %c32_i32 : f32
      %60 = arith.cmpf ugt, %59, %result_7 : f32
      %61 = arith.select %60, %59, %result_7 : f32
      %62 = arith.cmpf uno, %result_7, %result_7 : f32
      %63 = arith.select %62, %result_7, %61 : f32
      %64 = arith.cmpi eq, %20, %c0 : index
      scf.if %64 {
        %65 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        memref.store %63, %65[%19] : memref<32xf32, 3>
      }
    }
    gpu.barrier
    scf.if %23 {
      %24 = arith.subi %12, %20 : index
      %25 = arith.cmpi eq, %24, %c0 : index
      %26 = arith.subi %24, %c1 : index
      %27 = arith.divui %26, %c32 : index
      %28 = arith.addi %27, %c1 : index
      %29 = arith.select %25, %c0, %28 : index
      %30 = arith.remsi %29, %c4 : index
      %31 = arith.subi %29, %30 : index
      %32 = arith.muli %31, %c32 : index
      %33 = arith.addi %20, %32 : index
      %34 = arith.muli %22, %12 : index
      %35 = arith.muli %13, %14 : index
      %36 = arith.muli %35, %12 : index
      %37 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%36], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
      %38 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
      %39 = scf.for %arg5 = %20 to %33 step %c128 iter_args(%arg6 = %cst_0) -> (f32) {
        %52 = arith.addi %arg5, %c32 : index
        %53 = arith.addi %arg5, %c64 : index
        %54 = arith.addi %arg5, %c96 : index
        %55 = arith.addi %34, %arg5 : index
        %56 = arith.addi %34, %52 : index
        %57 = arith.addi %34, %53 : index
        %58 = arith.addi %34, %54 : index
        %59 = arith.divui %55, %12 : index
        %60 = arith.remui %59, %14 : index
        %61 = arith.divui %59, %14 : index
        %62 = arith.divui %56, %12 : index
        %63 = arith.remui %62, %14 : index
        %64 = arith.divui %62, %14 : index
        %65 = arith.divui %57, %12 : index
        %66 = arith.remui %65, %14 : index
        %67 = arith.divui %65, %14 : index
        %68 = arith.divui %58, %12 : index
        %69 = arith.remui %68, %14 : index
        %70 = arith.divui %68, %14 : index
        %71 = memref.load %37[%55] : memref<?xf32, "gpu">
        %72 = memref.load %37[%56] : memref<?xf32, "gpu">
        %73 = memref.load %37[%57] : memref<?xf32, "gpu">
        %74 = memref.load %37[%58] : memref<?xf32, "gpu">
        %75 = arith.muli %61, %14 : index
        %76 = arith.addi %75, %60 : index
        %77 = arith.muli %64, %14 : index
        %78 = arith.addi %77, %63 : index
        %79 = arith.muli %67, %14 : index
        %80 = arith.addi %79, %66 : index
        %81 = arith.muli %70, %14 : index
        %82 = arith.addi %81, %69 : index
        %83 = arith.remui %76, %c8 : index
        %84 = arith.remui %78, %c8 : index
        %85 = arith.remui %80, %c8 : index
        %86 = arith.remui %82, %c8 : index
        %87 = memref.load %38[%83] : memref<32xf32, 3>
        %88 = memref.load %38[%84] : memref<32xf32, 3>
        %89 = memref.load %38[%85] : memref<32xf32, 3>
        %90 = memref.load %38[%86] : memref<32xf32, 3>
        %91 = arith.subf %71, %87 : f32
        %92 = arith.subf %72, %88 : f32
        %93 = arith.subf %73, %89 : f32
        %94 = arith.subf %74, %90 : f32
        %95 = math.exp %91 : f32
        %96 = math.exp %92 : f32
        %97 = math.exp %93 : f32
        %98 = math.exp %94 : f32
        %99 = arith.addf %arg6, %95 : f32
        %100 = arith.addf %99, %96 : f32
        %101 = arith.addf %100, %97 : f32
        %102 = arith.addf %101, %98 : f32
        scf.yield %102 : f32
      }
      %40 = arith.muli %22, %12 : index
      %41 = arith.muli %13, %14 : index
      %42 = arith.muli %41, %12 : index
      %43 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%42], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
      %44 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
      %45 = scf.for %arg5 = %33 to %12 step %c32 iter_args(%arg6 = %39) -> (f32) {
        %52 = arith.addi %40, %arg5 : index
        %53 = arith.divui %52, %12 : index
        %54 = arith.remui %53, %14 : index
        %55 = arith.divui %53, %14 : index
        %56 = memref.load %43[%52] : memref<?xf32, "gpu">
        %57 = arith.muli %55, %14 : index
        %58 = arith.addi %57, %54 : index
        %59 = arith.remui %58, %c8 : index
        %60 = memref.load %44[%59] : memref<32xf32, 3>
        %61 = arith.subf %56, %60 : f32
        %62 = math.exp %61 : f32
        %63 = arith.addf %arg6, %62 : f32
        scf.yield %63 : f32
      }
      %result, %valid = gpu.shuffle  xor %45, %c1_i32, %c32_i32 : f32
      %46 = arith.addf %45, %result : f32
      %result_1, %valid_2 = gpu.shuffle  xor %46, %c2_i32, %c32_i32 : f32
      %47 = arith.addf %46, %result_1 : f32
      %result_3, %valid_4 = gpu.shuffle  xor %47, %c4_i32, %c32_i32 : f32
      %48 = arith.addf %47, %result_3 : f32
      %result_5, %valid_6 = gpu.shuffle  xor %48, %c8_i32, %c32_i32 : f32
      %49 = arith.addf %48, %result_5 : f32
      %result_7, %valid_8 = gpu.shuffle  xor %49, %c16_i32, %c32_i32 : f32
      %50 = arith.addf %49, %result_7 : f32
      %51 = arith.cmpi eq, %20, %c0 : index
      scf.if %51 {
        %52 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        memref.store %50, %52[%19] : memref<32xf32, 3>
      }
    }
    gpu.barrier
    scf.if %23 {
      %24 = arith.subi %12, %20 : index
      %25 = arith.cmpi eq, %24, %c0 : index
      %26 = arith.subi %24, %c1 : index
      %27 = arith.divui %26, %c32 : index
      %28 = arith.addi %27, %c1 : index
      %29 = arith.select %25, %c0, %28 : index
      %30 = arith.remsi %29, %c4 : index
      %31 = arith.subi %29, %30 : index
      %32 = arith.muli %31, %c32 : index
      %33 = arith.addi %20, %32 : index
      %34 = arith.muli %22, %12 : index
      %35 = arith.muli %13, %14 : index
      %36 = arith.muli %35, %12 : index
      %37 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%36], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
      %38 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
      %39 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
      %40 = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%36], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
      scf.for %arg5 = %20 to %33 step %c128 {
        %48 = arith.addi %arg5, %c32 : index
        %49 = arith.addi %arg5, %c64 : index
        %50 = arith.addi %arg5, %c96 : index
        %51 = arith.addi %34, %arg5 : index
        %52 = arith.addi %34, %48 : index
        %53 = arith.addi %34, %49 : index
        %54 = arith.addi %34, %50 : index
        %55 = arith.divui %51, %12 : index
        %56 = arith.remui %55, %14 : index
        %57 = arith.divui %55, %14 : index
        %58 = arith.divui %52, %12 : index
        %59 = arith.remui %58, %14 : index
        %60 = arith.divui %58, %14 : index
        %61 = arith.divui %53, %12 : index
        %62 = arith.remui %61, %14 : index
        %63 = arith.divui %61, %14 : index
        %64 = arith.divui %54, %12 : index
        %65 = arith.remui %64, %14 : index
        %66 = arith.divui %64, %14 : index
        %67 = memref.load %37[%51] : memref<?xf32, "gpu">
        %68 = memref.load %37[%52] : memref<?xf32, "gpu">
        %69 = memref.load %37[%53] : memref<?xf32, "gpu">
        %70 = memref.load %37[%54] : memref<?xf32, "gpu">
        %71 = arith.muli %57, %14 : index
        %72 = arith.addi %71, %56 : index
        %73 = arith.muli %60, %14 : index
        %74 = arith.addi %73, %59 : index
        %75 = arith.muli %63, %14 : index
        %76 = arith.addi %75, %62 : index
        %77 = arith.muli %66, %14 : index
        %78 = arith.addi %77, %65 : index
        %79 = arith.remui %72, %c8 : index
        %80 = arith.remui %74, %c8 : index
        %81 = arith.remui %76, %c8 : index
        %82 = arith.remui %78, %c8 : index
        %83 = memref.load %38[%79] : memref<32xf32, 3>
        %84 = memref.load %38[%80] : memref<32xf32, 3>
        %85 = memref.load %38[%81] : memref<32xf32, 3>
        %86 = memref.load %38[%82] : memref<32xf32, 3>
        %87 = arith.subf %67, %83 : f32
        %88 = arith.subf %68, %84 : f32
        %89 = arith.subf %69, %85 : f32
        %90 = arith.subf %70, %86 : f32
        %91 = math.exp %87 : f32
        %92 = math.exp %88 : f32
        %93 = math.exp %89 : f32
        %94 = math.exp %90 : f32
        %95 = memref.load %39[%79] : memref<32xf32, 3>
        %96 = memref.load %39[%80] : memref<32xf32, 3>
        %97 = memref.load %39[%81] : memref<32xf32, 3>
        %98 = memref.load %39[%82] : memref<32xf32, 3>
        %99 = arith.divf %91, %95 : f32
        %100 = arith.divf %92, %96 : f32
        %101 = arith.divf %93, %97 : f32
        %102 = arith.divf %94, %98 : f32
        memref.store %99, %40[%51] : memref<?xf32, "gpu">
        memref.store %100, %40[%52] : memref<?xf32, "gpu">
        memref.store %101, %40[%53] : memref<?xf32, "gpu">
        memref.store %102, %40[%54] : memref<?xf32, "gpu">
      }
      %41 = arith.muli %22, %12 : index
      %42 = arith.muli %13, %14 : index
      %43 = arith.muli %42, %12 : index
      %44 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%43], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
      %45 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
      %46 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
      %47 = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%43], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
      scf.for %arg5 = %33 to %12 step %c32 {
        %48 = arith.addi %41, %arg5 : index
        %49 = arith.divui %48, %12 : index
        %50 = arith.remui %49, %14 : index
        %51 = arith.divui %49, %14 : index
        %52 = memref.load %44[%48] : memref<?xf32, "gpu">
        %53 = arith.muli %51, %14 : index
        %54 = arith.addi %53, %50 : index
        %55 = arith.remui %54, %c8 : index
        %56 = memref.load %45[%55] : memref<32xf32, 3>
        %57 = arith.subf %52, %56 : f32
        %58 = math.exp %57 : f32
        %59 = memref.load %46[%55] : memref<32xf32, 3>
        %60 = arith.divf %58, %59 : f32
        memref.store %60, %47[%48] : memref<?xf32, "gpu">
      }
    }
    gpu.return
  }
}

// -----// IR Dump After SideEffectLoopInvariantCodeMotionPass (disc-side-effect-loop-invariant-code-motion) //----- //
gpu.func @main_kStitch_divide__13_1_0___1w1r(%arg0: memref<?x?x?xf32, "gpu">, %arg1: index, %arg2: memref<?x?x?xf32, "gpu">) workgroup(%arg3 : memref<32xf32, 3>, %arg4 : memref<32xf32, 3>) kernel {
  %cst = arith.constant -0.000000e+00 : f32
  %c16_i32 = arith.constant 16 : i32
  %c8_i32 = arith.constant 8 : i32
  %c4_i32 = arith.constant 4 : i32
  %c2_i32 = arith.constant 2 : i32
  %c32_i32 = arith.constant 32 : i32
  %c1_i32 = arith.constant 1 : i32
  %cst_0 = arith.constant 0xFF800000 : f32
  %c128 = arith.constant 128 : index
  %c96 = arith.constant 96 : index
  %c64 = arith.constant 64 : index
  %c4 = arith.constant 4 : index
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  cf.br ^bb1
^bb1:  // pred: ^bb0
  %0 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
  %1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
  %2 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
  %3 = gpu.block_id  x
  %4 = gpu.thread_id  x
  %5 = arith.divui %4, %c32 : index
  %6 = arith.remui %4, %c32 : index
  %7 = arith.muli %3, %c8 : index
  %8 = arith.addi %7, %5 : index
  %9 = arith.cmpi slt, %8, %arg1 : index
  scf.if %9 {
    %10 = arith.subi %0, %6 : index
    %11 = arith.cmpi eq, %10, %c0 : index
    %12 = arith.subi %10, %c1 : index
    %13 = arith.divui %12, %c32 : index
    %14 = arith.addi %13, %c1 : index
    %15 = arith.select %11, %c0, %14 : index
    %16 = arith.remsi %15, %c4 : index
    %17 = arith.subi %15, %16 : index
    %18 = arith.muli %17, %c32 : index
    %19 = arith.addi %6, %18 : index
    %20 = arith.muli %8, %0 : index
    %21 = arith.muli %1, %2 : index
    %22 = arith.muli %21, %0 : index
    %23 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%22], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %24 = scf.for %arg5 = %6 to %19 step %c128 iter_args(%arg6 = %cst_0) -> (f32) {
      %51 = arith.addi %arg5, %c32 : index
      %52 = arith.addi %arg5, %c64 : index
      %53 = arith.addi %arg5, %c96 : index
      %54 = arith.addi %20, %arg5 : index
      %55 = arith.addi %20, %51 : index
      %56 = arith.addi %20, %52 : index
      %57 = arith.addi %20, %53 : index
      %58 = memref.load %23[%54] : memref<?xf32, "gpu">
      %59 = memref.load %23[%55] : memref<?xf32, "gpu">
      %60 = memref.load %23[%56] : memref<?xf32, "gpu">
      %61 = memref.load %23[%57] : memref<?xf32, "gpu">
      %62 = arith.cmpf ugt, %arg6, %58 : f32
      %63 = arith.select %62, %arg6, %58 : f32
      %64 = arith.cmpf uno, %58, %58 : f32
      %65 = arith.select %64, %58, %63 : f32
      %66 = arith.cmpf ugt, %65, %59 : f32
      %67 = arith.select %66, %65, %59 : f32
      %68 = arith.cmpf uno, %59, %59 : f32
      %69 = arith.select %68, %59, %67 : f32
      %70 = arith.cmpf ugt, %69, %60 : f32
      %71 = arith.select %70, %69, %60 : f32
      %72 = arith.cmpf uno, %60, %60 : f32
      %73 = arith.select %72, %60, %71 : f32
      %74 = arith.cmpf ugt, %73, %61 : f32
      %75 = arith.select %74, %73, %61 : f32
      %76 = arith.cmpf uno, %61, %61 : f32
      %77 = arith.select %76, %61, %75 : f32
      scf.yield %77 : f32
    }
    %25 = arith.muli %8, %0 : index
    %26 = arith.muli %1, %2 : index
    %27 = arith.muli %26, %0 : index
    %28 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%27], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %29 = scf.for %arg5 = %19 to %0 step %c32 iter_args(%arg6 = %24) -> (f32) {
      %51 = arith.addi %25, %arg5 : index
      %52 = memref.load %28[%51] : memref<?xf32, "gpu">
      %53 = arith.cmpf ugt, %arg6, %52 : f32
      %54 = arith.select %53, %arg6, %52 : f32
      %55 = arith.cmpf uno, %52, %52 : f32
      %56 = arith.select %55, %52, %54 : f32
      scf.yield %56 : f32
    }
    %result, %valid = gpu.shuffle  xor %29, %c1_i32, %c32_i32 : f32
    %30 = arith.cmpf ugt, %29, %result : f32
    %31 = arith.select %30, %29, %result : f32
    %32 = arith.cmpf uno, %result, %result : f32
    %33 = arith.select %32, %result, %31 : f32
    %result_1, %valid_2 = gpu.shuffle  xor %33, %c2_i32, %c32_i32 : f32
    %34 = arith.cmpf ugt, %33, %result_1 : f32
    %35 = arith.select %34, %33, %result_1 : f32
    %36 = arith.cmpf uno, %result_1, %result_1 : f32
    %37 = arith.select %36, %result_1, %35 : f32
    %result_3, %valid_4 = gpu.shuffle  xor %37, %c4_i32, %c32_i32 : f32
    %38 = arith.cmpf ugt, %37, %result_3 : f32
    %39 = arith.select %38, %37, %result_3 : f32
    %40 = arith.cmpf uno, %result_3, %result_3 : f32
    %41 = arith.select %40, %result_3, %39 : f32
    %result_5, %valid_6 = gpu.shuffle  xor %41, %c8_i32, %c32_i32 : f32
    %42 = arith.cmpf ugt, %41, %result_5 : f32
    %43 = arith.select %42, %41, %result_5 : f32
    %44 = arith.cmpf uno, %result_5, %result_5 : f32
    %45 = arith.select %44, %result_5, %43 : f32
    %result_7, %valid_8 = gpu.shuffle  xor %45, %c16_i32, %c32_i32 : f32
    %46 = arith.cmpf ugt, %45, %result_7 : f32
    %47 = arith.select %46, %45, %result_7 : f32
    %48 = arith.cmpf uno, %result_7, %result_7 : f32
    %49 = arith.select %48, %result_7, %47 : f32
    %50 = arith.cmpi eq, %6, %c0 : index
    scf.if %50 {
      %51 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
      memref.store %49, %51[%5] : memref<32xf32, 3>
    }
  }
  gpu.barrier
  scf.if %9 {
    %10 = arith.subi %0, %6 : index
    %11 = arith.cmpi eq, %10, %c0 : index
    %12 = arith.subi %10, %c1 : index
    %13 = arith.divui %12, %c32 : index
    %14 = arith.addi %13, %c1 : index
    %15 = arith.select %11, %c0, %14 : index
    %16 = arith.remsi %15, %c4 : index
    %17 = arith.subi %15, %16 : index
    %18 = arith.muli %17, %c32 : index
    %19 = arith.addi %6, %18 : index
    %20 = arith.muli %8, %0 : index
    %21 = arith.muli %1, %2 : index
    %22 = arith.muli %21, %0 : index
    %23 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%22], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %24 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %25 = scf.for %arg5 = %6 to %19 step %c128 iter_args(%arg6 = %cst) -> (f32) {
      %38 = arith.addi %arg5, %c32 : index
      %39 = arith.addi %arg5, %c64 : index
      %40 = arith.addi %arg5, %c96 : index
      %41 = arith.addi %20, %arg5 : index
      %42 = arith.addi %20, %38 : index
      %43 = arith.addi %20, %39 : index
      %44 = arith.addi %20, %40 : index
      %45 = arith.divui %41, %0 : index
      %46 = arith.remui %45, %2 : index
      %47 = arith.divui %45, %2 : index
      %48 = arith.divui %42, %0 : index
      %49 = arith.remui %48, %2 : index
      %50 = arith.divui %48, %2 : index
      %51 = arith.divui %43, %0 : index
      %52 = arith.remui %51, %2 : index
      %53 = arith.divui %51, %2 : index
      %54 = arith.divui %44, %0 : index
      %55 = arith.remui %54, %2 : index
      %56 = arith.divui %54, %2 : index
      %57 = memref.load %23[%41] : memref<?xf32, "gpu">
      %58 = memref.load %23[%42] : memref<?xf32, "gpu">
      %59 = memref.load %23[%43] : memref<?xf32, "gpu">
      %60 = memref.load %23[%44] : memref<?xf32, "gpu">
      %61 = arith.muli %47, %2 : index
      %62 = arith.addi %61, %46 : index
      %63 = arith.muli %50, %2 : index
      %64 = arith.addi %63, %49 : index
      %65 = arith.muli %53, %2 : index
      %66 = arith.addi %65, %52 : index
      %67 = arith.muli %56, %2 : index
      %68 = arith.addi %67, %55 : index
      %69 = arith.remui %62, %c8 : index
      %70 = arith.remui %64, %c8 : index
      %71 = arith.remui %66, %c8 : index
      %72 = arith.remui %68, %c8 : index
      %73 = memref.load %24[%69] : memref<32xf32, 3>
      %74 = memref.load %24[%70] : memref<32xf32, 3>
      %75 = memref.load %24[%71] : memref<32xf32, 3>
      %76 = memref.load %24[%72] : memref<32xf32, 3>
      %77 = arith.subf %57, %73 : f32
      %78 = arith.subf %58, %74 : f32
      %79 = arith.subf %59, %75 : f32
      %80 = arith.subf %60, %76 : f32
      %81 = math.exp %77 : f32
      %82 = math.exp %78 : f32
      %83 = math.exp %79 : f32
      %84 = math.exp %80 : f32
      %85 = arith.addf %arg6, %81 : f32
      %86 = arith.addf %85, %82 : f32
      %87 = arith.addf %86, %83 : f32
      %88 = arith.addf %87, %84 : f32
      scf.yield %88 : f32
    }
    %26 = arith.muli %8, %0 : index
    %27 = arith.muli %1, %2 : index
    %28 = arith.muli %27, %0 : index
    %29 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%28], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %30 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %31 = scf.for %arg5 = %19 to %0 step %c32 iter_args(%arg6 = %25) -> (f32) {
      %38 = arith.addi %26, %arg5 : index
      %39 = arith.divui %38, %0 : index
      %40 = arith.remui %39, %2 : index
      %41 = arith.divui %39, %2 : index
      %42 = memref.load %29[%38] : memref<?xf32, "gpu">
      %43 = arith.muli %41, %2 : index
      %44 = arith.addi %43, %40 : index
      %45 = arith.remui %44, %c8 : index
      %46 = memref.load %30[%45] : memref<32xf32, 3>
      %47 = arith.subf %42, %46 : f32
      %48 = math.exp %47 : f32
      %49 = arith.addf %arg6, %48 : f32
      scf.yield %49 : f32
    }
    %result, %valid = gpu.shuffle  xor %31, %c1_i32, %c32_i32 : f32
    %32 = arith.addf %31, %result : f32
    %result_1, %valid_2 = gpu.shuffle  xor %32, %c2_i32, %c32_i32 : f32
    %33 = arith.addf %32, %result_1 : f32
    %result_3, %valid_4 = gpu.shuffle  xor %33, %c4_i32, %c32_i32 : f32
    %34 = arith.addf %33, %result_3 : f32
    %result_5, %valid_6 = gpu.shuffle  xor %34, %c8_i32, %c32_i32 : f32
    %35 = arith.addf %34, %result_5 : f32
    %result_7, %valid_8 = gpu.shuffle  xor %35, %c16_i32, %c32_i32 : f32
    %36 = arith.addf %35, %result_7 : f32
    %37 = arith.cmpi eq, %6, %c0 : index
    scf.if %37 {
      %38 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
      memref.store %36, %38[%5] : memref<32xf32, 3>
    }
  }
  gpu.barrier
  scf.if %9 {
    %10 = arith.subi %0, %6 : index
    %11 = arith.cmpi eq, %10, %c0 : index
    %12 = arith.subi %10, %c1 : index
    %13 = arith.divui %12, %c32 : index
    %14 = arith.addi %13, %c1 : index
    %15 = arith.select %11, %c0, %14 : index
    %16 = arith.remsi %15, %c4 : index
    %17 = arith.subi %15, %16 : index
    %18 = arith.muli %17, %c32 : index
    %19 = arith.addi %6, %18 : index
    %20 = arith.muli %8, %0 : index
    %21 = arith.muli %1, %2 : index
    %22 = arith.muli %21, %0 : index
    %23 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%22], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %24 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %25 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %26 = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%22], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    scf.for %arg5 = %6 to %19 step %c128 {
      %34 = arith.addi %arg5, %c32 : index
      %35 = arith.addi %arg5, %c64 : index
      %36 = arith.addi %arg5, %c96 : index
      %37 = arith.addi %20, %arg5 : index
      %38 = arith.addi %20, %34 : index
      %39 = arith.addi %20, %35 : index
      %40 = arith.addi %20, %36 : index
      %41 = arith.divui %37, %0 : index
      %42 = arith.remui %41, %2 : index
      %43 = arith.divui %41, %2 : index
      %44 = arith.divui %38, %0 : index
      %45 = arith.remui %44, %2 : index
      %46 = arith.divui %44, %2 : index
      %47 = arith.divui %39, %0 : index
      %48 = arith.remui %47, %2 : index
      %49 = arith.divui %47, %2 : index
      %50 = arith.divui %40, %0 : index
      %51 = arith.remui %50, %2 : index
      %52 = arith.divui %50, %2 : index
      %53 = memref.load %23[%37] : memref<?xf32, "gpu">
      %54 = memref.load %23[%38] : memref<?xf32, "gpu">
      %55 = memref.load %23[%39] : memref<?xf32, "gpu">
      %56 = memref.load %23[%40] : memref<?xf32, "gpu">
      %57 = arith.muli %43, %2 : index
      %58 = arith.addi %57, %42 : index
      %59 = arith.muli %46, %2 : index
      %60 = arith.addi %59, %45 : index
      %61 = arith.muli %49, %2 : index
      %62 = arith.addi %61, %48 : index
      %63 = arith.muli %52, %2 : index
      %64 = arith.addi %63, %51 : index
      %65 = arith.remui %58, %c8 : index
      %66 = arith.remui %60, %c8 : index
      %67 = arith.remui %62, %c8 : index
      %68 = arith.remui %64, %c8 : index
      %69 = memref.load %24[%65] : memref<32xf32, 3>
      %70 = memref.load %24[%66] : memref<32xf32, 3>
      %71 = memref.load %24[%67] : memref<32xf32, 3>
      %72 = memref.load %24[%68] : memref<32xf32, 3>
      %73 = arith.subf %53, %69 : f32
      %74 = arith.subf %54, %70 : f32
      %75 = arith.subf %55, %71 : f32
      %76 = arith.subf %56, %72 : f32
      %77 = math.exp %73 : f32
      %78 = math.exp %74 : f32
      %79 = math.exp %75 : f32
      %80 = math.exp %76 : f32
      %81 = memref.load %25[%65] : memref<32xf32, 3>
      %82 = memref.load %25[%66] : memref<32xf32, 3>
      %83 = memref.load %25[%67] : memref<32xf32, 3>
      %84 = memref.load %25[%68] : memref<32xf32, 3>
      %85 = arith.divf %77, %81 : f32
      %86 = arith.divf %78, %82 : f32
      %87 = arith.divf %79, %83 : f32
      %88 = arith.divf %80, %84 : f32
      memref.store %85, %26[%37] : memref<?xf32, "gpu">
      memref.store %86, %26[%38] : memref<?xf32, "gpu">
      memref.store %87, %26[%39] : memref<?xf32, "gpu">
      memref.store %88, %26[%40] : memref<?xf32, "gpu">
    }
    %27 = arith.muli %8, %0 : index
    %28 = arith.muli %1, %2 : index
    %29 = arith.muli %28, %0 : index
    %30 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%29], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %31 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %32 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %33 = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%29], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    scf.for %arg5 = %19 to %0 step %c32 {
      %34 = arith.addi %27, %arg5 : index
      %35 = arith.divui %34, %0 : index
      %36 = arith.remui %35, %2 : index
      %37 = arith.divui %35, %2 : index
      %38 = memref.load %30[%34] : memref<?xf32, "gpu">
      %39 = arith.muli %37, %2 : index
      %40 = arith.addi %39, %36 : index
      %41 = arith.remui %40, %c8 : index
      %42 = memref.load %31[%41] : memref<32xf32, 3>
      %43 = arith.subf %38, %42 : f32
      %44 = math.exp %43 : f32
      %45 = memref.load %32[%41] : memref<32xf32, 3>
      %46 = arith.divf %44, %45 : f32
      memref.store %46, %33[%34] : memref<?xf32, "gpu">
    }
  }
  gpu.return
}

// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kStitch_divide__13_1_0___1w1r(%arg0: memref<?x?x?xf32, "gpu">, %arg1: index, %arg2: memref<?x?x?xf32, "gpu">) workgroup(%arg3 : memref<32xf32, 3>, %arg4 : memref<32xf32, 3>) kernel {
    %cst = arith.constant -0.000000e+00 : f32
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c4_i32 = arith.constant 4 : i32
    %c2_i32 = arith.constant 2 : i32
    %c32_i32 = arith.constant 32 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_0 = arith.constant 0xFF800000 : f32
    %c128 = arith.constant 128 : index
    %c96 = arith.constant 96 : index
    %c64 = arith.constant 64 : index
    %c4 = arith.constant 4 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %0 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
    %1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
    %2 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
    %3 = gpu.block_id  x
    %4 = gpu.thread_id  x
    %5 = arith.divui %4, %c32 : index
    %6 = arith.remui %4, %c32 : index
    %7 = arith.muli %3, %c8 : index
    %8 = arith.addi %7, %5 : index
    %9 = arith.cmpi slt, %8, %arg1 : index
    scf.if %9 {
      %10 = arith.subi %0, %6 : index
      %11 = arith.cmpi eq, %10, %c0 : index
      %12 = arith.subi %10, %c1 : index
      %13 = arith.divui %12, %c32 : index
      %14 = arith.addi %13, %c1 : index
      %15 = arith.select %11, %c0, %14 : index
      %16 = arith.remsi %15, %c4 : index
      %17 = arith.subi %15, %16 : index
      %18 = arith.muli %17, %c32 : index
      %19 = arith.addi %6, %18 : index
      %20 = arith.muli %8, %0 : index
      %21 = arith.muli %1, %2 : index
      %22 = arith.muli %21, %0 : index
      %23 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%22], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
      %24 = scf.for %arg5 = %6 to %19 step %c128 iter_args(%arg6 = %cst_0) -> (f32) {
        %47 = arith.addi %arg5, %c32 : index
        %48 = arith.addi %arg5, %c64 : index
        %49 = arith.addi %arg5, %c96 : index
        %50 = arith.addi %20, %arg5 : index
        %51 = arith.addi %20, %47 : index
        %52 = arith.addi %20, %48 : index
        %53 = arith.addi %20, %49 : index
        %54 = memref.load %23[%50] : memref<?xf32, "gpu">
        %55 = memref.load %23[%51] : memref<?xf32, "gpu">
        %56 = memref.load %23[%52] : memref<?xf32, "gpu">
        %57 = memref.load %23[%53] : memref<?xf32, "gpu">
        %58 = arith.cmpf ugt, %arg6, %54 : f32
        %59 = arith.select %58, %arg6, %54 : f32
        %60 = arith.cmpf uno, %54, %54 : f32
        %61 = arith.select %60, %54, %59 : f32
        %62 = arith.cmpf ugt, %61, %55 : f32
        %63 = arith.select %62, %61, %55 : f32
        %64 = arith.cmpf uno, %55, %55 : f32
        %65 = arith.select %64, %55, %63 : f32
        %66 = arith.cmpf ugt, %65, %56 : f32
        %67 = arith.select %66, %65, %56 : f32
        %68 = arith.cmpf uno, %56, %56 : f32
        %69 = arith.select %68, %56, %67 : f32
        %70 = arith.cmpf ugt, %69, %57 : f32
        %71 = arith.select %70, %69, %57 : f32
        %72 = arith.cmpf uno, %57, %57 : f32
        %73 = arith.select %72, %57, %71 : f32
        scf.yield %73 : f32
      }
      %25 = scf.for %arg5 = %19 to %0 step %c32 iter_args(%arg6 = %24) -> (f32) {
        %47 = arith.addi %20, %arg5 : index
        %48 = memref.load %23[%47] : memref<?xf32, "gpu">
        %49 = arith.cmpf ugt, %arg6, %48 : f32
        %50 = arith.select %49, %arg6, %48 : f32
        %51 = arith.cmpf uno, %48, %48 : f32
        %52 = arith.select %51, %48, %50 : f32
        scf.yield %52 : f32
      }
      %result, %valid = gpu.shuffle  xor %25, %c1_i32, %c32_i32 : f32
      %26 = arith.cmpf ugt, %25, %result : f32
      %27 = arith.select %26, %25, %result : f32
      %28 = arith.cmpf uno, %result, %result : f32
      %29 = arith.select %28, %result, %27 : f32
      %result_1, %valid_2 = gpu.shuffle  xor %29, %c2_i32, %c32_i32 : f32
      %30 = arith.cmpf ugt, %29, %result_1 : f32
      %31 = arith.select %30, %29, %result_1 : f32
      %32 = arith.cmpf uno, %result_1, %result_1 : f32
      %33 = arith.select %32, %result_1, %31 : f32
      %result_3, %valid_4 = gpu.shuffle  xor %33, %c4_i32, %c32_i32 : f32
      %34 = arith.cmpf ugt, %33, %result_3 : f32
      %35 = arith.select %34, %33, %result_3 : f32
      %36 = arith.cmpf uno, %result_3, %result_3 : f32
      %37 = arith.select %36, %result_3, %35 : f32
      %result_5, %valid_6 = gpu.shuffle  xor %37, %c8_i32, %c32_i32 : f32
      %38 = arith.cmpf ugt, %37, %result_5 : f32
      %39 = arith.select %38, %37, %result_5 : f32
      %40 = arith.cmpf uno, %result_5, %result_5 : f32
      %41 = arith.select %40, %result_5, %39 : f32
      %result_7, %valid_8 = gpu.shuffle  xor %41, %c16_i32, %c32_i32 : f32
      %42 = arith.cmpf ugt, %41, %result_7 : f32
      %43 = arith.select %42, %41, %result_7 : f32
      %44 = arith.cmpf uno, %result_7, %result_7 : f32
      %45 = arith.select %44, %result_7, %43 : f32
      %46 = arith.cmpi eq, %6, %c0 : index
      scf.if %46 {
        %47 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        memref.store %45, %47[%5] : memref<32xf32, 3>
      }
    }
    gpu.barrier
    scf.if %9 {
      %10 = arith.subi %0, %6 : index
      %11 = arith.cmpi eq, %10, %c0 : index
      %12 = arith.subi %10, %c1 : index
      %13 = arith.divui %12, %c32 : index
      %14 = arith.addi %13, %c1 : index
      %15 = arith.select %11, %c0, %14 : index
      %16 = arith.remsi %15, %c4 : index
      %17 = arith.subi %15, %16 : index
      %18 = arith.muli %17, %c32 : index
      %19 = arith.addi %6, %18 : index
      %20 = arith.muli %8, %0 : index
      %21 = arith.muli %1, %2 : index
      %22 = arith.muli %21, %0 : index
      %23 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%22], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
      %24 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
      %25 = scf.for %arg5 = %6 to %19 step %c128 iter_args(%arg6 = %cst) -> (f32) {
        %33 = arith.addi %arg5, %c32 : index
        %34 = arith.addi %arg5, %c64 : index
        %35 = arith.addi %arg5, %c96 : index
        %36 = arith.addi %20, %arg5 : index
        %37 = arith.addi %20, %33 : index
        %38 = arith.addi %20, %34 : index
        %39 = arith.addi %20, %35 : index
        %40 = arith.divui %36, %0 : index
        %41 = arith.remui %40, %2 : index
        %42 = arith.divui %40, %2 : index
        %43 = arith.divui %37, %0 : index
        %44 = arith.remui %43, %2 : index
        %45 = arith.divui %43, %2 : index
        %46 = arith.divui %38, %0 : index
        %47 = arith.remui %46, %2 : index
        %48 = arith.divui %46, %2 : index
        %49 = arith.divui %39, %0 : index
        %50 = arith.remui %49, %2 : index
        %51 = arith.divui %49, %2 : index
        %52 = memref.load %23[%36] : memref<?xf32, "gpu">
        %53 = memref.load %23[%37] : memref<?xf32, "gpu">
        %54 = memref.load %23[%38] : memref<?xf32, "gpu">
        %55 = memref.load %23[%39] : memref<?xf32, "gpu">
        %56 = arith.muli %42, %2 : index
        %57 = arith.addi %56, %41 : index
        %58 = arith.muli %45, %2 : index
        %59 = arith.addi %58, %44 : index
        %60 = arith.muli %48, %2 : index
        %61 = arith.addi %60, %47 : index
        %62 = arith.muli %51, %2 : index
        %63 = arith.addi %62, %50 : index
        %64 = arith.remui %57, %c8 : index
        %65 = arith.remui %59, %c8 : index
        %66 = arith.remui %61, %c8 : index
        %67 = arith.remui %63, %c8 : index
        %68 = memref.load %24[%64] : memref<32xf32, 3>
        %69 = memref.load %24[%65] : memref<32xf32, 3>
        %70 = memref.load %24[%66] : memref<32xf32, 3>
        %71 = memref.load %24[%67] : memref<32xf32, 3>
        %72 = arith.subf %52, %68 : f32
        %73 = arith.subf %53, %69 : f32
        %74 = arith.subf %54, %70 : f32
        %75 = arith.subf %55, %71 : f32
        %76 = math.exp %72 : f32
        %77 = math.exp %73 : f32
        %78 = math.exp %74 : f32
        %79 = math.exp %75 : f32
        %80 = arith.addf %arg6, %76 : f32
        %81 = arith.addf %80, %77 : f32
        %82 = arith.addf %81, %78 : f32
        %83 = arith.addf %82, %79 : f32
        scf.yield %83 : f32
      }
      %26 = scf.for %arg5 = %19 to %0 step %c32 iter_args(%arg6 = %25) -> (f32) {
        %33 = arith.addi %20, %arg5 : index
        %34 = arith.divui %33, %0 : index
        %35 = arith.remui %34, %2 : index
        %36 = arith.divui %34, %2 : index
        %37 = memref.load %23[%33] : memref<?xf32, "gpu">
        %38 = arith.muli %36, %2 : index
        %39 = arith.addi %38, %35 : index
        %40 = arith.remui %39, %c8 : index
        %41 = memref.load %24[%40] : memref<32xf32, 3>
        %42 = arith.subf %37, %41 : f32
        %43 = math.exp %42 : f32
        %44 = arith.addf %arg6, %43 : f32
        scf.yield %44 : f32
      }
      %result, %valid = gpu.shuffle  xor %26, %c1_i32, %c32_i32 : f32
      %27 = arith.addf %26, %result : f32
      %result_1, %valid_2 = gpu.shuffle  xor %27, %c2_i32, %c32_i32 : f32
      %28 = arith.addf %27, %result_1 : f32
      %result_3, %valid_4 = gpu.shuffle  xor %28, %c4_i32, %c32_i32 : f32
      %29 = arith.addf %28, %result_3 : f32
      %result_5, %valid_6 = gpu.shuffle  xor %29, %c8_i32, %c32_i32 : f32
      %30 = arith.addf %29, %result_5 : f32
      %result_7, %valid_8 = gpu.shuffle  xor %30, %c16_i32, %c32_i32 : f32
      %31 = arith.addf %30, %result_7 : f32
      %32 = arith.cmpi eq, %6, %c0 : index
      scf.if %32 {
        %33 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
        memref.store %31, %33[%5] : memref<32xf32, 3>
      }
    }
    gpu.barrier
    scf.if %9 {
      %10 = arith.subi %0, %6 : index
      %11 = arith.cmpi eq, %10, %c0 : index
      %12 = arith.subi %10, %c1 : index
      %13 = arith.divui %12, %c32 : index
      %14 = arith.addi %13, %c1 : index
      %15 = arith.select %11, %c0, %14 : index
      %16 = arith.remsi %15, %c4 : index
      %17 = arith.subi %15, %16 : index
      %18 = arith.muli %17, %c32 : index
      %19 = arith.addi %6, %18 : index
      %20 = arith.muli %8, %0 : index
      %21 = arith.muli %1, %2 : index
      %22 = arith.muli %21, %0 : index
      %23 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%22], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
      %24 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
      %25 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
      %26 = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%22], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
      scf.for %arg5 = %6 to %19 step %c128 {
        %27 = arith.addi %arg5, %c32 : index
        %28 = arith.addi %arg5, %c64 : index
        %29 = arith.addi %arg5, %c96 : index
        %30 = arith.addi %20, %arg5 : index
        %31 = arith.addi %20, %27 : index
        %32 = arith.addi %20, %28 : index
        %33 = arith.addi %20, %29 : index
        %34 = arith.divui %30, %0 : index
        %35 = arith.remui %34, %2 : index
        %36 = arith.divui %34, %2 : index
        %37 = arith.divui %31, %0 : index
        %38 = arith.remui %37, %2 : index
        %39 = arith.divui %37, %2 : index
        %40 = arith.divui %32, %0 : index
        %41 = arith.remui %40, %2 : index
        %42 = arith.divui %40, %2 : index
        %43 = arith.divui %33, %0 : index
        %44 = arith.remui %43, %2 : index
        %45 = arith.divui %43, %2 : index
        %46 = memref.load %23[%30] : memref<?xf32, "gpu">
        %47 = memref.load %23[%31] : memref<?xf32, "gpu">
        %48 = memref.load %23[%32] : memref<?xf32, "gpu">
        %49 = memref.load %23[%33] : memref<?xf32, "gpu">
        %50 = arith.muli %36, %2 : index
        %51 = arith.addi %50, %35 : index
        %52 = arith.muli %39, %2 : index
        %53 = arith.addi %52, %38 : index
        %54 = arith.muli %42, %2 : index
        %55 = arith.addi %54, %41 : index
        %56 = arith.muli %45, %2 : index
        %57 = arith.addi %56, %44 : index
        %58 = arith.remui %51, %c8 : index
        %59 = arith.remui %53, %c8 : index
        %60 = arith.remui %55, %c8 : index
        %61 = arith.remui %57, %c8 : index
        %62 = memref.load %24[%58] : memref<32xf32, 3>
        %63 = memref.load %24[%59] : memref<32xf32, 3>
        %64 = memref.load %24[%60] : memref<32xf32, 3>
        %65 = memref.load %24[%61] : memref<32xf32, 3>
        %66 = arith.subf %46, %62 : f32
        %67 = arith.subf %47, %63 : f32
        %68 = arith.subf %48, %64 : f32
        %69 = arith.subf %49, %65 : f32
        %70 = math.exp %66 : f32
        %71 = math.exp %67 : f32
        %72 = math.exp %68 : f32
        %73 = math.exp %69 : f32
        %74 = memref.load %25[%58] : memref<32xf32, 3>
        %75 = memref.load %25[%59] : memref<32xf32, 3>
        %76 = memref.load %25[%60] : memref<32xf32, 3>
        %77 = memref.load %25[%61] : memref<32xf32, 3>
        %78 = arith.divf %70, %74 : f32
        %79 = arith.divf %71, %75 : f32
        %80 = arith.divf %72, %76 : f32
        %81 = arith.divf %73, %77 : f32
        memref.store %78, %26[%30] : memref<?xf32, "gpu">
        memref.store %79, %26[%31] : memref<?xf32, "gpu">
        memref.store %80, %26[%32] : memref<?xf32, "gpu">
        memref.store %81, %26[%33] : memref<?xf32, "gpu">
      }
      scf.for %arg5 = %19 to %0 step %c32 {
        %27 = arith.addi %20, %arg5 : index
        %28 = arith.divui %27, %0 : index
        %29 = arith.remui %28, %2 : index
        %30 = arith.divui %28, %2 : index
        %31 = memref.load %23[%27] : memref<?xf32, "gpu">
        %32 = arith.muli %30, %2 : index
        %33 = arith.addi %32, %29 : index
        %34 = arith.remui %33, %c8 : index
        %35 = memref.load %24[%34] : memref<32xf32, 3>
        %36 = arith.subf %31, %35 : f32
        %37 = math.exp %36 : f32
        %38 = memref.load %25[%34] : memref<32xf32, 3>
        %39 = arith.divf %37, %38 : f32
        memref.store %39, %26[%27] : memref<?xf32, "gpu">
      }
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kStitch_divide__13_1_0___1w1r(%arg0: memref<?x?x?xf32, "gpu">, %arg1: index, %arg2: memref<?x?x?xf32, "gpu">) workgroup(%arg3 : memref<32xf32, 3>, %arg4 : memref<32xf32, 3>) kernel {
    %cst = arith.constant -0.000000e+00 : f32
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c4_i32 = arith.constant 4 : i32
    %c2_i32 = arith.constant 2 : i32
    %c32_i32 = arith.constant 32 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_0 = arith.constant 0xFF800000 : f32
    %c128 = arith.constant 128 : index
    %c96 = arith.constant 96 : index
    %c64 = arith.constant 64 : index
    %c4 = arith.constant 4 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %0 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
    %1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
    %2 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
    %3 = gpu.block_id  x
    %4 = gpu.thread_id  x
    %5 = arith.divui %4, %c32 : index
    %6 = arith.remui %4, %c32 : index
    %7 = arith.muli %3, %c8 : index
    %8 = arith.addi %7, %5 : index
    %9 = arith.cmpi slt, %8, %arg1 : index
    cf.cond_br %9, ^bb2, ^bb11
  ^bb2:  // pred: ^bb1
    %10 = arith.subi %0, %6 : index
    %11 = arith.cmpi eq, %10, %c0 : index
    %12 = arith.subi %10, %c1 : index
    %13 = arith.divui %12, %c32 : index
    %14 = arith.addi %13, %c1 : index
    %15 = arith.select %11, %c0, %14 : index
    %16 = arith.remsi %15, %c4 : index
    %17 = arith.subi %15, %16 : index
    %18 = arith.muli %17, %c32 : index
    %19 = arith.addi %6, %18 : index
    %20 = arith.muli %8, %0 : index
    %21 = arith.muli %1, %2 : index
    %22 = arith.muli %21, %0 : index
    %23 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%22], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    cf.br ^bb3(%6, %cst_0 : index, f32)
  ^bb3(%24: index, %25: f32):  // 2 preds: ^bb2, ^bb4
    %26 = arith.cmpi slt, %24, %19 : index
    cf.cond_br %26, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %27 = arith.addi %24, %c32 : index
    %28 = arith.addi %24, %c64 : index
    %29 = arith.addi %24, %c96 : index
    %30 = arith.addi %20, %24 : index
    %31 = arith.addi %20, %27 : index
    %32 = arith.addi %20, %28 : index
    %33 = arith.addi %20, %29 : index
    %34 = memref.load %23[%30] : memref<?xf32, "gpu">
    %35 = memref.load %23[%31] : memref<?xf32, "gpu">
    %36 = memref.load %23[%32] : memref<?xf32, "gpu">
    %37 = memref.load %23[%33] : memref<?xf32, "gpu">
    %38 = arith.cmpf ugt, %25, %34 : f32
    %39 = arith.select %38, %25, %34 : f32
    %40 = arith.cmpf uno, %34, %34 : f32
    %41 = arith.select %40, %34, %39 : f32
    %42 = arith.cmpf ugt, %41, %35 : f32
    %43 = arith.select %42, %41, %35 : f32
    %44 = arith.cmpf uno, %35, %35 : f32
    %45 = arith.select %44, %35, %43 : f32
    %46 = arith.cmpf ugt, %45, %36 : f32
    %47 = arith.select %46, %45, %36 : f32
    %48 = arith.cmpf uno, %36, %36 : f32
    %49 = arith.select %48, %36, %47 : f32
    %50 = arith.cmpf ugt, %49, %37 : f32
    %51 = arith.select %50, %49, %37 : f32
    %52 = arith.cmpf uno, %37, %37 : f32
    %53 = arith.select %52, %37, %51 : f32
    %54 = arith.addi %24, %c128 : index
    cf.br ^bb3(%54, %53 : index, f32)
  ^bb5:  // pred: ^bb3
    cf.br ^bb6(%19, %25 : index, f32)
  ^bb6(%55: index, %56: f32):  // 2 preds: ^bb5, ^bb7
    %57 = arith.cmpi slt, %55, %0 : index
    cf.cond_br %57, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %58 = arith.addi %20, %55 : index
    %59 = memref.load %23[%58] : memref<?xf32, "gpu">
    %60 = arith.cmpf ugt, %56, %59 : f32
    %61 = arith.select %60, %56, %59 : f32
    %62 = arith.cmpf uno, %59, %59 : f32
    %63 = arith.select %62, %59, %61 : f32
    %64 = arith.addi %55, %c32 : index
    cf.br ^bb6(%64, %63 : index, f32)
  ^bb8:  // pred: ^bb6
    %result, %valid = gpu.shuffle  xor %56, %c1_i32, %c32_i32 : f32
    %65 = arith.cmpf ugt, %56, %result : f32
    %66 = arith.select %65, %56, %result : f32
    %67 = arith.cmpf uno, %result, %result : f32
    %68 = arith.select %67, %result, %66 : f32
    %result_1, %valid_2 = gpu.shuffle  xor %68, %c2_i32, %c32_i32 : f32
    %69 = arith.cmpf ugt, %68, %result_1 : f32
    %70 = arith.select %69, %68, %result_1 : f32
    %71 = arith.cmpf uno, %result_1, %result_1 : f32
    %72 = arith.select %71, %result_1, %70 : f32
    %result_3, %valid_4 = gpu.shuffle  xor %72, %c4_i32, %c32_i32 : f32
    %73 = arith.cmpf ugt, %72, %result_3 : f32
    %74 = arith.select %73, %72, %result_3 : f32
    %75 = arith.cmpf uno, %result_3, %result_3 : f32
    %76 = arith.select %75, %result_3, %74 : f32
    %result_5, %valid_6 = gpu.shuffle  xor %76, %c8_i32, %c32_i32 : f32
    %77 = arith.cmpf ugt, %76, %result_5 : f32
    %78 = arith.select %77, %76, %result_5 : f32
    %79 = arith.cmpf uno, %result_5, %result_5 : f32
    %80 = arith.select %79, %result_5, %78 : f32
    %result_7, %valid_8 = gpu.shuffle  xor %80, %c16_i32, %c32_i32 : f32
    %81 = arith.cmpf ugt, %80, %result_7 : f32
    %82 = arith.select %81, %80, %result_7 : f32
    %83 = arith.cmpf uno, %result_7, %result_7 : f32
    %84 = arith.select %83, %result_7, %82 : f32
    %85 = arith.cmpi eq, %6, %c0 : index
    cf.cond_br %85, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %86 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    memref.store %84, %86[%5] : memref<32xf32, 3>
    cf.br ^bb10
  ^bb10:  // 2 preds: ^bb8, ^bb9
    cf.br ^bb11
  ^bb11:  // 2 preds: ^bb1, ^bb10
    gpu.barrier
    cf.cond_br %9, ^bb12, ^bb21
  ^bb12:  // pred: ^bb11
    %87 = arith.subi %0, %6 : index
    %88 = arith.cmpi eq, %87, %c0 : index
    %89 = arith.subi %87, %c1 : index
    %90 = arith.divui %89, %c32 : index
    %91 = arith.addi %90, %c1 : index
    %92 = arith.select %88, %c0, %91 : index
    %93 = arith.remsi %92, %c4 : index
    %94 = arith.subi %92, %93 : index
    %95 = arith.muli %94, %c32 : index
    %96 = arith.addi %6, %95 : index
    %97 = arith.muli %8, %0 : index
    %98 = arith.muli %1, %2 : index
    %99 = arith.muli %98, %0 : index
    %100 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%99], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %101 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    cf.br ^bb13(%6, %cst : index, f32)
  ^bb13(%102: index, %103: f32):  // 2 preds: ^bb12, ^bb14
    %104 = arith.cmpi slt, %102, %96 : index
    cf.cond_br %104, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %105 = arith.addi %102, %c32 : index
    %106 = arith.addi %102, %c64 : index
    %107 = arith.addi %102, %c96 : index
    %108 = arith.addi %97, %102 : index
    %109 = arith.addi %97, %105 : index
    %110 = arith.addi %97, %106 : index
    %111 = arith.addi %97, %107 : index
    %112 = arith.divui %108, %0 : index
    %113 = arith.remui %112, %2 : index
    %114 = arith.divui %112, %2 : index
    %115 = arith.divui %109, %0 : index
    %116 = arith.remui %115, %2 : index
    %117 = arith.divui %115, %2 : index
    %118 = arith.divui %110, %0 : index
    %119 = arith.remui %118, %2 : index
    %120 = arith.divui %118, %2 : index
    %121 = arith.divui %111, %0 : index
    %122 = arith.remui %121, %2 : index
    %123 = arith.divui %121, %2 : index
    %124 = memref.load %100[%108] : memref<?xf32, "gpu">
    %125 = memref.load %100[%109] : memref<?xf32, "gpu">
    %126 = memref.load %100[%110] : memref<?xf32, "gpu">
    %127 = memref.load %100[%111] : memref<?xf32, "gpu">
    %128 = arith.muli %114, %2 : index
    %129 = arith.addi %128, %113 : index
    %130 = arith.muli %117, %2 : index
    %131 = arith.addi %130, %116 : index
    %132 = arith.muli %120, %2 : index
    %133 = arith.addi %132, %119 : index
    %134 = arith.muli %123, %2 : index
    %135 = arith.addi %134, %122 : index
    %136 = arith.remui %129, %c8 : index
    %137 = arith.remui %131, %c8 : index
    %138 = arith.remui %133, %c8 : index
    %139 = arith.remui %135, %c8 : index
    %140 = memref.load %101[%136] : memref<32xf32, 3>
    %141 = memref.load %101[%137] : memref<32xf32, 3>
    %142 = memref.load %101[%138] : memref<32xf32, 3>
    %143 = memref.load %101[%139] : memref<32xf32, 3>
    %144 = arith.subf %124, %140 : f32
    %145 = arith.subf %125, %141 : f32
    %146 = arith.subf %126, %142 : f32
    %147 = arith.subf %127, %143 : f32
    %148 = math.exp %144 : f32
    %149 = math.exp %145 : f32
    %150 = math.exp %146 : f32
    %151 = math.exp %147 : f32
    %152 = arith.addf %103, %148 : f32
    %153 = arith.addf %152, %149 : f32
    %154 = arith.addf %153, %150 : f32
    %155 = arith.addf %154, %151 : f32
    %156 = arith.addi %102, %c128 : index
    cf.br ^bb13(%156, %155 : index, f32)
  ^bb15:  // pred: ^bb13
    cf.br ^bb16(%96, %103 : index, f32)
  ^bb16(%157: index, %158: f32):  // 2 preds: ^bb15, ^bb17
    %159 = arith.cmpi slt, %157, %0 : index
    cf.cond_br %159, ^bb17, ^bb18
  ^bb17:  // pred: ^bb16
    %160 = arith.addi %97, %157 : index
    %161 = arith.divui %160, %0 : index
    %162 = arith.remui %161, %2 : index
    %163 = arith.divui %161, %2 : index
    %164 = memref.load %100[%160] : memref<?xf32, "gpu">
    %165 = arith.muli %163, %2 : index
    %166 = arith.addi %165, %162 : index
    %167 = arith.remui %166, %c8 : index
    %168 = memref.load %101[%167] : memref<32xf32, 3>
    %169 = arith.subf %164, %168 : f32
    %170 = math.exp %169 : f32
    %171 = arith.addf %158, %170 : f32
    %172 = arith.addi %157, %c32 : index
    cf.br ^bb16(%172, %171 : index, f32)
  ^bb18:  // pred: ^bb16
    %result_9, %valid_10 = gpu.shuffle  xor %158, %c1_i32, %c32_i32 : f32
    %173 = arith.addf %158, %result_9 : f32
    %result_11, %valid_12 = gpu.shuffle  xor %173, %c2_i32, %c32_i32 : f32
    %174 = arith.addf %173, %result_11 : f32
    %result_13, %valid_14 = gpu.shuffle  xor %174, %c4_i32, %c32_i32 : f32
    %175 = arith.addf %174, %result_13 : f32
    %result_15, %valid_16 = gpu.shuffle  xor %175, %c8_i32, %c32_i32 : f32
    %176 = arith.addf %175, %result_15 : f32
    %result_17, %valid_18 = gpu.shuffle  xor %176, %c16_i32, %c32_i32 : f32
    %177 = arith.addf %176, %result_17 : f32
    %178 = arith.cmpi eq, %6, %c0 : index
    cf.cond_br %178, ^bb19, ^bb20
  ^bb19:  // pred: ^bb18
    %179 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    memref.store %177, %179[%5] : memref<32xf32, 3>
    cf.br ^bb20
  ^bb20:  // 2 preds: ^bb18, ^bb19
    cf.br ^bb21
  ^bb21:  // 2 preds: ^bb11, ^bb20
    gpu.barrier
    cf.cond_br %9, ^bb22, ^bb29
  ^bb22:  // pred: ^bb21
    %180 = arith.subi %0, %6 : index
    %181 = arith.cmpi eq, %180, %c0 : index
    %182 = arith.subi %180, %c1 : index
    %183 = arith.divui %182, %c32 : index
    %184 = arith.addi %183, %c1 : index
    %185 = arith.select %181, %c0, %184 : index
    %186 = arith.remsi %185, %c4 : index
    %187 = arith.subi %185, %186 : index
    %188 = arith.muli %187, %c32 : index
    %189 = arith.addi %6, %188 : index
    %190 = arith.muli %8, %0 : index
    %191 = arith.muli %1, %2 : index
    %192 = arith.muli %191, %0 : index
    %193 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%192], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %194 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %195 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %196 = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%192], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    cf.br ^bb23(%6 : index)
  ^bb23(%197: index):  // 2 preds: ^bb22, ^bb24
    %198 = arith.cmpi slt, %197, %189 : index
    cf.cond_br %198, ^bb24, ^bb25
  ^bb24:  // pred: ^bb23
    %199 = arith.addi %197, %c32 : index
    %200 = arith.addi %197, %c64 : index
    %201 = arith.addi %197, %c96 : index
    %202 = arith.addi %190, %197 : index
    %203 = arith.addi %190, %199 : index
    %204 = arith.addi %190, %200 : index
    %205 = arith.addi %190, %201 : index
    %206 = arith.divui %202, %0 : index
    %207 = arith.remui %206, %2 : index
    %208 = arith.divui %206, %2 : index
    %209 = arith.divui %203, %0 : index
    %210 = arith.remui %209, %2 : index
    %211 = arith.divui %209, %2 : index
    %212 = arith.divui %204, %0 : index
    %213 = arith.remui %212, %2 : index
    %214 = arith.divui %212, %2 : index
    %215 = arith.divui %205, %0 : index
    %216 = arith.remui %215, %2 : index
    %217 = arith.divui %215, %2 : index
    %218 = memref.load %193[%202] : memref<?xf32, "gpu">
    %219 = memref.load %193[%203] : memref<?xf32, "gpu">
    %220 = memref.load %193[%204] : memref<?xf32, "gpu">
    %221 = memref.load %193[%205] : memref<?xf32, "gpu">
    %222 = arith.muli %208, %2 : index
    %223 = arith.addi %222, %207 : index
    %224 = arith.muli %211, %2 : index
    %225 = arith.addi %224, %210 : index
    %226 = arith.muli %214, %2 : index
    %227 = arith.addi %226, %213 : index
    %228 = arith.muli %217, %2 : index
    %229 = arith.addi %228, %216 : index
    %230 = arith.remui %223, %c8 : index
    %231 = arith.remui %225, %c8 : index
    %232 = arith.remui %227, %c8 : index
    %233 = arith.remui %229, %c8 : index
    %234 = memref.load %194[%230] : memref<32xf32, 3>
    %235 = memref.load %194[%231] : memref<32xf32, 3>
    %236 = memref.load %194[%232] : memref<32xf32, 3>
    %237 = memref.load %194[%233] : memref<32xf32, 3>
    %238 = arith.subf %218, %234 : f32
    %239 = arith.subf %219, %235 : f32
    %240 = arith.subf %220, %236 : f32
    %241 = arith.subf %221, %237 : f32
    %242 = math.exp %238 : f32
    %243 = math.exp %239 : f32
    %244 = math.exp %240 : f32
    %245 = math.exp %241 : f32
    %246 = memref.load %195[%230] : memref<32xf32, 3>
    %247 = memref.load %195[%231] : memref<32xf32, 3>
    %248 = memref.load %195[%232] : memref<32xf32, 3>
    %249 = memref.load %195[%233] : memref<32xf32, 3>
    %250 = arith.divf %242, %246 : f32
    %251 = arith.divf %243, %247 : f32
    %252 = arith.divf %244, %248 : f32
    %253 = arith.divf %245, %249 : f32
    memref.store %250, %196[%202] : memref<?xf32, "gpu">
    memref.store %251, %196[%203] : memref<?xf32, "gpu">
    memref.store %252, %196[%204] : memref<?xf32, "gpu">
    memref.store %253, %196[%205] : memref<?xf32, "gpu">
    %254 = arith.addi %197, %c128 : index
    cf.br ^bb23(%254 : index)
  ^bb25:  // pred: ^bb23
    cf.br ^bb26(%189 : index)
  ^bb26(%255: index):  // 2 preds: ^bb25, ^bb27
    %256 = arith.cmpi slt, %255, %0 : index
    cf.cond_br %256, ^bb27, ^bb28
  ^bb27:  // pred: ^bb26
    %257 = arith.addi %190, %255 : index
    %258 = arith.divui %257, %0 : index
    %259 = arith.remui %258, %2 : index
    %260 = arith.divui %258, %2 : index
    %261 = memref.load %193[%257] : memref<?xf32, "gpu">
    %262 = arith.muli %260, %2 : index
    %263 = arith.addi %262, %259 : index
    %264 = arith.remui %263, %c8 : index
    %265 = memref.load %194[%264] : memref<32xf32, 3>
    %266 = arith.subf %261, %265 : f32
    %267 = math.exp %266 : f32
    %268 = memref.load %195[%264] : memref<32xf32, 3>
    %269 = arith.divf %267, %268 : f32
    memref.store %269, %196[%257] : memref<?xf32, "gpu">
    %270 = arith.addi %255, %c32 : index
    cf.br ^bb26(%270 : index)
  ^bb28:  // pred: ^bb26
    cf.br ^bb29
  ^bb29:  // 2 preds: ^bb21, ^bb28
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kStitch_divide__13_1_0___1w1r(%arg0: memref<?x?x?xf32, "gpu">, %arg1: index, %arg2: memref<?x?x?xf32, "gpu">) workgroup(%arg3 : memref<32xf32, 3>, %arg4 : memref<32xf32, 3>) kernel {
    %cst = arith.constant -0.000000e+00 : f32
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c4_i32 = arith.constant 4 : i32
    %c2_i32 = arith.constant 2 : i32
    %c32_i32 = arith.constant 32 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_0 = arith.constant 0xFF800000 : f32
    %c128 = arith.constant 128 : index
    %c96 = arith.constant 96 : index
    %c64 = arith.constant 64 : index
    %c4 = arith.constant 4 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %0 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
    %1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
    %2 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
    %3 = gpu.block_id  x
    %4 = gpu.thread_id  x
    %5 = arith.divui %4, %c32 : index
    %6 = arith.remui %4, %c32 : index
    %7 = arith.muli %3, %c8 : index
    %8 = arith.addi %7, %5 : index
    %9 = arith.cmpi slt, %8, %arg1 : index
    cf.cond_br %9, ^bb2, ^bb11
  ^bb2:  // pred: ^bb1
    %10 = arith.subi %0, %6 : index
    %11 = arith.cmpi eq, %10, %c0 : index
    %12 = arith.subi %10, %c1 : index
    %13 = arith.divui %12, %c32 : index
    %14 = arith.addi %13, %c1 : index
    %15 = arith.select %11, %c0, %14 : index
    %16 = arith.remsi %15, %c4 : index
    %17 = arith.subi %15, %16 : index
    %18 = arith.muli %17, %c32 : index
    %19 = arith.addi %6, %18 : index
    %20 = arith.muli %8, %0 : index
    %21 = arith.muli %1, %2 : index
    %22 = arith.muli %21, %0 : index
    %23 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%22], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    cf.br ^bb3(%6, %cst_0 : index, f32)
  ^bb3(%24: index, %25: f32):  // 2 preds: ^bb2, ^bb4
    %26 = arith.cmpi slt, %24, %19 : index
    cf.cond_br %26, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %27 = arith.addi %24, %c32 : index
    %28 = arith.addi %24, %c64 : index
    %29 = arith.addi %24, %c96 : index
    %30 = arith.addi %20, %24 : index
    %31 = arith.addi %20, %27 : index
    %32 = arith.addi %20, %28 : index
    %33 = arith.addi %20, %29 : index
    %34 = memref.load %23[%30] : memref<?xf32, "gpu">
    %35 = memref.load %23[%31] : memref<?xf32, "gpu">
    %36 = memref.load %23[%32] : memref<?xf32, "gpu">
    %37 = memref.load %23[%33] : memref<?xf32, "gpu">
    %38 = arith.cmpf ugt, %25, %34 : f32
    %39 = arith.select %38, %25, %34 : f32
    %40 = arith.cmpf uno, %34, %34 : f32
    %41 = arith.select %40, %34, %39 : f32
    %42 = arith.cmpf ugt, %41, %35 : f32
    %43 = arith.select %42, %41, %35 : f32
    %44 = arith.cmpf uno, %35, %35 : f32
    %45 = arith.select %44, %35, %43 : f32
    %46 = arith.cmpf ugt, %45, %36 : f32
    %47 = arith.select %46, %45, %36 : f32
    %48 = arith.cmpf uno, %36, %36 : f32
    %49 = arith.select %48, %36, %47 : f32
    %50 = arith.cmpf ugt, %49, %37 : f32
    %51 = arith.select %50, %49, %37 : f32
    %52 = arith.cmpf uno, %37, %37 : f32
    %53 = arith.select %52, %37, %51 : f32
    %54 = arith.addi %24, %c128 : index
    cf.br ^bb3(%54, %53 : index, f32)
  ^bb5:  // pred: ^bb3
    cf.br ^bb6(%19, %25 : index, f32)
  ^bb6(%55: index, %56: f32):  // 2 preds: ^bb5, ^bb7
    %57 = arith.cmpi slt, %55, %0 : index
    cf.cond_br %57, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %58 = arith.addi %20, %55 : index
    %59 = memref.load %23[%58] : memref<?xf32, "gpu">
    %60 = arith.cmpf ugt, %56, %59 : f32
    %61 = arith.select %60, %56, %59 : f32
    %62 = arith.cmpf uno, %59, %59 : f32
    %63 = arith.select %62, %59, %61 : f32
    %64 = arith.addi %55, %c32 : index
    cf.br ^bb6(%64, %63 : index, f32)
  ^bb8:  // pred: ^bb6
    %result, %valid = gpu.shuffle  xor %56, %c1_i32, %c32_i32 : f32
    %65 = arith.cmpf ugt, %56, %result : f32
    %66 = arith.select %65, %56, %result : f32
    %67 = arith.cmpf uno, %result, %result : f32
    %68 = arith.select %67, %result, %66 : f32
    %result_1, %valid_2 = gpu.shuffle  xor %68, %c2_i32, %c32_i32 : f32
    %69 = arith.cmpf ugt, %68, %result_1 : f32
    %70 = arith.select %69, %68, %result_1 : f32
    %71 = arith.cmpf uno, %result_1, %result_1 : f32
    %72 = arith.select %71, %result_1, %70 : f32
    %result_3, %valid_4 = gpu.shuffle  xor %72, %c4_i32, %c32_i32 : f32
    %73 = arith.cmpf ugt, %72, %result_3 : f32
    %74 = arith.select %73, %72, %result_3 : f32
    %75 = arith.cmpf uno, %result_3, %result_3 : f32
    %76 = arith.select %75, %result_3, %74 : f32
    %result_5, %valid_6 = gpu.shuffle  xor %76, %c8_i32, %c32_i32 : f32
    %77 = arith.cmpf ugt, %76, %result_5 : f32
    %78 = arith.select %77, %76, %result_5 : f32
    %79 = arith.cmpf uno, %result_5, %result_5 : f32
    %80 = arith.select %79, %result_5, %78 : f32
    %result_7, %valid_8 = gpu.shuffle  xor %80, %c16_i32, %c32_i32 : f32
    %81 = arith.cmpf ugt, %80, %result_7 : f32
    %82 = arith.select %81, %80, %result_7 : f32
    %83 = arith.cmpf uno, %result_7, %result_7 : f32
    %84 = arith.select %83, %result_7, %82 : f32
    %85 = arith.cmpi eq, %6, %c0 : index
    cf.cond_br %85, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %86 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    memref.store %84, %86[%5] : memref<32xf32, 3>
    cf.br ^bb10
  ^bb10:  // 2 preds: ^bb8, ^bb9
    cf.br ^bb11
  ^bb11:  // 2 preds: ^bb1, ^bb10
    gpu.barrier
    cf.cond_br %9, ^bb12, ^bb21
  ^bb12:  // pred: ^bb11
    %87 = arith.subi %0, %6 : index
    %88 = arith.cmpi eq, %87, %c0 : index
    %89 = arith.subi %87, %c1 : index
    %90 = arith.divui %89, %c32 : index
    %91 = arith.addi %90, %c1 : index
    %92 = arith.select %88, %c0, %91 : index
    %93 = arith.remsi %92, %c4 : index
    %94 = arith.subi %92, %93 : index
    %95 = arith.muli %94, %c32 : index
    %96 = arith.addi %6, %95 : index
    %97 = arith.muli %8, %0 : index
    %98 = arith.muli %1, %2 : index
    %99 = arith.muli %98, %0 : index
    %100 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%99], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %101 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    cf.br ^bb13(%6, %cst : index, f32)
  ^bb13(%102: index, %103: f32):  // 2 preds: ^bb12, ^bb14
    %104 = arith.cmpi slt, %102, %96 : index
    cf.cond_br %104, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %105 = arith.addi %102, %c32 : index
    %106 = arith.addi %102, %c64 : index
    %107 = arith.addi %102, %c96 : index
    %108 = arith.addi %97, %102 : index
    %109 = arith.addi %97, %105 : index
    %110 = arith.addi %97, %106 : index
    %111 = arith.addi %97, %107 : index
    %112 = arith.divui %108, %0 : index
    %113 = arith.remui %112, %2 : index
    %114 = arith.divui %112, %2 : index
    %115 = arith.divui %109, %0 : index
    %116 = arith.remui %115, %2 : index
    %117 = arith.divui %115, %2 : index
    %118 = arith.divui %110, %0 : index
    %119 = arith.remui %118, %2 : index
    %120 = arith.divui %118, %2 : index
    %121 = arith.divui %111, %0 : index
    %122 = arith.remui %121, %2 : index
    %123 = arith.divui %121, %2 : index
    %124 = memref.load %100[%108] : memref<?xf32, "gpu">
    %125 = memref.load %100[%109] : memref<?xf32, "gpu">
    %126 = memref.load %100[%110] : memref<?xf32, "gpu">
    %127 = memref.load %100[%111] : memref<?xf32, "gpu">
    %128 = arith.muli %114, %2 : index
    %129 = arith.addi %128, %113 : index
    %130 = arith.muli %117, %2 : index
    %131 = arith.addi %130, %116 : index
    %132 = arith.muli %120, %2 : index
    %133 = arith.addi %132, %119 : index
    %134 = arith.muli %123, %2 : index
    %135 = arith.addi %134, %122 : index
    %136 = arith.remui %129, %c8 : index
    %137 = arith.remui %131, %c8 : index
    %138 = arith.remui %133, %c8 : index
    %139 = arith.remui %135, %c8 : index
    %140 = memref.load %101[%136] : memref<32xf32, 3>
    %141 = memref.load %101[%137] : memref<32xf32, 3>
    %142 = memref.load %101[%138] : memref<32xf32, 3>
    %143 = memref.load %101[%139] : memref<32xf32, 3>
    %144 = arith.subf %124, %140 : f32
    %145 = arith.subf %125, %141 : f32
    %146 = arith.subf %126, %142 : f32
    %147 = arith.subf %127, %143 : f32
    %148 = math.exp %144 : f32
    %149 = math.exp %145 : f32
    %150 = math.exp %146 : f32
    %151 = math.exp %147 : f32
    %152 = arith.addf %103, %148 : f32
    %153 = arith.addf %152, %149 : f32
    %154 = arith.addf %153, %150 : f32
    %155 = arith.addf %154, %151 : f32
    %156 = arith.addi %102, %c128 : index
    cf.br ^bb13(%156, %155 : index, f32)
  ^bb15:  // pred: ^bb13
    cf.br ^bb16(%96, %103 : index, f32)
  ^bb16(%157: index, %158: f32):  // 2 preds: ^bb15, ^bb17
    %159 = arith.cmpi slt, %157, %0 : index
    cf.cond_br %159, ^bb17, ^bb18
  ^bb17:  // pred: ^bb16
    %160 = arith.addi %97, %157 : index
    %161 = arith.divui %160, %0 : index
    %162 = arith.remui %161, %2 : index
    %163 = arith.divui %161, %2 : index
    %164 = memref.load %100[%160] : memref<?xf32, "gpu">
    %165 = arith.muli %163, %2 : index
    %166 = arith.addi %165, %162 : index
    %167 = arith.remui %166, %c8 : index
    %168 = memref.load %101[%167] : memref<32xf32, 3>
    %169 = arith.subf %164, %168 : f32
    %170 = math.exp %169 : f32
    %171 = arith.addf %158, %170 : f32
    %172 = arith.addi %157, %c32 : index
    cf.br ^bb16(%172, %171 : index, f32)
  ^bb18:  // pred: ^bb16
    %result_9, %valid_10 = gpu.shuffle  xor %158, %c1_i32, %c32_i32 : f32
    %173 = arith.addf %158, %result_9 : f32
    %result_11, %valid_12 = gpu.shuffle  xor %173, %c2_i32, %c32_i32 : f32
    %174 = arith.addf %173, %result_11 : f32
    %result_13, %valid_14 = gpu.shuffle  xor %174, %c4_i32, %c32_i32 : f32
    %175 = arith.addf %174, %result_13 : f32
    %result_15, %valid_16 = gpu.shuffle  xor %175, %c8_i32, %c32_i32 : f32
    %176 = arith.addf %175, %result_15 : f32
    %result_17, %valid_18 = gpu.shuffle  xor %176, %c16_i32, %c32_i32 : f32
    %177 = arith.addf %176, %result_17 : f32
    %178 = arith.cmpi eq, %6, %c0 : index
    cf.cond_br %178, ^bb19, ^bb20
  ^bb19:  // pred: ^bb18
    %179 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    memref.store %177, %179[%5] : memref<32xf32, 3>
    cf.br ^bb20
  ^bb20:  // 2 preds: ^bb18, ^bb19
    cf.br ^bb21
  ^bb21:  // 2 preds: ^bb11, ^bb20
    gpu.barrier
    cf.cond_br %9, ^bb22, ^bb29
  ^bb22:  // pred: ^bb21
    %180 = arith.subi %0, %6 : index
    %181 = arith.cmpi eq, %180, %c0 : index
    %182 = arith.subi %180, %c1 : index
    %183 = arith.divui %182, %c32 : index
    %184 = arith.addi %183, %c1 : index
    %185 = arith.select %181, %c0, %184 : index
    %186 = arith.remsi %185, %c4 : index
    %187 = arith.subi %185, %186 : index
    %188 = arith.muli %187, %c32 : index
    %189 = arith.addi %6, %188 : index
    %190 = arith.muli %8, %0 : index
    %191 = arith.muli %1, %2 : index
    %192 = arith.muli %191, %0 : index
    %193 = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [%192], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %194 = memref.reinterpret_cast %arg3 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %195 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [32], strides: [1] : memref<32xf32, 3> to memref<32xf32, 3>
    %196 = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%192], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    cf.br ^bb23(%6 : index)
  ^bb23(%197: index):  // 2 preds: ^bb22, ^bb24
    %198 = arith.cmpi slt, %197, %189 : index
    cf.cond_br %198, ^bb24, ^bb25
  ^bb24:  // pred: ^bb23
    %199 = arith.addi %197, %c32 : index
    %200 = arith.addi %197, %c64 : index
    %201 = arith.addi %197, %c96 : index
    %202 = arith.addi %190, %197 : index
    %203 = arith.addi %190, %199 : index
    %204 = arith.addi %190, %200 : index
    %205 = arith.addi %190, %201 : index
    %206 = arith.divui %202, %0 : index
    %207 = arith.remui %206, %2 : index
    %208 = arith.divui %206, %2 : index
    %209 = arith.divui %203, %0 : index
    %210 = arith.remui %209, %2 : index
    %211 = arith.divui %209, %2 : index
    %212 = arith.divui %204, %0 : index
    %213 = arith.remui %212, %2 : index
    %214 = arith.divui %212, %2 : index
    %215 = arith.divui %205, %0 : index
    %216 = arith.remui %215, %2 : index
    %217 = arith.divui %215, %2 : index
    %218 = memref.load %193[%202] : memref<?xf32, "gpu">
    %219 = memref.load %193[%203] : memref<?xf32, "gpu">
    %220 = memref.load %193[%204] : memref<?xf32, "gpu">
    %221 = memref.load %193[%205] : memref<?xf32, "gpu">
    %222 = arith.muli %208, %2 : index
    %223 = arith.addi %222, %207 : index
    %224 = arith.muli %211, %2 : index
    %225 = arith.addi %224, %210 : index
    %226 = arith.muli %214, %2 : index
    %227 = arith.addi %226, %213 : index
    %228 = arith.muli %217, %2 : index
    %229 = arith.addi %228, %216 : index
    %230 = arith.remui %223, %c8 : index
    %231 = arith.remui %225, %c8 : index
    %232 = arith.remui %227, %c8 : index
    %233 = arith.remui %229, %c8 : index
    %234 = memref.load %194[%230] : memref<32xf32, 3>
    %235 = memref.load %194[%231] : memref<32xf32, 3>
    %236 = memref.load %194[%232] : memref<32xf32, 3>
    %237 = memref.load %194[%233] : memref<32xf32, 3>
    %238 = arith.subf %218, %234 : f32
    %239 = arith.subf %219, %235 : f32
    %240 = arith.subf %220, %236 : f32
    %241 = arith.subf %221, %237 : f32
    %242 = math.exp %238 : f32
    %243 = math.exp %239 : f32
    %244 = math.exp %240 : f32
    %245 = math.exp %241 : f32
    %246 = memref.load %195[%230] : memref<32xf32, 3>
    %247 = memref.load %195[%231] : memref<32xf32, 3>
    %248 = memref.load %195[%232] : memref<32xf32, 3>
    %249 = memref.load %195[%233] : memref<32xf32, 3>
    %250 = arith.divf %242, %246 : f32
    %251 = arith.divf %243, %247 : f32
    %252 = arith.divf %244, %248 : f32
    %253 = arith.divf %245, %249 : f32
    memref.store %250, %196[%202] : memref<?xf32, "gpu">
    memref.store %251, %196[%203] : memref<?xf32, "gpu">
    memref.store %252, %196[%204] : memref<?xf32, "gpu">
    memref.store %253, %196[%205] : memref<?xf32, "gpu">
    %254 = arith.addi %197, %c128 : index
    cf.br ^bb23(%254 : index)
  ^bb25:  // pred: ^bb23
    cf.br ^bb26(%189 : index)
  ^bb26(%255: index):  // 2 preds: ^bb25, ^bb27
    %256 = arith.cmpi slt, %255, %0 : index
    cf.cond_br %256, ^bb27, ^bb28
  ^bb27:  // pred: ^bb26
    %257 = arith.addi %190, %255 : index
    %258 = arith.divui %257, %0 : index
    %259 = arith.remui %258, %2 : index
    %260 = arith.divui %258, %2 : index
    %261 = memref.load %193[%257] : memref<?xf32, "gpu">
    %262 = arith.muli %260, %2 : index
    %263 = arith.addi %262, %259 : index
    %264 = arith.remui %263, %c8 : index
    %265 = memref.load %194[%264] : memref<32xf32, 3>
    %266 = arith.subf %261, %265 : f32
    %267 = math.exp %266 : f32
    %268 = memref.load %195[%264] : memref<32xf32, 3>
    %269 = arith.divf %267, %268 : f32
    memref.store %269, %196[%257] : memref<?xf32, "gpu">
    %270 = arith.addi %255, %c32 : index
    cf.br ^bb26(%270 : index)
  ^bb28:  // pred: ^bb26
    cf.br ^bb29
  ^bb29:  // 2 preds: ^bb21, ^bb28
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel_0 {
  llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1w1r_0() {addr_space = 3 : i32} : !llvm.array<32 x f32>
  llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1w1r_1() {addr_space = 3 : i32} : !llvm.array<32 x f32>
  llvm.func @__nv_expf(f32) -> f32
  llvm.func @main_kStitch_divide__13_1_0___1w1r(%arg0: !llvm.ptr<f32>, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: !llvm.ptr<f32>, %arg11: !llvm.ptr<f32>, %arg12: i32, %arg13: i32, %arg14: i32, %arg15: i32, %arg16: i32, %arg17: i32, %arg18: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %3 = llvm.insertvalue %arg2, %2[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %4 = llvm.insertvalue %arg3, %3[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %5 = llvm.insertvalue %arg6, %4[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %6 = llvm.insertvalue %arg4, %5[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %7 = llvm.insertvalue %arg7, %6[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %8 = llvm.insertvalue %arg5, %7[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %9 = llvm.insertvalue %arg8, %8[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %10 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)>
    %11 = llvm.insertvalue %arg10, %10[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %12 = llvm.insertvalue %arg11, %11[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %13 = llvm.insertvalue %arg12, %12[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %14 = llvm.insertvalue %arg13, %13[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %15 = llvm.insertvalue %arg16, %14[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %16 = llvm.insertvalue %arg14, %15[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %17 = llvm.insertvalue %arg17, %16[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %18 = llvm.insertvalue %arg15, %17[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %19 = llvm.insertvalue %arg18, %18[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %20 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1w1r_0 : !llvm.ptr<array<32 x f32>, 3>
    %21 = llvm.getelementptr %20[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %22 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %23 = llvm.insertvalue %21, %22[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %24 = llvm.insertvalue %21, %23[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %25 = llvm.mlir.constant(0 : index) : i32
    %26 = llvm.insertvalue %25, %24[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %27 = llvm.mlir.constant(32 : index) : i32
    %28 = llvm.insertvalue %27, %26[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %29 = llvm.mlir.constant(1 : index) : i32
    %30 = llvm.insertvalue %29, %28[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %31 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1w1r_1 : !llvm.ptr<array<32 x f32>, 3>
    %32 = llvm.getelementptr %31[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %33 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %34 = llvm.insertvalue %32, %33[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %35 = llvm.insertvalue %32, %34[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %36 = llvm.mlir.constant(0 : index) : i32
    %37 = llvm.insertvalue %36, %35[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %38 = llvm.mlir.constant(32 : index) : i32
    %39 = llvm.insertvalue %38, %37[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %40 = llvm.mlir.constant(1 : index) : i32
    %41 = llvm.insertvalue %40, %39[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %42 = llvm.mlir.constant(-0.000000e+00 : f32) : f32
    %43 = llvm.mlir.constant(16 : i32) : i32
    %44 = llvm.mlir.constant(8 : i32) : i32
    %45 = llvm.mlir.constant(4 : i32) : i32
    %46 = llvm.mlir.constant(2 : i32) : i32
    %47 = llvm.mlir.constant(32 : i32) : i32
    %48 = llvm.mlir.constant(1 : i32) : i32
    %49 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %50 = llvm.mlir.constant(128 : index) : i32
    %51 = llvm.mlir.constant(96 : index) : i32
    %52 = llvm.mlir.constant(64 : index) : i32
    %53 = llvm.mlir.constant(4 : index) : i32
    %54 = llvm.mlir.constant(2 : index) : i32
    %55 = llvm.mlir.constant(8 : index) : i32
    %56 = llvm.mlir.constant(32 : index) : i32
    %57 = llvm.mlir.constant(0 : index) : i32
    %58 = llvm.mlir.constant(1 : index) : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %59 = llvm.extractvalue %9[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %60 = llvm.extractvalue %9[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %61 = llvm.extractvalue %9[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %62 = nvvm.read.ptx.sreg.ctaid.x : i32
    %63 = nvvm.read.ptx.sreg.tid.x : i32
    %64 = llvm.udiv %63, %56  : i32
    %65 = llvm.urem %63, %56  : i32
    %66 = llvm.mul %62, %55  : i32
    %67 = llvm.add %66, %64  : i32
    %68 = llvm.icmp "slt" %67, %arg9 : i32
    llvm.cond_br %68, ^bb2, ^bb11
  ^bb2:  // pred: ^bb1
    %69 = llvm.sub %59, %65  : i32
    %70 = llvm.icmp "eq" %69, %57 : i32
    %71 = llvm.sub %69, %58  : i32
    %72 = llvm.udiv %71, %56  : i32
    %73 = llvm.add %72, %58  : i32
    %74 = llvm.select %70, %57, %73 : i1, i32
    %75 = llvm.srem %74, %53  : i32
    %76 = llvm.sub %74, %75  : i32
    %77 = llvm.mul %76, %56  : i32
    %78 = llvm.add %65, %77  : i32
    %79 = llvm.mul %67, %59  : i32
    %80 = llvm.mul %60, %61  : i32
    %81 = llvm.mul %80, %59  : i32
    %82 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %83 = llvm.extractvalue %9[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %84 = llvm.extractvalue %9[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %85 = llvm.insertvalue %83, %82[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %86 = llvm.insertvalue %84, %85[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %87 = llvm.insertvalue %57, %86[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %88 = llvm.insertvalue %81, %87[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %89 = llvm.insertvalue %58, %88[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    llvm.br ^bb3(%65, %49 : i32, f32)
  ^bb3(%90: i32, %91: f32):  // 2 preds: ^bb2, ^bb4
    %92 = llvm.icmp "slt" %90, %78 : i32
    llvm.cond_br %92, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %93 = llvm.add %90, %56  : i32
    %94 = llvm.add %90, %52  : i32
    %95 = llvm.add %90, %51  : i32
    %96 = llvm.add %79, %90  : i32
    %97 = llvm.add %79, %93  : i32
    %98 = llvm.add %79, %94  : i32
    %99 = llvm.add %79, %95  : i32
    %100 = llvm.extractvalue %89[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %101 = llvm.getelementptr %100[%96] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %102 = llvm.load %101 : !llvm.ptr<f32>
    %103 = llvm.extractvalue %89[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %104 = llvm.getelementptr %103[%97] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %105 = llvm.load %104 : !llvm.ptr<f32>
    %106 = llvm.extractvalue %89[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %107 = llvm.getelementptr %106[%98] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %108 = llvm.load %107 : !llvm.ptr<f32>
    %109 = llvm.extractvalue %89[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %110 = llvm.getelementptr %109[%99] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %111 = llvm.load %110 : !llvm.ptr<f32>
    %112 = llvm.fcmp "ugt" %91, %102 : f32
    %113 = llvm.select %112, %91, %102 : i1, f32
    %114 = llvm.fcmp "uno" %102, %102 : f32
    %115 = llvm.select %114, %102, %113 : i1, f32
    %116 = llvm.fcmp "ugt" %115, %105 : f32
    %117 = llvm.select %116, %115, %105 : i1, f32
    %118 = llvm.fcmp "uno" %105, %105 : f32
    %119 = llvm.select %118, %105, %117 : i1, f32
    %120 = llvm.fcmp "ugt" %119, %108 : f32
    %121 = llvm.select %120, %119, %108 : i1, f32
    %122 = llvm.fcmp "uno" %108, %108 : f32
    %123 = llvm.select %122, %108, %121 : i1, f32
    %124 = llvm.fcmp "ugt" %123, %111 : f32
    %125 = llvm.select %124, %123, %111 : i1, f32
    %126 = llvm.fcmp "uno" %111, %111 : f32
    %127 = llvm.select %126, %111, %125 : i1, f32
    %128 = llvm.add %90, %50  : i32
    llvm.br ^bb3(%128, %127 : i32, f32)
  ^bb5:  // pred: ^bb3
    llvm.br ^bb6(%78, %91 : i32, f32)
  ^bb6(%129: i32, %130: f32):  // 2 preds: ^bb5, ^bb7
    %131 = llvm.icmp "slt" %129, %59 : i32
    llvm.cond_br %131, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %132 = llvm.add %79, %129  : i32
    %133 = llvm.extractvalue %89[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %134 = llvm.getelementptr %133[%132] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %135 = llvm.load %134 : !llvm.ptr<f32>
    %136 = llvm.fcmp "ugt" %130, %135 : f32
    %137 = llvm.select %136, %130, %135 : i1, f32
    %138 = llvm.fcmp "uno" %135, %135 : f32
    %139 = llvm.select %138, %135, %137 : i1, f32
    %140 = llvm.add %129, %56  : i32
    llvm.br ^bb6(%140, %139 : i32, f32)
  ^bb8:  // pred: ^bb6
    %141 = llvm.mlir.constant(1 : i32) : i32
    %142 = llvm.mlir.constant(-1 : i32) : i32
    %143 = llvm.mlir.constant(32 : i32) : i32
    %144 = llvm.sub %143, %47  : i32
    %145 = llvm.lshr %142, %144  : i32
    %146 = llvm.sub %47, %141  : i32
    %147 = nvvm.shfl.sync  bfly %145, %130, %48, %146 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %148 = llvm.extractvalue %147[0] : !llvm.struct<(f32, i1)> 
    %149 = llvm.extractvalue %147[1] : !llvm.struct<(f32, i1)> 
    %150 = llvm.fcmp "ugt" %130, %148 : f32
    %151 = llvm.select %150, %130, %148 : i1, f32
    %152 = llvm.fcmp "uno" %148, %148 : f32
    %153 = llvm.select %152, %148, %151 : i1, f32
    %154 = llvm.mlir.constant(1 : i32) : i32
    %155 = llvm.mlir.constant(-1 : i32) : i32
    %156 = llvm.mlir.constant(32 : i32) : i32
    %157 = llvm.sub %156, %47  : i32
    %158 = llvm.lshr %155, %157  : i32
    %159 = llvm.sub %47, %154  : i32
    %160 = nvvm.shfl.sync  bfly %158, %153, %46, %159 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %161 = llvm.extractvalue %160[0] : !llvm.struct<(f32, i1)> 
    %162 = llvm.extractvalue %160[1] : !llvm.struct<(f32, i1)> 
    %163 = llvm.fcmp "ugt" %153, %161 : f32
    %164 = llvm.select %163, %153, %161 : i1, f32
    %165 = llvm.fcmp "uno" %161, %161 : f32
    %166 = llvm.select %165, %161, %164 : i1, f32
    %167 = llvm.mlir.constant(1 : i32) : i32
    %168 = llvm.mlir.constant(-1 : i32) : i32
    %169 = llvm.mlir.constant(32 : i32) : i32
    %170 = llvm.sub %169, %47  : i32
    %171 = llvm.lshr %168, %170  : i32
    %172 = llvm.sub %47, %167  : i32
    %173 = nvvm.shfl.sync  bfly %171, %166, %45, %172 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %174 = llvm.extractvalue %173[0] : !llvm.struct<(f32, i1)> 
    %175 = llvm.extractvalue %173[1] : !llvm.struct<(f32, i1)> 
    %176 = llvm.fcmp "ugt" %166, %174 : f32
    %177 = llvm.select %176, %166, %174 : i1, f32
    %178 = llvm.fcmp "uno" %174, %174 : f32
    %179 = llvm.select %178, %174, %177 : i1, f32
    %180 = llvm.mlir.constant(1 : i32) : i32
    %181 = llvm.mlir.constant(-1 : i32) : i32
    %182 = llvm.mlir.constant(32 : i32) : i32
    %183 = llvm.sub %182, %47  : i32
    %184 = llvm.lshr %181, %183  : i32
    %185 = llvm.sub %47, %180  : i32
    %186 = nvvm.shfl.sync  bfly %184, %179, %44, %185 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %187 = llvm.extractvalue %186[0] : !llvm.struct<(f32, i1)> 
    %188 = llvm.extractvalue %186[1] : !llvm.struct<(f32, i1)> 
    %189 = llvm.fcmp "ugt" %179, %187 : f32
    %190 = llvm.select %189, %179, %187 : i1, f32
    %191 = llvm.fcmp "uno" %187, %187 : f32
    %192 = llvm.select %191, %187, %190 : i1, f32
    %193 = llvm.mlir.constant(1 : i32) : i32
    %194 = llvm.mlir.constant(-1 : i32) : i32
    %195 = llvm.mlir.constant(32 : i32) : i32
    %196 = llvm.sub %195, %47  : i32
    %197 = llvm.lshr %194, %196  : i32
    %198 = llvm.sub %47, %193  : i32
    %199 = nvvm.shfl.sync  bfly %197, %192, %43, %198 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %200 = llvm.extractvalue %199[0] : !llvm.struct<(f32, i1)> 
    %201 = llvm.extractvalue %199[1] : !llvm.struct<(f32, i1)> 
    %202 = llvm.fcmp "ugt" %192, %200 : f32
    %203 = llvm.select %202, %192, %200 : i1, f32
    %204 = llvm.fcmp "uno" %200, %200 : f32
    %205 = llvm.select %204, %200, %203 : i1, f32
    %206 = llvm.icmp "eq" %65, %57 : i32
    llvm.cond_br %206, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %207 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %208 = llvm.extractvalue %30[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %209 = llvm.extractvalue %30[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %210 = llvm.insertvalue %208, %207[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %211 = llvm.insertvalue %209, %210[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %212 = llvm.mlir.constant(0 : index) : i32
    %213 = llvm.insertvalue %212, %211[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %214 = llvm.mlir.constant(32 : index) : i32
    %215 = llvm.insertvalue %214, %213[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %216 = llvm.mlir.constant(1 : index) : i32
    %217 = llvm.insertvalue %216, %215[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %218 = llvm.extractvalue %217[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %219 = llvm.getelementptr %218[%64] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %205, %219 : !llvm.ptr<f32, 3>
    llvm.br ^bb10
  ^bb10:  // 2 preds: ^bb8, ^bb9
    llvm.br ^bb11
  ^bb11:  // 2 preds: ^bb1, ^bb10
    nvvm.barrier0
    llvm.cond_br %68, ^bb12, ^bb21
  ^bb12:  // pred: ^bb11
    %220 = llvm.sub %59, %65  : i32
    %221 = llvm.icmp "eq" %220, %57 : i32
    %222 = llvm.sub %220, %58  : i32
    %223 = llvm.udiv %222, %56  : i32
    %224 = llvm.add %223, %58  : i32
    %225 = llvm.select %221, %57, %224 : i1, i32
    %226 = llvm.srem %225, %53  : i32
    %227 = llvm.sub %225, %226  : i32
    %228 = llvm.mul %227, %56  : i32
    %229 = llvm.add %65, %228  : i32
    %230 = llvm.mul %67, %59  : i32
    %231 = llvm.mul %60, %61  : i32
    %232 = llvm.mul %231, %59  : i32
    %233 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %234 = llvm.extractvalue %9[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %235 = llvm.extractvalue %9[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %236 = llvm.insertvalue %234, %233[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %237 = llvm.insertvalue %235, %236[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %238 = llvm.insertvalue %57, %237[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %239 = llvm.insertvalue %232, %238[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %240 = llvm.insertvalue %58, %239[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %241 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %242 = llvm.extractvalue %30[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %243 = llvm.extractvalue %30[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %244 = llvm.insertvalue %242, %241[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %245 = llvm.insertvalue %243, %244[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %246 = llvm.mlir.constant(0 : index) : i32
    %247 = llvm.insertvalue %246, %245[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %248 = llvm.mlir.constant(32 : index) : i32
    %249 = llvm.insertvalue %248, %247[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %250 = llvm.mlir.constant(1 : index) : i32
    %251 = llvm.insertvalue %250, %249[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    llvm.br ^bb13(%65, %42 : i32, f32)
  ^bb13(%252: i32, %253: f32):  // 2 preds: ^bb12, ^bb14
    %254 = llvm.icmp "slt" %252, %229 : i32
    llvm.cond_br %254, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %255 = llvm.add %252, %56  : i32
    %256 = llvm.add %252, %52  : i32
    %257 = llvm.add %252, %51  : i32
    %258 = llvm.add %230, %252  : i32
    %259 = llvm.add %230, %255  : i32
    %260 = llvm.add %230, %256  : i32
    %261 = llvm.add %230, %257  : i32
    %262 = llvm.udiv %258, %59  : i32
    %263 = llvm.urem %262, %61  : i32
    %264 = llvm.udiv %262, %61  : i32
    %265 = llvm.udiv %259, %59  : i32
    %266 = llvm.urem %265, %61  : i32
    %267 = llvm.udiv %265, %61  : i32
    %268 = llvm.udiv %260, %59  : i32
    %269 = llvm.urem %268, %61  : i32
    %270 = llvm.udiv %268, %61  : i32
    %271 = llvm.udiv %261, %59  : i32
    %272 = llvm.urem %271, %61  : i32
    %273 = llvm.udiv %271, %61  : i32
    %274 = llvm.extractvalue %240[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %275 = llvm.getelementptr %274[%258] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %276 = llvm.load %275 : !llvm.ptr<f32>
    %277 = llvm.extractvalue %240[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %278 = llvm.getelementptr %277[%259] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %279 = llvm.load %278 : !llvm.ptr<f32>
    %280 = llvm.extractvalue %240[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %281 = llvm.getelementptr %280[%260] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %282 = llvm.load %281 : !llvm.ptr<f32>
    %283 = llvm.extractvalue %240[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %284 = llvm.getelementptr %283[%261] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %285 = llvm.load %284 : !llvm.ptr<f32>
    %286 = llvm.mul %264, %61  : i32
    %287 = llvm.add %286, %263  : i32
    %288 = llvm.mul %267, %61  : i32
    %289 = llvm.add %288, %266  : i32
    %290 = llvm.mul %270, %61  : i32
    %291 = llvm.add %290, %269  : i32
    %292 = llvm.mul %273, %61  : i32
    %293 = llvm.add %292, %272  : i32
    %294 = llvm.urem %287, %55  : i32
    %295 = llvm.urem %289, %55  : i32
    %296 = llvm.urem %291, %55  : i32
    %297 = llvm.urem %293, %55  : i32
    %298 = llvm.extractvalue %251[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %299 = llvm.getelementptr %298[%294] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %300 = llvm.load %299 : !llvm.ptr<f32, 3>
    %301 = llvm.extractvalue %251[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %302 = llvm.getelementptr %301[%295] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %303 = llvm.load %302 : !llvm.ptr<f32, 3>
    %304 = llvm.extractvalue %251[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %305 = llvm.getelementptr %304[%296] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %306 = llvm.load %305 : !llvm.ptr<f32, 3>
    %307 = llvm.extractvalue %251[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %308 = llvm.getelementptr %307[%297] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %309 = llvm.load %308 : !llvm.ptr<f32, 3>
    %310 = llvm.fsub %276, %300  : f32
    %311 = llvm.fsub %279, %303  : f32
    %312 = llvm.fsub %282, %306  : f32
    %313 = llvm.fsub %285, %309  : f32
    %314 = llvm.call @__nv_expf(%310) : (f32) -> f32
    %315 = llvm.call @__nv_expf(%311) : (f32) -> f32
    %316 = llvm.call @__nv_expf(%312) : (f32) -> f32
    %317 = llvm.call @__nv_expf(%313) : (f32) -> f32
    %318 = llvm.fadd %253, %314  : f32
    %319 = llvm.fadd %318, %315  : f32
    %320 = llvm.fadd %319, %316  : f32
    %321 = llvm.fadd %320, %317  : f32
    %322 = llvm.add %252, %50  : i32
    llvm.br ^bb13(%322, %321 : i32, f32)
  ^bb15:  // pred: ^bb13
    llvm.br ^bb16(%229, %253 : i32, f32)
  ^bb16(%323: i32, %324: f32):  // 2 preds: ^bb15, ^bb17
    %325 = llvm.icmp "slt" %323, %59 : i32
    llvm.cond_br %325, ^bb17, ^bb18
  ^bb17:  // pred: ^bb16
    %326 = llvm.add %230, %323  : i32
    %327 = llvm.udiv %326, %59  : i32
    %328 = llvm.urem %327, %61  : i32
    %329 = llvm.udiv %327, %61  : i32
    %330 = llvm.extractvalue %240[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %331 = llvm.getelementptr %330[%326] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %332 = llvm.load %331 : !llvm.ptr<f32>
    %333 = llvm.mul %329, %61  : i32
    %334 = llvm.add %333, %328  : i32
    %335 = llvm.urem %334, %55  : i32
    %336 = llvm.extractvalue %251[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %337 = llvm.getelementptr %336[%335] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %338 = llvm.load %337 : !llvm.ptr<f32, 3>
    %339 = llvm.fsub %332, %338  : f32
    %340 = llvm.call @__nv_expf(%339) : (f32) -> f32
    %341 = llvm.fadd %324, %340  : f32
    %342 = llvm.add %323, %56  : i32
    llvm.br ^bb16(%342, %341 : i32, f32)
  ^bb18:  // pred: ^bb16
    %343 = llvm.mlir.constant(1 : i32) : i32
    %344 = llvm.mlir.constant(-1 : i32) : i32
    %345 = llvm.mlir.constant(32 : i32) : i32
    %346 = llvm.sub %345, %47  : i32
    %347 = llvm.lshr %344, %346  : i32
    %348 = llvm.sub %47, %343  : i32
    %349 = nvvm.shfl.sync  bfly %347, %324, %48, %348 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %350 = llvm.extractvalue %349[0] : !llvm.struct<(f32, i1)> 
    %351 = llvm.extractvalue %349[1] : !llvm.struct<(f32, i1)> 
    %352 = llvm.fadd %324, %350  : f32
    %353 = llvm.mlir.constant(1 : i32) : i32
    %354 = llvm.mlir.constant(-1 : i32) : i32
    %355 = llvm.mlir.constant(32 : i32) : i32
    %356 = llvm.sub %355, %47  : i32
    %357 = llvm.lshr %354, %356  : i32
    %358 = llvm.sub %47, %353  : i32
    %359 = nvvm.shfl.sync  bfly %357, %352, %46, %358 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %360 = llvm.extractvalue %359[0] : !llvm.struct<(f32, i1)> 
    %361 = llvm.extractvalue %359[1] : !llvm.struct<(f32, i1)> 
    %362 = llvm.fadd %352, %360  : f32
    %363 = llvm.mlir.constant(1 : i32) : i32
    %364 = llvm.mlir.constant(-1 : i32) : i32
    %365 = llvm.mlir.constant(32 : i32) : i32
    %366 = llvm.sub %365, %47  : i32
    %367 = llvm.lshr %364, %366  : i32
    %368 = llvm.sub %47, %363  : i32
    %369 = nvvm.shfl.sync  bfly %367, %362, %45, %368 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %370 = llvm.extractvalue %369[0] : !llvm.struct<(f32, i1)> 
    %371 = llvm.extractvalue %369[1] : !llvm.struct<(f32, i1)> 
    %372 = llvm.fadd %362, %370  : f32
    %373 = llvm.mlir.constant(1 : i32) : i32
    %374 = llvm.mlir.constant(-1 : i32) : i32
    %375 = llvm.mlir.constant(32 : i32) : i32
    %376 = llvm.sub %375, %47  : i32
    %377 = llvm.lshr %374, %376  : i32
    %378 = llvm.sub %47, %373  : i32
    %379 = nvvm.shfl.sync  bfly %377, %372, %44, %378 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %380 = llvm.extractvalue %379[0] : !llvm.struct<(f32, i1)> 
    %381 = llvm.extractvalue %379[1] : !llvm.struct<(f32, i1)> 
    %382 = llvm.fadd %372, %380  : f32
    %383 = llvm.mlir.constant(1 : i32) : i32
    %384 = llvm.mlir.constant(-1 : i32) : i32
    %385 = llvm.mlir.constant(32 : i32) : i32
    %386 = llvm.sub %385, %47  : i32
    %387 = llvm.lshr %384, %386  : i32
    %388 = llvm.sub %47, %383  : i32
    %389 = nvvm.shfl.sync  bfly %387, %382, %43, %388 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %390 = llvm.extractvalue %389[0] : !llvm.struct<(f32, i1)> 
    %391 = llvm.extractvalue %389[1] : !llvm.struct<(f32, i1)> 
    %392 = llvm.fadd %382, %390  : f32
    %393 = llvm.icmp "eq" %65, %57 : i32
    llvm.cond_br %393, ^bb19, ^bb20
  ^bb19:  // pred: ^bb18
    %394 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %395 = llvm.extractvalue %41[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %396 = llvm.extractvalue %41[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %397 = llvm.insertvalue %395, %394[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %398 = llvm.insertvalue %396, %397[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %399 = llvm.mlir.constant(0 : index) : i32
    %400 = llvm.insertvalue %399, %398[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %401 = llvm.mlir.constant(32 : index) : i32
    %402 = llvm.insertvalue %401, %400[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %403 = llvm.mlir.constant(1 : index) : i32
    %404 = llvm.insertvalue %403, %402[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %405 = llvm.extractvalue %404[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %406 = llvm.getelementptr %405[%64] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %392, %406 : !llvm.ptr<f32, 3>
    llvm.br ^bb20
  ^bb20:  // 2 preds: ^bb18, ^bb19
    llvm.br ^bb21
  ^bb21:  // 2 preds: ^bb11, ^bb20
    nvvm.barrier0
    llvm.cond_br %68, ^bb22, ^bb29
  ^bb22:  // pred: ^bb21
    %407 = llvm.sub %59, %65  : i32
    %408 = llvm.icmp "eq" %407, %57 : i32
    %409 = llvm.sub %407, %58  : i32
    %410 = llvm.udiv %409, %56  : i32
    %411 = llvm.add %410, %58  : i32
    %412 = llvm.select %408, %57, %411 : i1, i32
    %413 = llvm.srem %412, %53  : i32
    %414 = llvm.sub %412, %413  : i32
    %415 = llvm.mul %414, %56  : i32
    %416 = llvm.add %65, %415  : i32
    %417 = llvm.mul %67, %59  : i32
    %418 = llvm.mul %60, %61  : i32
    %419 = llvm.mul %418, %59  : i32
    %420 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %421 = llvm.extractvalue %9[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %422 = llvm.extractvalue %9[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %423 = llvm.insertvalue %421, %420[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %424 = llvm.insertvalue %422, %423[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %425 = llvm.insertvalue %57, %424[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %426 = llvm.insertvalue %419, %425[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %427 = llvm.insertvalue %58, %426[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %428 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %429 = llvm.extractvalue %30[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %430 = llvm.extractvalue %30[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %431 = llvm.insertvalue %429, %428[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %432 = llvm.insertvalue %430, %431[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %433 = llvm.mlir.constant(0 : index) : i32
    %434 = llvm.insertvalue %433, %432[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %435 = llvm.mlir.constant(32 : index) : i32
    %436 = llvm.insertvalue %435, %434[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %437 = llvm.mlir.constant(1 : index) : i32
    %438 = llvm.insertvalue %437, %436[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %439 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %440 = llvm.extractvalue %41[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %441 = llvm.extractvalue %41[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %442 = llvm.insertvalue %440, %439[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %443 = llvm.insertvalue %441, %442[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %444 = llvm.mlir.constant(0 : index) : i32
    %445 = llvm.insertvalue %444, %443[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %446 = llvm.mlir.constant(32 : index) : i32
    %447 = llvm.insertvalue %446, %445[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %448 = llvm.mlir.constant(1 : index) : i32
    %449 = llvm.insertvalue %448, %447[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %450 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %451 = llvm.extractvalue %19[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %452 = llvm.extractvalue %19[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %453 = llvm.insertvalue %451, %450[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %454 = llvm.insertvalue %452, %453[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %455 = llvm.insertvalue %57, %454[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %456 = llvm.insertvalue %419, %455[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %457 = llvm.insertvalue %58, %456[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    llvm.br ^bb23(%65 : i32)
  ^bb23(%458: i32):  // 2 preds: ^bb22, ^bb24
    %459 = llvm.icmp "slt" %458, %416 : i32
    llvm.cond_br %459, ^bb24, ^bb25
  ^bb24:  // pred: ^bb23
    %460 = llvm.add %458, %56  : i32
    %461 = llvm.add %458, %52  : i32
    %462 = llvm.add %458, %51  : i32
    %463 = llvm.add %417, %458  : i32
    %464 = llvm.add %417, %460  : i32
    %465 = llvm.add %417, %461  : i32
    %466 = llvm.add %417, %462  : i32
    %467 = llvm.udiv %463, %59  : i32
    %468 = llvm.urem %467, %61  : i32
    %469 = llvm.udiv %467, %61  : i32
    %470 = llvm.udiv %464, %59  : i32
    %471 = llvm.urem %470, %61  : i32
    %472 = llvm.udiv %470, %61  : i32
    %473 = llvm.udiv %465, %59  : i32
    %474 = llvm.urem %473, %61  : i32
    %475 = llvm.udiv %473, %61  : i32
    %476 = llvm.udiv %466, %59  : i32
    %477 = llvm.urem %476, %61  : i32
    %478 = llvm.udiv %476, %61  : i32
    %479 = llvm.extractvalue %427[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %480 = llvm.getelementptr %479[%463] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %481 = llvm.load %480 : !llvm.ptr<f32>
    %482 = llvm.extractvalue %427[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %483 = llvm.getelementptr %482[%464] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %484 = llvm.load %483 : !llvm.ptr<f32>
    %485 = llvm.extractvalue %427[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %486 = llvm.getelementptr %485[%465] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %487 = llvm.load %486 : !llvm.ptr<f32>
    %488 = llvm.extractvalue %427[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %489 = llvm.getelementptr %488[%466] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %490 = llvm.load %489 : !llvm.ptr<f32>
    %491 = llvm.mul %469, %61  : i32
    %492 = llvm.add %491, %468  : i32
    %493 = llvm.mul %472, %61  : i32
    %494 = llvm.add %493, %471  : i32
    %495 = llvm.mul %475, %61  : i32
    %496 = llvm.add %495, %474  : i32
    %497 = llvm.mul %478, %61  : i32
    %498 = llvm.add %497, %477  : i32
    %499 = llvm.urem %492, %55  : i32
    %500 = llvm.urem %494, %55  : i32
    %501 = llvm.urem %496, %55  : i32
    %502 = llvm.urem %498, %55  : i32
    %503 = llvm.extractvalue %438[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %504 = llvm.getelementptr %503[%499] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %505 = llvm.load %504 : !llvm.ptr<f32, 3>
    %506 = llvm.extractvalue %438[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %507 = llvm.getelementptr %506[%500] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %508 = llvm.load %507 : !llvm.ptr<f32, 3>
    %509 = llvm.extractvalue %438[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %510 = llvm.getelementptr %509[%501] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %511 = llvm.load %510 : !llvm.ptr<f32, 3>
    %512 = llvm.extractvalue %438[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %513 = llvm.getelementptr %512[%502] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %514 = llvm.load %513 : !llvm.ptr<f32, 3>
    %515 = llvm.fsub %481, %505  : f32
    %516 = llvm.fsub %484, %508  : f32
    %517 = llvm.fsub %487, %511  : f32
    %518 = llvm.fsub %490, %514  : f32
    %519 = llvm.call @__nv_expf(%515) : (f32) -> f32
    %520 = llvm.call @__nv_expf(%516) : (f32) -> f32
    %521 = llvm.call @__nv_expf(%517) : (f32) -> f32
    %522 = llvm.call @__nv_expf(%518) : (f32) -> f32
    %523 = llvm.extractvalue %449[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %524 = llvm.getelementptr %523[%499] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %525 = llvm.load %524 : !llvm.ptr<f32, 3>
    %526 = llvm.extractvalue %449[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %527 = llvm.getelementptr %526[%500] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %528 = llvm.load %527 : !llvm.ptr<f32, 3>
    %529 = llvm.extractvalue %449[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %530 = llvm.getelementptr %529[%501] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %531 = llvm.load %530 : !llvm.ptr<f32, 3>
    %532 = llvm.extractvalue %449[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %533 = llvm.getelementptr %532[%502] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %534 = llvm.load %533 : !llvm.ptr<f32, 3>
    %535 = llvm.fdiv %519, %525  : f32
    %536 = llvm.fdiv %520, %528  : f32
    %537 = llvm.fdiv %521, %531  : f32
    %538 = llvm.fdiv %522, %534  : f32
    %539 = llvm.extractvalue %457[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %540 = llvm.getelementptr %539[%463] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %535, %540 : !llvm.ptr<f32>
    %541 = llvm.extractvalue %457[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %542 = llvm.getelementptr %541[%464] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %536, %542 : !llvm.ptr<f32>
    %543 = llvm.extractvalue %457[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %544 = llvm.getelementptr %543[%465] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %537, %544 : !llvm.ptr<f32>
    %545 = llvm.extractvalue %457[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %546 = llvm.getelementptr %545[%466] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %538, %546 : !llvm.ptr<f32>
    %547 = llvm.add %458, %50  : i32
    llvm.br ^bb23(%547 : i32)
  ^bb25:  // pred: ^bb23
    llvm.br ^bb26(%416 : i32)
  ^bb26(%548: i32):  // 2 preds: ^bb25, ^bb27
    %549 = llvm.icmp "slt" %548, %59 : i32
    llvm.cond_br %549, ^bb27, ^bb28
  ^bb27:  // pred: ^bb26
    %550 = llvm.add %417, %548  : i32
    %551 = llvm.udiv %550, %59  : i32
    %552 = llvm.urem %551, %61  : i32
    %553 = llvm.udiv %551, %61  : i32
    %554 = llvm.extractvalue %427[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %555 = llvm.getelementptr %554[%550] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %556 = llvm.load %555 : !llvm.ptr<f32>
    %557 = llvm.mul %553, %61  : i32
    %558 = llvm.add %557, %552  : i32
    %559 = llvm.urem %558, %55  : i32
    %560 = llvm.extractvalue %438[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %561 = llvm.getelementptr %560[%559] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %562 = llvm.load %561 : !llvm.ptr<f32, 3>
    %563 = llvm.fsub %556, %562  : f32
    %564 = llvm.call @__nv_expf(%563) : (f32) -> f32
    %565 = llvm.extractvalue %449[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %566 = llvm.getelementptr %565[%559] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %567 = llvm.load %566 : !llvm.ptr<f32, 3>
    %568 = llvm.fdiv %564, %567  : f32
    %569 = llvm.extractvalue %457[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %570 = llvm.getelementptr %569[%550] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %568, %570 : !llvm.ptr<f32>
    %571 = llvm.add %548, %56  : i32
    llvm.br ^bb26(%571 : i32)
  ^bb28:  // pred: ^bb26
    llvm.br ^bb29
  ^bb29:  // 2 preds: ^bb21, ^bb28
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kStitch_divide__13_1_0___1w1r(%arg0: !llvm.ptr<f32>, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: !llvm.ptr<f32>, %arg11: !llvm.ptr<f32>, %arg12: i32, %arg13: i32, %arg14: i32, %arg15: i32, %arg16: i32, %arg17: i32, %arg18: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(-1 : i32) : i32
  %1 = llvm.mlir.constant(8 : index) : i32
  %2 = llvm.mlir.constant(4 : index) : i32
  %3 = llvm.mlir.constant(64 : index) : i32
  %4 = llvm.mlir.constant(96 : index) : i32
  %5 = llvm.mlir.constant(128 : index) : i32
  %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
  %7 = llvm.mlir.constant(1 : i32) : i32
  %8 = llvm.mlir.constant(32 : i32) : i32
  %9 = llvm.mlir.constant(2 : i32) : i32
  %10 = llvm.mlir.constant(4 : i32) : i32
  %11 = llvm.mlir.constant(8 : i32) : i32
  %12 = llvm.mlir.constant(16 : i32) : i32
  %13 = llvm.mlir.constant(-0.000000e+00 : f32) : f32
  %14 = llvm.mlir.constant(1 : index) : i32
  %15 = llvm.mlir.constant(32 : index) : i32
  %16 = llvm.mlir.constant(0 : index) : i32
  %17 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1w1r_0 : !llvm.ptr<array<32 x f32>, 3>
  %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
  %19 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1w1r_1 : !llvm.ptr<array<32 x f32>, 3>
  %20 = llvm.getelementptr %19[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %21 = nvvm.read.ptx.sreg.ctaid.x : i32
  %22 = nvvm.read.ptx.sreg.tid.x : i32
  %23 = llvm.udiv %22, %15  : i32
  %24 = llvm.urem %22, %15  : i32
  %25 = llvm.mul %21, %1  : i32
  %26 = llvm.add %25, %23  : i32
  %27 = llvm.icmp "slt" %26, %arg9 : i32
  llvm.cond_br %27, ^bb2, ^bb11
^bb2:  // pred: ^bb1
  %28 = llvm.sub %arg5, %24  : i32
  %29 = llvm.icmp "eq" %28, %16 : i32
  %30 = llvm.sub %28, %14  : i32
  %31 = llvm.udiv %30, %15  : i32
  %32 = llvm.add %31, %14  : i32
  %33 = llvm.select %29, %16, %32 : i1, i32
  %34 = llvm.srem %33, %2  : i32
  %35 = llvm.sub %33, %34  : i32
  %36 = llvm.mul %35, %15  : i32
  %37 = llvm.add %24, %36  : i32
  %38 = llvm.mul %26, %arg5  : i32
  llvm.br ^bb3(%24, %6 : i32, f32)
^bb3(%39: i32, %40: f32):  // 2 preds: ^bb2, ^bb4
  %41 = llvm.icmp "slt" %39, %37 : i32
  llvm.cond_br %41, ^bb4, ^bb5
^bb4:  // pred: ^bb3
  %42 = llvm.add %39, %15  : i32
  %43 = llvm.add %39, %3  : i32
  %44 = llvm.add %39, %4  : i32
  %45 = llvm.add %38, %39  : i32
  %46 = llvm.add %38, %42  : i32
  %47 = llvm.add %38, %43  : i32
  %48 = llvm.add %38, %44  : i32
  %49 = llvm.getelementptr %arg1[%45] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %50 = llvm.load %49 : !llvm.ptr<f32>
  %51 = llvm.getelementptr %arg1[%46] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %52 = llvm.load %51 : !llvm.ptr<f32>
  %53 = llvm.getelementptr %arg1[%47] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %54 = llvm.load %53 : !llvm.ptr<f32>
  %55 = llvm.getelementptr %arg1[%48] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %56 = llvm.load %55 : !llvm.ptr<f32>
  %57 = llvm.fcmp "ugt" %40, %50 : f32
  %58 = llvm.select %57, %40, %50 : i1, f32
  %59 = llvm.fcmp "uno" %50, %50 : f32
  %60 = llvm.select %59, %50, %58 : i1, f32
  %61 = llvm.fcmp "ugt" %60, %52 : f32
  %62 = llvm.select %61, %60, %52 : i1, f32
  %63 = llvm.fcmp "uno" %52, %52 : f32
  %64 = llvm.select %63, %52, %62 : i1, f32
  %65 = llvm.fcmp "ugt" %64, %54 : f32
  %66 = llvm.select %65, %64, %54 : i1, f32
  %67 = llvm.fcmp "uno" %54, %54 : f32
  %68 = llvm.select %67, %54, %66 : i1, f32
  %69 = llvm.fcmp "ugt" %68, %56 : f32
  %70 = llvm.select %69, %68, %56 : i1, f32
  %71 = llvm.fcmp "uno" %56, %56 : f32
  %72 = llvm.select %71, %56, %70 : i1, f32
  %73 = llvm.add %39, %5  : i32
  llvm.br ^bb3(%73, %72 : i32, f32)
^bb5:  // pred: ^bb3
  llvm.br ^bb6(%37, %40 : i32, f32)
^bb6(%74: i32, %75: f32):  // 2 preds: ^bb5, ^bb7
  %76 = llvm.icmp "slt" %74, %arg5 : i32
  llvm.cond_br %76, ^bb7, ^bb8
^bb7:  // pred: ^bb6
  %77 = llvm.add %38, %74  : i32
  %78 = llvm.getelementptr %arg1[%77] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %79 = llvm.load %78 : !llvm.ptr<f32>
  %80 = llvm.fcmp "ugt" %75, %79 : f32
  %81 = llvm.select %80, %75, %79 : i1, f32
  %82 = llvm.fcmp "uno" %79, %79 : f32
  %83 = llvm.select %82, %79, %81 : i1, f32
  %84 = llvm.add %74, %15  : i32
  llvm.br ^bb6(%84, %83 : i32, f32)
^bb8:  // pred: ^bb6
  %85 = llvm.sub %8, %8  : i32
  %86 = llvm.lshr %0, %85  : i32
  %87 = llvm.sub %8, %7  : i32
  %88 = nvvm.shfl.sync  bfly %86, %75, %7, %87 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %89 = llvm.extractvalue %88[0] : !llvm.struct<(f32, i1)> 
  %90 = llvm.fcmp "ugt" %75, %89 : f32
  %91 = llvm.select %90, %75, %89 : i1, f32
  %92 = llvm.fcmp "uno" %89, %89 : f32
  %93 = llvm.select %92, %89, %91 : i1, f32
  %94 = llvm.sub %8, %8  : i32
  %95 = llvm.lshr %0, %94  : i32
  %96 = llvm.sub %8, %7  : i32
  %97 = nvvm.shfl.sync  bfly %95, %93, %9, %96 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %98 = llvm.extractvalue %97[0] : !llvm.struct<(f32, i1)> 
  %99 = llvm.fcmp "ugt" %93, %98 : f32
  %100 = llvm.select %99, %93, %98 : i1, f32
  %101 = llvm.fcmp "uno" %98, %98 : f32
  %102 = llvm.select %101, %98, %100 : i1, f32
  %103 = llvm.sub %8, %8  : i32
  %104 = llvm.lshr %0, %103  : i32
  %105 = llvm.sub %8, %7  : i32
  %106 = nvvm.shfl.sync  bfly %104, %102, %10, %105 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %107 = llvm.extractvalue %106[0] : !llvm.struct<(f32, i1)> 
  %108 = llvm.fcmp "ugt" %102, %107 : f32
  %109 = llvm.select %108, %102, %107 : i1, f32
  %110 = llvm.fcmp "uno" %107, %107 : f32
  %111 = llvm.select %110, %107, %109 : i1, f32
  %112 = llvm.sub %8, %8  : i32
  %113 = llvm.lshr %0, %112  : i32
  %114 = llvm.sub %8, %7  : i32
  %115 = nvvm.shfl.sync  bfly %113, %111, %11, %114 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %116 = llvm.extractvalue %115[0] : !llvm.struct<(f32, i1)> 
  %117 = llvm.fcmp "ugt" %111, %116 : f32
  %118 = llvm.select %117, %111, %116 : i1, f32
  %119 = llvm.fcmp "uno" %116, %116 : f32
  %120 = llvm.select %119, %116, %118 : i1, f32
  %121 = llvm.sub %8, %8  : i32
  %122 = llvm.lshr %0, %121  : i32
  %123 = llvm.sub %8, %7  : i32
  %124 = nvvm.shfl.sync  bfly %122, %120, %12, %123 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %125 = llvm.extractvalue %124[0] : !llvm.struct<(f32, i1)> 
  %126 = llvm.fcmp "ugt" %120, %125 : f32
  %127 = llvm.select %126, %120, %125 : i1, f32
  %128 = llvm.fcmp "uno" %125, %125 : f32
  %129 = llvm.select %128, %125, %127 : i1, f32
  %130 = llvm.icmp "eq" %24, %16 : i32
  llvm.cond_br %130, ^bb9, ^bb10
^bb9:  // pred: ^bb8
  %131 = llvm.getelementptr %18[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %129, %131 : !llvm.ptr<f32, 3>
  llvm.br ^bb10
^bb10:  // 2 preds: ^bb8, ^bb9
  llvm.br ^bb11
^bb11:  // 2 preds: ^bb1, ^bb10
  nvvm.barrier0
  llvm.cond_br %27, ^bb12, ^bb21
^bb12:  // pred: ^bb11
  %132 = llvm.sub %arg5, %24  : i32
  %133 = llvm.icmp "eq" %132, %16 : i32
  %134 = llvm.sub %132, %14  : i32
  %135 = llvm.udiv %134, %15  : i32
  %136 = llvm.add %135, %14  : i32
  %137 = llvm.select %133, %16, %136 : i1, i32
  %138 = llvm.srem %137, %2  : i32
  %139 = llvm.sub %137, %138  : i32
  %140 = llvm.mul %139, %15  : i32
  %141 = llvm.add %24, %140  : i32
  %142 = llvm.mul %26, %arg5  : i32
  llvm.br ^bb13(%24, %13 : i32, f32)
^bb13(%143: i32, %144: f32):  // 2 preds: ^bb12, ^bb14
  %145 = llvm.icmp "slt" %143, %141 : i32
  llvm.cond_br %145, ^bb14, ^bb15
^bb14:  // pred: ^bb13
  %146 = llvm.add %143, %15  : i32
  %147 = llvm.add %143, %3  : i32
  %148 = llvm.add %143, %4  : i32
  %149 = llvm.add %142, %143  : i32
  %150 = llvm.add %142, %146  : i32
  %151 = llvm.add %142, %147  : i32
  %152 = llvm.add %142, %148  : i32
  %153 = llvm.udiv %149, %arg5  : i32
  %154 = llvm.urem %153, %arg4  : i32
  %155 = llvm.udiv %153, %arg4  : i32
  %156 = llvm.udiv %150, %arg5  : i32
  %157 = llvm.urem %156, %arg4  : i32
  %158 = llvm.udiv %156, %arg4  : i32
  %159 = llvm.udiv %151, %arg5  : i32
  %160 = llvm.urem %159, %arg4  : i32
  %161 = llvm.udiv %159, %arg4  : i32
  %162 = llvm.udiv %152, %arg5  : i32
  %163 = llvm.urem %162, %arg4  : i32
  %164 = llvm.udiv %162, %arg4  : i32
  %165 = llvm.getelementptr %arg1[%149] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %166 = llvm.load %165 : !llvm.ptr<f32>
  %167 = llvm.getelementptr %arg1[%150] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %168 = llvm.load %167 : !llvm.ptr<f32>
  %169 = llvm.getelementptr %arg1[%151] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %170 = llvm.load %169 : !llvm.ptr<f32>
  %171 = llvm.getelementptr %arg1[%152] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %172 = llvm.load %171 : !llvm.ptr<f32>
  %173 = llvm.mul %155, %arg4  : i32
  %174 = llvm.add %173, %154  : i32
  %175 = llvm.mul %158, %arg4  : i32
  %176 = llvm.add %175, %157  : i32
  %177 = llvm.mul %161, %arg4  : i32
  %178 = llvm.add %177, %160  : i32
  %179 = llvm.mul %164, %arg4  : i32
  %180 = llvm.add %179, %163  : i32
  %181 = llvm.urem %174, %1  : i32
  %182 = llvm.urem %176, %1  : i32
  %183 = llvm.urem %178, %1  : i32
  %184 = llvm.urem %180, %1  : i32
  %185 = llvm.getelementptr %18[%181] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %186 = llvm.load %185 : !llvm.ptr<f32, 3>
  %187 = llvm.getelementptr %18[%182] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %188 = llvm.load %187 : !llvm.ptr<f32, 3>
  %189 = llvm.getelementptr %18[%183] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %190 = llvm.load %189 : !llvm.ptr<f32, 3>
  %191 = llvm.getelementptr %18[%184] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %192 = llvm.load %191 : !llvm.ptr<f32, 3>
  %193 = llvm.fsub %166, %186  : f32
  %194 = llvm.fsub %168, %188  : f32
  %195 = llvm.fsub %170, %190  : f32
  %196 = llvm.fsub %172, %192  : f32
  %197 = llvm.call @__nv_expf(%193) : (f32) -> f32
  %198 = llvm.call @__nv_expf(%194) : (f32) -> f32
  %199 = llvm.call @__nv_expf(%195) : (f32) -> f32
  %200 = llvm.call @__nv_expf(%196) : (f32) -> f32
  %201 = llvm.fadd %144, %197  : f32
  %202 = llvm.fadd %201, %198  : f32
  %203 = llvm.fadd %202, %199  : f32
  %204 = llvm.fadd %203, %200  : f32
  %205 = llvm.add %143, %5  : i32
  llvm.br ^bb13(%205, %204 : i32, f32)
^bb15:  // pred: ^bb13
  llvm.br ^bb16(%141, %144 : i32, f32)
^bb16(%206: i32, %207: f32):  // 2 preds: ^bb15, ^bb17
  %208 = llvm.icmp "slt" %206, %arg5 : i32
  llvm.cond_br %208, ^bb17, ^bb18
^bb17:  // pred: ^bb16
  %209 = llvm.add %142, %206  : i32
  %210 = llvm.udiv %209, %arg5  : i32
  %211 = llvm.urem %210, %arg4  : i32
  %212 = llvm.udiv %210, %arg4  : i32
  %213 = llvm.getelementptr %arg1[%209] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %214 = llvm.load %213 : !llvm.ptr<f32>
  %215 = llvm.mul %212, %arg4  : i32
  %216 = llvm.add %215, %211  : i32
  %217 = llvm.urem %216, %1  : i32
  %218 = llvm.getelementptr %18[%217] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %219 = llvm.load %218 : !llvm.ptr<f32, 3>
  %220 = llvm.fsub %214, %219  : f32
  %221 = llvm.call @__nv_expf(%220) : (f32) -> f32
  %222 = llvm.fadd %207, %221  : f32
  %223 = llvm.add %206, %15  : i32
  llvm.br ^bb16(%223, %222 : i32, f32)
^bb18:  // pred: ^bb16
  %224 = llvm.sub %8, %8  : i32
  %225 = llvm.lshr %0, %224  : i32
  %226 = llvm.sub %8, %7  : i32
  %227 = nvvm.shfl.sync  bfly %225, %207, %7, %226 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %228 = llvm.extractvalue %227[0] : !llvm.struct<(f32, i1)> 
  %229 = llvm.fadd %207, %228  : f32
  %230 = llvm.sub %8, %8  : i32
  %231 = llvm.lshr %0, %230  : i32
  %232 = llvm.sub %8, %7  : i32
  %233 = nvvm.shfl.sync  bfly %231, %229, %9, %232 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %234 = llvm.extractvalue %233[0] : !llvm.struct<(f32, i1)> 
  %235 = llvm.fadd %229, %234  : f32
  %236 = llvm.sub %8, %8  : i32
  %237 = llvm.lshr %0, %236  : i32
  %238 = llvm.sub %8, %7  : i32
  %239 = nvvm.shfl.sync  bfly %237, %235, %10, %238 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %240 = llvm.extractvalue %239[0] : !llvm.struct<(f32, i1)> 
  %241 = llvm.fadd %235, %240  : f32
  %242 = llvm.sub %8, %8  : i32
  %243 = llvm.lshr %0, %242  : i32
  %244 = llvm.sub %8, %7  : i32
  %245 = nvvm.shfl.sync  bfly %243, %241, %11, %244 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %246 = llvm.extractvalue %245[0] : !llvm.struct<(f32, i1)> 
  %247 = llvm.fadd %241, %246  : f32
  %248 = llvm.sub %8, %8  : i32
  %249 = llvm.lshr %0, %248  : i32
  %250 = llvm.sub %8, %7  : i32
  %251 = nvvm.shfl.sync  bfly %249, %247, %12, %250 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
  %252 = llvm.extractvalue %251[0] : !llvm.struct<(f32, i1)> 
  %253 = llvm.fadd %247, %252  : f32
  %254 = llvm.icmp "eq" %24, %16 : i32
  llvm.cond_br %254, ^bb19, ^bb20
^bb19:  // pred: ^bb18
  %255 = llvm.getelementptr %20[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %253, %255 : !llvm.ptr<f32, 3>
  llvm.br ^bb20
^bb20:  // 2 preds: ^bb18, ^bb19
  llvm.br ^bb21
^bb21:  // 2 preds: ^bb11, ^bb20
  nvvm.barrier0
  llvm.cond_br %27, ^bb22, ^bb29
^bb22:  // pred: ^bb21
  %256 = llvm.sub %arg5, %24  : i32
  %257 = llvm.icmp "eq" %256, %16 : i32
  %258 = llvm.sub %256, %14  : i32
  %259 = llvm.udiv %258, %15  : i32
  %260 = llvm.add %259, %14  : i32
  %261 = llvm.select %257, %16, %260 : i1, i32
  %262 = llvm.srem %261, %2  : i32
  %263 = llvm.sub %261, %262  : i32
  %264 = llvm.mul %263, %15  : i32
  %265 = llvm.add %24, %264  : i32
  %266 = llvm.mul %26, %arg5  : i32
  llvm.br ^bb23(%24 : i32)
^bb23(%267: i32):  // 2 preds: ^bb22, ^bb24
  %268 = llvm.icmp "slt" %267, %265 : i32
  llvm.cond_br %268, ^bb24, ^bb25
^bb24:  // pred: ^bb23
  %269 = llvm.add %267, %15  : i32
  %270 = llvm.add %267, %3  : i32
  %271 = llvm.add %267, %4  : i32
  %272 = llvm.add %266, %267  : i32
  %273 = llvm.add %266, %269  : i32
  %274 = llvm.add %266, %270  : i32
  %275 = llvm.add %266, %271  : i32
  %276 = llvm.udiv %272, %arg5  : i32
  %277 = llvm.urem %276, %arg4  : i32
  %278 = llvm.udiv %276, %arg4  : i32
  %279 = llvm.udiv %273, %arg5  : i32
  %280 = llvm.urem %279, %arg4  : i32
  %281 = llvm.udiv %279, %arg4  : i32
  %282 = llvm.udiv %274, %arg5  : i32
  %283 = llvm.urem %282, %arg4  : i32
  %284 = llvm.udiv %282, %arg4  : i32
  %285 = llvm.udiv %275, %arg5  : i32
  %286 = llvm.urem %285, %arg4  : i32
  %287 = llvm.udiv %285, %arg4  : i32
  %288 = llvm.getelementptr %arg1[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %289 = llvm.load %288 : !llvm.ptr<f32>
  %290 = llvm.getelementptr %arg1[%273] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %291 = llvm.load %290 : !llvm.ptr<f32>
  %292 = llvm.getelementptr %arg1[%274] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %293 = llvm.load %292 : !llvm.ptr<f32>
  %294 = llvm.getelementptr %arg1[%275] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %295 = llvm.load %294 : !llvm.ptr<f32>
  %296 = llvm.mul %278, %arg4  : i32
  %297 = llvm.add %296, %277  : i32
  %298 = llvm.mul %281, %arg4  : i32
  %299 = llvm.add %298, %280  : i32
  %300 = llvm.mul %284, %arg4  : i32
  %301 = llvm.add %300, %283  : i32
  %302 = llvm.mul %287, %arg4  : i32
  %303 = llvm.add %302, %286  : i32
  %304 = llvm.urem %297, %1  : i32
  %305 = llvm.urem %299, %1  : i32
  %306 = llvm.urem %301, %1  : i32
  %307 = llvm.urem %303, %1  : i32
  %308 = llvm.getelementptr %18[%304] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %309 = llvm.load %308 : !llvm.ptr<f32, 3>
  %310 = llvm.getelementptr %18[%305] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %311 = llvm.load %310 : !llvm.ptr<f32, 3>
  %312 = llvm.getelementptr %18[%306] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %313 = llvm.load %312 : !llvm.ptr<f32, 3>
  %314 = llvm.getelementptr %18[%307] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %315 = llvm.load %314 : !llvm.ptr<f32, 3>
  %316 = llvm.fsub %289, %309  : f32
  %317 = llvm.fsub %291, %311  : f32
  %318 = llvm.fsub %293, %313  : f32
  %319 = llvm.fsub %295, %315  : f32
  %320 = llvm.call @__nv_expf(%316) : (f32) -> f32
  %321 = llvm.call @__nv_expf(%317) : (f32) -> f32
  %322 = llvm.call @__nv_expf(%318) : (f32) -> f32
  %323 = llvm.call @__nv_expf(%319) : (f32) -> f32
  %324 = llvm.getelementptr %20[%304] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %325 = llvm.load %324 : !llvm.ptr<f32, 3>
  %326 = llvm.getelementptr %20[%305] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %327 = llvm.load %326 : !llvm.ptr<f32, 3>
  %328 = llvm.getelementptr %20[%306] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %329 = llvm.load %328 : !llvm.ptr<f32, 3>
  %330 = llvm.getelementptr %20[%307] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %331 = llvm.load %330 : !llvm.ptr<f32, 3>
  %332 = llvm.fdiv %320, %325  : f32
  %333 = llvm.fdiv %321, %327  : f32
  %334 = llvm.fdiv %322, %329  : f32
  %335 = llvm.fdiv %323, %331  : f32
  %336 = llvm.getelementptr %arg11[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  llvm.store %332, %336 : !llvm.ptr<f32>
  %337 = llvm.getelementptr %arg11[%273] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  llvm.store %333, %337 : !llvm.ptr<f32>
  %338 = llvm.getelementptr %arg11[%274] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  llvm.store %334, %338 : !llvm.ptr<f32>
  %339 = llvm.getelementptr %arg11[%275] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  llvm.store %335, %339 : !llvm.ptr<f32>
  %340 = llvm.add %267, %5  : i32
  llvm.br ^bb23(%340 : i32)
^bb25:  // pred: ^bb23
  llvm.br ^bb26(%265 : i32)
^bb26(%341: i32):  // 2 preds: ^bb25, ^bb27
  %342 = llvm.icmp "slt" %341, %arg5 : i32
  llvm.cond_br %342, ^bb27, ^bb28
^bb27:  // pred: ^bb26
  %343 = llvm.add %266, %341  : i32
  %344 = llvm.udiv %343, %arg5  : i32
  %345 = llvm.urem %344, %arg4  : i32
  %346 = llvm.udiv %344, %arg4  : i32
  %347 = llvm.getelementptr %arg1[%343] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %348 = llvm.load %347 : !llvm.ptr<f32>
  %349 = llvm.mul %346, %arg4  : i32
  %350 = llvm.add %349, %345  : i32
  %351 = llvm.urem %350, %1  : i32
  %352 = llvm.getelementptr %18[%351] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %353 = llvm.load %352 : !llvm.ptr<f32, 3>
  %354 = llvm.fsub %348, %353  : f32
  %355 = llvm.call @__nv_expf(%354) : (f32) -> f32
  %356 = llvm.getelementptr %20[%351] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %357 = llvm.load %356 : !llvm.ptr<f32, 3>
  %358 = llvm.fdiv %355, %357  : f32
  %359 = llvm.getelementptr %arg11[%343] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  llvm.store %358, %359 : !llvm.ptr<f32>
  %360 = llvm.add %341, %15  : i32
  llvm.br ^bb26(%360 : i32)
^bb28:  // pred: ^bb26
  llvm.br ^bb29
^bb29:  // 2 preds: ^bb21, ^bb28
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel_0 {
  llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1w1r_0() {addr_space = 3 : i32} : !llvm.array<32 x f32>
  llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1w1r_1() {addr_space = 3 : i32} : !llvm.array<32 x f32>
  llvm.func @__nv_expf(f32) -> f32
  llvm.func @main_kStitch_divide__13_1_0___1w1r(%arg0: !llvm.ptr<f32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: !llvm.ptr<f32>) attributes {disc.elimargs = [0 : index, 2 : index, 3 : index, 6 : index, 7 : index, 8 : index, 10 : index, 12 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(-1 : i32) : i32
    %1 = llvm.mlir.constant(8 : index) : i32
    %2 = llvm.mlir.constant(4 : index) : i32
    %3 = llvm.mlir.constant(64 : index) : i32
    %4 = llvm.mlir.constant(96 : index) : i32
    %5 = llvm.mlir.constant(128 : index) : i32
    %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %7 = llvm.mlir.constant(1 : i32) : i32
    %8 = llvm.mlir.constant(32 : i32) : i32
    %9 = llvm.mlir.constant(2 : i32) : i32
    %10 = llvm.mlir.constant(4 : i32) : i32
    %11 = llvm.mlir.constant(8 : i32) : i32
    %12 = llvm.mlir.constant(16 : i32) : i32
    %13 = llvm.mlir.constant(-0.000000e+00 : f32) : f32
    %14 = llvm.mlir.constant(1 : index) : i32
    %15 = llvm.mlir.constant(32 : index) : i32
    %16 = llvm.mlir.constant(0 : index) : i32
    %17 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1w1r_0 : !llvm.ptr<array<32 x f32>, 3>
    %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %19 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1w1r_1 : !llvm.ptr<array<32 x f32>, 3>
    %20 = llvm.getelementptr %19[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %21 = nvvm.read.ptx.sreg.ctaid.x : i32
    %22 = nvvm.read.ptx.sreg.tid.x : i32
    %23 = llvm.udiv %22, %15  : i32
    %24 = llvm.urem %22, %15  : i32
    %25 = llvm.mul %21, %1  : i32
    %26 = llvm.add %25, %23  : i32
    %27 = llvm.icmp "slt" %26, %arg3 : i32
    llvm.cond_br %27, ^bb2, ^bb11
  ^bb2:  // pred: ^bb1
    %28 = llvm.sub %arg2, %24  : i32
    %29 = llvm.icmp "eq" %28, %16 : i32
    %30 = llvm.sub %28, %14  : i32
    %31 = llvm.udiv %30, %15  : i32
    %32 = llvm.add %31, %14  : i32
    %33 = llvm.select %29, %16, %32 : i1, i32
    %34 = llvm.srem %33, %2  : i32
    %35 = llvm.sub %33, %34  : i32
    %36 = llvm.mul %35, %15  : i32
    %37 = llvm.add %24, %36  : i32
    %38 = llvm.mul %26, %arg2  : i32
    llvm.br ^bb3(%24, %6 : i32, f32)
  ^bb3(%39: i32, %40: f32):  // 2 preds: ^bb2, ^bb4
    %41 = llvm.icmp "slt" %39, %37 : i32
    llvm.cond_br %41, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %42 = llvm.add %39, %15  : i32
    %43 = llvm.add %39, %3  : i32
    %44 = llvm.add %39, %4  : i32
    %45 = llvm.add %38, %39  : i32
    %46 = llvm.add %38, %42  : i32
    %47 = llvm.add %38, %43  : i32
    %48 = llvm.add %38, %44  : i32
    %49 = llvm.getelementptr %arg0[%45] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %50 = llvm.load %49 : !llvm.ptr<f32>
    %51 = llvm.getelementptr %arg0[%46] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %52 = llvm.load %51 : !llvm.ptr<f32>
    %53 = llvm.getelementptr %arg0[%47] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %54 = llvm.load %53 : !llvm.ptr<f32>
    %55 = llvm.getelementptr %arg0[%48] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %56 = llvm.load %55 : !llvm.ptr<f32>
    %57 = llvm.fcmp "ugt" %40, %50 : f32
    %58 = llvm.select %57, %40, %50 : i1, f32
    %59 = llvm.fcmp "uno" %50, %50 : f32
    %60 = llvm.select %59, %50, %58 : i1, f32
    %61 = llvm.fcmp "ugt" %60, %52 : f32
    %62 = llvm.select %61, %60, %52 : i1, f32
    %63 = llvm.fcmp "uno" %52, %52 : f32
    %64 = llvm.select %63, %52, %62 : i1, f32
    %65 = llvm.fcmp "ugt" %64, %54 : f32
    %66 = llvm.select %65, %64, %54 : i1, f32
    %67 = llvm.fcmp "uno" %54, %54 : f32
    %68 = llvm.select %67, %54, %66 : i1, f32
    %69 = llvm.fcmp "ugt" %68, %56 : f32
    %70 = llvm.select %69, %68, %56 : i1, f32
    %71 = llvm.fcmp "uno" %56, %56 : f32
    %72 = llvm.select %71, %56, %70 : i1, f32
    %73 = llvm.add %39, %5  : i32
    llvm.br ^bb3(%73, %72 : i32, f32)
  ^bb5:  // pred: ^bb3
    llvm.br ^bb6(%37, %40 : i32, f32)
  ^bb6(%74: i32, %75: f32):  // 2 preds: ^bb5, ^bb7
    %76 = llvm.icmp "slt" %74, %arg2 : i32
    llvm.cond_br %76, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %77 = llvm.add %38, %74  : i32
    %78 = llvm.getelementptr %arg0[%77] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %79 = llvm.load %78 : !llvm.ptr<f32>
    %80 = llvm.fcmp "ugt" %75, %79 : f32
    %81 = llvm.select %80, %75, %79 : i1, f32
    %82 = llvm.fcmp "uno" %79, %79 : f32
    %83 = llvm.select %82, %79, %81 : i1, f32
    %84 = llvm.add %74, %15  : i32
    llvm.br ^bb6(%84, %83 : i32, f32)
  ^bb8:  // pred: ^bb6
    %85 = llvm.sub %8, %8  : i32
    %86 = llvm.lshr %0, %85  : i32
    %87 = llvm.sub %8, %7  : i32
    %88 = nvvm.shfl.sync  bfly %86, %75, %7, %87 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %89 = llvm.extractvalue %88[0] : !llvm.struct<(f32, i1)> 
    %90 = llvm.fcmp "ugt" %75, %89 : f32
    %91 = llvm.select %90, %75, %89 : i1, f32
    %92 = llvm.fcmp "uno" %89, %89 : f32
    %93 = llvm.select %92, %89, %91 : i1, f32
    %94 = llvm.sub %8, %8  : i32
    %95 = llvm.lshr %0, %94  : i32
    %96 = llvm.sub %8, %7  : i32
    %97 = nvvm.shfl.sync  bfly %95, %93, %9, %96 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %98 = llvm.extractvalue %97[0] : !llvm.struct<(f32, i1)> 
    %99 = llvm.fcmp "ugt" %93, %98 : f32
    %100 = llvm.select %99, %93, %98 : i1, f32
    %101 = llvm.fcmp "uno" %98, %98 : f32
    %102 = llvm.select %101, %98, %100 : i1, f32
    %103 = llvm.sub %8, %8  : i32
    %104 = llvm.lshr %0, %103  : i32
    %105 = llvm.sub %8, %7  : i32
    %106 = nvvm.shfl.sync  bfly %104, %102, %10, %105 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %107 = llvm.extractvalue %106[0] : !llvm.struct<(f32, i1)> 
    %108 = llvm.fcmp "ugt" %102, %107 : f32
    %109 = llvm.select %108, %102, %107 : i1, f32
    %110 = llvm.fcmp "uno" %107, %107 : f32
    %111 = llvm.select %110, %107, %109 : i1, f32
    %112 = llvm.sub %8, %8  : i32
    %113 = llvm.lshr %0, %112  : i32
    %114 = llvm.sub %8, %7  : i32
    %115 = nvvm.shfl.sync  bfly %113, %111, %11, %114 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %116 = llvm.extractvalue %115[0] : !llvm.struct<(f32, i1)> 
    %117 = llvm.fcmp "ugt" %111, %116 : f32
    %118 = llvm.select %117, %111, %116 : i1, f32
    %119 = llvm.fcmp "uno" %116, %116 : f32
    %120 = llvm.select %119, %116, %118 : i1, f32
    %121 = llvm.sub %8, %8  : i32
    %122 = llvm.lshr %0, %121  : i32
    %123 = llvm.sub %8, %7  : i32
    %124 = nvvm.shfl.sync  bfly %122, %120, %12, %123 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %125 = llvm.extractvalue %124[0] : !llvm.struct<(f32, i1)> 
    %126 = llvm.fcmp "ugt" %120, %125 : f32
    %127 = llvm.select %126, %120, %125 : i1, f32
    %128 = llvm.fcmp "uno" %125, %125 : f32
    %129 = llvm.select %128, %125, %127 : i1, f32
    %130 = llvm.icmp "eq" %24, %16 : i32
    llvm.cond_br %130, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %131 = llvm.getelementptr %18[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %129, %131 : !llvm.ptr<f32, 3>
    llvm.br ^bb10
  ^bb10:  // 2 preds: ^bb8, ^bb9
    llvm.br ^bb11
  ^bb11:  // 2 preds: ^bb1, ^bb10
    nvvm.barrier0
    llvm.cond_br %27, ^bb12, ^bb21
  ^bb12:  // pred: ^bb11
    %132 = llvm.sub %arg2, %24  : i32
    %133 = llvm.icmp "eq" %132, %16 : i32
    %134 = llvm.sub %132, %14  : i32
    %135 = llvm.udiv %134, %15  : i32
    %136 = llvm.add %135, %14  : i32
    %137 = llvm.select %133, %16, %136 : i1, i32
    %138 = llvm.srem %137, %2  : i32
    %139 = llvm.sub %137, %138  : i32
    %140 = llvm.mul %139, %15  : i32
    %141 = llvm.add %24, %140  : i32
    %142 = llvm.mul %26, %arg2  : i32
    llvm.br ^bb13(%24, %13 : i32, f32)
  ^bb13(%143: i32, %144: f32):  // 2 preds: ^bb12, ^bb14
    %145 = llvm.icmp "slt" %143, %141 : i32
    llvm.cond_br %145, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %146 = llvm.add %143, %15  : i32
    %147 = llvm.add %143, %3  : i32
    %148 = llvm.add %143, %4  : i32
    %149 = llvm.add %142, %143  : i32
    %150 = llvm.add %142, %146  : i32
    %151 = llvm.add %142, %147  : i32
    %152 = llvm.add %142, %148  : i32
    %153 = llvm.udiv %149, %arg2  : i32
    %154 = llvm.urem %153, %arg1  : i32
    %155 = llvm.udiv %153, %arg1  : i32
    %156 = llvm.udiv %150, %arg2  : i32
    %157 = llvm.urem %156, %arg1  : i32
    %158 = llvm.udiv %156, %arg1  : i32
    %159 = llvm.udiv %151, %arg2  : i32
    %160 = llvm.urem %159, %arg1  : i32
    %161 = llvm.udiv %159, %arg1  : i32
    %162 = llvm.udiv %152, %arg2  : i32
    %163 = llvm.urem %162, %arg1  : i32
    %164 = llvm.udiv %162, %arg1  : i32
    %165 = llvm.getelementptr %arg0[%149] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %166 = llvm.load %165 : !llvm.ptr<f32>
    %167 = llvm.getelementptr %arg0[%150] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %168 = llvm.load %167 : !llvm.ptr<f32>
    %169 = llvm.getelementptr %arg0[%151] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %170 = llvm.load %169 : !llvm.ptr<f32>
    %171 = llvm.getelementptr %arg0[%152] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %172 = llvm.load %171 : !llvm.ptr<f32>
    %173 = llvm.mul %155, %arg1  : i32
    %174 = llvm.add %173, %154  : i32
    %175 = llvm.mul %158, %arg1  : i32
    %176 = llvm.add %175, %157  : i32
    %177 = llvm.mul %161, %arg1  : i32
    %178 = llvm.add %177, %160  : i32
    %179 = llvm.mul %164, %arg1  : i32
    %180 = llvm.add %179, %163  : i32
    %181 = llvm.urem %174, %1  : i32
    %182 = llvm.urem %176, %1  : i32
    %183 = llvm.urem %178, %1  : i32
    %184 = llvm.urem %180, %1  : i32
    %185 = llvm.getelementptr %18[%181] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %186 = llvm.load %185 : !llvm.ptr<f32, 3>
    %187 = llvm.getelementptr %18[%182] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %188 = llvm.load %187 : !llvm.ptr<f32, 3>
    %189 = llvm.getelementptr %18[%183] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %190 = llvm.load %189 : !llvm.ptr<f32, 3>
    %191 = llvm.getelementptr %18[%184] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %192 = llvm.load %191 : !llvm.ptr<f32, 3>
    %193 = llvm.fsub %166, %186  : f32
    %194 = llvm.fsub %168, %188  : f32
    %195 = llvm.fsub %170, %190  : f32
    %196 = llvm.fsub %172, %192  : f32
    %197 = llvm.call @__nv_expf(%193) : (f32) -> f32
    %198 = llvm.call @__nv_expf(%194) : (f32) -> f32
    %199 = llvm.call @__nv_expf(%195) : (f32) -> f32
    %200 = llvm.call @__nv_expf(%196) : (f32) -> f32
    %201 = llvm.fadd %144, %197  : f32
    %202 = llvm.fadd %201, %198  : f32
    %203 = llvm.fadd %202, %199  : f32
    %204 = llvm.fadd %203, %200  : f32
    %205 = llvm.add %143, %5  : i32
    llvm.br ^bb13(%205, %204 : i32, f32)
  ^bb15:  // pred: ^bb13
    llvm.br ^bb16(%141, %144 : i32, f32)
  ^bb16(%206: i32, %207: f32):  // 2 preds: ^bb15, ^bb17
    %208 = llvm.icmp "slt" %206, %arg2 : i32
    llvm.cond_br %208, ^bb17, ^bb18
  ^bb17:  // pred: ^bb16
    %209 = llvm.add %142, %206  : i32
    %210 = llvm.udiv %209, %arg2  : i32
    %211 = llvm.urem %210, %arg1  : i32
    %212 = llvm.udiv %210, %arg1  : i32
    %213 = llvm.getelementptr %arg0[%209] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %214 = llvm.load %213 : !llvm.ptr<f32>
    %215 = llvm.mul %212, %arg1  : i32
    %216 = llvm.add %215, %211  : i32
    %217 = llvm.urem %216, %1  : i32
    %218 = llvm.getelementptr %18[%217] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %219 = llvm.load %218 : !llvm.ptr<f32, 3>
    %220 = llvm.fsub %214, %219  : f32
    %221 = llvm.call @__nv_expf(%220) : (f32) -> f32
    %222 = llvm.fadd %207, %221  : f32
    %223 = llvm.add %206, %15  : i32
    llvm.br ^bb16(%223, %222 : i32, f32)
  ^bb18:  // pred: ^bb16
    %224 = llvm.sub %8, %8  : i32
    %225 = llvm.lshr %0, %224  : i32
    %226 = llvm.sub %8, %7  : i32
    %227 = nvvm.shfl.sync  bfly %225, %207, %7, %226 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %228 = llvm.extractvalue %227[0] : !llvm.struct<(f32, i1)> 
    %229 = llvm.fadd %207, %228  : f32
    %230 = llvm.sub %8, %8  : i32
    %231 = llvm.lshr %0, %230  : i32
    %232 = llvm.sub %8, %7  : i32
    %233 = nvvm.shfl.sync  bfly %231, %229, %9, %232 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %234 = llvm.extractvalue %233[0] : !llvm.struct<(f32, i1)> 
    %235 = llvm.fadd %229, %234  : f32
    %236 = llvm.sub %8, %8  : i32
    %237 = llvm.lshr %0, %236  : i32
    %238 = llvm.sub %8, %7  : i32
    %239 = nvvm.shfl.sync  bfly %237, %235, %10, %238 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %240 = llvm.extractvalue %239[0] : !llvm.struct<(f32, i1)> 
    %241 = llvm.fadd %235, %240  : f32
    %242 = llvm.sub %8, %8  : i32
    %243 = llvm.lshr %0, %242  : i32
    %244 = llvm.sub %8, %7  : i32
    %245 = nvvm.shfl.sync  bfly %243, %241, %11, %244 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %246 = llvm.extractvalue %245[0] : !llvm.struct<(f32, i1)> 
    %247 = llvm.fadd %241, %246  : f32
    %248 = llvm.sub %8, %8  : i32
    %249 = llvm.lshr %0, %248  : i32
    %250 = llvm.sub %8, %7  : i32
    %251 = nvvm.shfl.sync  bfly %249, %247, %12, %250 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %252 = llvm.extractvalue %251[0] : !llvm.struct<(f32, i1)> 
    %253 = llvm.fadd %247, %252  : f32
    %254 = llvm.icmp "eq" %24, %16 : i32
    llvm.cond_br %254, ^bb19, ^bb20
  ^bb19:  // pred: ^bb18
    %255 = llvm.getelementptr %20[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %253, %255 : !llvm.ptr<f32, 3>
    llvm.br ^bb20
  ^bb20:  // 2 preds: ^bb18, ^bb19
    llvm.br ^bb21
  ^bb21:  // 2 preds: ^bb11, ^bb20
    nvvm.barrier0
    llvm.cond_br %27, ^bb22, ^bb29
  ^bb22:  // pred: ^bb21
    %256 = llvm.sub %arg2, %24  : i32
    %257 = llvm.icmp "eq" %256, %16 : i32
    %258 = llvm.sub %256, %14  : i32
    %259 = llvm.udiv %258, %15  : i32
    %260 = llvm.add %259, %14  : i32
    %261 = llvm.select %257, %16, %260 : i1, i32
    %262 = llvm.srem %261, %2  : i32
    %263 = llvm.sub %261, %262  : i32
    %264 = llvm.mul %263, %15  : i32
    %265 = llvm.add %24, %264  : i32
    %266 = llvm.mul %26, %arg2  : i32
    llvm.br ^bb23(%24 : i32)
  ^bb23(%267: i32):  // 2 preds: ^bb22, ^bb24
    %268 = llvm.icmp "slt" %267, %265 : i32
    llvm.cond_br %268, ^bb24, ^bb25
  ^bb24:  // pred: ^bb23
    %269 = llvm.add %267, %15  : i32
    %270 = llvm.add %267, %3  : i32
    %271 = llvm.add %267, %4  : i32
    %272 = llvm.add %266, %267  : i32
    %273 = llvm.add %266, %269  : i32
    %274 = llvm.add %266, %270  : i32
    %275 = llvm.add %266, %271  : i32
    %276 = llvm.udiv %272, %arg2  : i32
    %277 = llvm.urem %276, %arg1  : i32
    %278 = llvm.udiv %276, %arg1  : i32
    %279 = llvm.udiv %273, %arg2  : i32
    %280 = llvm.urem %279, %arg1  : i32
    %281 = llvm.udiv %279, %arg1  : i32
    %282 = llvm.udiv %274, %arg2  : i32
    %283 = llvm.urem %282, %arg1  : i32
    %284 = llvm.udiv %282, %arg1  : i32
    %285 = llvm.udiv %275, %arg2  : i32
    %286 = llvm.urem %285, %arg1  : i32
    %287 = llvm.udiv %285, %arg1  : i32
    %288 = llvm.getelementptr %arg0[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %289 = llvm.load %288 : !llvm.ptr<f32>
    %290 = llvm.getelementptr %arg0[%273] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %291 = llvm.load %290 : !llvm.ptr<f32>
    %292 = llvm.getelementptr %arg0[%274] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %293 = llvm.load %292 : !llvm.ptr<f32>
    %294 = llvm.getelementptr %arg0[%275] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %295 = llvm.load %294 : !llvm.ptr<f32>
    %296 = llvm.mul %278, %arg1  : i32
    %297 = llvm.add %296, %277  : i32
    %298 = llvm.mul %281, %arg1  : i32
    %299 = llvm.add %298, %280  : i32
    %300 = llvm.mul %284, %arg1  : i32
    %301 = llvm.add %300, %283  : i32
    %302 = llvm.mul %287, %arg1  : i32
    %303 = llvm.add %302, %286  : i32
    %304 = llvm.urem %297, %1  : i32
    %305 = llvm.urem %299, %1  : i32
    %306 = llvm.urem %301, %1  : i32
    %307 = llvm.urem %303, %1  : i32
    %308 = llvm.getelementptr %18[%304] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %309 = llvm.load %308 : !llvm.ptr<f32, 3>
    %310 = llvm.getelementptr %18[%305] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %311 = llvm.load %310 : !llvm.ptr<f32, 3>
    %312 = llvm.getelementptr %18[%306] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %313 = llvm.load %312 : !llvm.ptr<f32, 3>
    %314 = llvm.getelementptr %18[%307] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %315 = llvm.load %314 : !llvm.ptr<f32, 3>
    %316 = llvm.fsub %289, %309  : f32
    %317 = llvm.fsub %291, %311  : f32
    %318 = llvm.fsub %293, %313  : f32
    %319 = llvm.fsub %295, %315  : f32
    %320 = llvm.call @__nv_expf(%316) : (f32) -> f32
    %321 = llvm.call @__nv_expf(%317) : (f32) -> f32
    %322 = llvm.call @__nv_expf(%318) : (f32) -> f32
    %323 = llvm.call @__nv_expf(%319) : (f32) -> f32
    %324 = llvm.getelementptr %20[%304] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %325 = llvm.load %324 : !llvm.ptr<f32, 3>
    %326 = llvm.getelementptr %20[%305] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %327 = llvm.load %326 : !llvm.ptr<f32, 3>
    %328 = llvm.getelementptr %20[%306] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %329 = llvm.load %328 : !llvm.ptr<f32, 3>
    %330 = llvm.getelementptr %20[%307] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %331 = llvm.load %330 : !llvm.ptr<f32, 3>
    %332 = llvm.fdiv %320, %325  : f32
    %333 = llvm.fdiv %321, %327  : f32
    %334 = llvm.fdiv %322, %329  : f32
    %335 = llvm.fdiv %323, %331  : f32
    %336 = llvm.getelementptr %arg4[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %332, %336 : !llvm.ptr<f32>
    %337 = llvm.getelementptr %arg4[%273] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %333, %337 : !llvm.ptr<f32>
    %338 = llvm.getelementptr %arg4[%274] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %334, %338 : !llvm.ptr<f32>
    %339 = llvm.getelementptr %arg4[%275] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %335, %339 : !llvm.ptr<f32>
    %340 = llvm.add %267, %5  : i32
    llvm.br ^bb23(%340 : i32)
  ^bb25:  // pred: ^bb23
    llvm.br ^bb26(%265 : i32)
  ^bb26(%341: i32):  // 2 preds: ^bb25, ^bb27
    %342 = llvm.icmp "slt" %341, %arg2 : i32
    llvm.cond_br %342, ^bb27, ^bb28
  ^bb27:  // pred: ^bb26
    %343 = llvm.add %266, %341  : i32
    %344 = llvm.udiv %343, %arg2  : i32
    %345 = llvm.urem %344, %arg1  : i32
    %346 = llvm.udiv %344, %arg1  : i32
    %347 = llvm.getelementptr %arg0[%343] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %348 = llvm.load %347 : !llvm.ptr<f32>
    %349 = llvm.mul %346, %arg1  : i32
    %350 = llvm.add %349, %345  : i32
    %351 = llvm.urem %350, %1  : i32
    %352 = llvm.getelementptr %18[%351] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %353 = llvm.load %352 : !llvm.ptr<f32, 3>
    %354 = llvm.fsub %348, %353  : f32
    %355 = llvm.call @__nv_expf(%354) : (f32) -> f32
    %356 = llvm.getelementptr %20[%351] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %357 = llvm.load %356 : !llvm.ptr<f32, 3>
    %358 = llvm.fdiv %355, %357  : f32
    %359 = llvm.getelementptr %arg4[%343] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %358, %359 : !llvm.ptr<f32>
    %360 = llvm.add %341, %15  : i32
    llvm.br ^bb26(%360 : i32)
  ^bb28:  // pred: ^bb26
    llvm.br ^bb29
  ^bb29:  // 2 preds: ^bb21, ^bb28
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\A0 \00\00\00\00\00\00\02\00\01\01@\00\00\00` \00\00\00\00\00\00^ \00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(K\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80J\08\00\11G\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F5)_shndx\00.nv.info\00.text.main_kStitch_divide__13_1_0___1w1r2\00\0F,\00\15oshared.\00\15\9Fconstant01\00\12\FD\01debug_frame\00.rel\11\00\1Aa\12\00!nv&\00oaction%\01 \0F\94\00\0F\0FH\01q\1F$$\00\0F\FF\16$__cuda_sm70_shflsync_bfly_p\00$____wg_G\00\0F\00\09\00/261\00\19_1__28\EA\01\1Fo_param\F1\01.\0F\01\00\06\8CU\00\00\00\03\00\0A\00\01\00\11\AA\18\00,\0B\00\01\00\A1\D8\00\00\00\22\00\0A\00@=\0F\001\00@\01\07\000\00z\010\00,\09\00\01\00\11\B2\18\00,\04\00\01\00\11\E2\18\00,\07\00\01\00f2\00\00\00\12\10\90\00!\80>\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\130\C0\00\10\04\9B\00R\04<\00\00\00E\005\04\08\0F<\00&8\00H\00\04p\00L\84\80\80(o\00\10\08\15\00\81\16\85\80\80(\09\05\0C2\00\068\00\22\18\00\01\00\22p\00\01\00\048\01\1308\011\04/\08\06\01\10&\0C\00R#\08\00\03\00\01\00'\04\12\0C\00\17\11\0C\00\12#0\00\04$\00\17\07$\00\05\0C\00!7\044\05q\015\00\00\04\0A\08~\01\A2`\01 \00\03\19 \00\04\17\A2\00u\04\00\18\00\00\F0!\10\00u\03\00\10\00\00\F0\11\10\009\02\00\0C\10\009\01\00\08\10\00\01\01\00\F1_\F0!\00\03\1B\FF\00\04(P\00\C0\12\00\00\00\13\00\00@\13\00\00\80\13\00\00\C0\13\00\00\10+\00\000+\00\00P+\00\00p+\00\00\90+\00\00\00:\00\00p:\00\00\E0:\00\00P;\00\00\C0;\00\00\10<\00\00P<\00\00\90<\00\00\D0<\00\00 =\00\00\04\1C\0C\00\F0+\00\00`6\00\00\C09\00\00\04\1E\04s\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\D8\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00\C8\00\01\001\02\00\00\C8\01*H\00\10\00\0F\01\00\FF\D6@$v\01\FF\8F\05\B1\FF\00\8E\07\00\C4\0F\00\19y\0D\18\00 \00!y\05\A1\0E\00\B9z\04\00\00F\00\00\B3\05\90\E2\0F\00Ey\00\00\10\14!\00 \80\03\10\00c$t\0B\FF\1F\00@\00\10\E2@\00\03:\04\B1%\00\00\00b\0E\00$t\1A\FF\01\00\04 \00\F1\04x\0A\FF\05\00\00\00\0D\16\01\00\00\E4\1F\00\12x\0D\0D@\00\B0\C0\8E\07\00\C6\0F\00$x\0C\03\A9\02 \0A\020\00\D0/\00\0Cz\00\0D\00[\00\00pR\F2p\00g\04$x\0A\0A\04p\00P\0Cr\00\0D\FF\F0\04`R\F4\03\00\E4\0F0\00\81\0C\00\\\00\00pb\F6\10\00@\12r\00\FF\D2\00\A2\FF3\8E\07\00\D6\0F\00G94\04\01\C0\00p\EA\0F\00\10z\02\00`\00B\FF\E0\FF\07\E0\00H\01\00\E0\0C\E0\00R\13\FF\00\00\80\C0\00\93\E4\0F\00\10x\02\02 \000\00\80\C8\0F\00\12x\02\02\80\E0\00!\C0\8E\10\00a\07r\02\02\FF\00@\00\11\00 \001r\0F\02\80\00\12\FC \00\00\B0\00\00\F2\05\E6pb\F0\03\00\DA\0F\00G\09\00\00`\0C\90\00@$x\02\0F\AB\05!\0D\0A\E0\00v\10Ey\02\00\F0\06\90\001\1Cx\00\01\00?p\F0\F0\A0\00\02\B2\0Cx\00\02\80\01\00\00pB\F8 \000r\11\FF\90\00\10\0D \00\96\D8\0F\00G\C9\00\00\90\06p\00\05P\002\E1\F0\03\E0\00a\10\0F\80\FE\FF\FF\E0\00\10\C4`\00%\16\FFp\01p\E4\0F\00$z\09\0C0\01\22\11\02\D0\00\90%v\02\09\00X\00\00\16\10\00p\CA\0F\00\81y\0E\020\00\C5\00\19\1E\0C\00\A2\00\00\10x\05\090\01\84\E4\0F\04\10x\07\09@\10\00\10\C6@\00*\04\05@\00%\15\04@\00u\E2\02\00%v\06\07 \00\86\E2\0F\08\10x\09\09`P\00g\00\10x\17\11\80\10\00E\81y\12\06@\00fb\09\00%v\08\A0\00\10\C8\C0\00\11\1B\C0\003\17\02\8E0\00%\17\080\00\10d0\00&\02\1Bp\00h\1F\04\10x\19\1B\F0\016\81y\14\E0\00 b\01\C0\00\16\190\00y/\00\10x\1D\1B@0\00\16\19\D0\00 b\03\D0\00\16\1D0\00j\0F\09\10x\1B\1B\D0\00F\1D\11\00\01\10\00<\81y\18\D0\00\06\A0\00\01\D0\00\11\1F\D0\00\12\1DP\00F\00\81y\1B\D0\00\11f\D0\00\1A\1F\D0\00*\1D\1F\D0\00\1C\1C\D0\00\1A\1D\D0\00*!\1F\D0\00\1C\1D\D0\00\1A!\D0\00*\1F\1F\D0\00F!\11\80\01\10\00<\81y\1E\D0\00\06\A0\00\01\D0\00\11!\D0\00\15!\D0\00\17\1F\D0\00\00p\00*\02!\D0\00%#!\D0\00\01@\02\17\02\D0\00\01`\02\1A#\D0\009#!@0\00*\05\040\00\17\060\00y\0F\01\10x!!`0\00\16\06\C0\00\10\220\00\18\08\00\01`\00\0Br\00\0E\0E \00\B1\80\F8\03\00\E4O\08\0Br\00\13\10\000\C0\FA\03\80\04A\08\C2\0E\0E\7F\042\00\80\06\10\01\16\13\E0\00 \A2\0E@\00!\15\15\1F\00P\FA\03\00\E4\8F@\00%\12\12P\00 \0F\02`\00\01 \00\B1\C0\FC\03\00\D2\0F\00\08\D2\15\15`\00!\00\00`\01U\0Br\00\17\17@\00\01\10\00$\15\120\00\10\C8\80\00!\12\12@\00\060\00&\14\14`\00\00\10\00(\12\170\001\D2\17\17@\00\060\00*\19\19`\00)\17\14`\00!\14\14@\00\060\00*\18\18`\00)\14\19`\00!\19\19@\00\060\00*\1B\1B`\00)\19\18`\00!\18\18@\00\060\00*\1C\1C`\00)\18\1B`\00!\1B\1B@\00\060\00*\1D\1D`\00)\1B\1C`\00!\1C\1C@\00\060\00*\1E\1E`\00)\1C\1D`\00!\1D\1D@\00\060\00*\1F\1F`\00)\1D\1E`\00!\1E\1E@\00\060\00*\02\02`\00)\1E\1F`\00!\1F\1F@\00\060\00*\05\05`\00)\1F\02`\00!\02\02@\00\060\00&\06\06`\00\10\01p\00\19\05`\00!\05\05@\00\030\00c\10x\11\11\00\02\F0\02\03p\00\19\06`\00!\06\06@\00\030\00P\0Cr\00\11\10@\06$b\FC\90\00%\13\13\A0\00\01\00\03\11\06\10\00\22\C0\F8\00\031\D2\13\13P\000\00\00\060\03`G\E9\00\00\90\F9A\07\11\83\90\06#Ay\8E\00\0B\10\073\11\0A\8E\A0\07C\02\00p\03 \00\13\E8\F0\06\13\00\F0\06\12\DA\E0\06'@\03@\00+t\03\C0\06\1B\0E\C0\06 \04\0E\C0\033\03\02\8E\E0\03*\02\04\C0\06%\06\0E`\04\01\C0\06*\08\0E\C0\06*\06\06@\00*\19\06\C0\06%\08\08 \00\01\C0\06/\14\0E\C0\06\0B*\0E\08 \05%\14\14@\00\01 \05\1B\10\C0\06%\15\140\00\10d\90\00'\16\10p\00V\04\10x\04\10 \05k\1F\00\81y\16\16\F0\04\16\040\00\00 \05*\04\10 \05*\07\060\00\17\040\00\00\C0\00'\08\10 \05[\01\81y\04\04 \05\08\00\01\07\F0\02\15\F0 \05\18\02 \021\82\02\020\02/\00\00 \05\05%\19\19\D0\02&\8F\08p\05\14\F0 \05$\02\19p\05\00 \051\C2\19\19`\00\03p\05\06`\05\14\F8\B0\02\15\19\A0\05\00\E0\02J\82\0E\0E\190\00&\16\16`\00\16\00\80\05\030\00\14\C2\80\05\050\00*\07\07`\00)\15\16`\00:\16\16\150\00&\04\04`\00\00\A0\03)\16\07`\004\07\07\160\00\11\C80\00)\07\04P\007\04\04\07P\00\0E\D0\09\19\11P\08\06\A0\03\14\F8\80\01$\04\13P\00\00\80\01D\C2\13\13\04P\00\1E\C4\90\03\00\F0\03\01\C0\0A1\16p\00p\03X\89\00\00\A0\01p\03\1B\09p\03\1B\08p\03\11\02\10\02\16\09\A0\02/\08 \C0\07\05%\A2\0Ep\02\15\09p\02/\06\08\C0\07\06%\E2\0E\90\03\060\00?\08\08`\C0\07\0B\06\90\00\10\CC \00(\09\08\F0\02\06\B0\02\1B\F8\B0\02*\FA\03\90\05Q\F0\03\00\D2\8F\80\05\02\C0\02/\80\06\80\05\0B\04\C0\01\04\80\05\050\00*\09\09\D0\02\03\80\05\1B\FA\80\05\05\10\02$\06\09@\03\01P\00!\09\090\00\00@\03\10\CA\00\0C\11\13\00\0C\13\09\E0\0DIAy\01\00\D0\01Sz\00\0F\00[\90\0C\03\00\0D#\90\04 \00\12\D8\A0\0C)p\040\0D\010\005\FF\E1\FF\90\05#`\02 \00\0C\A0\0C\03\90\0C\1B`\A0\05& \02P\00c$v\0E\FF\00[0\02\1B\E2\C0\02\10\C8\C0\01H\0E\0E\A0\FF\90\0C\1B\10`\02\11\02`\02\16\0F`\02\10\02\D0\013\10\02\8E\F0\01\1C\08\90\0C\15\0Fp\02\02\90\0C\18\0F\D0\051$z\05P\001\05\02\8E \001$z\07\10\00\12\07\10\00\05\B0\0C\15\10p\00%\06\07\10\00\1E\E4\90\02W\10x\09\0F``\00\09\80\02\16$\C0\00\01\80\02*\C8\1F\C0\00\020\05+\08\08\80\02\19\08\B0\03(\08\08p\02K\81y\13\020\05*\05\05P\0A$\08\05\D0\07\0F\A0\02\00E\10x\0F\0F\90\06\12\CE \08\01p\00!\00\00\F0\00\0F\10\08\10/\0F\0E\10\08,.\10\FE\10\08\0F\80\02\09#0\01 \00\12\E6p\02\1B p\02(\00\01\A0\04\1F\04@\02\12\15\04@\02\19\06P\01\09@\02\1B\CA0\02\16\C8 \02\16\04\C0\01%\06\06\D0\03\02\C0\01\18\06\00\01\22\82\06\10\01$\00\00\C0\01\18\04p\00\09\10\03\01\D0\05\16\0F\B0\02\09`\01\1B\F8`\01\06\D0\05\01p\00\0F\D0\05\05\04\00\04\07\D0\05.\80\00@\09\0F0\01\0D\15\030\01\16\040\01+\A4\0E\E0\06\02\00\01\19\04\00\01\12\04\80\06\0B\B0\04\1F\04\B0\04\04aGy\00\7F\12'\10\00\10\0B\10\00\FB\00\89\7F\02\13\00\1F \0C\00\00\0E\00\00$\0E`\08\1F\1F`\08\0B\10\CA@\00X\03\02\00\1F@@\00*\03\03@\00)\02\03@\00!\03\03P\00\05@\00X\04\03\00\1F\80@\00*\04\04@\00\1C\03\F0\00\01P\00\05@\00g\05\04\00\1F\00\0D@\00*\05\05@\00)\04\05@\00!\05\05P\00\05@\00a\06\05\00\1F\00\0E@\00ab\00\00$r\02@\011\05\00\8Ep\04(\0B\A2\E0\03,/\08\A0\05T\0B\A2\00\02\060\02\00@\08#\A8\00 \00 p\02\10\02\00\A0\02\14\02@\02q\CA\0F\00\88\A3\00\0A0\00\00\86\16i\E4\03\00Ay\00\A0\01\22Hy\88\18\04\10\066\1D{\00\01\00\12\EC@\14\11P!\0E\02 \00\00\90\13/0\17\90\13\0B&\B0\07\E0\03\0F\80\13\1F\16\03\80\13\010\13\01\1F\00#\80\FF \01\00\90\13\11\03\B0\04\11\F8\00\01@\0Cz\00\03\80\00\01\10\07\000\14VI\00\00 \07\A0\00\09@\00\00\10\03&r\04`\13\01\A0\0B\11\18 \03\01\00\04\1B\E4@\13\10\C6\C0\00%\11\18\C0\00w\E2\0F\04%v\12\18@\10X\04\10x\10\18\90\06G%v\0E\11 \00V\08\81y\06\12 \04\10\22`\06\15\18`\06\01\A0\06*\14\100\00%\05\0E0\00 \E6\10 \0C\17\09 \00\00\E0\0B\06@\0C h\090\07\18\16\10\121\06{\19\90\01\C1\00\90 \00\000\0E\00\08s\19\00 \0B\00\F5\0D $\1E\A0\07Q\19\FE\FF\FF\0F\80\00`\CC\1F\00\05s\0F\8A\02\01\AC\19 \00d\90\025\0E\FF\FF@\04\15\1F\E0\03\10\0F\D0\0Dp\C8/\00$z\13\13`\00\22\FF\02@\01@'r\0F\0F\A0\03\22\0E\00\90\09T'r\12\0F\18@\00\16\C8 \04%\12\0Ap\01\01@\00\13\18p\01V'r\14\0F\11p\01P\01\0Cz\00\18 \00$p`\10\15\02\90\00\15\14@\00%\11\0E\A0\0A\01@\006\0E\0F\10@\00\00\10\02\12\11@\00 \FC\03\10\000\10J\18P\00\11\80\F0\00\16\E2\90\00%\0E\0A0\00\03p\00\12\FAp\00!z\10\A0\00\04\D0\07T\10H\12\12\01\B0\01\10\E2p\007\16\0F\09p\00E\10j\11\11`\00\02\90\02\1A\10\C0\00\11\18p\00\13\16\C0\00'\10XP\00\18\E4\B0\00\04\80\00%\0F\18`\08\00\80\005h\14\140\00\00@\00;J\10\10p\00\15\0F\F0\00\10\CA`\00\070\00\09\90\00\020\04I\10H\0E\0EP\00;j\0F\0FP\00\01\E0\09\22pP`\032\12z\09\F0\09\133\F0\00\17X@\00\09\80\00\02`\00V\07r\12\09\12 \07g\04\07r\14\09\14\10\00H\10h\16\16P\01T$x\0F\12\04P\01\10\E2@\00!\0E\09\90\02\03`\08W$x\10\14\04p\01@\12x\0F\0F\C0\11\22\FF\E2\00\02,\10XP\00(\0E\0EP\00H\12x\10\100\00W\07r\09\09\16`\00T\84y\0F\0F\00 \05u\A2\0E\00\12x\0E\0E0\00\01\E0\18(\09\09P\00E\84y\10\100\00\10\E80\00%\09\090\00\01 \00%\0E\0E \00\10j\10\00%\09\09\10\00!\22\0E\00\18S\9D\98\BB;\FF0\07\84$t\15\FF\00\00|C`\00W\10x\04\04\80\A0\01E\0Cr\00\04\F0\04\84\E2\0F\00!r\0F\06\0F\F3\07P\C8O\00#t\0F\00B\00?\13 \F0\19T!r\10\05\10 \00 \C6\8F \00t\06\01\00@K\15@ \004\07\07\0E \00u\E2\0F\02#t\05\10@\00\B3\E4\0F\08!t\0E\06\7F\00@\CB \00X\00#t\12\07`\00G#t\05\05P\00e\08!r\08\08\09P\00\C4\1F\00#x\0E\0F;\AA\B8?\0E\080\00)\12\12\80\00:t\11\05`\00)\09\08`\00\94x\0E\0F`p\A52\0E\000\00)\14\120\000x\11\10`\00\15\11`\00%\0F\09`\00\10\C4 \00 \14\07 \00&\14\080\00\00P\00\22\11\00\10\00W\08s\0E\00\0E\90\01J!t\09\0F`\00 \14\070\00\01\B4\02\10\C6\10\00 \07\08P\00\22\09\08\10\00T\08s\10\00\11@\00@b\0E\00#\B0\0ECp\A52\07@\00@$x\07\06\0F\01\13\FF\10\05W\08s\09\00\14`\02E$x\05\05 \00\00p\00Cr\02\07\0E\C8\1D\00\90\05X\08s\11\00\080\02:x\12\120\004\05\05\100\00!\E2/p\13\05 \00\11\C6 \00!\12\09P\08\03 \02Dr\02\02\11\10\00\22\E2\1F`\0B\09\C0\13\0A\B0\09\01P\0E\13\0E\10\00\13\E2P\0E\18\0E\F0\07,\04\03\D0\0B/\A0\07P\0E\0A\1C\04P\0E(`\07P\0E\12\04`\04\0FP\0E\06(\04\04P\0EV\10x\09\03 \F0\03F\04$t\18\B0\07\01@\035\05\03@ \00Q\1F\04$z\12\E0\07\01\F0\0A\01 \0E\16\09\C0\0D\01\80\07C\0E\12\00X\90\06\01@\00%\11\03\A0\07\01\D0\07'\14\09 \00F\08\81y\07\A0\07%\A6\00\E0\13\12\05 \00F\04\81y\08\A0\07 \22\07\B0\06\18\0C\B0\06\04\F0\13\22\18\02\10\06G%v\18\11`\00\00\C0\0C\06\D0\07 h#\A0\0E\18\18\E0\106\06{\13\E0\07*b\0E\B0\07\84\E2\1F\00\10x\03\03\80\C0\00\01\80\02\10\13w!\03\00\08l.\00\10x\1B\13\00\08\14\1B\00\08\10\E4P\00\17\15\F0\07j\8F\00$z\15\15\F0\079\16\0F\15\F0\074\0E\16\12\80\00\00\A0\07&r\0F`\07\01\A0\078\13\16\10 \00!z\120\06$\12\02 \00 \0F\16\B0\02\07\C0\07\1A\12@\07\16\14\90\00%\E4\0F\A0\00%\13\0A\E0\1C\18\14\D0\01E$z\10\15\C0\07\02P\00\1A\09\10\08(\12\12\F0\06-\10H\D0\06\15\12\D0\06\01\F0\074\12\16\11\A0\00\1B\E2\E0\07\16\C6\A0\00\22\12\0A \00I\10j\09\09`\005h\0F\0F`\00\11\E2\C0\08\18\14\10\026$t\14\00\06\1F\E4`\07\02\15\09\90\00\1F\C4\00\08\02\15\11\E0\00\00@\00;H\13\13@\00\06\D0\07\01 \08/\0F\0F \08\0A\0B\D0\086\12z\10\00\08\00 \00\1Ah\C0\08A\07r\0E\10\1F\04!\00\00\B0\0ET\07r\0F\10\0F\10\00\010\08)\13\13\00\01)x\09\A0\07\0D\00\098x\0F\0F \00A\07r\13\10\C0\02\02@\08\0Ap\07\1B\E4\10\08\01@\00%\13\13@\00\11\E4\90\07$\09\00P\05*\A8\0E\80\09\0C\00\08\10&p\007\12\10\12\D0\00I\12x\10\13`\00\08\C0\07\00\C0\00\030\0E\04\A0\07\00\B0\05\0A\80\00\01o\19\02`\00\10fP\00%\12\12P\00\10\CC \00\02/\1C\05\90\06Ir\0E\07\0E\E0\07*\07\0E\E0\074\08\08\0F \00p\C6\0F\01#t\07\07\00\07!\14@ \00H\08#t\0FP\07d\08!r\11\06\110\00y\E4\0F\02!t\09\07\F0\068t\06\0F@\00 $x\0F\17\07p\06:t\10\11P\009\05\05\12\F0\07%\09\0E \07\01\B0\07\1B\0F@\08*\10\10\A0\00'\12\05P\00\01@\00\00@\07$\09\00\10\00\11\0Fp\07$\0F\08P\00*\0E\10P\00'\12\12P\00Y\00$x\06\06\B0\00!x\0F\90\07%\0F\00P\08\17\11\90\08I\04!t\08P\08\00\A0\07\14\09\80\01!$\0E0\00\08\80\08@#x\08\05@\00\01\D5\13\12\C8\10\00\00 \00\15\08@\007\0F\00\0F\10\08E$x\05\10\90\00\1B\CEp\08\85\A2\0E\00#r\07\07\09\C0\07\10\1F\C0\07\06\E0\07\01\B0\057\08\00\08\10\021#r\06\E1\09\013\07!\C8/\00\08\11\0E\9D\02\06\E0\07\1B\08\E0\07.\D0\F8\90\13\0F\C0\07\09)0\04\90\13\1D\04\90\13\17\04@\00'x\05\80\07h\1F\00$t\08\FF\F0\02\04\10\07\06p\07\16\11 \07\01\00\07\11\06\00\07\22\08\02P\0EE%v\08\11\10\00\01\00\07\16\05p\15 \A8\00\A0\12\18\08\A0\1B6\06{\12\00\07!\22\0F\F0\06\18@\00\15Es\12\00\12\F0\06\00 \15&\0E\12\F0\06*\0F\01\F0\0E\1A\09@\074\E4\0F\01 \0E\02\00\07.\0F\02\F0\0E\16\13\F0\0E\00\D0\06&t\0E\10\04\01\D0\064\06\13\10P\00\10\C8@\0F\11\09P\00\10\06P\00u\E4/\00'r\07\13P\06\01\10\07%\10\09\B0\06\00@\01!r\080\00#\07\0A\10\00)t\0F \0C\06\B0\05#\FC\03P\06\15\08P\06\00\F0\0C&z\08\B0\05\1B\C8`\05\10\CE\E0\05\0A@\06E\10h\06\06\10\05\1B\E4\E0\06\00 \00?Z\11\11 \0E\03\02p\06\06`\00\12\F0\A0\06\10X\90\04\06p\15'\10H`\00\01\F0\11!\06\08\BC\02\03\80\16'\10\080\00\1B\E4\00\0A\03\E0\03\14\040\01\01\10\06!\07\08\0C\03!\00\00P\00I\12x\06\06\A0\055x\07\070\00\10\CA \00(\08\07 \00\22\84y_\13\04\10\06E\84y\09\08\10\00\10\E2\80\05\10\050\17\08\80\05\01\B0\04$\0E \80\059\09\04\09`\0D\00\C0\00D@K\0F@\A0\04%\0E\090\00\10\C6\C0\04\18\04p\05X\04#t\0E\0E0\00\09p\05\10\E4`\04\11\04p\04$\04\08\00\05%\06\0E@\00\12\C6 \00\00\80\04\22\04\00 \00@#x\06\090\00$\06\08P\00%\05\0EP\00\12\C6 \00\000\00\13\060\00T\08s\04\00\04\F0\00\01 \127\06\00\06\C0\04\00\90\0C\14\04\A0\04 \C8\1F\10\00$\05\06\10\00/\C6/\90\16\01\1C\03\90\16&@\02 \00\0E\C0\17\16\03 \04\01\00\0B%\04\03\80\179\CC\1F\00\90\1Ef\A2\00\00\06{\09\F0\03\10\E2\A0\0A4\06\FF\FFP\00'\E2/\90\02\03@\08&t\050\03&\CA\1F\E0\05\01\10\04 \8E\00\D0&\0A\00\0B4\07\00\07\10\04%$\0E\C0\0A\01\80\03\00\C0\03:z\0F\0F\00\041\06\07\0F\80\05\02\B0\05T'r\06\06\03\90\00\16\C8\C0\03#\06\0A\10\007z\03\08\10\05\010\05\050\04\17\C6@\01!`\F0p\05I\10\0A\03\03\A0\03\1D\08\A0\03\15\03\A0\03\1B\DA`\03\00\10\09)\DA\06\00\04\00 \03\14\07\B0)\1B\CAP\03\10\CA\10\034\03\06\00\00\02\10\A4\00\03\10\03p\10\07\00\03\01\7F\103?\08 \00\070t\04\04\C0\02\22\05@\10\00E!t\08\04\A0\02\02 \07\1B\030\07\18\030\07E$x\03\04\A0\02\1B\CA\F0\06\11$\80\02'\03\08\10\0F\0FP\18\01(\E2\10P\18%\03\02P\18\02\C0\00\04\F0\17C\00\00\CAO\F0\17\17@ \00\11\04\B0\09\06 \00&\05\04\10\18\10\1E\00\04\02\C0\17\01 \00\14\1F\B0\17\03\F0\17Q.\00!r\06\10\03\06 \00%\07\06\D0\17`d\00\00!\A2\07\01\10\03 \00\11/\80\17/\07\80\80\17';M9\00\C0\07\11\00\10\03$\FF\E0\C0\07H\00\00@\0Ap\174\00\00 \10\02\01\00\02\00\FD\16\08p\17\00\FC\01\010\00\03p\17%\00\00p\17\02p\12\11\0D\80\05$b\F2\E0\05\01p\00\05`\17V\19\00\00\C0\09\90\00\04p\1D\11\0D\E0\02\11op\17\05`\02\11\C6\A0\07\15\07\90\00\01@\17\11\0E\80\1D$\02\02\00\04%\10\03\10\00\01\00\08\16\04`\0F \A8\02\E0\16\18\10P\0FY\10x\09\07@\10\1E&\05\07\B0\0F\00\90\1D*\12\09`\00*\0A\05`\00\07\90\17zj\01\00\81y\0A\0A@\17\17\14`\08\0C`\0F\00p#4\0D\0D\80p\00\01\B0\099\14\00\14`\17,\15\14`\0F\19\15`\0F\02\E0+\06`\0F\01`\07\07`\04\11\12\10\17\07`\0F$\12\07\80\00\01`\04\1B\10`\0F)\0F\12\80\048z\0B\10\00\1F\040\00\15\0F0\004\11\12\090\00\02P\08\12\0BP\04\12\F6\D0\0E8z\10\10\B0\04W'r\12\12\05\D0\01\06 \08\19\F2\A0\0F$\11\0Ap\00\16\16\10\0F\00\C0\049:\0B\0B\D0\04\1D8\E0\0E\15\0B\D0\04\11\E2\C0\00\06\D0\0F\18\E2p\08\12\F6`\08\1A\1A\C0\08\06@\00*\FA\03\C0\08\13\F4P\00'\10\16\A0\11:\00\10\18\00\0F6\12z\140\05\00\A0\00\08\10\10\18\C4P\00\02\10\09R\07r\0E\14\0E@\03\11\050\00\1BZ\E0\00\17(`\00\0F\90\16\00\06\B0\00\13\F2P\004\0F\14\0FP\00\01p\0E\16\13\80\16\1B\E2`\0A\1B\E2\00\10\0F0\0F\00H\10X\11\11\80\00T\84y\15\13\00`\05\1B\E2 \01\0F@\0F\00f\84y\0B\13\00\80\80\0C\09\00\19\00\80\01\1C\18`\00%\17\0F`\00\10\E6\D0\004\11\14\11\D0\00\04`\0F\04P\00Wd\0E\00\10(P\00\000\025x\11\11\C0\00\01\10\014\12\14\12@\00\01\B0\06\16\10P\0F\11\E2@\0F\15\11\B0\00\1B\C8\E0\17\01p\00%\19\12\90\00\1Bh\80\0F\22\E8\0E\C0\17\16\80\10\01 \0Bx\C56\94\80~\00B\FC\03\00\C4\1F\10\002\00\00\E2\A0\01B\0Bx\00\0F \00\000\01#/\04\10\00A\00\00\E2\F6P\0BB h\0B\0B\866\11@P\05! \98\10\00\11K\10\00u\E2\0F\00 (\0F\0F \00\01\F0\06\10\0Ba\00\020\04\00 \008\B8\0F\0F0\00Q!r\15\04\15a\00\00 \00\11O@\09\08\00\0CV!r\17\08\17\D0\0FW\01#t\13\15\A0\0AX\08\84y\08\12\C0\01E#t\1B\17 \00\00`\0A\10t\90\114@K\040\175t\1B\1B\10\00\10\E4\D0\07%\14\13\D0\07\01p\007\19\06\190\18E!t\16\1B \00\01\B0\0A(\14\15p\17 $x\CF $\80\00\B0\00@#x\16\17 \00&\16\080\00\05`\17\01`\00%\11\0A\A0\10|\E2\8F\00$x\1B\1B@\00\000\00\13\160\00X\08s\15\00\14\B0\0A8t\17\19\00\01H#t\13\11\80\0BE#t\17\17\E0\00\02 \00\06\00\01\10\E4P\007\16\00\16\A0\0FI!t\0A\17\00\01)t\04\10\01E$x\17\17\A0\00\01\D0\00 \0A\19\E0\00\22\0A\08 \00H#x\04\11\C0\0B\00\90\12\08\D0\00R r\06\06\15\EA7\01\C0\17\00@\00\00\E0\00&\0A\00@\00\08\E0\0B\00\F0\00\15\0F0\02e\02\00 h\06\06P\02\02\C0\02\12\08\A0\02\14\F8\C0\02\15\10\D0\02\01p\005\16\1B\16p\00\14O0\002\00\00\E20\0BH \98\06\06\80\021\08s\0A;4\04@\02K (\16\16p\00\12\10@\00\13\F4`\007\0F\0B\06\D0\00\01`%\16^p\07H \B8\16\16`\00\09`\08\00 \008H\08\08`\00\09\A0\0Cxb\0E\00 h\10\10 \00F r\15\15\D0\00h\8F\00 \D8\08\08`\001\86y\000\11\CA\04\19\10\0C\00\E6\05\00 \A8\10\10\E0\00\03`\19\00P\01\84\22\07\00 r\0A\17\0AP\00u\C8\1F\00 H\0A\0Ap\00\12\C6\F0\19\15\100\00D\0E\00 rp)\02\F0\00H \D8\0A\0A`\00\01\C02\04\00\01z\E4\8F\00 h\13\13\C0\00%\11\11p\00z\C6\0F\01 \A8\13\13@\00%\0A\03@\00\01\C0\08(\02\05P\01T\86y\00\0A\15\E0\00E\E2\05\00  \15\00P\00\10\1F \00$\08\11 \00 \E8\05\10\00'\02\130\00hG\99\00\00@\F6 \12\01=\00\05\D0\099M\09\00\10\009\06{\04\80\0D\00p\0A#0\03 \00\09\A0\07\03P\06\09\80\10\16O\D0\01\00@\01\10\A4\80\0D&\02\04 \09uO\00\05s\03\00\02 \09\15\04\10#\02`\03QO\00$r\05\10\00\10\03P\08 \C8\9F\C0%\1A\050\091\09\03\050\0C\02\10\00\00\E06\15\04@\00\00\00\08\11\04\A0\0A\16\00P\01\01\A0*3\0B\02\8Ep\0E\19\02p(9'r\05\F0\1D\09P\0B\01\F0\17\11\07\90\00\13\05\D0\116$t\0A\A0\05\01p\00*\06\07P\22\1C\07\F0\0D\1A\06\F0\0D)\06\06@\088\08\05\05 \07\060\00\12\F20\00\17\18 \00\01\D0\0B\12\05 (#\00\000\07(\05\05\E0\0DE\12x\03\05 \07\84\CA\1F\00\84y\05\03\00`\03\10\E8 \07F\06\03\00\80\80\0DB\0Bx\00\06@\04\00\C0\1B#\1F\04\10\00\02\10\07W\D6\0F\00 \08\80\04\1B\C8@\04&\CC\0FP\10#\10\00\E0\051\02\02\05A\00\04P\0E\01?+\11?\15\0C\00 \0E\03P\1E\22\0A@\10\008!t\07 \1E\19\040\1D\01\D0\10 \07\02`\05#\07\08@\005x\07\02\80\1D\00\E0(\22s\02\D0\0F\03\D0\00$ r\B0+\11@ \15\01\00\02\13^\00\02\10\C6\D0\00(\05\05\A0\03\09\F0\0C\00 \005\98\05\05\E0\00\00\F0\00#r\05@)\02\D0\07\00`\03\15\05`\03\11\01\00\11/p\FD`\03\08\0B\10\00\04\F0\02\13\13p\02\81\02x\04\00\10:\00\00\F5\05\12\E2\D0'\14\01\D0\01E\C4\0F\00D\80\03\11\C0@\00\00P(\14\07p&\00\C0\17Gt\03\FF\02\C0\02\06\E0.\01\90\1DR\04\08r\13\070\04\02\90!\00p\00\14\80p\00\00\10\07$r\02 \00\14\00p\00)\C0\02p\00\12\02p\00\15\F0p\00\1F\04p\00\02\00\90\06\02P\00\01?\00\00F\16\03p\00\1B\F0p\00\03 \00\14\02p\00\1FPp\00\0C\1F\08p\00\05\01\E0\00\09p\00\01P\01#`;p\00\1F\C4p\00\03/\E0\01p\00\0B\1F\10p\00\17\01\E0\00/\D0;\E0\00\0B\17pp\00\04\D0\12\13\07`\00fGy\00\00\F0\D7@\02\09\00\02\03 \02& <\D0\00\00P\00\15 P\00\11?p\03\01\A0\00\16\00@\02\1B\02@\00\1B`@\00&\E0\00\90\00\0F@\00\01\1B\04@\00\1B\A0@\00\1F\A0@\00\0C\1B\08@\00\1B\E0@\00\19`@\00\1D\06@\00\1B\10@\00&0=@\00\04@\03\17\06 \03\17\10P\00\00P\01(`\EEP\01\00 \14\06P\00fHs\00\00\1A\00\A040\89s\07@*A\0C\0B\00\0E@\04`Py\00\04\80\C2@\00R\C3\03\00\EA\1F\A0\01\13\FFP\00f\C0\0F\00\18y\00\01\00\0F\10\00\D0\0F\01\00-\00\AB\02.\03\00\01\00\22@\00\01\00=%\01\000\00\08\01\00\1F\0B@\00\04\13e)\00\1F\F1@\00\0C\00\DC\04.\02\00\01\00\13XU\00$\C0\00\AD\03\00T\02\02G\06\00\01\00\03\948/\00\E6T\00\00\00\01\00\02\A5!\02\00E\0F\80\00\09\11)\06\00\05\EFC\05\01\00\13\F0@\00\11T\06\00\06\E4\00*\04\00\01\00/[\00@\00\03\12D\95\066\00\DC\00@\00\1F\0A@\00\00\00-'\1F\0B@\00\00\11 \B5\07\0E\C0\00\1B\08\08\00\11\F3\9C2\0E\01\00\13\F8@\00& \00\80\00\17\048\00\02\94\033\00\00\B5\14\01\0C\84\01\13\18\F5\03\00|<\04\01\00\0F\C0\00\01\132@\00\15\06R\00\03\01\00\03\95\00&\80>\80\00j\07\00\00&\80\00\01\00\13\87\94\00*\03\00\01\00\040J\01]\00\0F\80\00\08\13\06l\1D\04hJ\0C\01\00\1B\A8\08\00\17\08W\00\17\05\E8\00\0C\01\00*h@\08\00\088\00\18\06\A0\00\0F\01\00\05\00)\00\B0\00\00\00\08\00\00\00\00\00\00\00\00\00"} {
  llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1w1r_0() {addr_space = 3 : i32} : !llvm.array<32 x f32>
  llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1w1r_1() {addr_space = 3 : i32} : !llvm.array<32 x f32>
  llvm.func @__nv_expf(f32) -> f32
  llvm.func @main_kStitch_divide__13_1_0___1w1r(%arg0: !llvm.ptr<f32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: !llvm.ptr<f32>) attributes {disc.elimargs = [0 : index, 2 : index, 3 : index, 6 : index, 7 : index, 8 : index, 10 : index, 12 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(-1 : i32) : i32
    %1 = llvm.mlir.constant(8 : index) : i32
    %2 = llvm.mlir.constant(4 : index) : i32
    %3 = llvm.mlir.constant(64 : index) : i32
    %4 = llvm.mlir.constant(96 : index) : i32
    %5 = llvm.mlir.constant(128 : index) : i32
    %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %7 = llvm.mlir.constant(1 : i32) : i32
    %8 = llvm.mlir.constant(32 : i32) : i32
    %9 = llvm.mlir.constant(2 : i32) : i32
    %10 = llvm.mlir.constant(4 : i32) : i32
    %11 = llvm.mlir.constant(8 : i32) : i32
    %12 = llvm.mlir.constant(16 : i32) : i32
    %13 = llvm.mlir.constant(-0.000000e+00 : f32) : f32
    %14 = llvm.mlir.constant(1 : index) : i32
    %15 = llvm.mlir.constant(32 : index) : i32
    %16 = llvm.mlir.constant(0 : index) : i32
    %17 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1w1r_0 : !llvm.ptr<array<32 x f32>, 3>
    %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %19 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1w1r_1 : !llvm.ptr<array<32 x f32>, 3>
    %20 = llvm.getelementptr %19[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %21 = nvvm.read.ptx.sreg.ctaid.x : i32
    %22 = nvvm.read.ptx.sreg.tid.x : i32
    %23 = llvm.udiv %22, %15  : i32
    %24 = llvm.urem %22, %15  : i32
    %25 = llvm.mul %21, %1  : i32
    %26 = llvm.add %25, %23  : i32
    %27 = llvm.icmp "slt" %26, %arg3 : i32
    llvm.cond_br %27, ^bb2, ^bb11
  ^bb2:  // pred: ^bb1
    %28 = llvm.sub %arg2, %24  : i32
    %29 = llvm.icmp "eq" %28, %16 : i32
    %30 = llvm.sub %28, %14  : i32
    %31 = llvm.udiv %30, %15  : i32
    %32 = llvm.add %31, %14  : i32
    %33 = llvm.select %29, %16, %32 : i1, i32
    %34 = llvm.srem %33, %2  : i32
    %35 = llvm.sub %33, %34  : i32
    %36 = llvm.mul %35, %15  : i32
    %37 = llvm.add %24, %36  : i32
    %38 = llvm.mul %26, %arg2  : i32
    llvm.br ^bb3(%24, %6 : i32, f32)
  ^bb3(%39: i32, %40: f32):  // 2 preds: ^bb2, ^bb4
    %41 = llvm.icmp "slt" %39, %37 : i32
    llvm.cond_br %41, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %42 = llvm.add %39, %15  : i32
    %43 = llvm.add %39, %3  : i32
    %44 = llvm.add %39, %4  : i32
    %45 = llvm.add %38, %39  : i32
    %46 = llvm.add %38, %42  : i32
    %47 = llvm.add %38, %43  : i32
    %48 = llvm.add %38, %44  : i32
    %49 = llvm.getelementptr %arg0[%45] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %50 = llvm.load %49 : !llvm.ptr<f32>
    %51 = llvm.getelementptr %arg0[%46] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %52 = llvm.load %51 : !llvm.ptr<f32>
    %53 = llvm.getelementptr %arg0[%47] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %54 = llvm.load %53 : !llvm.ptr<f32>
    %55 = llvm.getelementptr %arg0[%48] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %56 = llvm.load %55 : !llvm.ptr<f32>
    %57 = llvm.fcmp "ugt" %40, %50 : f32
    %58 = llvm.select %57, %40, %50 : i1, f32
    %59 = llvm.fcmp "uno" %50, %50 : f32
    %60 = llvm.select %59, %50, %58 : i1, f32
    %61 = llvm.fcmp "ugt" %60, %52 : f32
    %62 = llvm.select %61, %60, %52 : i1, f32
    %63 = llvm.fcmp "uno" %52, %52 : f32
    %64 = llvm.select %63, %52, %62 : i1, f32
    %65 = llvm.fcmp "ugt" %64, %54 : f32
    %66 = llvm.select %65, %64, %54 : i1, f32
    %67 = llvm.fcmp "uno" %54, %54 : f32
    %68 = llvm.select %67, %54, %66 : i1, f32
    %69 = llvm.fcmp "ugt" %68, %56 : f32
    %70 = llvm.select %69, %68, %56 : i1, f32
    %71 = llvm.fcmp "uno" %56, %56 : f32
    %72 = llvm.select %71, %56, %70 : i1, f32
    %73 = llvm.add %39, %5  : i32
    llvm.br ^bb3(%73, %72 : i32, f32)
  ^bb5:  // pred: ^bb3
    llvm.br ^bb6(%37, %40 : i32, f32)
  ^bb6(%74: i32, %75: f32):  // 2 preds: ^bb5, ^bb7
    %76 = llvm.icmp "slt" %74, %arg2 : i32
    llvm.cond_br %76, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %77 = llvm.add %38, %74  : i32
    %78 = llvm.getelementptr %arg0[%77] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %79 = llvm.load %78 : !llvm.ptr<f32>
    %80 = llvm.fcmp "ugt" %75, %79 : f32
    %81 = llvm.select %80, %75, %79 : i1, f32
    %82 = llvm.fcmp "uno" %79, %79 : f32
    %83 = llvm.select %82, %79, %81 : i1, f32
    %84 = llvm.add %74, %15  : i32
    llvm.br ^bb6(%84, %83 : i32, f32)
  ^bb8:  // pred: ^bb6
    %85 = llvm.sub %8, %8  : i32
    %86 = llvm.lshr %0, %85  : i32
    %87 = llvm.sub %8, %7  : i32
    %88 = nvvm.shfl.sync  bfly %86, %75, %7, %87 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %89 = llvm.extractvalue %88[0] : !llvm.struct<(f32, i1)> 
    %90 = llvm.fcmp "ugt" %75, %89 : f32
    %91 = llvm.select %90, %75, %89 : i1, f32
    %92 = llvm.fcmp "uno" %89, %89 : f32
    %93 = llvm.select %92, %89, %91 : i1, f32
    %94 = llvm.sub %8, %8  : i32
    %95 = llvm.lshr %0, %94  : i32
    %96 = llvm.sub %8, %7  : i32
    %97 = nvvm.shfl.sync  bfly %95, %93, %9, %96 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %98 = llvm.extractvalue %97[0] : !llvm.struct<(f32, i1)> 
    %99 = llvm.fcmp "ugt" %93, %98 : f32
    %100 = llvm.select %99, %93, %98 : i1, f32
    %101 = llvm.fcmp "uno" %98, %98 : f32
    %102 = llvm.select %101, %98, %100 : i1, f32
    %103 = llvm.sub %8, %8  : i32
    %104 = llvm.lshr %0, %103  : i32
    %105 = llvm.sub %8, %7  : i32
    %106 = nvvm.shfl.sync  bfly %104, %102, %10, %105 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %107 = llvm.extractvalue %106[0] : !llvm.struct<(f32, i1)> 
    %108 = llvm.fcmp "ugt" %102, %107 : f32
    %109 = llvm.select %108, %102, %107 : i1, f32
    %110 = llvm.fcmp "uno" %107, %107 : f32
    %111 = llvm.select %110, %107, %109 : i1, f32
    %112 = llvm.sub %8, %8  : i32
    %113 = llvm.lshr %0, %112  : i32
    %114 = llvm.sub %8, %7  : i32
    %115 = nvvm.shfl.sync  bfly %113, %111, %11, %114 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %116 = llvm.extractvalue %115[0] : !llvm.struct<(f32, i1)> 
    %117 = llvm.fcmp "ugt" %111, %116 : f32
    %118 = llvm.select %117, %111, %116 : i1, f32
    %119 = llvm.fcmp "uno" %116, %116 : f32
    %120 = llvm.select %119, %116, %118 : i1, f32
    %121 = llvm.sub %8, %8  : i32
    %122 = llvm.lshr %0, %121  : i32
    %123 = llvm.sub %8, %7  : i32
    %124 = nvvm.shfl.sync  bfly %122, %120, %12, %123 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %125 = llvm.extractvalue %124[0] : !llvm.struct<(f32, i1)> 
    %126 = llvm.fcmp "ugt" %120, %125 : f32
    %127 = llvm.select %126, %120, %125 : i1, f32
    %128 = llvm.fcmp "uno" %125, %125 : f32
    %129 = llvm.select %128, %125, %127 : i1, f32
    %130 = llvm.icmp "eq" %24, %16 : i32
    llvm.cond_br %130, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %131 = llvm.getelementptr %18[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %129, %131 : !llvm.ptr<f32, 3>
    llvm.br ^bb10
  ^bb10:  // 2 preds: ^bb8, ^bb9
    llvm.br ^bb11
  ^bb11:  // 2 preds: ^bb1, ^bb10
    nvvm.barrier0
    llvm.cond_br %27, ^bb12, ^bb21
  ^bb12:  // pred: ^bb11
    %132 = llvm.sub %arg2, %24  : i32
    %133 = llvm.icmp "eq" %132, %16 : i32
    %134 = llvm.sub %132, %14  : i32
    %135 = llvm.udiv %134, %15  : i32
    %136 = llvm.add %135, %14  : i32
    %137 = llvm.select %133, %16, %136 : i1, i32
    %138 = llvm.srem %137, %2  : i32
    %139 = llvm.sub %137, %138  : i32
    %140 = llvm.mul %139, %15  : i32
    %141 = llvm.add %24, %140  : i32
    %142 = llvm.mul %26, %arg2  : i32
    llvm.br ^bb13(%24, %13 : i32, f32)
  ^bb13(%143: i32, %144: f32):  // 2 preds: ^bb12, ^bb14
    %145 = llvm.icmp "slt" %143, %141 : i32
    llvm.cond_br %145, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %146 = llvm.add %143, %15  : i32
    %147 = llvm.add %143, %3  : i32
    %148 = llvm.add %143, %4  : i32
    %149 = llvm.add %142, %143  : i32
    %150 = llvm.add %142, %146  : i32
    %151 = llvm.add %142, %147  : i32
    %152 = llvm.add %142, %148  : i32
    %153 = llvm.udiv %149, %arg2  : i32
    %154 = llvm.urem %153, %arg1  : i32
    %155 = llvm.udiv %153, %arg1  : i32
    %156 = llvm.udiv %150, %arg2  : i32
    %157 = llvm.urem %156, %arg1  : i32
    %158 = llvm.udiv %156, %arg1  : i32
    %159 = llvm.udiv %151, %arg2  : i32
    %160 = llvm.urem %159, %arg1  : i32
    %161 = llvm.udiv %159, %arg1  : i32
    %162 = llvm.udiv %152, %arg2  : i32
    %163 = llvm.urem %162, %arg1  : i32
    %164 = llvm.udiv %162, %arg1  : i32
    %165 = llvm.getelementptr %arg0[%149] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %166 = llvm.load %165 : !llvm.ptr<f32>
    %167 = llvm.getelementptr %arg0[%150] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %168 = llvm.load %167 : !llvm.ptr<f32>
    %169 = llvm.getelementptr %arg0[%151] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %170 = llvm.load %169 : !llvm.ptr<f32>
    %171 = llvm.getelementptr %arg0[%152] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %172 = llvm.load %171 : !llvm.ptr<f32>
    %173 = llvm.mul %155, %arg1  : i32
    %174 = llvm.add %173, %154  : i32
    %175 = llvm.mul %158, %arg1  : i32
    %176 = llvm.add %175, %157  : i32
    %177 = llvm.mul %161, %arg1  : i32
    %178 = llvm.add %177, %160  : i32
    %179 = llvm.mul %164, %arg1  : i32
    %180 = llvm.add %179, %163  : i32
    %181 = llvm.urem %174, %1  : i32
    %182 = llvm.urem %176, %1  : i32
    %183 = llvm.urem %178, %1  : i32
    %184 = llvm.urem %180, %1  : i32
    %185 = llvm.getelementptr %18[%181] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %186 = llvm.load %185 : !llvm.ptr<f32, 3>
    %187 = llvm.getelementptr %18[%182] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %188 = llvm.load %187 : !llvm.ptr<f32, 3>
    %189 = llvm.getelementptr %18[%183] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %190 = llvm.load %189 : !llvm.ptr<f32, 3>
    %191 = llvm.getelementptr %18[%184] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %192 = llvm.load %191 : !llvm.ptr<f32, 3>
    %193 = llvm.fsub %166, %186  : f32
    %194 = llvm.fsub %168, %188  : f32
    %195 = llvm.fsub %170, %190  : f32
    %196 = llvm.fsub %172, %192  : f32
    %197 = llvm.call @__nv_expf(%193) : (f32) -> f32
    %198 = llvm.call @__nv_expf(%194) : (f32) -> f32
    %199 = llvm.call @__nv_expf(%195) : (f32) -> f32
    %200 = llvm.call @__nv_expf(%196) : (f32) -> f32
    %201 = llvm.fadd %144, %197  : f32
    %202 = llvm.fadd %201, %198  : f32
    %203 = llvm.fadd %202, %199  : f32
    %204 = llvm.fadd %203, %200  : f32
    %205 = llvm.add %143, %5  : i32
    llvm.br ^bb13(%205, %204 : i32, f32)
  ^bb15:  // pred: ^bb13
    llvm.br ^bb16(%141, %144 : i32, f32)
  ^bb16(%206: i32, %207: f32):  // 2 preds: ^bb15, ^bb17
    %208 = llvm.icmp "slt" %206, %arg2 : i32
    llvm.cond_br %208, ^bb17, ^bb18
  ^bb17:  // pred: ^bb16
    %209 = llvm.add %142, %206  : i32
    %210 = llvm.udiv %209, %arg2  : i32
    %211 = llvm.urem %210, %arg1  : i32
    %212 = llvm.udiv %210, %arg1  : i32
    %213 = llvm.getelementptr %arg0[%209] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %214 = llvm.load %213 : !llvm.ptr<f32>
    %215 = llvm.mul %212, %arg1  : i32
    %216 = llvm.add %215, %211  : i32
    %217 = llvm.urem %216, %1  : i32
    %218 = llvm.getelementptr %18[%217] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %219 = llvm.load %218 : !llvm.ptr<f32, 3>
    %220 = llvm.fsub %214, %219  : f32
    %221 = llvm.call @__nv_expf(%220) : (f32) -> f32
    %222 = llvm.fadd %207, %221  : f32
    %223 = llvm.add %206, %15  : i32
    llvm.br ^bb16(%223, %222 : i32, f32)
  ^bb18:  // pred: ^bb16
    %224 = llvm.sub %8, %8  : i32
    %225 = llvm.lshr %0, %224  : i32
    %226 = llvm.sub %8, %7  : i32
    %227 = nvvm.shfl.sync  bfly %225, %207, %7, %226 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %228 = llvm.extractvalue %227[0] : !llvm.struct<(f32, i1)> 
    %229 = llvm.fadd %207, %228  : f32
    %230 = llvm.sub %8, %8  : i32
    %231 = llvm.lshr %0, %230  : i32
    %232 = llvm.sub %8, %7  : i32
    %233 = nvvm.shfl.sync  bfly %231, %229, %9, %232 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %234 = llvm.extractvalue %233[0] : !llvm.struct<(f32, i1)> 
    %235 = llvm.fadd %229, %234  : f32
    %236 = llvm.sub %8, %8  : i32
    %237 = llvm.lshr %0, %236  : i32
    %238 = llvm.sub %8, %7  : i32
    %239 = nvvm.shfl.sync  bfly %237, %235, %10, %238 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %240 = llvm.extractvalue %239[0] : !llvm.struct<(f32, i1)> 
    %241 = llvm.fadd %235, %240  : f32
    %242 = llvm.sub %8, %8  : i32
    %243 = llvm.lshr %0, %242  : i32
    %244 = llvm.sub %8, %7  : i32
    %245 = nvvm.shfl.sync  bfly %243, %241, %11, %244 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %246 = llvm.extractvalue %245[0] : !llvm.struct<(f32, i1)> 
    %247 = llvm.fadd %241, %246  : f32
    %248 = llvm.sub %8, %8  : i32
    %249 = llvm.lshr %0, %248  : i32
    %250 = llvm.sub %8, %7  : i32
    %251 = nvvm.shfl.sync  bfly %249, %247, %12, %250 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
    %252 = llvm.extractvalue %251[0] : !llvm.struct<(f32, i1)> 
    %253 = llvm.fadd %247, %252  : f32
    %254 = llvm.icmp "eq" %24, %16 : i32
    llvm.cond_br %254, ^bb19, ^bb20
  ^bb19:  // pred: ^bb18
    %255 = llvm.getelementptr %20[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %253, %255 : !llvm.ptr<f32, 3>
    llvm.br ^bb20
  ^bb20:  // 2 preds: ^bb18, ^bb19
    llvm.br ^bb21
  ^bb21:  // 2 preds: ^bb11, ^bb20
    nvvm.barrier0
    llvm.cond_br %27, ^bb22, ^bb29
  ^bb22:  // pred: ^bb21
    %256 = llvm.sub %arg2, %24  : i32
    %257 = llvm.icmp "eq" %256, %16 : i32
    %258 = llvm.sub %256, %14  : i32
    %259 = llvm.udiv %258, %15  : i32
    %260 = llvm.add %259, %14  : i32
    %261 = llvm.select %257, %16, %260 : i1, i32
    %262 = llvm.srem %261, %2  : i32
    %263 = llvm.sub %261, %262  : i32
    %264 = llvm.mul %263, %15  : i32
    %265 = llvm.add %24, %264  : i32
    %266 = llvm.mul %26, %arg2  : i32
    llvm.br ^bb23(%24 : i32)
  ^bb23(%267: i32):  // 2 preds: ^bb22, ^bb24
    %268 = llvm.icmp "slt" %267, %265 : i32
    llvm.cond_br %268, ^bb24, ^bb25
  ^bb24:  // pred: ^bb23
    %269 = llvm.add %267, %15  : i32
    %270 = llvm.add %267, %3  : i32
    %271 = llvm.add %267, %4  : i32
    %272 = llvm.add %266, %267  : i32
    %273 = llvm.add %266, %269  : i32
    %274 = llvm.add %266, %270  : i32
    %275 = llvm.add %266, %271  : i32
    %276 = llvm.udiv %272, %arg2  : i32
    %277 = llvm.urem %276, %arg1  : i32
    %278 = llvm.udiv %276, %arg1  : i32
    %279 = llvm.udiv %273, %arg2  : i32
    %280 = llvm.urem %279, %arg1  : i32
    %281 = llvm.udiv %279, %arg1  : i32
    %282 = llvm.udiv %274, %arg2  : i32
    %283 = llvm.urem %282, %arg1  : i32
    %284 = llvm.udiv %282, %arg1  : i32
    %285 = llvm.udiv %275, %arg2  : i32
    %286 = llvm.urem %285, %arg1  : i32
    %287 = llvm.udiv %285, %arg1  : i32
    %288 = llvm.getelementptr %arg0[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %289 = llvm.load %288 : !llvm.ptr<f32>
    %290 = llvm.getelementptr %arg0[%273] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %291 = llvm.load %290 : !llvm.ptr<f32>
    %292 = llvm.getelementptr %arg0[%274] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %293 = llvm.load %292 : !llvm.ptr<f32>
    %294 = llvm.getelementptr %arg0[%275] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %295 = llvm.load %294 : !llvm.ptr<f32>
    %296 = llvm.mul %278, %arg1  : i32
    %297 = llvm.add %296, %277  : i32
    %298 = llvm.mul %281, %arg1  : i32
    %299 = llvm.add %298, %280  : i32
    %300 = llvm.mul %284, %arg1  : i32
    %301 = llvm.add %300, %283  : i32
    %302 = llvm.mul %287, %arg1  : i32
    %303 = llvm.add %302, %286  : i32
    %304 = llvm.urem %297, %1  : i32
    %305 = llvm.urem %299, %1  : i32
    %306 = llvm.urem %301, %1  : i32
    %307 = llvm.urem %303, %1  : i32
    %308 = llvm.getelementptr %18[%304] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %309 = llvm.load %308 : !llvm.ptr<f32, 3>
    %310 = llvm.getelementptr %18[%305] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %311 = llvm.load %310 : !llvm.ptr<f32, 3>
    %312 = llvm.getelementptr %18[%306] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %313 = llvm.load %312 : !llvm.ptr<f32, 3>
    %314 = llvm.getelementptr %18[%307] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %315 = llvm.load %314 : !llvm.ptr<f32, 3>
    %316 = llvm.fsub %289, %309  : f32
    %317 = llvm.fsub %291, %311  : f32
    %318 = llvm.fsub %293, %313  : f32
    %319 = llvm.fsub %295, %315  : f32
    %320 = llvm.call @__nv_expf(%316) : (f32) -> f32
    %321 = llvm.call @__nv_expf(%317) : (f32) -> f32
    %322 = llvm.call @__nv_expf(%318) : (f32) -> f32
    %323 = llvm.call @__nv_expf(%319) : (f32) -> f32
    %324 = llvm.getelementptr %20[%304] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %325 = llvm.load %324 : !llvm.ptr<f32, 3>
    %326 = llvm.getelementptr %20[%305] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %327 = llvm.load %326 : !llvm.ptr<f32, 3>
    %328 = llvm.getelementptr %20[%306] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %329 = llvm.load %328 : !llvm.ptr<f32, 3>
    %330 = llvm.getelementptr %20[%307] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %331 = llvm.load %330 : !llvm.ptr<f32, 3>
    %332 = llvm.fdiv %320, %325  : f32
    %333 = llvm.fdiv %321, %327  : f32
    %334 = llvm.fdiv %322, %329  : f32
    %335 = llvm.fdiv %323, %331  : f32
    %336 = llvm.getelementptr %arg4[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %332, %336 : !llvm.ptr<f32>
    %337 = llvm.getelementptr %arg4[%273] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %333, %337 : !llvm.ptr<f32>
    %338 = llvm.getelementptr %arg4[%274] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %334, %338 : !llvm.ptr<f32>
    %339 = llvm.getelementptr %arg4[%275] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %335, %339 : !llvm.ptr<f32>
    %340 = llvm.add %267, %5  : i32
    llvm.br ^bb23(%340 : i32)
  ^bb25:  // pred: ^bb23
    llvm.br ^bb26(%265 : i32)
  ^bb26(%341: i32):  // 2 preds: ^bb25, ^bb27
    %342 = llvm.icmp "slt" %341, %arg2 : i32
    llvm.cond_br %342, ^bb27, ^bb28
  ^bb27:  // pred: ^bb26
    %343 = llvm.add %266, %341  : i32
    %344 = llvm.udiv %343, %arg2  : i32
    %345 = llvm.urem %344, %arg1  : i32
    %346 = llvm.udiv %344, %arg1  : i32
    %347 = llvm.getelementptr %arg0[%343] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %348 = llvm.load %347 : !llvm.ptr<f32>
    %349 = llvm.mul %346, %arg1  : i32
    %350 = llvm.add %349, %345  : i32
    %351 = llvm.urem %350, %1  : i32
    %352 = llvm.getelementptr %18[%351] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %353 = llvm.load %352 : !llvm.ptr<f32, 3>
    %354 = llvm.fsub %348, %353  : f32
    %355 = llvm.call @__nv_expf(%354) : (f32) -> f32
    %356 = llvm.getelementptr %20[%351] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %357 = llvm.load %356 : !llvm.ptr<f32, 3>
    %358 = llvm.fdiv %355, %357  : f32
    %359 = llvm.getelementptr %arg4[%343] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %358, %359 : !llvm.ptr<f32>
    %360 = llvm.add %341, %15  : i32
    llvm.br ^bb26(%360 : i32)
  ^bb28:  // pred: ^bb26
    llvm.br ^bb29
  ^bb29:  // 2 preds: ^bb21, ^bb28
    llvm.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c8 = arith.constant 8 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c1024 = arith.constant 1024 : index
    %c6 = arith.constant 6 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
    %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
    %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
    %4 = arith.index_cast %2 : index to i32
    %5 = arith.index_cast %1 : index to i32
    %6 = arith.muli %4, %5 : i32
    %7 = arith.index_cast %6 : i32 to index
    %8 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    %9 = arith.cmpi slt, %7, %c3 : index
    %10 = arith.cmpi sge, %3, %c1024 : index
    %11 = arith.cmpi slt, %7, %c6 : index
    %12 = arith.andi %11, %10 : i1
    %13 = arith.cmpi sge, %3, %c512 : index
    %14 = arith.cmpi sge, %7, %c6 : index
    %15 = arith.andi %14, %13 : i1
    %16 = arith.ori %9, %12 : i1
    %17 = arith.ori %16, %15 : i1
    cf.cond_br %17, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    gpu.launch_func  @main_kernel::@main_kStitch_divide__13_1_0___1b1r blocks in (%7, %c1, %c1) threads in (%c256, %c1, %c1) args(%0 : memref<?x?x?xf32, "gpu">, %8 : memref<?x?x?xf32, "gpu">)
    cf.br ^bb3
  ^bb2:  // pred: ^bb0
    %18 = arith.cmpi eq, %7, %c0 : index
    %19 = arith.subi %7, %c1 : index
    %20 = arith.divui %19, %c8 : index
    %21 = arith.addi %20, %c1 : index
    %22 = arith.select %18, %c0, %21 : index
    gpu.launch_func  @main_kernel_0::@main_kStitch_divide__13_1_0___1w1r blocks in (%22, %c1, %c1) threads in (%c256, %c1, %c1) args(%0 : memref<?x?x?xf32, "gpu">, %7 : index, %8 : memref<?x?x?xf32, "gpu">)
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    "disc_ral.dispatch"(%arg0, %c0, %8) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\00\E8\19\00\00\00\00\00\00\02\00\01\01@\00\00\00\A8\19\00\00\00\00\00\00\A2\19\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8:\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\002@:\00\01\00\117\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0D\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F5)_shndx\00.nv.info\00.text.main_kStitch_divide__13_1_0___1b1r2\00\0F,\00\15oshared.\00\15\9Fconstant21\00\1D\1F01\00\12\FD\01debug_frame\00.rel\11\00\1Aa\12\00!nv&\00oactionV\01 \0F\94\00\0F\0Fy\01\A2a__ocg_3\00/\00$0\00\0F\FF\16$__cuda_sm70_shflsync_bfly_p\00$____wg_G\00\0F@_0_$\0D\00/265\00\19_1__281\00\19\122f\00/305\00\19_3__32\91\02\1Fo_param\98\02.\0F\01\00\06\8CU\00\00\00\03\00\0B\00\01\00\11\AA\18\00,\0C\00\01\00\11\D8\18\00,\09\00\01\00\A1\15\01\00\00\22\00\0B\00`,\0F\001\00 \01\07\000\00!\020\00,\0A\00\01\00\11Y\18\00,\04\00\01\00\11\89\18\00,\07\00\01\00f2\00\00\00\12\10\A8\00!\80-\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13P\C0\00\10\04\9B\00R\044\00\00\00E\005\04\D4\0A<\00&8\00H\00\04p\00L\88\80\80(o\00\10\08\15\00x\16\89\80\80(\09\05J\01\004\00\22\18\00\01\00\22p\00\01\00\048\01\13\108\01\90\04/\08\00\08\00\00\00$\0C\00%#\08*\01'\04\12\0C\00\17\11\0C\00\12#0\00\04$\00\17\08$\00\05\0C\00!7\04$\06\90\015\00\00\04\0A\08\00\05x\00\A3\01\18\00\03\19\18\00\04\17\0CH\06U\10\00\00\F0!\10\00\10\01>\00%\F0\11\10\00\01\01\00\F09\F0!\00\03\1B\FF\00\04(X\00 \12\00\00p\12\00\00\B0\12\00\00\00\13\00\00`\13\00\00\90\14\00\00\D0\14\00\00\10\15\00\000!\00\00`!\00\00\80!\00\00\A0!\00\00\C0!\00\00P\22\00\00p\22\00\00\90\08\00\F4\16*\00\00\E0*\00\00`+\00\00\B0+\00\00\F0+\00\000,\00\00\04\1C\08\00@(\00\00@*\00\00\04\1E\E6\00C\00\00K\00\01\00\94\02\02\08\10\0A/\22\00\00\DF\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\E0\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00@\00\00\00\14\FB\01\22\C8\00\01\00\13\02\D1\00*H\00\10\00/\04\00\01\00\FF|@$v\01\FF\D7\04\C1\FF\00\8E\07\00\C4\0F\00\19y\02\00\01\00\F1\00!\00\00\00\22\0E\00\B9z\04\00\00F\00\00\FB\04\90\E2\0F\00Ey\00\00\10\0D!\00 \80\03\10\00\81$t\0C\FF\00\00\80\FF@\00\F3\05\E2\0F\00\10z\00\02\00Z\00\00\FF\E1\FF\07\00\E4\1F\04\0C\10\00\A0pR\F0\03\00\E4\0F\00\10xl\00\F0\07\00\00\FF\E0\FF\07\00\C8\0F\00\12x\03\00\00\FC\FF\FF\FF\C0\8E\07 \002\19y\00\01\00\10%\9D\00\83\0E\00\07r\03\03\FF\00\01\00\000\000r\03\03\A2\001\FF\FC\8E@\00P\0Cr\00\02\03\80\04\D3b\F2\03\00\DA\0F\00G\19\00\00`\0C\A0\00a\EA\0F\00\10r\04 \00\01\90\00\01\C0\00F\01\00\F0\06\C0\001\1Cx\00\01\00?p\F0\F0\D0\00\02@\0Cx\00\04?\062pB\F4 \00Ar\0E\FF\FF\D0\02\C6\8E\07\00\D8\0F\00G\A9\00\00\90\06p\00\05P\00\14\E1\F0\00a\0D\03\00\F4\FF\FF\F0\00b\C4\0F\00\02x\10\04\03\12\0FP\01@$z\0B\00 \01 \0E\02\D0\00\B0\1F\00%v\04\0B\00X\00\00\10\10\00a\CA\0F\00\81y\0F\C3\05\E3\00\19\1E\0C\00\A2\00\00\10x\07\0B\00\01@\01\93\E4\0F\04\10x\09\0B\00\02\10\00z\C6\0F\00%v\06\07@\00%\12\06@\00u\E2\02\00%v\08\09 \00\95\E2\0F\08\10x\0B\0B\00\03P\00v\00\10x\13\0E\00\04\10\00E\81y\11\08@\00fb\09\00%v\0A\A0\00\10\C8\C0\00\11\15\C0\00\12\13P\00U\00\81y\14\0A0\00\10f0\00&\04\15p\00e\1F\04\10x\17\15\C0\00\10\C80\00\16\13\E0\00 b\01\C0\00\16\170\00\88/\00\10x\1D\15\00\020\00\16\16\D0\00zb\03\00%v\1C\1D\D0\00*\1F\15\D0\00E\09\0E\00\08\10\00Z\01\81y\15\1C\D0\00%\1E\1F@\00\01\D0\00\11\19\D0\00\15\09\D0\00%\18\1E0\00\10f`\01' \19p\00\00\80\01\1A\19\D0\00&\17 `\00\1C\0Fp\01Z\00\10x\0B\19\D0\00\1F\1A`\01\07\01\D0\00&\05\19\D0\00\10\1F\00\024\0E\00\0C\10\00j/\00\81y\19\0A@\01%\04\05@\00\01\D0\00\11\1B\D0\00\15\07\D0\00\16\1Cp\01z&\03\01%v\06\1B\D0\00%\1D\1B\D0\00\01@\02\16\07p\01\11\22\D0\00\1A\1D\D0\00H\1D\1B\00\020\00*\08\080\00\17\0A0\00\00\C0\00H\1B\1B\00\030\00*\0B\0A0\00(\04\1B\00\02Q\0Br\00\0F\0F_\03\A1\F4\03\00\E4O\08\0Br\00\0C\10\00\B2\C0\F6\03\00\D6\0F\00\08\A2\0F\0F\D2\07\22\80\05\B0\02\16\0C\E0\00 \A2\0E@\00%\12\12@\00\10\8F@\00!\11\11\10\00\10\F6\A0\03\10\02`\00\01 \00P\C0\F8\03\00\D2P\00!\12\12`\000\00\00\06P\00U\0Br\00\14\14@\00\01\10\00$\12\110\00q\C8\0F\00\08\B2\11\11@\00\060\00&\13\13`\00\00\10\00(\11\140\001\A2\14\14@\00\060\00*\16\16`\00)\14\13`\00!\13\13@\00\060\00*\15\15`\00)\13\16`\00!\16\16@\00\060\00*\18\18`\00)\16\15`\00!\15\15@\00\060\00*\17\17`\00)\15\18`\00!\18\18@\00\060\00*\1A\1A`\00)\18\17`\00!\17\17@\00\060\00*\19\19`\00)\17\1A`\00!\1A\1A@\00\060\00&\1C\1C`\00\10\01p\00\19\19`\00!\19\19@\00\060\00*\07\07`\00)\19\1C`\00!\1C\1C@\00\060\00*\08\08\C0\00)\1C\07`\00!\07\07@\00\060\00*\0B\0B`\00)\07\08`\00!\08\08@\00\030\00f\10x\0E\0E\00\10\90\04\00p\00\19\0B`\00!\0B\0B@\00\030\00Q\0Cr\00\0E\0D\D0\06\14\F8`\00*\0C\0C\00\03)\0B\0C\00\03!\0C\0CP\00\00\00\03\90\E2\0F\00G\C9\00\00\90\F9\9A\0B\11\83\90\06#Ay\89\0B\03\A0\06@$x\04\03\12\005\0E\0A\8E\10\07#p\03 \00\13\E8\F0\06\13\04\F0\06\12\DA\E0\06'@\03@\00St\14\FF\04\000\08\10\E4`\04\1C\0A\C0\06\10\0A\C0\033\14\02\8E\E0\03\1B\0D\C0\06%\06\0A`\04\01\C0\06+\08\0A\C0\06\1A\06@\00\1C\10\C0\06\15\08 \00\01 \05&\0A\0A \05\00p\01\1B\11\C0\06\1C\0F`\05\06\A0\00\01 \05\11\11\C0\00\15\11 \05\16\12\90\04\10f`\06&\04\11p\00\00\C0\06*\13\11 \05*\05\04\F0\05&\06\130\00!/\000\00\09 \05+\06\06 \06\070\00j\0F\01\10x\11\11 \05\1C\09P\05\07\90\00\01 \02\12\0D\8F\09\15\F0 \05\01\10\00\22\C0\F4 \021\82\0D\0D0\02$\00\00 \05\1B\0A \05%\10\10@\00*\8F\08p\05\01 \05$\0D\10p\02\00 \05\22\82\10\FF\09\03p\05\06`\05#\F0\03\10\00$\10\0F0\00\12\C8\A0\05\1A\100\00&\05\05`\00\16\00\80\05\030\00\14\82\80\05\050\00*\06\06`\00)\12\05`\00:\05\05\120\00*\09\090\04)\05\06`\004\06\06\050\00\11\C80\00\11\060\00#\C0\F0P\00!\09\090\00\00\AB\0C\1F\E4\D0\09\01\18\0EP\08\00@\00%\0C\0C\10\04\01\80\01(\09\0C\80\011\B2\0C\0C`\00\00\80\01\1E\C4\90\03\00\F0\03\00_\03Ap\16p\00p\03V\89\00\00\A0\01 \000\02x\0B\CF\02\070\0A\0Fp\03\03\15\0B\C0\07\07`\03\0C\A0\02%\A2\0E`\03\030\00\19\00\80\03\0C\A0\02%\E2\0Ep\03\060\00\07p\03\0F\A0\02\03\06\90\00\10\CC \00(\0A\0A\F0\02\06\00\02\06\B0\02\05\D0\01\09\F0\01\BF\F4\03\00\D2\8F\00\08\82\05\05\0C\E0\01\02\1F\F0\E0\01\03\17\A2\E0\01\020\00*\0A\0A\D0\02(\06\090\00\14\82\F0\01\05\10\02)\09\0A\10\02!\0A\0A0\00\00\10\02\10\CA\00\0C\11\0C\00\0C\13\0A0\0D\14A\B0\0C\04\D0\010z\00\03\A0\01Dpb\F4\030\0D#\B0\04 \00\10\E2\F0\0C0\05\02\1F@\05\00\F0\0C\96\D6\0F\00G)\00\00\80\04@\001$r\07`\00'\03\00\A0\05\16\02@\00\09\B0\0C\10\C6p\0D%\04\07p\0D\13\C8\C0\05\10\03\C0\05\12\F6\C0\05V\B9\00\00 \02`\00\10\02\D0\0D\13ZP\02\0F\E0\02\02F\04\04\00\FD\B0\0C;$t\06\F0\05\16\08P\0A%\C8\1F \023\06\02\8E\10\02*\0D\08\F0\05%\0B\07\90\02\01\F0\05(\0F\07\F0\05\04\00\0D\010\02a\E4\0F\04$z\0F\10\00\12\0F\10\00\05\10\0B\13\06\B0\05E%v\0E\0F\10\00\1B\E4`\02 \E2\0E\00\066\07\00\03`\00E\81y\0F\0E \00\16$\C0\00\1F\11\C0\00\01\09P\05\05\D0\03$\0C\0D\B0\07\01\D0\03\05P\05\12\06\80\00+\0C\08P\05\03p\02\14\F8P\05\12\0DP\02\11\FA`\01\06`\05\01\F0\02U\01\10x\07\07\B0\06q\CE\0F\00\08\C2\0A\0Ap\00Q\00\80\06\00\C80\00$\0A\0F@\00\00\90\02D\B2\0F\0F\0A \00\020\08!\07\040\08*\FA\03\90\04\14\F8\C0\00$\0F\0C\F0\02\00\C0\00\22\C2\0C@\0B\040\08_\D9\00\00\10\FE0\08\07\09`\02\03\90\02\140\A0\04\13\E6p\02\1A\01p\02(\00\010\08\1F\0A@\02\12\1F\0A@\02\02\1A\0E@\02\1B\CA0\02\16\C8 \02\1D\0A\C0\01\1B\F0\C0\01\1B\F6\10\07/\80\05\10\07\01\0E\10\03\16\07\B0\02\09`\01\05 \02)\0D\0C \02\12\0Cp\00\00 \02\0F\F0\05\01!z\00P\01\08\F0\05(\80\00\F0\05\1B\09\F0\05\1B\060\01\11\06\C0\05\15\090\01\16\09\B0\05+\A4\0E \05\02\00\01\19\09\00\014\09\09\0C\00\01\07\D0\04\1F\09\D0\04\04\F1\00\89\7F\07\0C\00\1F \0C\00\00\0E\00\00b\0E\D0\04&`\03\90\04\06p\0B\00p\00\11/p\00\19\07p\00(\07\07p\00r\89\7F\04\07\00\1F@P\00\11d\B0\00*\04\04@\00)\07\04@\009\04\04\07@\00X\09\04\00\1F\80@\00*\09\09@\00\1C\04\F0\00\17\04\F0\01@\19x\04\FF \06@\02\16\01\00 \04\87\89\7F\06\09\00\1F\00\0DP\00*\06\06P\00)\09\06P\007\06\06\09P\00@\0Cr\00\05@\01!pR\D0\03\0DP\12\81\89\7F\0B\06\00\1F\00\0E`\00ft\0E\00$\B4\07\10\03\00\80\0A\10\B6\EF\02\12\80\E0\01\00 \03\12t\00\06#\FF\00\D0\02\14\B2p\0C\12\F8\90\00G\B2\00\06\0BP\043\1C\B8\00p\00Ap\02\00\E4\90\03\10\050\01 p@0\00`\0F\04\19x\05\05\FF\05\10\FF\C5\00\01\C0\094\0B\0B\06@\04\12\E2\E0\05\17\07p\001\0Cx\00\A0\061p@\FA`\06#\88\B3\B0\0C\00\A5\0Cf\E8\0F\00\1D{\00\01\00\89\EC\0F\00GY\00\00@\B0\08\00\9E\04$\80\FF\C0\021\82x\06\F0\13\01\01\00\93\CA\0F\00\84\C9\0A\05\00\04P\00\81\A2\02\00Gy\00\06\C2Q\0E\22\80\0Bp\02C\09\0A\00\07p\02/\A4N\D0\02\01\02\90\07%\C0\FC\E0\01\02\10\05\11\00\10\03c\89\7F\08\09\00\07`\02'\A4\0E\E0\0D\05@\00)\09\08@\00!\08\08P\00\05@\00C\0C\08\00\07`\02a\E2\04\00$r\0B\90\011\08\00\8EP\06$\0B\B2\00\04\11\FC\E0\05\09\F0\01\0A\90\01\01\10\00\00\C0\01\14\0Cp\00\10\DAp\00\04\B0\0D\02p\00c\88\B3\00\FF\0C\00\10\01-\E4\07\80\03\22Hyh\19\01\10\00\1E\E4\80\01T\84y\08\FF\00@\00\22\A2Np\08\07\A0\03!\02x\DE\00\14\80\90\01\00 \15&\80\03`\00B$t\09\FF\B1\01\03\80\02&r\0A\E0\14\10\C6@\00\1B\13\90\04\02\80\05\15\0A\90\04\11\0C\80\05\02\00\14[\8F\04\10x\0E\A0\14\19\10\A0\14J\81y\0D\0C`\0A&\0E\0E@\00\00p\05\16\12\90\14\01\10\0E%\10\10 \00<\E4\0F\08\D0\07G%v\12\12 \00\00\B0\14\15\10 \00\10h\10\00%\13\12\10\00\B4\A2N\00$t\16\FF\9D\98\BB;\F0\02\01\D0\0A\04\80\07\01\10\03a\15\FF\00\00|C \00\11\C6\E0\03!\0A\03`\07\02\D0\03c!r\14\08\0D\00\E4\06`\C8\8F\00#t\0B\9F\112?\16  \01T!r\0D\08\0F \00\D2\C6\0F\01#t\0B\0B\01\00@K\15@ \00E#t\0C\0D0\00\D4\C6\0F\00!t\0F\0B\7F\00@\CB\00\00@\004\11\08\11@\00u\E2\0F\02#t\0C\0C@\00\D4\E4\0F\08#x\0F\14;\AA\B8?\0F\08P\00'\10\11\80\00e\08!r\13\08\13@\00hO\00!t\0E\0C`\00\00@\00e`p\A52\0F\00@\00\17\10\A0\00W\08#t\14\13P\00P\00#x\0E\0Dp\00\12\0Ep\000\04$x\FF\02\15\800\04I!t\12\10`\007t\14\14P\00P\00#x\0D\0Dp\00\13\0E \00T\08s\0F\00\0F\B0\02p\22\0E\00#x\12\11`\00#\12\08\00\018t\0E\14P\00E$x\10\10p\00!\C4\0F0\00\00P\00$\12\00`\00%\0E\13\A0\00`\E4\0F\04\08s\0D\CD\05\02`\00\10\A4`\00(\13\13\80\00T\19x\0E\0B\17\10\05\10\CA\90\004\11\00\120\00\81\E2\0E\00#r\0F\0E\0F\10\07\00\10\00e\1F\00\19x\09\140\00\10\CC0\008\16\00\13\C0\00Cr\0D\0C\0D\1F\03q\00\C8O\00#r\10\91\01#\0D\00 \020r\09\09`\13\13\10P\00\01\E0\0D\18\FCp\09\0Ap\07\00\C0\03&\90\07\C0\03\00 \0C&p\07 \00\04\90\04\08 \0C&\10\040\00\0E \0C+\0A\0B \0C\12\0A \0C\02\90\04XG\E9\00\00\C0\10\041v\0A\FF0\00\03\E0\01\09\10\09\13\C80\03\07 \0C;\02x\12 \04\16\0C\A0\09k\E2\9F\00\10x\0F\10\04\16\11\10\04p\E2\0F\04%v\0C\0C\C0\03$\12\02\90\03\19\13\00\04\0B\10\0C\1C\04@\04\09\C0\11\16\E4 \0C\22\12\02`\07E%v\10\11\10\00\0F@\04\001$z\13@\00\0F@\04\02\10d@\00%\12\13@\00\1F\CCP\04\11\11\0B\B1\0F\03\E0\00L\00$t\17P\04!\0B\0AP\04/\FC\03P\04\03\16\0CP\044\C8\0F\00\00\04$\17@ \04\19\15`\04:!t\0F\E0\03:t\0D\15\10\04\0AP\04E\19x\0C\0C\D0\02\01@\03\09P\04W\04#t\0D\0Dp\00U\08#t\0E\11P\00\03\80\04\08@\04\0Ep\04'\10\0D\90\00X\04#t\0E\0EP\00H$x\0D\0D@\04\0Ep\04 \10\15\90\00\01~\1Ex\E2\0F\04!t\12\0E\E0\00\0D@\04\03p\04&\17@@\00\00\A0\00\02p\03\1A\0F`\04\10\C4`\04\18\16`\04F\04\19x\14\E0\03\1E\E2`\04T\08s\10\00\10p\00\10\A2P\04\11\16p\04#\16\08\A0\01!x\16`\04\22\16\00\D0\05;\08s\12P\04(\0C\0CP\04\10$\90\1A\04\00\01\03P\04\19\16P\04\11\0Cp\13\15\0CP\045\09\09\12\10\00\01P\04'\14\16P\00\00\C0\03/p\FC\C0\0D\09\07\00\04\040\04\19\02\C0\0D\010\12\08\10\04'\E0\01@\00\08\C0\03\01\D0\02\1B\0E\D0\0D\07\F0\03/\E4\9F\C0\0F\00\02\E0\03\1B\0E\C0\0F\15\0Ep\03\07\10\08*\E8\0E\B0\0F\10\22p\00\19\13`\03 \02x\1F\03!|C\05\02\1B\E4\A0\04\13\C4\80\03\06`\04G\00!r\0A`\03\10\CF\F0\06\00\7F\034?\13 \C0\02\1B\12\B0\07\02\10\07\14\14P\025t\13\120\00\01\B0\07*\11\10\E0\02(\13\130\00\09\B0\06\10\E4 \02 \11\0A0\02\22\11\08 \00E!t\0D\13@\00\12\C6 \00\00@\02\22\11\00 \00@#x\0D\120\00$\0D\08P\00%\0A\13P\00\12\C6 \00\000\00\13\0D0\00\00\C0\06\14\11@\0290\0E\00\00\07\00\B0\02Dr\09\10\110\02 \C8\1F\10\00$\0A\0D\10\00/\C6O\A0\0E\01\1C\0B\A0\0E\17\F0 \00?\02x\0D\90\14\02\15\0B\10\0A\02\00\14\15\0D0\05\0A\E0\1B6$t\0D\D0\01\00@\02&t\0F \05\01\90\015\08\08\0B\B0\01PO\01#t\0C\CA\005?\0D  \05\01\10\05\22\0F@\80\03J!t\0D\0C\D0\03\07\E0\04\020\01\15\08P\01\12\C8\10\00\05@\01*\CC\0F0\01\11$0\01\15\0C \01-\CC\1F\A0\07\01`\0C\04\10\0F\03p\0B&\80\01\80\07%!rP\0C@\00\00\CAO\00\0E6\08\00\1F\90\0C2!r\0B\E0\00\05 \00%\0A\0B\D0\0E\01 \00\11\0A \06\06 \00%\0D\0A\A0\0E\01 \00\13\0C \12\00 \00\10\CF \00\15\0C`\0E\00 \00\12\B2\90\08\04@\00c\88\B3\00\04\0D(\89#\0F\C0\0D\04\17\B0\E0\006$t\04\10\0C\0F\C0\0D\02C\04\05\00(0\01!\E2\04\C0\0D(2\09\C0\0D%\05\04\C0\0D\93d\EE\00!r\05\05\04\00\01\00 \CA/p\0F\15\05\A0\0D\11d\F0\008\05\06\00 \00%\07\0A\80\0D\8A\A4\02\00!\B2\07\07\0A\D0\003\FF\07$\80\00/\E4\050\0D 4\05\FF\00@\00!b.\C0\01&0\05\10\05!\84y\00\01\05\C0\05\000\0D(\00\05 \01\08\10\15Q\1F\00$z\0D\00\03\15\02\00\03 \0E\0D\00\03\15\06\90\17%\07\0D@\05 \E4O0 \15\0D\A0\04\0C\A0\14\01\A0\17'\10\07@\00\00\D0\04\18\0D\10\09E%v\12\09 \00\01\D0\08*\11\10\10\0D'\14\0B \00\0A\B0\08\01\10\0D%\15\14\10\00p\E2\8E\00\02x\16\00\90\03\05\D0\03\08\B0\08\00\80\10\10x\FE\00\D0\80\00\00\E2\F2\03\00\E2/\04 x\18\10\00A>\00\00@p\03\03 \004~\00B\80\05C\02\02\00\04\B0\00`\E4\0F\00\08r\181\00\02\01\00P\CE\0F\00 \98\8F\1E#\80K@\00\00\D0\02&\04\0F\F0\03X\00#t\08\0C\10\08T!r\0F\04\11 \00\01\A0\05(\08\08\F0\07V!r\13\04\13 \00W\02!t\11\08\10\04W\00#t\0A\0FP\00Z\08#t\12\13\D0\08)\11\0C\A0\05Dr\15\04\15P\00\10\E2p\04%\0A\0Ap\00\010\00)\0E\0C\B0\05:t\0C\120\09\18\11\80\09\00\10\09\18\0A\A0\04\10$@\0B\04\D0\05\01 \00\1B\12\C0\09'\14\11P\00\00\D0\08\17\0F\10\09@\00\08s\0E\1A\03\060\08:x\12\13@\0D\09\E0\08\01@\00\08\10\09\0E\00\0A*\12\13P\0D%\16\15\E0\08\01P\05(\0F\080\00\00p\06\14\10\80\00xb\0E\00$x\14\14\D0\00\00@\00\00P\00\22\16\000\00\01\00#\13\\\90\02\01p\0D7\13\00\12\A0\06R r\0E\0F\0E\EA/\01\A0\06E \08\0E\0E`\02\10\C60\00\19\15 \09T r\0A\0A\110\00u\E2/\00 \98\0E\0EP\02z\C6\0F\00 \08\0A\0A@\00C\17\00\18\00\81\0D\10b@\005\0C\0C\13@\00jO\00 \98\0A\0A@\00%\0C\0C@\00\84\E2\0F\00 r\14\14\150\00u\C6\1F\00 \98\0C\0C0\00\00 \008\08\14\140\00\06\90&\02\80\0DI \98\14\140\00#r\15\82*!@\00P\035r\11\17\00\01\010\01%\0C\0D0\01\010\006\13\17\0A \00d\04 r\17\17\14\10\00\02\F0\0C\17\070\00P\08\86y\00\0C \01a\04\19\10\0C\00\E6P%\18\0BP\00T\86y\00\0E\13 \00 \E8\01\10\00*\08\15\10\00$\06\17\10\00!\E2\01\00\08(\00\FB`\0A\0Ap\059M)\00\10\00\0E0\04/\02\050\04\03\22\F4\03\90\0AF\00\00\B0\01\80\050\08r\021\00\02\82\03\010\04%\02\02@\01\00\00\19 s\0C?\05\02\D0\01/f\100\17\00\11\02\90\05\15\03\90\05%\06\020\17*\CCO\D0$k\A2\0E\00$t\08`\0A\00\9E\07\06`\0Ac\10x\03\03\00\01\D0\04\18\C8\E0\1B\13\F0\F0\075\05\04\070\04\12\CFp\00\13\\p\00\00\80\08 t\08\CF\003?\08 \80\08\03\C0\04\22\0B@\10\00J!t\0A\08 \04%\08\08P\03\01\C0\08 \0A\05\90\03#\0A\08@\000x\0A\05`\03\01\14\02\01\10\017\05\00\0A\C0\081 r\05q\00\05P\03&(\05\80\01\10\C8P\01(\05\05P\01E r\05\0C0\00!\CA/\00\02\1B\05\00\02/\90\FE\00\02\08\0B\10\00\00\E0\09\00\0B\01\06P\01C\08\00\80*\A0\01T\C4\0F\00Dy`\0CP\C0\03\00\EA\7F\A0\15\15\0A \15\00 \0EGt\0D\FF\02@\00\06p\19\01`\15Q\04\08r\0A\0C\F0\00\00@\150\E4\0F\04`\00\14\F0`\00\01p\06\05 \00\05p\00\15pp\00\17\0Fp\00\01P\00\00\10\16\06\D0\15\10\C4@\006\0B\0C\0A\00 \19\04\F0\0A\12\E4 \00\03\C0\09\00@\00\01\E0\00#p+ \00\16\C6@\15\13\0B\B0\12\00\80\00&\F0\00\80\00\10G\F0\02\16\E9P\01\040\00\11\04\F0\00\11\8F`\0B\17\01p\00\00`\00*\C0+@\01&\A0\00@\01c!r\0A\04\0C\00\01\00\01\C0\07\0A@\01\00\80\01*\00,@\00\17`\90\002!r\0A\10\01\07@\00\1B\04@\00\1B@@\00\17 @\002\02r\07\9C\0E\12\0F@\00\00\E0\00&@\F6\E0\00\03\80\16\05 \00AHs\00\000\03\000\02Q\E8\0F\00\89s\D0\0AA\0C\06\00\0E\C0\02`Py\00\08`\D3@\00Q\C3\03\00\EA\1FP\00#\F0\FFP\00f\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-\11\01\89\02\0E\01\00\22@\00\01\00=V\01\000\00\08\01\00\1F\0B@\00\04\13\96)\00?\98\02\00@\00\0A\00L\06.\02\00\01\00\1105\03\02h3\13\00$\00\13\07\A7\04\00\01\00\03\14(>\00\17\01T\00\00\01\00*\08\05@\00\0F\80\00\05\1A)\7F2\05\01\00\13\E0@\00\11T\06\00\06\E4\00*\04\00\01\00\1F[@\00\0414\06\00\01\00&\C0\00@\00\1F\0B@\00\00!G\01\18\00\0D@\00/\F8\06\C0\00\03\1B\08\08\00#$\01l\0E\0B\01\00\12\D0%\03\03l\03\02\80\00\17\048\00\22\10\00\01\00\13\B5\14\01\0C\84\01*\F0\07h\00\1F\00\C0\00\04\1F\E6@\00\04\13\F8@\00?x\01\00@\00\0A\132@\00\15\06\92\00\02\01\00*\80\09\E04\12\03\8C\00:$\80\00\01\00\13\87\A4\00+\03\00\01\00\03\F09/H\00\80\00\0B\13\06\9F\0D\05(:\0B\01\00*\A8\00\08\00\04\10\01\13\018\00\04(\01\0C\01\00*\10/\08\00\088\00\17\06\A0\00\0F\01\00\05\04\B8\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_0() {addr_space = 3 : i32} : !llvm.array<32 x f32>
    llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_1() {addr_space = 3 : i32} : !llvm.array<8 x f32>
    llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_2() {addr_space = 3 : i32} : !llvm.array<32 x f32>
    llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_3() {addr_space = 3 : i32} : !llvm.array<8 x f32>
    llvm.func @__nv_expf(f32) -> f32
    llvm.func @main_kStitch_divide__13_1_0___1b1r(%arg0: !llvm.ptr<f32>, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [0 : index, 2 : index, 3 : index, 4 : index, 6 : index, 7 : index, 8 : index, 9 : index, 11 : index, 12 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(-1 : i32) : i32
      %1 = llvm.mlir.constant(256 : index) : i32
      %2 = llvm.mlir.constant(4 : index) : i32
      %3 = llvm.mlir.constant(512 : index) : i32
      %4 = llvm.mlir.constant(768 : index) : i32
      %5 = llvm.mlir.constant(1024 : index) : i32
      %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %7 = llvm.mlir.constant(1 : i32) : i32
      %8 = llvm.mlir.constant(32 : i32) : i32
      %9 = llvm.mlir.constant(2 : i32) : i32
      %10 = llvm.mlir.constant(4 : i32) : i32
      %11 = llvm.mlir.constant(8 : i32) : i32
      %12 = llvm.mlir.constant(16 : i32) : i32
      %13 = llvm.mlir.constant(-0.000000e+00 : f32) : f32
      %14 = llvm.mlir.constant(8 : index) : i32
      %15 = llvm.mlir.constant(1 : index) : i32
      %16 = llvm.mlir.constant(32 : index) : i32
      %17 = llvm.mlir.constant(0 : index) : i32
      %18 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_0 : !llvm.ptr<array<32 x f32>, 3>
      %19 = llvm.getelementptr %18[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %20 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_1 : !llvm.ptr<array<8 x f32>, 3>
      %21 = llvm.getelementptr %20[0, 0] : (!llvm.ptr<array<8 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %22 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_2 : !llvm.ptr<array<32 x f32>, 3>
      %23 = llvm.getelementptr %22[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %24 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_3 : !llvm.ptr<array<8 x f32>, 3>
      %25 = llvm.getelementptr %24[0, 0] : (!llvm.ptr<array<8 x f32>, 3>) -> !llvm.ptr<f32, 3>
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %26 = nvvm.read.ptx.sreg.ctaid.x : i32
      %27 = nvvm.read.ptx.sreg.tid.x : i32
      %28 = llvm.udiv %27, %16  : i32
      %29 = llvm.urem %27, %16  : i32
      %30 = llvm.sub %arg1, %27  : i32
      %31 = llvm.icmp "eq" %30, %17 : i32
      %32 = llvm.sub %30, %15  : i32
      %33 = llvm.udiv %32, %1  : i32
      %34 = llvm.add %33, %15  : i32
      %35 = llvm.select %31, %17, %34 : i1, i32
      %36 = llvm.srem %35, %2  : i32
      %37 = llvm.sub %35, %36  : i32
      %38 = llvm.mul %37, %1  : i32
      %39 = llvm.add %27, %38  : i32
      %40 = llvm.mul %26, %arg1  : i32
      llvm.br ^bb2(%27, %6 : i32, f32)
    ^bb2(%41: i32, %42: f32):  // 2 preds: ^bb1, ^bb3
      %43 = llvm.icmp "slt" %41, %39 : i32
      llvm.cond_br %43, ^bb3, ^bb4
    ^bb3:  // pred: ^bb2
      %44 = llvm.add %41, %1  : i32
      %45 = llvm.add %41, %3  : i32
      %46 = llvm.add %41, %4  : i32
      %47 = llvm.add %40, %41  : i32
      %48 = llvm.add %40, %44  : i32
      %49 = llvm.add %40, %45  : i32
      %50 = llvm.add %40, %46  : i32
      %51 = llvm.getelementptr %arg0[%47] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %52 = llvm.load %51 : !llvm.ptr<f32>
      %53 = llvm.getelementptr %arg0[%48] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %54 = llvm.load %53 : !llvm.ptr<f32>
      %55 = llvm.getelementptr %arg0[%49] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %56 = llvm.load %55 : !llvm.ptr<f32>
      %57 = llvm.getelementptr %arg0[%50] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %58 = llvm.load %57 : !llvm.ptr<f32>
      %59 = llvm.fcmp "ugt" %42, %52 : f32
      %60 = llvm.select %59, %42, %52 : i1, f32
      %61 = llvm.fcmp "uno" %52, %52 : f32
      %62 = llvm.select %61, %52, %60 : i1, f32
      %63 = llvm.fcmp "ugt" %62, %54 : f32
      %64 = llvm.select %63, %62, %54 : i1, f32
      %65 = llvm.fcmp "uno" %54, %54 : f32
      %66 = llvm.select %65, %54, %64 : i1, f32
      %67 = llvm.fcmp "ugt" %66, %56 : f32
      %68 = llvm.select %67, %66, %56 : i1, f32
      %69 = llvm.fcmp "uno" %56, %56 : f32
      %70 = llvm.select %69, %56, %68 : i1, f32
      %71 = llvm.fcmp "ugt" %70, %58 : f32
      %72 = llvm.select %71, %70, %58 : i1, f32
      %73 = llvm.fcmp "uno" %58, %58 : f32
      %74 = llvm.select %73, %58, %72 : i1, f32
      %75 = llvm.add %41, %5  : i32
      llvm.br ^bb2(%75, %74 : i32, f32)
    ^bb4:  // pred: ^bb2
      llvm.br ^bb5(%39, %42 : i32, f32)
    ^bb5(%76: i32, %77: f32):  // 2 preds: ^bb4, ^bb6
      %78 = llvm.icmp "slt" %76, %arg1 : i32
      llvm.cond_br %78, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %79 = llvm.add %40, %76  : i32
      %80 = llvm.getelementptr %arg0[%79] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %81 = llvm.load %80 : !llvm.ptr<f32>
      %82 = llvm.fcmp "ugt" %77, %81 : f32
      %83 = llvm.select %82, %77, %81 : i1, f32
      %84 = llvm.fcmp "uno" %81, %81 : f32
      %85 = llvm.select %84, %81, %83 : i1, f32
      %86 = llvm.add %76, %1  : i32
      llvm.br ^bb5(%86, %85 : i32, f32)
    ^bb7:  // pred: ^bb5
      %87 = llvm.sub %8, %8  : i32
      %88 = llvm.lshr %0, %87  : i32
      %89 = llvm.sub %8, %7  : i32
      %90 = nvvm.shfl.sync  bfly %88, %77, %7, %89 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %91 = llvm.extractvalue %90[0] : !llvm.struct<(f32, i1)> 
      %92 = llvm.fcmp "ugt" %77, %91 : f32
      %93 = llvm.select %92, %77, %91 : i1, f32
      %94 = llvm.fcmp "uno" %91, %91 : f32
      %95 = llvm.select %94, %91, %93 : i1, f32
      %96 = llvm.sub %8, %8  : i32
      %97 = llvm.lshr %0, %96  : i32
      %98 = llvm.sub %8, %7  : i32
      %99 = nvvm.shfl.sync  bfly %97, %95, %9, %98 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %100 = llvm.extractvalue %99[0] : !llvm.struct<(f32, i1)> 
      %101 = llvm.fcmp "ugt" %95, %100 : f32
      %102 = llvm.select %101, %95, %100 : i1, f32
      %103 = llvm.fcmp "uno" %100, %100 : f32
      %104 = llvm.select %103, %100, %102 : i1, f32
      %105 = llvm.sub %8, %8  : i32
      %106 = llvm.lshr %0, %105  : i32
      %107 = llvm.sub %8, %7  : i32
      %108 = nvvm.shfl.sync  bfly %106, %104, %10, %107 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %109 = llvm.extractvalue %108[0] : !llvm.struct<(f32, i1)> 
      %110 = llvm.fcmp "ugt" %104, %109 : f32
      %111 = llvm.select %110, %104, %109 : i1, f32
      %112 = llvm.fcmp "uno" %109, %109 : f32
      %113 = llvm.select %112, %109, %111 : i1, f32
      %114 = llvm.sub %8, %8  : i32
      %115 = llvm.lshr %0, %114  : i32
      %116 = llvm.sub %8, %7  : i32
      %117 = nvvm.shfl.sync  bfly %115, %113, %11, %116 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %118 = llvm.extractvalue %117[0] : !llvm.struct<(f32, i1)> 
      %119 = llvm.fcmp "ugt" %113, %118 : f32
      %120 = llvm.select %119, %113, %118 : i1, f32
      %121 = llvm.fcmp "uno" %118, %118 : f32
      %122 = llvm.select %121, %118, %120 : i1, f32
      %123 = llvm.sub %8, %8  : i32
      %124 = llvm.lshr %0, %123  : i32
      %125 = llvm.sub %8, %7  : i32
      %126 = nvvm.shfl.sync  bfly %124, %122, %12, %125 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %127 = llvm.extractvalue %126[0] : !llvm.struct<(f32, i1)> 
      %128 = llvm.fcmp "ugt" %122, %127 : f32
      %129 = llvm.select %128, %122, %127 : i1, f32
      %130 = llvm.fcmp "uno" %127, %127 : f32
      %131 = llvm.select %130, %127, %129 : i1, f32
      %132 = llvm.icmp "eq" %29, %17 : i32
      llvm.cond_br %132, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %133 = llvm.getelementptr %21[%28] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %131, %133 : !llvm.ptr<f32, 3>
      llvm.br ^bb9
    ^bb9:  // 2 preds: ^bb7, ^bb8
      nvvm.barrier0
      %134 = llvm.icmp "slt" %27, %16 : i32
      llvm.cond_br %134, ^bb10, ^bb17
    ^bb10:  // pred: ^bb9
      %135 = llvm.icmp "slt" %29, %14 : i32
      llvm.cond_br %135, ^bb11, ^bb12
    ^bb11:  // pred: ^bb10
      %136 = llvm.getelementptr %21[%29] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %137 = llvm.load %136 : !llvm.ptr<f32, 3>
      llvm.br ^bb13(%137 : f32)
    ^bb12:  // pred: ^bb10
      llvm.br ^bb13(%6 : f32)
    ^bb13(%138: f32):  // 2 preds: ^bb11, ^bb12
      llvm.br ^bb14
    ^bb14:  // pred: ^bb13
      %139 = llvm.sub %8, %11  : i32
      %140 = llvm.lshr %0, %139  : i32
      %141 = llvm.sub %11, %7  : i32
      %142 = nvvm.shfl.sync  bfly %140, %138, %7, %141 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %143 = llvm.extractvalue %142[0] : !llvm.struct<(f32, i1)> 
      %144 = llvm.fcmp "ugt" %138, %143 : f32
      %145 = llvm.select %144, %138, %143 : i1, f32
      %146 = llvm.fcmp "uno" %143, %143 : f32
      %147 = llvm.select %146, %143, %145 : i1, f32
      %148 = llvm.sub %8, %11  : i32
      %149 = llvm.lshr %0, %148  : i32
      %150 = llvm.sub %11, %7  : i32
      %151 = nvvm.shfl.sync  bfly %149, %147, %9, %150 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %152 = llvm.extractvalue %151[0] : !llvm.struct<(f32, i1)> 
      %153 = llvm.fcmp "ugt" %147, %152 : f32
      %154 = llvm.select %153, %147, %152 : i1, f32
      %155 = llvm.fcmp "uno" %152, %152 : f32
      %156 = llvm.select %155, %152, %154 : i1, f32
      %157 = llvm.sub %8, %11  : i32
      %158 = llvm.lshr %0, %157  : i32
      %159 = llvm.sub %11, %7  : i32
      %160 = nvvm.shfl.sync  bfly %158, %156, %10, %159 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %161 = llvm.extractvalue %160[0] : !llvm.struct<(f32, i1)> 
      %162 = llvm.fcmp "ugt" %156, %161 : f32
      %163 = llvm.select %162, %156, %161 : i1, f32
      %164 = llvm.fcmp "uno" %161, %161 : f32
      %165 = llvm.select %164, %161, %163 : i1, f32
      llvm.cond_br %132, ^bb15, ^bb16
    ^bb15:  // pred: ^bb14
      llvm.store %165, %19 : !llvm.ptr<f32, 3>
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb14, ^bb15
      llvm.br ^bb17
    ^bb17:  // 2 preds: ^bb9, ^bb16
      nvvm.barrier0
      %166 = llvm.load %19 : !llvm.ptr<f32, 3>
      llvm.br ^bb18(%27, %13 : i32, f32)
    ^bb18(%167: i32, %168: f32):  // 2 preds: ^bb17, ^bb19
      %169 = llvm.icmp "slt" %167, %39 : i32
      llvm.cond_br %169, ^bb19, ^bb20
    ^bb19:  // pred: ^bb18
      %170 = llvm.add %167, %1  : i32
      %171 = llvm.add %167, %3  : i32
      %172 = llvm.add %167, %4  : i32
      %173 = llvm.add %40, %167  : i32
      %174 = llvm.add %40, %170  : i32
      %175 = llvm.add %40, %171  : i32
      %176 = llvm.add %40, %172  : i32
      %177 = llvm.getelementptr %arg0[%173] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %178 = llvm.load %177 : !llvm.ptr<f32>
      %179 = llvm.getelementptr %arg0[%174] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %180 = llvm.load %179 : !llvm.ptr<f32>
      %181 = llvm.getelementptr %arg0[%175] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %182 = llvm.load %181 : !llvm.ptr<f32>
      %183 = llvm.getelementptr %arg0[%176] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %184 = llvm.load %183 : !llvm.ptr<f32>
      %185 = llvm.fsub %178, %166  : f32
      %186 = llvm.fsub %180, %166  : f32
      %187 = llvm.fsub %182, %166  : f32
      %188 = llvm.fsub %184, %166  : f32
      %189 = llvm.call @__nv_expf(%185) : (f32) -> f32
      %190 = llvm.call @__nv_expf(%186) : (f32) -> f32
      %191 = llvm.call @__nv_expf(%187) : (f32) -> f32
      %192 = llvm.call @__nv_expf(%188) : (f32) -> f32
      %193 = llvm.fadd %168, %189  : f32
      %194 = llvm.fadd %193, %190  : f32
      %195 = llvm.fadd %194, %191  : f32
      %196 = llvm.fadd %195, %192  : f32
      %197 = llvm.add %167, %5  : i32
      llvm.br ^bb18(%197, %196 : i32, f32)
    ^bb20:  // pred: ^bb18
      %198 = llvm.load %19 : !llvm.ptr<f32, 3>
      llvm.br ^bb21(%39, %168 : i32, f32)
    ^bb21(%199: i32, %200: f32):  // 2 preds: ^bb20, ^bb22
      %201 = llvm.icmp "slt" %199, %arg1 : i32
      llvm.cond_br %201, ^bb22, ^bb23
    ^bb22:  // pred: ^bb21
      %202 = llvm.add %40, %199  : i32
      %203 = llvm.getelementptr %arg0[%202] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %204 = llvm.load %203 : !llvm.ptr<f32>
      %205 = llvm.fsub %204, %198  : f32
      %206 = llvm.call @__nv_expf(%205) : (f32) -> f32
      %207 = llvm.fadd %200, %206  : f32
      %208 = llvm.add %199, %1  : i32
      llvm.br ^bb21(%208, %207 : i32, f32)
    ^bb23:  // pred: ^bb21
      %209 = llvm.sub %8, %8  : i32
      %210 = llvm.lshr %0, %209  : i32
      %211 = llvm.sub %8, %7  : i32
      %212 = nvvm.shfl.sync  bfly %210, %200, %7, %211 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %213 = llvm.extractvalue %212[0] : !llvm.struct<(f32, i1)> 
      %214 = llvm.fadd %200, %213  : f32
      %215 = llvm.sub %8, %8  : i32
      %216 = llvm.lshr %0, %215  : i32
      %217 = llvm.sub %8, %7  : i32
      %218 = nvvm.shfl.sync  bfly %216, %214, %9, %217 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %219 = llvm.extractvalue %218[0] : !llvm.struct<(f32, i1)> 
      %220 = llvm.fadd %214, %219  : f32
      %221 = llvm.sub %8, %8  : i32
      %222 = llvm.lshr %0, %221  : i32
      %223 = llvm.sub %8, %7  : i32
      %224 = nvvm.shfl.sync  bfly %222, %220, %10, %223 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %225 = llvm.extractvalue %224[0] : !llvm.struct<(f32, i1)> 
      %226 = llvm.fadd %220, %225  : f32
      %227 = llvm.sub %8, %8  : i32
      %228 = llvm.lshr %0, %227  : i32
      %229 = llvm.sub %8, %7  : i32
      %230 = nvvm.shfl.sync  bfly %228, %226, %11, %229 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %231 = llvm.extractvalue %230[0] : !llvm.struct<(f32, i1)> 
      %232 = llvm.fadd %226, %231  : f32
      %233 = llvm.sub %8, %8  : i32
      %234 = llvm.lshr %0, %233  : i32
      %235 = llvm.sub %8, %7  : i32
      %236 = nvvm.shfl.sync  bfly %234, %232, %12, %235 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %237 = llvm.extractvalue %236[0] : !llvm.struct<(f32, i1)> 
      %238 = llvm.fadd %232, %237  : f32
      llvm.cond_br %132, ^bb24, ^bb25
    ^bb24:  // pred: ^bb23
      %239 = llvm.getelementptr %25[%28] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %238, %239 : !llvm.ptr<f32, 3>
      llvm.br ^bb25
    ^bb25:  // 2 preds: ^bb23, ^bb24
      nvvm.barrier0
      llvm.cond_br %134, ^bb26, ^bb33
    ^bb26:  // pred: ^bb25
      %240 = llvm.icmp "slt" %29, %14 : i32
      llvm.cond_br %240, ^bb27, ^bb28
    ^bb27:  // pred: ^bb26
      %241 = llvm.getelementptr %25[%29] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %242 = llvm.load %241 : !llvm.ptr<f32, 3>
      llvm.br ^bb29(%242 : f32)
    ^bb28:  // pred: ^bb26
      llvm.br ^bb29(%13 : f32)
    ^bb29(%243: f32):  // 2 preds: ^bb27, ^bb28
      llvm.br ^bb30
    ^bb30:  // pred: ^bb29
      %244 = llvm.sub %8, %11  : i32
      %245 = llvm.lshr %0, %244  : i32
      %246 = llvm.sub %11, %7  : i32
      %247 = nvvm.shfl.sync  bfly %245, %243, %7, %246 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %248 = llvm.extractvalue %247[0] : !llvm.struct<(f32, i1)> 
      %249 = llvm.fadd %243, %248  : f32
      %250 = llvm.sub %8, %11  : i32
      %251 = llvm.lshr %0, %250  : i32
      %252 = llvm.sub %11, %7  : i32
      %253 = nvvm.shfl.sync  bfly %251, %249, %9, %252 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %254 = llvm.extractvalue %253[0] : !llvm.struct<(f32, i1)> 
      %255 = llvm.fadd %249, %254  : f32
      %256 = llvm.sub %8, %11  : i32
      %257 = llvm.lshr %0, %256  : i32
      %258 = llvm.sub %11, %7  : i32
      %259 = nvvm.shfl.sync  bfly %257, %255, %10, %258 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %260 = llvm.extractvalue %259[0] : !llvm.struct<(f32, i1)> 
      %261 = llvm.fadd %255, %260  : f32
      llvm.cond_br %132, ^bb31, ^bb32
    ^bb31:  // pred: ^bb30
      llvm.store %261, %23 : !llvm.ptr<f32, 3>
      llvm.br ^bb32
    ^bb32:  // 2 preds: ^bb30, ^bb31
      llvm.br ^bb33
    ^bb33:  // 2 preds: ^bb25, ^bb32
      nvvm.barrier0
      %262 = llvm.load %19 : !llvm.ptr<f32, 3>
      %263 = llvm.load %23 : !llvm.ptr<f32, 3>
      llvm.br ^bb34(%27 : i32)
    ^bb34(%264: i32):  // 2 preds: ^bb33, ^bb35
      %265 = llvm.icmp "slt" %264, %39 : i32
      llvm.cond_br %265, ^bb35, ^bb36
    ^bb35:  // pred: ^bb34
      %266 = llvm.add %264, %1  : i32
      %267 = llvm.add %264, %3  : i32
      %268 = llvm.add %264, %4  : i32
      %269 = llvm.add %40, %264  : i32
      %270 = llvm.add %40, %266  : i32
      %271 = llvm.add %40, %267  : i32
      %272 = llvm.add %40, %268  : i32
      %273 = llvm.getelementptr %arg0[%269] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %274 = llvm.load %273 : !llvm.ptr<f32>
      %275 = llvm.getelementptr %arg0[%270] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %276 = llvm.load %275 : !llvm.ptr<f32>
      %277 = llvm.getelementptr %arg0[%271] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %278 = llvm.load %277 : !llvm.ptr<f32>
      %279 = llvm.getelementptr %arg0[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %280 = llvm.load %279 : !llvm.ptr<f32>
      %281 = llvm.fsub %274, %262  : f32
      %282 = llvm.fsub %276, %262  : f32
      %283 = llvm.fsub %278, %262  : f32
      %284 = llvm.fsub %280, %262  : f32
      %285 = llvm.call @__nv_expf(%281) : (f32) -> f32
      %286 = llvm.call @__nv_expf(%282) : (f32) -> f32
      %287 = llvm.call @__nv_expf(%283) : (f32) -> f32
      %288 = llvm.call @__nv_expf(%284) : (f32) -> f32
      %289 = llvm.fdiv %285, %263  : f32
      %290 = llvm.fdiv %286, %263  : f32
      %291 = llvm.fdiv %287, %263  : f32
      %292 = llvm.fdiv %288, %263  : f32
      %293 = llvm.getelementptr %arg2[%269] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %289, %293 : !llvm.ptr<f32>
      %294 = llvm.getelementptr %arg2[%270] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %290, %294 : !llvm.ptr<f32>
      %295 = llvm.getelementptr %arg2[%271] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %291, %295 : !llvm.ptr<f32>
      %296 = llvm.getelementptr %arg2[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %292, %296 : !llvm.ptr<f32>
      %297 = llvm.add %264, %5  : i32
      llvm.br ^bb34(%297 : i32)
    ^bb36:  // pred: ^bb34
      %298 = llvm.load %19 : !llvm.ptr<f32, 3>
      %299 = llvm.load %23 : !llvm.ptr<f32, 3>
      llvm.br ^bb37(%39 : i32)
    ^bb37(%300: i32):  // 2 preds: ^bb36, ^bb38
      %301 = llvm.icmp "slt" %300, %arg1 : i32
      llvm.cond_br %301, ^bb38, ^bb39
    ^bb38:  // pred: ^bb37
      %302 = llvm.add %40, %300  : i32
      %303 = llvm.getelementptr %arg0[%302] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %304 = llvm.load %303 : !llvm.ptr<f32>
      %305 = llvm.fsub %304, %298  : f32
      %306 = llvm.call @__nv_expf(%305) : (f32) -> f32
      %307 = llvm.fdiv %306, %299  : f32
      %308 = llvm.getelementptr %arg2[%302] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %307, %308 : !llvm.ptr<f32>
      %309 = llvm.add %300, %1  : i32
      llvm.br ^bb37(%309 : i32)
    ^bb39:  // pred: ^bb37
      llvm.return
    }
  }
  gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\A0 \00\00\00\00\00\00\02\00\01\01@\00\00\00` \00\00\00\00\00\00^ \00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(K\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80J\08\00\11G\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F5)_shndx\00.nv.info\00.text.main_kStitch_divide__13_1_0___1w1r2\00\0F,\00\15oshared.\00\15\9Fconstant01\00\12\FD\01debug_frame\00.rel\11\00\1Aa\12\00!nv&\00oaction%\01 \0F\94\00\0F\0FH\01q\1F$$\00\0F\FF\16$__cuda_sm70_shflsync_bfly_p\00$____wg_G\00\0F\00\09\00/261\00\19_1__28\EA\01\1Fo_param\F1\01.\0F\01\00\06\8CU\00\00\00\03\00\0A\00\01\00\11\AA\18\00,\0B\00\01\00\A1\D8\00\00\00\22\00\0A\00@=\0F\001\00@\01\07\000\00z\010\00,\09\00\01\00\11\B2\18\00,\04\00\01\00\11\E2\18\00,\07\00\01\00f2\00\00\00\12\10\90\00!\80>\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\130\C0\00\10\04\9B\00R\04<\00\00\00E\005\04\08\0F<\00&8\00H\00\04p\00L\84\80\80(o\00\10\08\15\00\81\16\85\80\80(\09\05\0C2\00\068\00\22\18\00\01\00\22p\00\01\00\048\01\1308\011\04/\08\06\01\10&\0C\00R#\08\00\03\00\01\00'\04\12\0C\00\17\11\0C\00\12#0\00\04$\00\17\07$\00\05\0C\00!7\044\05q\015\00\00\04\0A\08~\01\A2`\01 \00\03\19 \00\04\17\A2\00u\04\00\18\00\00\F0!\10\00u\03\00\10\00\00\F0\11\10\009\02\00\0C\10\009\01\00\08\10\00\01\01\00\F1_\F0!\00\03\1B\FF\00\04(P\00\C0\12\00\00\00\13\00\00@\13\00\00\80\13\00\00\C0\13\00\00\10+\00\000+\00\00P+\00\00p+\00\00\90+\00\00\00:\00\00p:\00\00\E0:\00\00P;\00\00\C0;\00\00\10<\00\00P<\00\00\90<\00\00\D0<\00\00 =\00\00\04\1C\0C\00\F0+\00\00`6\00\00\C09\00\00\04\1E\04s\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\D8\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00\C8\00\01\001\02\00\00\C8\01*H\00\10\00\0F\01\00\FF\D6@$v\01\FF\8F\05\B1\FF\00\8E\07\00\C4\0F\00\19y\0D\18\00 \00!y\05\A1\0E\00\B9z\04\00\00F\00\00\B3\05\90\E2\0F\00Ey\00\00\10\14!\00 \80\03\10\00c$t\0B\FF\1F\00@\00\10\E2@\00\03:\04\B1%\00\00\00b\0E\00$t\1A\FF\01\00\04 \00\F1\04x\0A\FF\05\00\00\00\0D\16\01\00\00\E4\1F\00\12x\0D\0D@\00\B0\C0\8E\07\00\C6\0F\00$x\0C\03\A9\02 \0A\020\00\D0/\00\0Cz\00\0D\00[\00\00pR\F2p\00g\04$x\0A\0A\04p\00P\0Cr\00\0D\FF\F0\04`R\F4\03\00\E4\0F0\00\81\0C\00\\\00\00pb\F6\10\00@\12r\00\FF\D2\00\A2\FF3\8E\07\00\D6\0F\00G94\04\01\C0\00p\EA\0F\00\10z\02\00`\00B\FF\E0\FF\07\E0\00H\01\00\E0\0C\E0\00R\13\FF\00\00\80\C0\00\93\E4\0F\00\10x\02\02 \000\00\80\C8\0F\00\12x\02\02\80\E0\00!\C0\8E\10\00a\07r\02\02\FF\00@\00\11\00 \001r\0F\02\80\00\12\FC \00\00\B0\00\00\F2\05\E6pb\F0\03\00\DA\0F\00G\09\00\00`\0C\90\00@$x\02\0F\AB\05!\0D\0A\E0\00v\10Ey\02\00\F0\06\90\001\1Cx\00\01\00?p\F0\F0\A0\00\02\B2\0Cx\00\02\80\01\00\00pB\F8 \000r\11\FF\90\00\10\0D \00\96\D8\0F\00G\C9\00\00\90\06p\00\05P\002\E1\F0\03\E0\00a\10\0F\80\FE\FF\FF\E0\00\10\C4`\00%\16\FFp\01p\E4\0F\00$z\09\0C0\01\22\11\02\D0\00\90%v\02\09\00X\00\00\16\10\00p\CA\0F\00\81y\0E\020\00\C5\00\19\1E\0C\00\A2\00\00\10x\05\090\01\84\E4\0F\04\10x\07\09@\10\00\10\C6@\00*\04\05@\00%\15\04@\00u\E2\02\00%v\06\07 \00\86\E2\0F\08\10x\09\09`P\00g\00\10x\17\11\80\10\00E\81y\12\06@\00fb\09\00%v\08\A0\00\10\C8\C0\00\11\1B\C0\003\17\02\8E0\00%\17\080\00\10d0\00&\02\1Bp\00h\1F\04\10x\19\1B\F0\016\81y\14\E0\00 b\01\C0\00\16\190\00y/\00\10x\1D\1B@0\00\16\19\D0\00 b\03\D0\00\16\1D0\00j\0F\09\10x\1B\1B\D0\00F\1D\11\00\01\10\00<\81y\18\D0\00\06\A0\00\01\D0\00\11\1F\D0\00\12\1DP\00F\00\81y\1B\D0\00\11f\D0\00\1A\1F\D0\00*\1D\1F\D0\00\1C\1C\D0\00\1A\1D\D0\00*!\1F\D0\00\1C\1D\D0\00\1A!\D0\00*\1F\1F\D0\00F!\11\80\01\10\00<\81y\1E\D0\00\06\A0\00\01\D0\00\11!\D0\00\15!\D0\00\17\1F\D0\00\00p\00*\02!\D0\00%#!\D0\00\01@\02\17\02\D0\00\01`\02\1A#\D0\009#!@0\00*\05\040\00\17\060\00y\0F\01\10x!!`0\00\16\06\C0\00\10\220\00\18\08\00\01`\00\0Br\00\0E\0E \00\B1\80\F8\03\00\E4O\08\0Br\00\13\10\000\C0\FA\03\80\04A\08\C2\0E\0E\7F\042\00\80\06\10\01\16\13\E0\00 \A2\0E@\00!\15\15\1F\00P\FA\03\00\E4\8F@\00%\12\12P\00 \0F\02`\00\01 \00\B1\C0\FC\03\00\D2\0F\00\08\D2\15\15`\00!\00\00`\01U\0Br\00\17\17@\00\01\10\00$\15\120\00\10\C8\80\00!\12\12@\00\060\00&\14\14`\00\00\10\00(\12\170\001\D2\17\17@\00\060\00*\19\19`\00)\17\14`\00!\14\14@\00\060\00*\18\18`\00)\14\19`\00!\19\19@\00\060\00*\1B\1B`\00)\19\18`\00!\18\18@\00\060\00*\1C\1C`\00)\18\1B`\00!\1B\1B@\00\060\00*\1D\1D`\00)\1B\1C`\00!\1C\1C@\00\060\00*\1E\1E`\00)\1C\1D`\00!\1D\1D@\00\060\00*\1F\1F`\00)\1D\1E`\00!\1E\1E@\00\060\00*\02\02`\00)\1E\1F`\00!\1F\1F@\00\060\00*\05\05`\00)\1F\02`\00!\02\02@\00\060\00&\06\06`\00\10\01p\00\19\05`\00!\05\05@\00\030\00c\10x\11\11\00\02\F0\02\03p\00\19\06`\00!\06\06@\00\030\00P\0Cr\00\11\10@\06$b\FC\90\00%\13\13\A0\00\01\00\03\11\06\10\00\22\C0\F8\00\031\D2\13\13P\000\00\00\060\03`G\E9\00\00\90\F9A\07\11\83\90\06#Ay\8E\00\0B\10\073\11\0A\8E\A0\07C\02\00p\03 \00\13\E8\F0\06\13\00\F0\06\12\DA\E0\06'@\03@\00+t\03\C0\06\1B\0E\C0\06 \04\0E\C0\033\03\02\8E\E0\03*\02\04\C0\06%\06\0E`\04\01\C0\06*\08\0E\C0\06*\06\06@\00*\19\06\C0\06%\08\08 \00\01\C0\06/\14\0E\C0\06\0B*\0E\08 \05%\14\14@\00\01 \05\1B\10\C0\06%\15\140\00\10d\90\00'\16\10p\00V\04\10x\04\10 \05k\1F\00\81y\16\16\F0\04\16\040\00\00 \05*\04\10 \05*\07\060\00\17\040\00\00\C0\00'\08\10 \05[\01\81y\04\04 \05\08\00\01\07\F0\02\15\F0 \05\18\02 \021\82\02\020\02/\00\00 \05\05%\19\19\D0\02&\8F\08p\05\14\F0 \05$\02\19p\05\00 \051\C2\19\19`\00\03p\05\06`\05\14\F8\B0\02\15\19\A0\05\00\E0\02J\82\0E\0E\190\00&\16\16`\00\16\00\80\05\030\00\14\C2\80\05\050\00*\07\07`\00)\15\16`\00:\16\16\150\00&\04\04`\00\00\A0\03)\16\07`\004\07\07\160\00\11\C80\00)\07\04P\007\04\04\07P\00\0E\D0\09\19\11P\08\06\A0\03\14\F8\80\01$\04\13P\00\00\80\01D\C2\13\13\04P\00\1E\C4\90\03\00\F0\03\01\C0\0A1\16p\00p\03X\89\00\00\A0\01p\03\1B\09p\03\1B\08p\03\11\02\10\02\16\09\A0\02/\08 \C0\07\05%\A2\0Ep\02\15\09p\02/\06\08\C0\07\06%\E2\0E\90\03\060\00?\08\08`\C0\07\0B\06\90\00\10\CC \00(\09\08\F0\02\06\B0\02\1B\F8\B0\02*\FA\03\90\05Q\F0\03\00\D2\8F\80\05\02\C0\02/\80\06\80\05\0B\04\C0\01\04\80\05\050\00*\09\09\D0\02\03\80\05\1B\FA\80\05\05\10\02$\06\09@\03\01P\00!\09\090\00\00@\03\10\CA\00\0C\11\13\00\0C\13\09\E0\0DIAy\01\00\D0\01Sz\00\0F\00[\90\0C\03\00\0D#\90\04 \00\12\D8\A0\0C)p\040\0D\010\005\FF\E1\FF\90\05#`\02 \00\0C\A0\0C\03\90\0C\1B`\A0\05& \02P\00c$v\0E\FF\00[0\02\1B\E2\C0\02\10\C8\C0\01H\0E\0E\A0\FF\90\0C\1B\10`\02\11\02`\02\16\0F`\02\10\02\D0\013\10\02\8E\F0\01\1C\08\90\0C\15\0Fp\02\02\90\0C\18\0F\D0\051$z\05P\001\05\02\8E \001$z\07\10\00\12\07\10\00\05\B0\0C\15\10p\00%\06\07\10\00\1E\E4\90\02W\10x\09\0F``\00\09\80\02\16$\C0\00\01\80\02*\C8\1F\C0\00\020\05+\08\08\80\02\19\08\B0\03(\08\08p\02K\81y\13\020\05*\05\05P\0A$\08\05\D0\07\0F\A0\02\00E\10x\0F\0F\90\06\12\CE \08\01p\00!\00\00\F0\00\0F\10\08\10/\0F\0E\10\08,.\10\FE\10\08\0F\80\02\09#0\01 \00\12\E6p\02\1B p\02(\00\01\A0\04\1F\04@\02\12\15\04@\02\19\06P\01\09@\02\1B\CA0\02\16\C8 \02\16\04\C0\01%\06\06\D0\03\02\C0\01\18\06\00\01\22\82\06\10\01$\00\00\C0\01\18\04p\00\09\10\03\01\D0\05\16\0F\B0\02\09`\01\1B\F8`\01\06\D0\05\01p\00\0F\D0\05\05\04\00\04\07\D0\05.\80\00@\09\0F0\01\0D\15\030\01\16\040\01+\A4\0E\E0\06\02\00\01\19\04\00\01\12\04\80\06\0B\B0\04\1F\04\B0\04\04aGy\00\7F\12'\10\00\10\0B\10\00\FB\00\89\7F\02\13\00\1F \0C\00\00\0E\00\00$\0E`\08\1F\1F`\08\0B\10\CA@\00X\03\02\00\1F@@\00*\03\03@\00)\02\03@\00!\03\03P\00\05@\00X\04\03\00\1F\80@\00*\04\04@\00\1C\03\F0\00\01P\00\05@\00g\05\04\00\1F\00\0D@\00*\05\05@\00)\04\05@\00!\05\05P\00\05@\00a\06\05\00\1F\00\0E@\00ab\00\00$r\02@\011\05\00\8Ep\04(\0B\A2\E0\03,/\08\A0\05T\0B\A2\00\02\060\02\00@\08#\A8\00 \00 p\02\10\02\00\A0\02\14\02@\02q\CA\0F\00\88\A3\00\0A0\00\00\86\16i\E4\03\00Ay\00\A0\01\22Hy\88\18\04\10\066\1D{\00\01\00\12\EC@\14\11P!\0E\02 \00\00\90\13/0\17\90\13\0B&\B0\07\E0\03\0F\80\13\1F\16\03\80\13\010\13\01\1F\00#\80\FF \01\00\90\13\11\03\B0\04\11\F8\00\01@\0Cz\00\03\80\00\01\10\07\000\14VI\00\00 \07\A0\00\09@\00\00\10\03&r\04`\13\01\A0\0B\11\18 \03\01\00\04\1B\E4@\13\10\C6\C0\00%\11\18\C0\00w\E2\0F\04%v\12\18@\10X\04\10x\10\18\90\06G%v\0E\11 \00V\08\81y\06\12 \04\10\22`\06\15\18`\06\01\A0\06*\14\100\00%\05\0E0\00 \E6\10 \0C\17\09 \00\00\E0\0B\06@\0C h\090\07\18\16\10\121\06{\19\90\01\C1\00\90 \00\000\0E\00\08s\19\00 \0B\00\F5\0D $\1E\A0\07Q\19\FE\FF\FF\0F\80\00`\CC\1F\00\05s\0F\8A\02\01\AC\19 \00d\90\025\0E\FF\FF@\04\15\1F\E0\03\10\0F\D0\0Dp\C8/\00$z\13\13`\00\22\FF\02@\01@'r\0F\0F\A0\03\22\0E\00\90\09T'r\12\0F\18@\00\16\C8 \04%\12\0Ap\01\01@\00\13\18p\01V'r\14\0F\11p\01P\01\0Cz\00\18 \00$p`\10\15\02\90\00\15\14@\00%\11\0E\A0\0A\01@\006\0E\0F\10@\00\00\10\02\12\11@\00 \FC\03\10\000\10J\18P\00\11\80\F0\00\16\E2\90\00%\0E\0A0\00\03p\00\12\FAp\00!z\10\A0\00\04\D0\07T\10H\12\12\01\B0\01\10\E2p\007\16\0F\09p\00E\10j\11\11`\00\02\90\02\1A\10\C0\00\11\18p\00\13\16\C0\00'\10XP\00\18\E4\B0\00\04\80\00%\0F\18`\08\00\80\005h\14\140\00\00@\00;J\10\10p\00\15\0F\F0\00\10\CA`\00\070\00\09\90\00\020\04I\10H\0E\0EP\00;j\0F\0FP\00\01\E0\09\22pP`\032\12z\09\F0\09\133\F0\00\17X@\00\09\80\00\02`\00V\07r\12\09\12 \07g\04\07r\14\09\14\10\00H\10h\16\16P\01T$x\0F\12\04P\01\10\E2@\00!\0E\09\90\02\03`\08W$x\10\14\04p\01@\12x\0F\0F\C0\11\22\FF\E2\00\02,\10XP\00(\0E\0EP\00H\12x\10\100\00W\07r\09\09\16`\00T\84y\0F\0F\00 \05u\A2\0E\00\12x\0E\0E0\00\01\E0\18(\09\09P\00E\84y\10\100\00\10\E80\00%\09\090\00\01 \00%\0E\0E \00\10j\10\00%\09\09\10\00!\22\0E\00\18S\9D\98\BB;\FF0\07\84$t\15\FF\00\00|C`\00W\10x\04\04\80\A0\01E\0Cr\00\04\F0\04\84\E2\0F\00!r\0F\06\0F\F3\07P\C8O\00#t\0F\00B\00?\13 \F0\19T!r\10\05\10 \00 \C6\8F \00t\06\01\00@K\15@ \004\07\07\0E \00u\E2\0F\02#t\05\10@\00\B3\E4\0F\08!t\0E\06\7F\00@\CB \00X\00#t\12\07`\00G#t\05\05P\00e\08!r\08\08\09P\00\C4\1F\00#x\0E\0F;\AA\B8?\0E\080\00)\12\12\80\00:t\11\05`\00)\09\08`\00\94x\0E\0F`p\A52\0E\000\00)\14\120\000x\11\10`\00\15\11`\00%\0F\09`\00\10\C4 \00 \14\07 \00&\14\080\00\00P\00\22\11\00\10\00W\08s\0E\00\0E\90\01J!t\09\0F`\00 \14\070\00\01\B4\02\10\C6\10\00 \07\08P\00\22\09\08\10\00T\08s\10\00\11@\00@b\0E\00#\B0\0ECp\A52\07@\00@$x\07\06\0F\01\13\FF\10\05W\08s\09\00\14`\02E$x\05\05 \00\00p\00Cr\02\07\0E\C8\1D\00\90\05X\08s\11\00\080\02:x\12\120\004\05\05\100\00!\E2/p\13\05 \00\11\C6 \00!\12\09P\08\03 \02Dr\02\02\11\10\00\22\E2\1F`\0B\09\C0\13\0A\B0\09\01P\0E\13\0E\10\00\13\E2P\0E\18\0E\F0\07,\04\03\D0\0B/\A0\07P\0E\0A\1C\04P\0E(`\07P\0E\12\04`\04\0FP\0E\06(\04\04P\0EV\10x\09\03 \F0\03F\04$t\18\B0\07\01@\035\05\03@ \00Q\1F\04$z\12\E0\07\01\F0\0A\01 \0E\16\09\C0\0D\01\80\07C\0E\12\00X\90\06\01@\00%\11\03\A0\07\01\D0\07'\14\09 \00F\08\81y\07\A0\07%\A6\00\E0\13\12\05 \00F\04\81y\08\A0\07 \22\07\B0\06\18\0C\B0\06\04\F0\13\22\18\02\10\06G%v\18\11`\00\00\C0\0C\06\D0\07 h#\A0\0E\18\18\E0\106\06{\13\E0\07*b\0E\B0\07\84\E2\1F\00\10x\03\03\80\C0\00\01\80\02\10\13w!\03\00\08l.\00\10x\1B\13\00\08\14\1B\00\08\10\E4P\00\17\15\F0\07j\8F\00$z\15\15\F0\079\16\0F\15\F0\074\0E\16\12\80\00\00\A0\07&r\0F`\07\01\A0\078\13\16\10 \00!z\120\06$\12\02 \00 \0F\16\B0\02\07\C0\07\1A\12@\07\16\14\90\00%\E4\0F\A0\00%\13\0A\E0\1C\18\14\D0\01E$z\10\15\C0\07\02P\00\1A\09\10\08(\12\12\F0\06-\10H\D0\06\15\12\D0\06\01\F0\074\12\16\11\A0\00\1B\E2\E0\07\16\C6\A0\00\22\12\0A \00I\10j\09\09`\005h\0F\0F`\00\11\E2\C0\08\18\14\10\026$t\14\00\06\1F\E4`\07\02\15\09\90\00\1F\C4\00\08\02\15\11\E0\00\00@\00;H\13\13@\00\06\D0\07\01 \08/\0F\0F \08\0A\0B\D0\086\12z\10\00\08\00 \00\1Ah\C0\08A\07r\0E\10\1F\04!\00\00\B0\0ET\07r\0F\10\0F\10\00\010\08)\13\13\00\01)x\09\A0\07\0D\00\098x\0F\0F \00A\07r\13\10\C0\02\02@\08\0Ap\07\1B\E4\10\08\01@\00%\13\13@\00\11\E4\90\07$\09\00P\05*\A8\0E\80\09\0C\00\08\10&p\007\12\10\12\D0\00I\12x\10\13`\00\08\C0\07\00\C0\00\030\0E\04\A0\07\00\B0\05\0A\80\00\01o\19\02`\00\10fP\00%\12\12P\00\10\CC \00\02/\1C\05\90\06Ir\0E\07\0E\E0\07*\07\0E\E0\074\08\08\0F \00p\C6\0F\01#t\07\07\00\07!\14@ \00H\08#t\0FP\07d\08!r\11\06\110\00y\E4\0F\02!t\09\07\F0\068t\06\0F@\00 $x\0F\17\07p\06:t\10\11P\009\05\05\12\F0\07%\09\0E \07\01\B0\07\1B\0F@\08*\10\10\A0\00'\12\05P\00\01@\00\00@\07$\09\00\10\00\11\0Fp\07$\0F\08P\00*\0E\10P\00'\12\12P\00Y\00$x\06\06\B0\00!x\0F\90\07%\0F\00P\08\17\11\90\08I\04!t\08P\08\00\A0\07\14\09\80\01!$\0E0\00\08\80\08@#x\08\05@\00\01\D5\13\12\C8\10\00\00 \00\15\08@\007\0F\00\0F\10\08E$x\05\10\90\00\1B\CEp\08\85\A2\0E\00#r\07\07\09\C0\07\10\1F\C0\07\06\E0\07\01\B0\057\08\00\08\10\021#r\06\E1\09\013\07!\C8/\00\08\11\0E\9D\02\06\E0\07\1B\08\E0\07.\D0\F8\90\13\0F\C0\07\09)0\04\90\13\1D\04\90\13\17\04@\00'x\05\80\07h\1F\00$t\08\FF\F0\02\04\10\07\06p\07\16\11 \07\01\00\07\11\06\00\07\22\08\02P\0EE%v\08\11\10\00\01\00\07\16\05p\15 \A8\00\A0\12\18\08\A0\1B6\06{\12\00\07!\22\0F\F0\06\18@\00\15Es\12\00\12\F0\06\00 \15&\0E\12\F0\06*\0F\01\F0\0E\1A\09@\074\E4\0F\01 \0E\02\00\07.\0F\02\F0\0E\16\13\F0\0E\00\D0\06&t\0E\10\04\01\D0\064\06\13\10P\00\10\C8@\0F\11\09P\00\10\06P\00u\E4/\00'r\07\13P\06\01\10\07%\10\09\B0\06\00@\01!r\080\00#\07\0A\10\00)t\0F \0C\06\B0\05#\FC\03P\06\15\08P\06\00\F0\0C&z\08\B0\05\1B\C8`\05\10\CE\E0\05\0A@\06E\10h\06\06\10\05\1B\E4\E0\06\00 \00?Z\11\11 \0E\03\02p\06\06`\00\12\F0\A0\06\10X\90\04\06p\15'\10H`\00\01\F0\11!\06\08\BC\02\03\80\16'\10\080\00\1B\E4\00\0A\03\E0\03\14\040\01\01\10\06!\07\08\0C\03!\00\00P\00I\12x\06\06\A0\055x\07\070\00\10\CA \00(\08\07 \00\22\84y_\13\04\10\06E\84y\09\08\10\00\10\E2\80\05\10\050\17\08\80\05\01\B0\04$\0E \80\059\09\04\09`\0D\00\C0\00D@K\0F@\A0\04%\0E\090\00\10\C6\C0\04\18\04p\05X\04#t\0E\0E0\00\09p\05\10\E4`\04\11\04p\04$\04\08\00\05%\06\0E@\00\12\C6 \00\00\80\04\22\04\00 \00@#x\06\090\00$\06\08P\00%\05\0EP\00\12\C6 \00\000\00\13\060\00T\08s\04\00\04\F0\00\01 \127\06\00\06\C0\04\00\90\0C\14\04\A0\04 \C8\1F\10\00$\05\06\10\00/\C6/\90\16\01\1C\03\90\16&@\02 \00\0E\C0\17\16\03 \04\01\00\0B%\04\03\80\179\CC\1F\00\90\1Ef\A2\00\00\06{\09\F0\03\10\E2\A0\0A4\06\FF\FFP\00'\E2/\90\02\03@\08&t\050\03&\CA\1F\E0\05\01\10\04 \8E\00\D0&\0A\00\0B4\07\00\07\10\04%$\0E\C0\0A\01\80\03\00\C0\03:z\0F\0F\00\041\06\07\0F\80\05\02\B0\05T'r\06\06\03\90\00\16\C8\C0\03#\06\0A\10\007z\03\08\10\05\010\05\050\04\17\C6@\01!`\F0p\05I\10\0A\03\03\A0\03\1D\08\A0\03\15\03\A0\03\1B\DA`\03\00\10\09)\DA\06\00\04\00 \03\14\07\B0)\1B\CAP\03\10\CA\10\034\03\06\00\00\02\10\A4\00\03\10\03p\10\07\00\03\01\7F\103?\08 \00\070t\04\04\C0\02\22\05@\10\00E!t\08\04\A0\02\02 \07\1B\030\07\18\030\07E$x\03\04\A0\02\1B\CA\F0\06\11$\80\02'\03\08\10\0F\0FP\18\01(\E2\10P\18%\03\02P\18\02\C0\00\04\F0\17C\00\00\CAO\F0\17\17@ \00\11\04\B0\09\06 \00&\05\04\10\18\10\1E\00\04\02\C0\17\01 \00\14\1F\B0\17\03\F0\17Q.\00!r\06\10\03\06 \00%\07\06\D0\17`d\00\00!\A2\07\01\10\03 \00\11/\80\17/\07\80\80\17';M9\00\C0\07\11\00\10\03$\FF\E0\C0\07H\00\00@\0Ap\174\00\00 \10\02\01\00\02\00\FD\16\08p\17\00\FC\01\010\00\03p\17%\00\00p\17\02p\12\11\0D\80\05$b\F2\E0\05\01p\00\05`\17V\19\00\00\C0\09\90\00\04p\1D\11\0D\E0\02\11op\17\05`\02\11\C6\A0\07\15\07\90\00\01@\17\11\0E\80\1D$\02\02\00\04%\10\03\10\00\01\00\08\16\04`\0F \A8\02\E0\16\18\10P\0FY\10x\09\07@\10\1E&\05\07\B0\0F\00\90\1D*\12\09`\00*\0A\05`\00\07\90\17zj\01\00\81y\0A\0A@\17\17\14`\08\0C`\0F\00p#4\0D\0D\80p\00\01\B0\099\14\00\14`\17,\15\14`\0F\19\15`\0F\02\E0+\06`\0F\01`\07\07`\04\11\12\10\17\07`\0F$\12\07\80\00\01`\04\1B\10`\0F)\0F\12\80\048z\0B\10\00\1F\040\00\15\0F0\004\11\12\090\00\02P\08\12\0BP\04\12\F6\D0\0E8z\10\10\B0\04W'r\12\12\05\D0\01\06 \08\19\F2\A0\0F$\11\0Ap\00\16\16\10\0F\00\C0\049:\0B\0B\D0\04\1D8\E0\0E\15\0B\D0\04\11\E2\C0\00\06\D0\0F\18\E2p\08\12\F6`\08\1A\1A\C0\08\06@\00*\FA\03\C0\08\13\F4P\00'\10\16\A0\11:\00\10\18\00\0F6\12z\140\05\00\A0\00\08\10\10\18\C4P\00\02\10\09R\07r\0E\14\0E@\03\11\050\00\1BZ\E0\00\17(`\00\0F\90\16\00\06\B0\00\13\F2P\004\0F\14\0FP\00\01p\0E\16\13\80\16\1B\E2`\0A\1B\E2\00\10\0F0\0F\00H\10X\11\11\80\00T\84y\15\13\00`\05\1B\E2 \01\0F@\0F\00f\84y\0B\13\00\80\80\0C\09\00\19\00\80\01\1C\18`\00%\17\0F`\00\10\E6\D0\004\11\14\11\D0\00\04`\0F\04P\00Wd\0E\00\10(P\00\000\025x\11\11\C0\00\01\10\014\12\14\12@\00\01\B0\06\16\10P\0F\11\E2@\0F\15\11\B0\00\1B\C8\E0\17\01p\00%\19\12\90\00\1Bh\80\0F\22\E8\0E\C0\17\16\80\10\01 \0Bx\C56\94\80~\00B\FC\03\00\C4\1F\10\002\00\00\E2\A0\01B\0Bx\00\0F \00\000\01#/\04\10\00A\00\00\E2\F6P\0BB h\0B\0B\866\11@P\05! \98\10\00\11K\10\00u\E2\0F\00 (\0F\0F \00\01\F0\06\10\0Ba\00\020\04\00 \008\B8\0F\0F0\00Q!r\15\04\15a\00\00 \00\11O@\09\08\00\0CV!r\17\08\17\D0\0FW\01#t\13\15\A0\0AX\08\84y\08\12\C0\01E#t\1B\17 \00\00`\0A\10t\90\114@K\040\175t\1B\1B\10\00\10\E4\D0\07%\14\13\D0\07\01p\007\19\06\190\18E!t\16\1B \00\01\B0\0A(\14\15p\17 $x\CF $\80\00\B0\00@#x\16\17 \00&\16\080\00\05`\17\01`\00%\11\0A\A0\10|\E2\8F\00$x\1B\1B@\00\000\00\13\160\00X\08s\15\00\14\B0\0A8t\17\19\00\01H#t\13\11\80\0BE#t\17\17\E0\00\02 \00\06\00\01\10\E4P\007\16\00\16\A0\0FI!t\0A\17\00\01)t\04\10\01E$x\17\17\A0\00\01\D0\00 \0A\19\E0\00\22\0A\08 \00H#x\04\11\C0\0B\00\90\12\08\D0\00R r\06\06\15\EA7\01\C0\17\00@\00\00\E0\00&\0A\00@\00\08\E0\0B\00\F0\00\15\0F0\02e\02\00 h\06\06P\02\02\C0\02\12\08\A0\02\14\F8\C0\02\15\10\D0\02\01p\005\16\1B\16p\00\14O0\002\00\00\E20\0BH \98\06\06\80\021\08s\0A;4\04@\02K (\16\16p\00\12\10@\00\13\F4`\007\0F\0B\06\D0\00\01`%\16^p\07H \B8\16\16`\00\09`\08\00 \008H\08\08`\00\09\A0\0Cxb\0E\00 h\10\10 \00F r\15\15\D0\00h\8F\00 \D8\08\08`\001\86y\000\11\CA\04\19\10\0C\00\E6\05\00 \A8\10\10\E0\00\03`\19\00P\01\84\22\07\00 r\0A\17\0AP\00u\C8\1F\00 H\0A\0Ap\00\12\C6\F0\19\15\100\00D\0E\00 rp)\02\F0\00H \D8\0A\0A`\00\01\C02\04\00\01z\E4\8F\00 h\13\13\C0\00%\11\11p\00z\C6\0F\01 \A8\13\13@\00%\0A\03@\00\01\C0\08(\02\05P\01T\86y\00\0A\15\E0\00E\E2\05\00  \15\00P\00\10\1F \00$\08\11 \00 \E8\05\10\00'\02\130\00hG\99\00\00@\F6 \12\01=\00\05\D0\099M\09\00\10\009\06{\04\80\0D\00p\0A#0\03 \00\09\A0\07\03P\06\09\80\10\16O\D0\01\00@\01\10\A4\80\0D&\02\04 \09uO\00\05s\03\00\02 \09\15\04\10#\02`\03QO\00$r\05\10\00\10\03P\08 \C8\9F\C0%\1A\050\091\09\03\050\0C\02\10\00\00\E06\15\04@\00\00\00\08\11\04\A0\0A\16\00P\01\01\A0*3\0B\02\8Ep\0E\19\02p(9'r\05\F0\1D\09P\0B\01\F0\17\11\07\90\00\13\05\D0\116$t\0A\A0\05\01p\00*\06\07P\22\1C\07\F0\0D\1A\06\F0\0D)\06\06@\088\08\05\05 \07\060\00\12\F20\00\17\18 \00\01\D0\0B\12\05 (#\00\000\07(\05\05\E0\0DE\12x\03\05 \07\84\CA\1F\00\84y\05\03\00`\03\10\E8 \07F\06\03\00\80\80\0DB\0Bx\00\06@\04\00\C0\1B#\1F\04\10\00\02\10\07W\D6\0F\00 \08\80\04\1B\C8@\04&\CC\0FP\10#\10\00\E0\051\02\02\05A\00\04P\0E\01?+\11?\15\0C\00 \0E\03P\1E\22\0A@\10\008!t\07 \1E\19\040\1D\01\D0\10 \07\02`\05#\07\08@\005x\07\02\80\1D\00\E0(\22s\02\D0\0F\03\D0\00$ r\B0+\11@ \15\01\00\02\13^\00\02\10\C6\D0\00(\05\05\A0\03\09\F0\0C\00 \005\98\05\05\E0\00\00\F0\00#r\05@)\02\D0\07\00`\03\15\05`\03\11\01\00\11/p\FD`\03\08\0B\10\00\04\F0\02\13\13p\02\81\02x\04\00\10:\00\00\F5\05\12\E2\D0'\14\01\D0\01E\C4\0F\00D\80\03\11\C0@\00\00P(\14\07p&\00\C0\17Gt\03\FF\02\C0\02\06\E0.\01\90\1DR\04\08r\13\070\04\02\90!\00p\00\14\80p\00\00\10\07$r\02 \00\14\00p\00)\C0\02p\00\12\02p\00\15\F0p\00\1F\04p\00\02\00\90\06\02P\00\01?\00\00F\16\03p\00\1B\F0p\00\03 \00\14\02p\00\1FPp\00\0C\1F\08p\00\05\01\E0\00\09p\00\01P\01#`;p\00\1F\C4p\00\03/\E0\01p\00\0B\1F\10p\00\17\01\E0\00/\D0;\E0\00\0B\17pp\00\04\D0\12\13\07`\00fGy\00\00\F0\D7@\02\09\00\02\03 \02& <\D0\00\00P\00\15 P\00\11?p\03\01\A0\00\16\00@\02\1B\02@\00\1B`@\00&\E0\00\90\00\0F@\00\01\1B\04@\00\1B\A0@\00\1F\A0@\00\0C\1B\08@\00\1B\E0@\00\19`@\00\1D\06@\00\1B\10@\00&0=@\00\04@\03\17\06 \03\17\10P\00\00P\01(`\EEP\01\00 \14\06P\00fHs\00\00\1A\00\A040\89s\07@*A\0C\0B\00\0E@\04`Py\00\04\80\C2@\00R\C3\03\00\EA\1F\A0\01\13\FFP\00f\C0\0F\00\18y\00\01\00\0F\10\00\D0\0F\01\00-\00\AB\02.\03\00\01\00\22@\00\01\00=%\01\000\00\08\01\00\1F\0B@\00\04\13e)\00\1F\F1@\00\0C\00\DC\04.\02\00\01\00\13XU\00$\C0\00\AD\03\00T\02\02G\06\00\01\00\03\948/\00\E6T\00\00\00\01\00\02\A5!\02\00E\0F\80\00\09\11)\06\00\05\EFC\05\01\00\13\F0@\00\11T\06\00\06\E4\00*\04\00\01\00/[\00@\00\03\12D\95\066\00\DC\00@\00\1F\0A@\00\00\00-'\1F\0B@\00\00\11 \B5\07\0E\C0\00\1B\08\08\00\11\F3\9C2\0E\01\00\13\F8@\00& \00\80\00\17\048\00\02\94\033\00\00\B5\14\01\0C\84\01\13\18\F5\03\00|<\04\01\00\0F\C0\00\01\132@\00\15\06R\00\03\01\00\03\95\00&\80>\80\00j\07\00\00&\80\00\01\00\13\87\94\00*\03\00\01\00\040J\01]\00\0F\80\00\08\13\06l\1D\04hJ\0C\01\00\1B\A8\08\00\17\08W\00\17\05\E8\00\0C\01\00*h@\08\00\088\00\18\06\A0\00\0F\01\00\05\00)\00\B0\00\00\00\08\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1w1r_0() {addr_space = 3 : i32} : !llvm.array<32 x f32>
    llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1w1r_1() {addr_space = 3 : i32} : !llvm.array<32 x f32>
    llvm.func @__nv_expf(f32) -> f32
    llvm.func @main_kStitch_divide__13_1_0___1w1r(%arg0: !llvm.ptr<f32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: !llvm.ptr<f32>) attributes {disc.elimargs = [0 : index, 2 : index, 3 : index, 6 : index, 7 : index, 8 : index, 10 : index, 12 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(-1 : i32) : i32
      %1 = llvm.mlir.constant(8 : index) : i32
      %2 = llvm.mlir.constant(4 : index) : i32
      %3 = llvm.mlir.constant(64 : index) : i32
      %4 = llvm.mlir.constant(96 : index) : i32
      %5 = llvm.mlir.constant(128 : index) : i32
      %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %7 = llvm.mlir.constant(1 : i32) : i32
      %8 = llvm.mlir.constant(32 : i32) : i32
      %9 = llvm.mlir.constant(2 : i32) : i32
      %10 = llvm.mlir.constant(4 : i32) : i32
      %11 = llvm.mlir.constant(8 : i32) : i32
      %12 = llvm.mlir.constant(16 : i32) : i32
      %13 = llvm.mlir.constant(-0.000000e+00 : f32) : f32
      %14 = llvm.mlir.constant(1 : index) : i32
      %15 = llvm.mlir.constant(32 : index) : i32
      %16 = llvm.mlir.constant(0 : index) : i32
      %17 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1w1r_0 : !llvm.ptr<array<32 x f32>, 3>
      %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %19 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1w1r_1 : !llvm.ptr<array<32 x f32>, 3>
      %20 = llvm.getelementptr %19[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %21 = nvvm.read.ptx.sreg.ctaid.x : i32
      %22 = nvvm.read.ptx.sreg.tid.x : i32
      %23 = llvm.udiv %22, %15  : i32
      %24 = llvm.urem %22, %15  : i32
      %25 = llvm.mul %21, %1  : i32
      %26 = llvm.add %25, %23  : i32
      %27 = llvm.icmp "slt" %26, %arg3 : i32
      llvm.cond_br %27, ^bb2, ^bb11
    ^bb2:  // pred: ^bb1
      %28 = llvm.sub %arg2, %24  : i32
      %29 = llvm.icmp "eq" %28, %16 : i32
      %30 = llvm.sub %28, %14  : i32
      %31 = llvm.udiv %30, %15  : i32
      %32 = llvm.add %31, %14  : i32
      %33 = llvm.select %29, %16, %32 : i1, i32
      %34 = llvm.srem %33, %2  : i32
      %35 = llvm.sub %33, %34  : i32
      %36 = llvm.mul %35, %15  : i32
      %37 = llvm.add %24, %36  : i32
      %38 = llvm.mul %26, %arg2  : i32
      llvm.br ^bb3(%24, %6 : i32, f32)
    ^bb3(%39: i32, %40: f32):  // 2 preds: ^bb2, ^bb4
      %41 = llvm.icmp "slt" %39, %37 : i32
      llvm.cond_br %41, ^bb4, ^bb5
    ^bb4:  // pred: ^bb3
      %42 = llvm.add %39, %15  : i32
      %43 = llvm.add %39, %3  : i32
      %44 = llvm.add %39, %4  : i32
      %45 = llvm.add %38, %39  : i32
      %46 = llvm.add %38, %42  : i32
      %47 = llvm.add %38, %43  : i32
      %48 = llvm.add %38, %44  : i32
      %49 = llvm.getelementptr %arg0[%45] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %50 = llvm.load %49 : !llvm.ptr<f32>
      %51 = llvm.getelementptr %arg0[%46] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %52 = llvm.load %51 : !llvm.ptr<f32>
      %53 = llvm.getelementptr %arg0[%47] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %54 = llvm.load %53 : !llvm.ptr<f32>
      %55 = llvm.getelementptr %arg0[%48] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %56 = llvm.load %55 : !llvm.ptr<f32>
      %57 = llvm.fcmp "ugt" %40, %50 : f32
      %58 = llvm.select %57, %40, %50 : i1, f32
      %59 = llvm.fcmp "uno" %50, %50 : f32
      %60 = llvm.select %59, %50, %58 : i1, f32
      %61 = llvm.fcmp "ugt" %60, %52 : f32
      %62 = llvm.select %61, %60, %52 : i1, f32
      %63 = llvm.fcmp "uno" %52, %52 : f32
      %64 = llvm.select %63, %52, %62 : i1, f32
      %65 = llvm.fcmp "ugt" %64, %54 : f32
      %66 = llvm.select %65, %64, %54 : i1, f32
      %67 = llvm.fcmp "uno" %54, %54 : f32
      %68 = llvm.select %67, %54, %66 : i1, f32
      %69 = llvm.fcmp "ugt" %68, %56 : f32
      %70 = llvm.select %69, %68, %56 : i1, f32
      %71 = llvm.fcmp "uno" %56, %56 : f32
      %72 = llvm.select %71, %56, %70 : i1, f32
      %73 = llvm.add %39, %5  : i32
      llvm.br ^bb3(%73, %72 : i32, f32)
    ^bb5:  // pred: ^bb3
      llvm.br ^bb6(%37, %40 : i32, f32)
    ^bb6(%74: i32, %75: f32):  // 2 preds: ^bb5, ^bb7
      %76 = llvm.icmp "slt" %74, %arg2 : i32
      llvm.cond_br %76, ^bb7, ^bb8
    ^bb7:  // pred: ^bb6
      %77 = llvm.add %38, %74  : i32
      %78 = llvm.getelementptr %arg0[%77] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %79 = llvm.load %78 : !llvm.ptr<f32>
      %80 = llvm.fcmp "ugt" %75, %79 : f32
      %81 = llvm.select %80, %75, %79 : i1, f32
      %82 = llvm.fcmp "uno" %79, %79 : f32
      %83 = llvm.select %82, %79, %81 : i1, f32
      %84 = llvm.add %74, %15  : i32
      llvm.br ^bb6(%84, %83 : i32, f32)
    ^bb8:  // pred: ^bb6
      %85 = llvm.sub %8, %8  : i32
      %86 = llvm.lshr %0, %85  : i32
      %87 = llvm.sub %8, %7  : i32
      %88 = nvvm.shfl.sync  bfly %86, %75, %7, %87 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %89 = llvm.extractvalue %88[0] : !llvm.struct<(f32, i1)> 
      %90 = llvm.fcmp "ugt" %75, %89 : f32
      %91 = llvm.select %90, %75, %89 : i1, f32
      %92 = llvm.fcmp "uno" %89, %89 : f32
      %93 = llvm.select %92, %89, %91 : i1, f32
      %94 = llvm.sub %8, %8  : i32
      %95 = llvm.lshr %0, %94  : i32
      %96 = llvm.sub %8, %7  : i32
      %97 = nvvm.shfl.sync  bfly %95, %93, %9, %96 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %98 = llvm.extractvalue %97[0] : !llvm.struct<(f32, i1)> 
      %99 = llvm.fcmp "ugt" %93, %98 : f32
      %100 = llvm.select %99, %93, %98 : i1, f32
      %101 = llvm.fcmp "uno" %98, %98 : f32
      %102 = llvm.select %101, %98, %100 : i1, f32
      %103 = llvm.sub %8, %8  : i32
      %104 = llvm.lshr %0, %103  : i32
      %105 = llvm.sub %8, %7  : i32
      %106 = nvvm.shfl.sync  bfly %104, %102, %10, %105 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %107 = llvm.extractvalue %106[0] : !llvm.struct<(f32, i1)> 
      %108 = llvm.fcmp "ugt" %102, %107 : f32
      %109 = llvm.select %108, %102, %107 : i1, f32
      %110 = llvm.fcmp "uno" %107, %107 : f32
      %111 = llvm.select %110, %107, %109 : i1, f32
      %112 = llvm.sub %8, %8  : i32
      %113 = llvm.lshr %0, %112  : i32
      %114 = llvm.sub %8, %7  : i32
      %115 = nvvm.shfl.sync  bfly %113, %111, %11, %114 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %116 = llvm.extractvalue %115[0] : !llvm.struct<(f32, i1)> 
      %117 = llvm.fcmp "ugt" %111, %116 : f32
      %118 = llvm.select %117, %111, %116 : i1, f32
      %119 = llvm.fcmp "uno" %116, %116 : f32
      %120 = llvm.select %119, %116, %118 : i1, f32
      %121 = llvm.sub %8, %8  : i32
      %122 = llvm.lshr %0, %121  : i32
      %123 = llvm.sub %8, %7  : i32
      %124 = nvvm.shfl.sync  bfly %122, %120, %12, %123 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %125 = llvm.extractvalue %124[0] : !llvm.struct<(f32, i1)> 
      %126 = llvm.fcmp "ugt" %120, %125 : f32
      %127 = llvm.select %126, %120, %125 : i1, f32
      %128 = llvm.fcmp "uno" %125, %125 : f32
      %129 = llvm.select %128, %125, %127 : i1, f32
      %130 = llvm.icmp "eq" %24, %16 : i32
      llvm.cond_br %130, ^bb9, ^bb10
    ^bb9:  // pred: ^bb8
      %131 = llvm.getelementptr %18[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %129, %131 : !llvm.ptr<f32, 3>
      llvm.br ^bb10
    ^bb10:  // 2 preds: ^bb8, ^bb9
      llvm.br ^bb11
    ^bb11:  // 2 preds: ^bb1, ^bb10
      nvvm.barrier0
      llvm.cond_br %27, ^bb12, ^bb21
    ^bb12:  // pred: ^bb11
      %132 = llvm.sub %arg2, %24  : i32
      %133 = llvm.icmp "eq" %132, %16 : i32
      %134 = llvm.sub %132, %14  : i32
      %135 = llvm.udiv %134, %15  : i32
      %136 = llvm.add %135, %14  : i32
      %137 = llvm.select %133, %16, %136 : i1, i32
      %138 = llvm.srem %137, %2  : i32
      %139 = llvm.sub %137, %138  : i32
      %140 = llvm.mul %139, %15  : i32
      %141 = llvm.add %24, %140  : i32
      %142 = llvm.mul %26, %arg2  : i32
      llvm.br ^bb13(%24, %13 : i32, f32)
    ^bb13(%143: i32, %144: f32):  // 2 preds: ^bb12, ^bb14
      %145 = llvm.icmp "slt" %143, %141 : i32
      llvm.cond_br %145, ^bb14, ^bb15
    ^bb14:  // pred: ^bb13
      %146 = llvm.add %143, %15  : i32
      %147 = llvm.add %143, %3  : i32
      %148 = llvm.add %143, %4  : i32
      %149 = llvm.add %142, %143  : i32
      %150 = llvm.add %142, %146  : i32
      %151 = llvm.add %142, %147  : i32
      %152 = llvm.add %142, %148  : i32
      %153 = llvm.udiv %149, %arg2  : i32
      %154 = llvm.urem %153, %arg1  : i32
      %155 = llvm.udiv %153, %arg1  : i32
      %156 = llvm.udiv %150, %arg2  : i32
      %157 = llvm.urem %156, %arg1  : i32
      %158 = llvm.udiv %156, %arg1  : i32
      %159 = llvm.udiv %151, %arg2  : i32
      %160 = llvm.urem %159, %arg1  : i32
      %161 = llvm.udiv %159, %arg1  : i32
      %162 = llvm.udiv %152, %arg2  : i32
      %163 = llvm.urem %162, %arg1  : i32
      %164 = llvm.udiv %162, %arg1  : i32
      %165 = llvm.getelementptr %arg0[%149] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %166 = llvm.load %165 : !llvm.ptr<f32>
      %167 = llvm.getelementptr %arg0[%150] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %168 = llvm.load %167 : !llvm.ptr<f32>
      %169 = llvm.getelementptr %arg0[%151] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %170 = llvm.load %169 : !llvm.ptr<f32>
      %171 = llvm.getelementptr %arg0[%152] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %172 = llvm.load %171 : !llvm.ptr<f32>
      %173 = llvm.mul %155, %arg1  : i32
      %174 = llvm.add %173, %154  : i32
      %175 = llvm.mul %158, %arg1  : i32
      %176 = llvm.add %175, %157  : i32
      %177 = llvm.mul %161, %arg1  : i32
      %178 = llvm.add %177, %160  : i32
      %179 = llvm.mul %164, %arg1  : i32
      %180 = llvm.add %179, %163  : i32
      %181 = llvm.urem %174, %1  : i32
      %182 = llvm.urem %176, %1  : i32
      %183 = llvm.urem %178, %1  : i32
      %184 = llvm.urem %180, %1  : i32
      %185 = llvm.getelementptr %18[%181] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %186 = llvm.load %185 : !llvm.ptr<f32, 3>
      %187 = llvm.getelementptr %18[%182] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %188 = llvm.load %187 : !llvm.ptr<f32, 3>
      %189 = llvm.getelementptr %18[%183] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %190 = llvm.load %189 : !llvm.ptr<f32, 3>
      %191 = llvm.getelementptr %18[%184] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %192 = llvm.load %191 : !llvm.ptr<f32, 3>
      %193 = llvm.fsub %166, %186  : f32
      %194 = llvm.fsub %168, %188  : f32
      %195 = llvm.fsub %170, %190  : f32
      %196 = llvm.fsub %172, %192  : f32
      %197 = llvm.call @__nv_expf(%193) : (f32) -> f32
      %198 = llvm.call @__nv_expf(%194) : (f32) -> f32
      %199 = llvm.call @__nv_expf(%195) : (f32) -> f32
      %200 = llvm.call @__nv_expf(%196) : (f32) -> f32
      %201 = llvm.fadd %144, %197  : f32
      %202 = llvm.fadd %201, %198  : f32
      %203 = llvm.fadd %202, %199  : f32
      %204 = llvm.fadd %203, %200  : f32
      %205 = llvm.add %143, %5  : i32
      llvm.br ^bb13(%205, %204 : i32, f32)
    ^bb15:  // pred: ^bb13
      llvm.br ^bb16(%141, %144 : i32, f32)
    ^bb16(%206: i32, %207: f32):  // 2 preds: ^bb15, ^bb17
      %208 = llvm.icmp "slt" %206, %arg2 : i32
      llvm.cond_br %208, ^bb17, ^bb18
    ^bb17:  // pred: ^bb16
      %209 = llvm.add %142, %206  : i32
      %210 = llvm.udiv %209, %arg2  : i32
      %211 = llvm.urem %210, %arg1  : i32
      %212 = llvm.udiv %210, %arg1  : i32
      %213 = llvm.getelementptr %arg0[%209] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %214 = llvm.load %213 : !llvm.ptr<f32>
      %215 = llvm.mul %212, %arg1  : i32
      %216 = llvm.add %215, %211  : i32
      %217 = llvm.urem %216, %1  : i32
      %218 = llvm.getelementptr %18[%217] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %219 = llvm.load %218 : !llvm.ptr<f32, 3>
      %220 = llvm.fsub %214, %219  : f32
      %221 = llvm.call @__nv_expf(%220) : (f32) -> f32
      %222 = llvm.fadd %207, %221  : f32
      %223 = llvm.add %206, %15  : i32
      llvm.br ^bb16(%223, %222 : i32, f32)
    ^bb18:  // pred: ^bb16
      %224 = llvm.sub %8, %8  : i32
      %225 = llvm.lshr %0, %224  : i32
      %226 = llvm.sub %8, %7  : i32
      %227 = nvvm.shfl.sync  bfly %225, %207, %7, %226 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %228 = llvm.extractvalue %227[0] : !llvm.struct<(f32, i1)> 
      %229 = llvm.fadd %207, %228  : f32
      %230 = llvm.sub %8, %8  : i32
      %231 = llvm.lshr %0, %230  : i32
      %232 = llvm.sub %8, %7  : i32
      %233 = nvvm.shfl.sync  bfly %231, %229, %9, %232 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %234 = llvm.extractvalue %233[0] : !llvm.struct<(f32, i1)> 
      %235 = llvm.fadd %229, %234  : f32
      %236 = llvm.sub %8, %8  : i32
      %237 = llvm.lshr %0, %236  : i32
      %238 = llvm.sub %8, %7  : i32
      %239 = nvvm.shfl.sync  bfly %237, %235, %10, %238 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %240 = llvm.extractvalue %239[0] : !llvm.struct<(f32, i1)> 
      %241 = llvm.fadd %235, %240  : f32
      %242 = llvm.sub %8, %8  : i32
      %243 = llvm.lshr %0, %242  : i32
      %244 = llvm.sub %8, %7  : i32
      %245 = nvvm.shfl.sync  bfly %243, %241, %11, %244 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %246 = llvm.extractvalue %245[0] : !llvm.struct<(f32, i1)> 
      %247 = llvm.fadd %241, %246  : f32
      %248 = llvm.sub %8, %8  : i32
      %249 = llvm.lshr %0, %248  : i32
      %250 = llvm.sub %8, %7  : i32
      %251 = nvvm.shfl.sync  bfly %249, %247, %12, %250 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %252 = llvm.extractvalue %251[0] : !llvm.struct<(f32, i1)> 
      %253 = llvm.fadd %247, %252  : f32
      %254 = llvm.icmp "eq" %24, %16 : i32
      llvm.cond_br %254, ^bb19, ^bb20
    ^bb19:  // pred: ^bb18
      %255 = llvm.getelementptr %20[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %253, %255 : !llvm.ptr<f32, 3>
      llvm.br ^bb20
    ^bb20:  // 2 preds: ^bb18, ^bb19
      llvm.br ^bb21
    ^bb21:  // 2 preds: ^bb11, ^bb20
      nvvm.barrier0
      llvm.cond_br %27, ^bb22, ^bb29
    ^bb22:  // pred: ^bb21
      %256 = llvm.sub %arg2, %24  : i32
      %257 = llvm.icmp "eq" %256, %16 : i32
      %258 = llvm.sub %256, %14  : i32
      %259 = llvm.udiv %258, %15  : i32
      %260 = llvm.add %259, %14  : i32
      %261 = llvm.select %257, %16, %260 : i1, i32
      %262 = llvm.srem %261, %2  : i32
      %263 = llvm.sub %261, %262  : i32
      %264 = llvm.mul %263, %15  : i32
      %265 = llvm.add %24, %264  : i32
      %266 = llvm.mul %26, %arg2  : i32
      llvm.br ^bb23(%24 : i32)
    ^bb23(%267: i32):  // 2 preds: ^bb22, ^bb24
      %268 = llvm.icmp "slt" %267, %265 : i32
      llvm.cond_br %268, ^bb24, ^bb25
    ^bb24:  // pred: ^bb23
      %269 = llvm.add %267, %15  : i32
      %270 = llvm.add %267, %3  : i32
      %271 = llvm.add %267, %4  : i32
      %272 = llvm.add %266, %267  : i32
      %273 = llvm.add %266, %269  : i32
      %274 = llvm.add %266, %270  : i32
      %275 = llvm.add %266, %271  : i32
      %276 = llvm.udiv %272, %arg2  : i32
      %277 = llvm.urem %276, %arg1  : i32
      %278 = llvm.udiv %276, %arg1  : i32
      %279 = llvm.udiv %273, %arg2  : i32
      %280 = llvm.urem %279, %arg1  : i32
      %281 = llvm.udiv %279, %arg1  : i32
      %282 = llvm.udiv %274, %arg2  : i32
      %283 = llvm.urem %282, %arg1  : i32
      %284 = llvm.udiv %282, %arg1  : i32
      %285 = llvm.udiv %275, %arg2  : i32
      %286 = llvm.urem %285, %arg1  : i32
      %287 = llvm.udiv %285, %arg1  : i32
      %288 = llvm.getelementptr %arg0[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %289 = llvm.load %288 : !llvm.ptr<f32>
      %290 = llvm.getelementptr %arg0[%273] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %291 = llvm.load %290 : !llvm.ptr<f32>
      %292 = llvm.getelementptr %arg0[%274] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %293 = llvm.load %292 : !llvm.ptr<f32>
      %294 = llvm.getelementptr %arg0[%275] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %295 = llvm.load %294 : !llvm.ptr<f32>
      %296 = llvm.mul %278, %arg1  : i32
      %297 = llvm.add %296, %277  : i32
      %298 = llvm.mul %281, %arg1  : i32
      %299 = llvm.add %298, %280  : i32
      %300 = llvm.mul %284, %arg1  : i32
      %301 = llvm.add %300, %283  : i32
      %302 = llvm.mul %287, %arg1  : i32
      %303 = llvm.add %302, %286  : i32
      %304 = llvm.urem %297, %1  : i32
      %305 = llvm.urem %299, %1  : i32
      %306 = llvm.urem %301, %1  : i32
      %307 = llvm.urem %303, %1  : i32
      %308 = llvm.getelementptr %18[%304] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %309 = llvm.load %308 : !llvm.ptr<f32, 3>
      %310 = llvm.getelementptr %18[%305] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %311 = llvm.load %310 : !llvm.ptr<f32, 3>
      %312 = llvm.getelementptr %18[%306] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %313 = llvm.load %312 : !llvm.ptr<f32, 3>
      %314 = llvm.getelementptr %18[%307] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %315 = llvm.load %314 : !llvm.ptr<f32, 3>
      %316 = llvm.fsub %289, %309  : f32
      %317 = llvm.fsub %291, %311  : f32
      %318 = llvm.fsub %293, %313  : f32
      %319 = llvm.fsub %295, %315  : f32
      %320 = llvm.call @__nv_expf(%316) : (f32) -> f32
      %321 = llvm.call @__nv_expf(%317) : (f32) -> f32
      %322 = llvm.call @__nv_expf(%318) : (f32) -> f32
      %323 = llvm.call @__nv_expf(%319) : (f32) -> f32
      %324 = llvm.getelementptr %20[%304] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %325 = llvm.load %324 : !llvm.ptr<f32, 3>
      %326 = llvm.getelementptr %20[%305] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %327 = llvm.load %326 : !llvm.ptr<f32, 3>
      %328 = llvm.getelementptr %20[%306] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %329 = llvm.load %328 : !llvm.ptr<f32, 3>
      %330 = llvm.getelementptr %20[%307] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %331 = llvm.load %330 : !llvm.ptr<f32, 3>
      %332 = llvm.fdiv %320, %325  : f32
      %333 = llvm.fdiv %321, %327  : f32
      %334 = llvm.fdiv %322, %329  : f32
      %335 = llvm.fdiv %323, %331  : f32
      %336 = llvm.getelementptr %arg4[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %332, %336 : !llvm.ptr<f32>
      %337 = llvm.getelementptr %arg4[%273] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %333, %337 : !llvm.ptr<f32>
      %338 = llvm.getelementptr %arg4[%274] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %334, %338 : !llvm.ptr<f32>
      %339 = llvm.getelementptr %arg4[%275] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %335, %339 : !llvm.ptr<f32>
      %340 = llvm.add %267, %5  : i32
      llvm.br ^bb23(%340 : i32)
    ^bb25:  // pred: ^bb23
      llvm.br ^bb26(%265 : i32)
    ^bb26(%341: i32):  // 2 preds: ^bb25, ^bb27
      %342 = llvm.icmp "slt" %341, %arg2 : i32
      llvm.cond_br %342, ^bb27, ^bb28
    ^bb27:  // pred: ^bb26
      %343 = llvm.add %266, %341  : i32
      %344 = llvm.udiv %343, %arg2  : i32
      %345 = llvm.urem %344, %arg1  : i32
      %346 = llvm.udiv %344, %arg1  : i32
      %347 = llvm.getelementptr %arg0[%343] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %348 = llvm.load %347 : !llvm.ptr<f32>
      %349 = llvm.mul %346, %arg1  : i32
      %350 = llvm.add %349, %345  : i32
      %351 = llvm.urem %350, %1  : i32
      %352 = llvm.getelementptr %18[%351] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %353 = llvm.load %352 : !llvm.ptr<f32, 3>
      %354 = llvm.fsub %348, %353  : f32
      %355 = llvm.call @__nv_expf(%354) : (f32) -> f32
      %356 = llvm.getelementptr %20[%351] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %357 = llvm.load %356 : !llvm.ptr<f32, 3>
      %358 = llvm.fdiv %355, %357  : f32
      %359 = llvm.getelementptr %arg4[%343] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %358, %359 : !llvm.ptr<f32>
      %360 = llvm.add %341, %15  : i32
      llvm.br ^bb26(%360 : i32)
    ^bb28:  // pred: ^bb26
      llvm.br ^bb29
    ^bb29:  // 2 preds: ^bb21, ^bb28
      llvm.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c8 = arith.constant 8 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c1024 = arith.constant 1024 : index
    %c6 = arith.constant 6 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
    %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
    %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
    %4 = arith.index_cast %2 : index to i32
    %5 = arith.index_cast %1 : index to i32
    %6 = arith.muli %4, %5 : i32
    %7 = arith.index_cast %6 : i32 to index
    %8 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    %9 = arith.cmpi slt, %7, %c3 : index
    %10 = arith.cmpi sge, %3, %c1024 : index
    %11 = arith.cmpi slt, %7, %c6 : index
    %12 = arith.andi %11, %10 : i1
    %13 = arith.cmpi sge, %3, %c512 : index
    %14 = arith.cmpi sge, %7, %c6 : index
    %15 = arith.andi %14, %13 : i1
    %16 = arith.ori %9, %12 : i1
    %17 = arith.ori %16, %15 : i1
    cf.cond_br %17, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    gpu.launch_func  @main_kernel::@main_kStitch_divide__13_1_0___1b1r blocks in (%7, %c1, %c1) threads in (%c256, %c1, %c1) args(%0 : memref<?x?x?xf32, "gpu">, %8 : memref<?x?x?xf32, "gpu">)
    cf.br ^bb3
  ^bb2:  // pred: ^bb0
    %18 = arith.cmpi eq, %7, %c0 : index
    %19 = arith.subi %7, %c1 : index
    %20 = arith.divui %19, %c8 : index
    %21 = arith.addi %20, %c1 : index
    %22 = arith.select %18, %c0, %21 : index
    gpu.launch_func  @main_kernel_0::@main_kStitch_divide__13_1_0___1w1r blocks in (%22, %c1, %c1) threads in (%c256, %c1, %c1) args(%0 : memref<?x?x?xf32, "gpu">, %7 : index, %8 : memref<?x?x?xf32, "gpu">)
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    "disc_ral.dispatch"(%arg0, %c0, %8) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\00\E8\19\00\00\00\00\00\00\02\00\01\01@\00\00\00\A8\19\00\00\00\00\00\00\A2\19\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8:\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\002@:\00\01\00\117\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0D\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F5)_shndx\00.nv.info\00.text.main_kStitch_divide__13_1_0___1b1r2\00\0F,\00\15oshared.\00\15\9Fconstant21\00\1D\1F01\00\12\FD\01debug_frame\00.rel\11\00\1Aa\12\00!nv&\00oactionV\01 \0F\94\00\0F\0Fy\01\A2a__ocg_3\00/\00$0\00\0F\FF\16$__cuda_sm70_shflsync_bfly_p\00$____wg_G\00\0F@_0_$\0D\00/265\00\19_1__281\00\19\122f\00/305\00\19_3__32\91\02\1Fo_param\98\02.\0F\01\00\06\8CU\00\00\00\03\00\0B\00\01\00\11\AA\18\00,\0C\00\01\00\11\D8\18\00,\09\00\01\00\A1\15\01\00\00\22\00\0B\00`,\0F\001\00 \01\07\000\00!\020\00,\0A\00\01\00\11Y\18\00,\04\00\01\00\11\89\18\00,\07\00\01\00f2\00\00\00\12\10\A8\00!\80-\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13P\C0\00\10\04\9B\00R\044\00\00\00E\005\04\D4\0A<\00&8\00H\00\04p\00L\88\80\80(o\00\10\08\15\00x\16\89\80\80(\09\05J\01\004\00\22\18\00\01\00\22p\00\01\00\048\01\13\108\01\90\04/\08\00\08\00\00\00$\0C\00%#\08*\01'\04\12\0C\00\17\11\0C\00\12#0\00\04$\00\17\08$\00\05\0C\00!7\04$\06\90\015\00\00\04\0A\08\00\05x\00\A3\01\18\00\03\19\18\00\04\17\0CH\06U\10\00\00\F0!\10\00\10\01>\00%\F0\11\10\00\01\01\00\F09\F0!\00\03\1B\FF\00\04(X\00 \12\00\00p\12\00\00\B0\12\00\00\00\13\00\00`\13\00\00\90\14\00\00\D0\14\00\00\10\15\00\000!\00\00`!\00\00\80!\00\00\A0!\00\00\C0!\00\00P\22\00\00p\22\00\00\90\08\00\F4\16*\00\00\E0*\00\00`+\00\00\B0+\00\00\F0+\00\000,\00\00\04\1C\08\00@(\00\00@*\00\00\04\1E\E6\00C\00\00K\00\01\00\94\02\02\08\10\0A/\22\00\00\DF\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\E0\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00@\00\00\00\14\FB\01\22\C8\00\01\00\13\02\D1\00*H\00\10\00/\04\00\01\00\FF|@$v\01\FF\D7\04\C1\FF\00\8E\07\00\C4\0F\00\19y\02\00\01\00\F1\00!\00\00\00\22\0E\00\B9z\04\00\00F\00\00\FB\04\90\E2\0F\00Ey\00\00\10\0D!\00 \80\03\10\00\81$t\0C\FF\00\00\80\FF@\00\F3\05\E2\0F\00\10z\00\02\00Z\00\00\FF\E1\FF\07\00\E4\1F\04\0C\10\00\A0pR\F0\03\00\E4\0F\00\10xl\00\F0\07\00\00\FF\E0\FF\07\00\C8\0F\00\12x\03\00\00\FC\FF\FF\FF\C0\8E\07 \002\19y\00\01\00\10%\9D\00\83\0E\00\07r\03\03\FF\00\01\00\000\000r\03\03\A2\001\FF\FC\8E@\00P\0Cr\00\02\03\80\04\D3b\F2\03\00\DA\0F\00G\19\00\00`\0C\A0\00a\EA\0F\00\10r\04 \00\01\90\00\01\C0\00F\01\00\F0\06\C0\001\1Cx\00\01\00?p\F0\F0\D0\00\02@\0Cx\00\04?\062pB\F4 \00Ar\0E\FF\FF\D0\02\C6\8E\07\00\D8\0F\00G\A9\00\00\90\06p\00\05P\00\14\E1\F0\00a\0D\03\00\F4\FF\FF\F0\00b\C4\0F\00\02x\10\04\03\12\0FP\01@$z\0B\00 \01 \0E\02\D0\00\B0\1F\00%v\04\0B\00X\00\00\10\10\00a\CA\0F\00\81y\0F\C3\05\E3\00\19\1E\0C\00\A2\00\00\10x\07\0B\00\01@\01\93\E4\0F\04\10x\09\0B\00\02\10\00z\C6\0F\00%v\06\07@\00%\12\06@\00u\E2\02\00%v\08\09 \00\95\E2\0F\08\10x\0B\0B\00\03P\00v\00\10x\13\0E\00\04\10\00E\81y\11\08@\00fb\09\00%v\0A\A0\00\10\C8\C0\00\11\15\C0\00\12\13P\00U\00\81y\14\0A0\00\10f0\00&\04\15p\00e\1F\04\10x\17\15\C0\00\10\C80\00\16\13\E0\00 b\01\C0\00\16\170\00\88/\00\10x\1D\15\00\020\00\16\16\D0\00zb\03\00%v\1C\1D\D0\00*\1F\15\D0\00E\09\0E\00\08\10\00Z\01\81y\15\1C\D0\00%\1E\1F@\00\01\D0\00\11\19\D0\00\15\09\D0\00%\18\1E0\00\10f`\01' \19p\00\00\80\01\1A\19\D0\00&\17 `\00\1C\0Fp\01Z\00\10x\0B\19\D0\00\1F\1A`\01\07\01\D0\00&\05\19\D0\00\10\1F\00\024\0E\00\0C\10\00j/\00\81y\19\0A@\01%\04\05@\00\01\D0\00\11\1B\D0\00\15\07\D0\00\16\1Cp\01z&\03\01%v\06\1B\D0\00%\1D\1B\D0\00\01@\02\16\07p\01\11\22\D0\00\1A\1D\D0\00H\1D\1B\00\020\00*\08\080\00\17\0A0\00\00\C0\00H\1B\1B\00\030\00*\0B\0A0\00(\04\1B\00\02Q\0Br\00\0F\0F_\03\A1\F4\03\00\E4O\08\0Br\00\0C\10\00\B2\C0\F6\03\00\D6\0F\00\08\A2\0F\0F\D2\07\22\80\05\B0\02\16\0C\E0\00 \A2\0E@\00%\12\12@\00\10\8F@\00!\11\11\10\00\10\F6\A0\03\10\02`\00\01 \00P\C0\F8\03\00\D2P\00!\12\12`\000\00\00\06P\00U\0Br\00\14\14@\00\01\10\00$\12\110\00q\C8\0F\00\08\B2\11\11@\00\060\00&\13\13`\00\00\10\00(\11\140\001\A2\14\14@\00\060\00*\16\16`\00)\14\13`\00!\13\13@\00\060\00*\15\15`\00)\13\16`\00!\16\16@\00\060\00*\18\18`\00)\16\15`\00!\15\15@\00\060\00*\17\17`\00)\15\18`\00!\18\18@\00\060\00*\1A\1A`\00)\18\17`\00!\17\17@\00\060\00*\19\19`\00)\17\1A`\00!\1A\1A@\00\060\00&\1C\1C`\00\10\01p\00\19\19`\00!\19\19@\00\060\00*\07\07`\00)\19\1C`\00!\1C\1C@\00\060\00*\08\08\C0\00)\1C\07`\00!\07\07@\00\060\00*\0B\0B`\00)\07\08`\00!\08\08@\00\030\00f\10x\0E\0E\00\10\90\04\00p\00\19\0B`\00!\0B\0B@\00\030\00Q\0Cr\00\0E\0D\D0\06\14\F8`\00*\0C\0C\00\03)\0B\0C\00\03!\0C\0CP\00\00\00\03\90\E2\0F\00G\C9\00\00\90\F9\9A\0B\11\83\90\06#Ay\89\0B\03\A0\06@$x\04\03\12\005\0E\0A\8E\10\07#p\03 \00\13\E8\F0\06\13\04\F0\06\12\DA\E0\06'@\03@\00St\14\FF\04\000\08\10\E4`\04\1C\0A\C0\06\10\0A\C0\033\14\02\8E\E0\03\1B\0D\C0\06%\06\0A`\04\01\C0\06+\08\0A\C0\06\1A\06@\00\1C\10\C0\06\15\08 \00\01 \05&\0A\0A \05\00p\01\1B\11\C0\06\1C\0F`\05\06\A0\00\01 \05\11\11\C0\00\15\11 \05\16\12\90\04\10f`\06&\04\11p\00\00\C0\06*\13\11 \05*\05\04\F0\05&\06\130\00!/\000\00\09 \05+\06\06 \06\070\00j\0F\01\10x\11\11 \05\1C\09P\05\07\90\00\01 \02\12\0D\8F\09\15\F0 \05\01\10\00\22\C0\F4 \021\82\0D\0D0\02$\00\00 \05\1B\0A \05%\10\10@\00*\8F\08p\05\01 \05$\0D\10p\02\00 \05\22\82\10\FF\09\03p\05\06`\05#\F0\03\10\00$\10\0F0\00\12\C8\A0\05\1A\100\00&\05\05`\00\16\00\80\05\030\00\14\82\80\05\050\00*\06\06`\00)\12\05`\00:\05\05\120\00*\09\090\04)\05\06`\004\06\06\050\00\11\C80\00\11\060\00#\C0\F0P\00!\09\090\00\00\AB\0C\1F\E4\D0\09\01\18\0EP\08\00@\00%\0C\0C\10\04\01\80\01(\09\0C\80\011\B2\0C\0C`\00\00\80\01\1E\C4\90\03\00\F0\03\00_\03Ap\16p\00p\03V\89\00\00\A0\01 \000\02x\0B\CF\02\070\0A\0Fp\03\03\15\0B\C0\07\07`\03\0C\A0\02%\A2\0E`\03\030\00\19\00\80\03\0C\A0\02%\E2\0Ep\03\060\00\07p\03\0F\A0\02\03\06\90\00\10\CC \00(\0A\0A\F0\02\06\00\02\06\B0\02\05\D0\01\09\F0\01\BF\F4\03\00\D2\8F\00\08\82\05\05\0C\E0\01\02\1F\F0\E0\01\03\17\A2\E0\01\020\00*\0A\0A\D0\02(\06\090\00\14\82\F0\01\05\10\02)\09\0A\10\02!\0A\0A0\00\00\10\02\10\CA\00\0C\11\0C\00\0C\13\0A0\0D\14A\B0\0C\04\D0\010z\00\03\A0\01Dpb\F4\030\0D#\B0\04 \00\10\E2\F0\0C0\05\02\1F@\05\00\F0\0C\96\D6\0F\00G)\00\00\80\04@\001$r\07`\00'\03\00\A0\05\16\02@\00\09\B0\0C\10\C6p\0D%\04\07p\0D\13\C8\C0\05\10\03\C0\05\12\F6\C0\05V\B9\00\00 \02`\00\10\02\D0\0D\13ZP\02\0F\E0\02\02F\04\04\00\FD\B0\0C;$t\06\F0\05\16\08P\0A%\C8\1F \023\06\02\8E\10\02*\0D\08\F0\05%\0B\07\90\02\01\F0\05(\0F\07\F0\05\04\00\0D\010\02a\E4\0F\04$z\0F\10\00\12\0F\10\00\05\10\0B\13\06\B0\05E%v\0E\0F\10\00\1B\E4`\02 \E2\0E\00\066\07\00\03`\00E\81y\0F\0E \00\16$\C0\00\1F\11\C0\00\01\09P\05\05\D0\03$\0C\0D\B0\07\01\D0\03\05P\05\12\06\80\00+\0C\08P\05\03p\02\14\F8P\05\12\0DP\02\11\FA`\01\06`\05\01\F0\02U\01\10x\07\07\B0\06q\CE\0F\00\08\C2\0A\0Ap\00Q\00\80\06\00\C80\00$\0A\0F@\00\00\90\02D\B2\0F\0F\0A \00\020\08!\07\040\08*\FA\03\90\04\14\F8\C0\00$\0F\0C\F0\02\00\C0\00\22\C2\0C@\0B\040\08_\D9\00\00\10\FE0\08\07\09`\02\03\90\02\140\A0\04\13\E6p\02\1A\01p\02(\00\010\08\1F\0A@\02\12\1F\0A@\02\02\1A\0E@\02\1B\CA0\02\16\C8 \02\1D\0A\C0\01\1B\F0\C0\01\1B\F6\10\07/\80\05\10\07\01\0E\10\03\16\07\B0\02\09`\01\05 \02)\0D\0C \02\12\0Cp\00\00 \02\0F\F0\05\01!z\00P\01\08\F0\05(\80\00\F0\05\1B\09\F0\05\1B\060\01\11\06\C0\05\15\090\01\16\09\B0\05+\A4\0E \05\02\00\01\19\09\00\014\09\09\0C\00\01\07\D0\04\1F\09\D0\04\04\F1\00\89\7F\07\0C\00\1F \0C\00\00\0E\00\00b\0E\D0\04&`\03\90\04\06p\0B\00p\00\11/p\00\19\07p\00(\07\07p\00r\89\7F\04\07\00\1F@P\00\11d\B0\00*\04\04@\00)\07\04@\009\04\04\07@\00X\09\04\00\1F\80@\00*\09\09@\00\1C\04\F0\00\17\04\F0\01@\19x\04\FF \06@\02\16\01\00 \04\87\89\7F\06\09\00\1F\00\0DP\00*\06\06P\00)\09\06P\007\06\06\09P\00@\0Cr\00\05@\01!pR\D0\03\0DP\12\81\89\7F\0B\06\00\1F\00\0E`\00ft\0E\00$\B4\07\10\03\00\80\0A\10\B6\EF\02\12\80\E0\01\00 \03\12t\00\06#\FF\00\D0\02\14\B2p\0C\12\F8\90\00G\B2\00\06\0BP\043\1C\B8\00p\00Ap\02\00\E4\90\03\10\050\01 p@0\00`\0F\04\19x\05\05\FF\05\10\FF\C5\00\01\C0\094\0B\0B\06@\04\12\E2\E0\05\17\07p\001\0Cx\00\A0\061p@\FA`\06#\88\B3\B0\0C\00\A5\0Cf\E8\0F\00\1D{\00\01\00\89\EC\0F\00GY\00\00@\B0\08\00\9E\04$\80\FF\C0\021\82x\06\F0\13\01\01\00\93\CA\0F\00\84\C9\0A\05\00\04P\00\81\A2\02\00Gy\00\06\C2Q\0E\22\80\0Bp\02C\09\0A\00\07p\02/\A4N\D0\02\01\02\90\07%\C0\FC\E0\01\02\10\05\11\00\10\03c\89\7F\08\09\00\07`\02'\A4\0E\E0\0D\05@\00)\09\08@\00!\08\08P\00\05@\00C\0C\08\00\07`\02a\E2\04\00$r\0B\90\011\08\00\8EP\06$\0B\B2\00\04\11\FC\E0\05\09\F0\01\0A\90\01\01\10\00\00\C0\01\14\0Cp\00\10\DAp\00\04\B0\0D\02p\00c\88\B3\00\FF\0C\00\10\01-\E4\07\80\03\22Hyh\19\01\10\00\1E\E4\80\01T\84y\08\FF\00@\00\22\A2Np\08\07\A0\03!\02x\DE\00\14\80\90\01\00 \15&\80\03`\00B$t\09\FF\B1\01\03\80\02&r\0A\E0\14\10\C6@\00\1B\13\90\04\02\80\05\15\0A\90\04\11\0C\80\05\02\00\14[\8F\04\10x\0E\A0\14\19\10\A0\14J\81y\0D\0C`\0A&\0E\0E@\00\00p\05\16\12\90\14\01\10\0E%\10\10 \00<\E4\0F\08\D0\07G%v\12\12 \00\00\B0\14\15\10 \00\10h\10\00%\13\12\10\00\B4\A2N\00$t\16\FF\9D\98\BB;\F0\02\01\D0\0A\04\80\07\01\10\03a\15\FF\00\00|C \00\11\C6\E0\03!\0A\03`\07\02\D0\03c!r\14\08\0D\00\E4\06`\C8\8F\00#t\0B\9F\112?\16  \01T!r\0D\08\0F \00\D2\C6\0F\01#t\0B\0B\01\00@K\15@ \00E#t\0C\0D0\00\D4\C6\0F\00!t\0F\0B\7F\00@\CB\00\00@\004\11\08\11@\00u\E2\0F\02#t\0C\0C@\00\D4\E4\0F\08#x\0F\14;\AA\B8?\0F\08P\00'\10\11\80\00e\08!r\13\08\13@\00hO\00!t\0E\0C`\00\00@\00e`p\A52\0F\00@\00\17\10\A0\00W\08#t\14\13P\00P\00#x\0E\0Dp\00\12\0Ep\000\04$x\FF\02\15\800\04I!t\12\10`\007t\14\14P\00P\00#x\0D\0Dp\00\13\0E \00T\08s\0F\00\0F\B0\02p\22\0E\00#x\12\11`\00#\12\08\00\018t\0E\14P\00E$x\10\10p\00!\C4\0F0\00\00P\00$\12\00`\00%\0E\13\A0\00`\E4\0F\04\08s\0D\CD\05\02`\00\10\A4`\00(\13\13\80\00T\19x\0E\0B\17\10\05\10\CA\90\004\11\00\120\00\81\E2\0E\00#r\0F\0E\0F\10\07\00\10\00e\1F\00\19x\09\140\00\10\CC0\008\16\00\13\C0\00Cr\0D\0C\0D\1F\03q\00\C8O\00#r\10\91\01#\0D\00 \020r\09\09`\13\13\10P\00\01\E0\0D\18\FCp\09\0Ap\07\00\C0\03&\90\07\C0\03\00 \0C&p\07 \00\04\90\04\08 \0C&\10\040\00\0E \0C+\0A\0B \0C\12\0A \0C\02\90\04XG\E9\00\00\C0\10\041v\0A\FF0\00\03\E0\01\09\10\09\13\C80\03\07 \0C;\02x\12 \04\16\0C\A0\09k\E2\9F\00\10x\0F\10\04\16\11\10\04p\E2\0F\04%v\0C\0C\C0\03$\12\02\90\03\19\13\00\04\0B\10\0C\1C\04@\04\09\C0\11\16\E4 \0C\22\12\02`\07E%v\10\11\10\00\0F@\04\001$z\13@\00\0F@\04\02\10d@\00%\12\13@\00\1F\CCP\04\11\11\0B\B1\0F\03\E0\00L\00$t\17P\04!\0B\0AP\04/\FC\03P\04\03\16\0CP\044\C8\0F\00\00\04$\17@ \04\19\15`\04:!t\0F\E0\03:t\0D\15\10\04\0AP\04E\19x\0C\0C\D0\02\01@\03\09P\04W\04#t\0D\0Dp\00U\08#t\0E\11P\00\03\80\04\08@\04\0Ep\04'\10\0D\90\00X\04#t\0E\0EP\00H$x\0D\0D@\04\0Ep\04 \10\15\90\00\01~\1Ex\E2\0F\04!t\12\0E\E0\00\0D@\04\03p\04&\17@@\00\00\A0\00\02p\03\1A\0F`\04\10\C4`\04\18\16`\04F\04\19x\14\E0\03\1E\E2`\04T\08s\10\00\10p\00\10\A2P\04\11\16p\04#\16\08\A0\01!x\16`\04\22\16\00\D0\05;\08s\12P\04(\0C\0CP\04\10$\90\1A\04\00\01\03P\04\19\16P\04\11\0Cp\13\15\0CP\045\09\09\12\10\00\01P\04'\14\16P\00\00\C0\03/p\FC\C0\0D\09\07\00\04\040\04\19\02\C0\0D\010\12\08\10\04'\E0\01@\00\08\C0\03\01\D0\02\1B\0E\D0\0D\07\F0\03/\E4\9F\C0\0F\00\02\E0\03\1B\0E\C0\0F\15\0Ep\03\07\10\08*\E8\0E\B0\0F\10\22p\00\19\13`\03 \02x\1F\03!|C\05\02\1B\E4\A0\04\13\C4\80\03\06`\04G\00!r\0A`\03\10\CF\F0\06\00\7F\034?\13 \C0\02\1B\12\B0\07\02\10\07\14\14P\025t\13\120\00\01\B0\07*\11\10\E0\02(\13\130\00\09\B0\06\10\E4 \02 \11\0A0\02\22\11\08 \00E!t\0D\13@\00\12\C6 \00\00@\02\22\11\00 \00@#x\0D\120\00$\0D\08P\00%\0A\13P\00\12\C6 \00\000\00\13\0D0\00\00\C0\06\14\11@\0290\0E\00\00\07\00\B0\02Dr\09\10\110\02 \C8\1F\10\00$\0A\0D\10\00/\C6O\A0\0E\01\1C\0B\A0\0E\17\F0 \00?\02x\0D\90\14\02\15\0B\10\0A\02\00\14\15\0D0\05\0A\E0\1B6$t\0D\D0\01\00@\02&t\0F \05\01\90\015\08\08\0B\B0\01PO\01#t\0C\CA\005?\0D  \05\01\10\05\22\0F@\80\03J!t\0D\0C\D0\03\07\E0\04\020\01\15\08P\01\12\C8\10\00\05@\01*\CC\0F0\01\11$0\01\15\0C \01-\CC\1F\A0\07\01`\0C\04\10\0F\03p\0B&\80\01\80\07%!rP\0C@\00\00\CAO\00\0E6\08\00\1F\90\0C2!r\0B\E0\00\05 \00%\0A\0B\D0\0E\01 \00\11\0A \06\06 \00%\0D\0A\A0\0E\01 \00\13\0C \12\00 \00\10\CF \00\15\0C`\0E\00 \00\12\B2\90\08\04@\00c\88\B3\00\04\0D(\89#\0F\C0\0D\04\17\B0\E0\006$t\04\10\0C\0F\C0\0D\02C\04\05\00(0\01!\E2\04\C0\0D(2\09\C0\0D%\05\04\C0\0D\93d\EE\00!r\05\05\04\00\01\00 \CA/p\0F\15\05\A0\0D\11d\F0\008\05\06\00 \00%\07\0A\80\0D\8A\A4\02\00!\B2\07\07\0A\D0\003\FF\07$\80\00/\E4\050\0D 4\05\FF\00@\00!b.\C0\01&0\05\10\05!\84y\00\01\05\C0\05\000\0D(\00\05 \01\08\10\15Q\1F\00$z\0D\00\03\15\02\00\03 \0E\0D\00\03\15\06\90\17%\07\0D@\05 \E4O0 \15\0D\A0\04\0C\A0\14\01\A0\17'\10\07@\00\00\D0\04\18\0D\10\09E%v\12\09 \00\01\D0\08*\11\10\10\0D'\14\0B \00\0A\B0\08\01\10\0D%\15\14\10\00p\E2\8E\00\02x\16\00\90\03\05\D0\03\08\B0\08\00\80\10\10x\FE\00\D0\80\00\00\E2\F2\03\00\E2/\04 x\18\10\00A>\00\00@p\03\03 \004~\00B\80\05C\02\02\00\04\B0\00`\E4\0F\00\08r\181\00\02\01\00P\CE\0F\00 \98\8F\1E#\80K@\00\00\D0\02&\04\0F\F0\03X\00#t\08\0C\10\08T!r\0F\04\11 \00\01\A0\05(\08\08\F0\07V!r\13\04\13 \00W\02!t\11\08\10\04W\00#t\0A\0FP\00Z\08#t\12\13\D0\08)\11\0C\A0\05Dr\15\04\15P\00\10\E2p\04%\0A\0Ap\00\010\00)\0E\0C\B0\05:t\0C\120\09\18\11\80\09\00\10\09\18\0A\A0\04\10$@\0B\04\D0\05\01 \00\1B\12\C0\09'\14\11P\00\00\D0\08\17\0F\10\09@\00\08s\0E\1A\03\060\08:x\12\13@\0D\09\E0\08\01@\00\08\10\09\0E\00\0A*\12\13P\0D%\16\15\E0\08\01P\05(\0F\080\00\00p\06\14\10\80\00xb\0E\00$x\14\14\D0\00\00@\00\00P\00\22\16\000\00\01\00#\13\\\90\02\01p\0D7\13\00\12\A0\06R r\0E\0F\0E\EA/\01\A0\06E \08\0E\0E`\02\10\C60\00\19\15 \09T r\0A\0A\110\00u\E2/\00 \98\0E\0EP\02z\C6\0F\00 \08\0A\0A@\00C\17\00\18\00\81\0D\10b@\005\0C\0C\13@\00jO\00 \98\0A\0A@\00%\0C\0C@\00\84\E2\0F\00 r\14\14\150\00u\C6\1F\00 \98\0C\0C0\00\00 \008\08\14\140\00\06\90&\02\80\0DI \98\14\140\00#r\15\82*!@\00P\035r\11\17\00\01\010\01%\0C\0D0\01\010\006\13\17\0A \00d\04 r\17\17\14\10\00\02\F0\0C\17\070\00P\08\86y\00\0C \01a\04\19\10\0C\00\E6P%\18\0BP\00T\86y\00\0E\13 \00 \E8\01\10\00*\08\15\10\00$\06\17\10\00!\E2\01\00\08(\00\FB`\0A\0Ap\059M)\00\10\00\0E0\04/\02\050\04\03\22\F4\03\90\0AF\00\00\B0\01\80\050\08r\021\00\02\82\03\010\04%\02\02@\01\00\00\19 s\0C?\05\02\D0\01/f\100\17\00\11\02\90\05\15\03\90\05%\06\020\17*\CCO\D0$k\A2\0E\00$t\08`\0A\00\9E\07\06`\0Ac\10x\03\03\00\01\D0\04\18\C8\E0\1B\13\F0\F0\075\05\04\070\04\12\CFp\00\13\\p\00\00\80\08 t\08\CF\003?\08 \80\08\03\C0\04\22\0B@\10\00J!t\0A\08 \04%\08\08P\03\01\C0\08 \0A\05\90\03#\0A\08@\000x\0A\05`\03\01\14\02\01\10\017\05\00\0A\C0\081 r\05q\00\05P\03&(\05\80\01\10\C8P\01(\05\05P\01E r\05\0C0\00!\CA/\00\02\1B\05\00\02/\90\FE\00\02\08\0B\10\00\00\E0\09\00\0B\01\06P\01C\08\00\80*\A0\01T\C4\0F\00Dy`\0CP\C0\03\00\EA\7F\A0\15\15\0A \15\00 \0EGt\0D\FF\02@\00\06p\19\01`\15Q\04\08r\0A\0C\F0\00\00@\150\E4\0F\04`\00\14\F0`\00\01p\06\05 \00\05p\00\15pp\00\17\0Fp\00\01P\00\00\10\16\06\D0\15\10\C4@\006\0B\0C\0A\00 \19\04\F0\0A\12\E4 \00\03\C0\09\00@\00\01\E0\00#p+ \00\16\C6@\15\13\0B\B0\12\00\80\00&\F0\00\80\00\10G\F0\02\16\E9P\01\040\00\11\04\F0\00\11\8F`\0B\17\01p\00\00`\00*\C0+@\01&\A0\00@\01c!r\0A\04\0C\00\01\00\01\C0\07\0A@\01\00\80\01*\00,@\00\17`\90\002!r\0A\10\01\07@\00\1B\04@\00\1B@@\00\17 @\002\02r\07\9C\0E\12\0F@\00\00\E0\00&@\F6\E0\00\03\80\16\05 \00AHs\00\000\03\000\02Q\E8\0F\00\89s\D0\0AA\0C\06\00\0E\C0\02`Py\00\08`\D3@\00Q\C3\03\00\EA\1FP\00#\F0\FFP\00f\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-\11\01\89\02\0E\01\00\22@\00\01\00=V\01\000\00\08\01\00\1F\0B@\00\04\13\96)\00?\98\02\00@\00\0A\00L\06.\02\00\01\00\1105\03\02h3\13\00$\00\13\07\A7\04\00\01\00\03\14(>\00\17\01T\00\00\01\00*\08\05@\00\0F\80\00\05\1A)\7F2\05\01\00\13\E0@\00\11T\06\00\06\E4\00*\04\00\01\00\1F[@\00\0414\06\00\01\00&\C0\00@\00\1F\0B@\00\00!G\01\18\00\0D@\00/\F8\06\C0\00\03\1B\08\08\00#$\01l\0E\0B\01\00\12\D0%\03\03l\03\02\80\00\17\048\00\22\10\00\01\00\13\B5\14\01\0C\84\01*\F0\07h\00\1F\00\C0\00\04\1F\E6@\00\04\13\F8@\00?x\01\00@\00\0A\132@\00\15\06\92\00\02\01\00*\80\09\E04\12\03\8C\00:$\80\00\01\00\13\87\A4\00+\03\00\01\00\03\F09/H\00\80\00\0B\13\06\9F\0D\05(:\0B\01\00*\A8\00\08\00\04\10\01\13\018\00\04(\01\0C\01\00*\10/\08\00\088\00\17\06\A0\00\0F\01\00\05\04\B8\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_0() {addr_space = 3 : i32} : !llvm.array<32 x f32>
    llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_1() {addr_space = 3 : i32} : !llvm.array<8 x f32>
    llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_2() {addr_space = 3 : i32} : !llvm.array<32 x f32>
    llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_3() {addr_space = 3 : i32} : !llvm.array<8 x f32>
    llvm.func @__nv_expf(f32) -> f32
    llvm.func @main_kStitch_divide__13_1_0___1b1r(%arg0: !llvm.ptr<f32>, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [0 : index, 2 : index, 3 : index, 4 : index, 6 : index, 7 : index, 8 : index, 9 : index, 11 : index, 12 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(-1 : i32) : i32
      %1 = llvm.mlir.constant(256 : index) : i32
      %2 = llvm.mlir.constant(4 : index) : i32
      %3 = llvm.mlir.constant(512 : index) : i32
      %4 = llvm.mlir.constant(768 : index) : i32
      %5 = llvm.mlir.constant(1024 : index) : i32
      %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %7 = llvm.mlir.constant(1 : i32) : i32
      %8 = llvm.mlir.constant(32 : i32) : i32
      %9 = llvm.mlir.constant(2 : i32) : i32
      %10 = llvm.mlir.constant(4 : i32) : i32
      %11 = llvm.mlir.constant(8 : i32) : i32
      %12 = llvm.mlir.constant(16 : i32) : i32
      %13 = llvm.mlir.constant(-0.000000e+00 : f32) : f32
      %14 = llvm.mlir.constant(8 : index) : i32
      %15 = llvm.mlir.constant(1 : index) : i32
      %16 = llvm.mlir.constant(32 : index) : i32
      %17 = llvm.mlir.constant(0 : index) : i32
      %18 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_0 : !llvm.ptr<array<32 x f32>, 3>
      %19 = llvm.getelementptr %18[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %20 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_1 : !llvm.ptr<array<8 x f32>, 3>
      %21 = llvm.getelementptr %20[0, 0] : (!llvm.ptr<array<8 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %22 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_2 : !llvm.ptr<array<32 x f32>, 3>
      %23 = llvm.getelementptr %22[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %24 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_3 : !llvm.ptr<array<8 x f32>, 3>
      %25 = llvm.getelementptr %24[0, 0] : (!llvm.ptr<array<8 x f32>, 3>) -> !llvm.ptr<f32, 3>
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %26 = nvvm.read.ptx.sreg.ctaid.x : i32
      %27 = nvvm.read.ptx.sreg.tid.x : i32
      %28 = llvm.udiv %27, %16  : i32
      %29 = llvm.urem %27, %16  : i32
      %30 = llvm.sub %arg1, %27  : i32
      %31 = llvm.icmp "eq" %30, %17 : i32
      %32 = llvm.sub %30, %15  : i32
      %33 = llvm.udiv %32, %1  : i32
      %34 = llvm.add %33, %15  : i32
      %35 = llvm.select %31, %17, %34 : i1, i32
      %36 = llvm.srem %35, %2  : i32
      %37 = llvm.sub %35, %36  : i32
      %38 = llvm.mul %37, %1  : i32
      %39 = llvm.add %27, %38  : i32
      %40 = llvm.mul %26, %arg1  : i32
      llvm.br ^bb2(%27, %6 : i32, f32)
    ^bb2(%41: i32, %42: f32):  // 2 preds: ^bb1, ^bb3
      %43 = llvm.icmp "slt" %41, %39 : i32
      llvm.cond_br %43, ^bb3, ^bb4
    ^bb3:  // pred: ^bb2
      %44 = llvm.add %41, %1  : i32
      %45 = llvm.add %41, %3  : i32
      %46 = llvm.add %41, %4  : i32
      %47 = llvm.add %40, %41  : i32
      %48 = llvm.add %40, %44  : i32
      %49 = llvm.add %40, %45  : i32
      %50 = llvm.add %40, %46  : i32
      %51 = llvm.getelementptr %arg0[%47] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %52 = llvm.load %51 : !llvm.ptr<f32>
      %53 = llvm.getelementptr %arg0[%48] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %54 = llvm.load %53 : !llvm.ptr<f32>
      %55 = llvm.getelementptr %arg0[%49] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %56 = llvm.load %55 : !llvm.ptr<f32>
      %57 = llvm.getelementptr %arg0[%50] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %58 = llvm.load %57 : !llvm.ptr<f32>
      %59 = llvm.fcmp "ugt" %42, %52 : f32
      %60 = llvm.select %59, %42, %52 : i1, f32
      %61 = llvm.fcmp "uno" %52, %52 : f32
      %62 = llvm.select %61, %52, %60 : i1, f32
      %63 = llvm.fcmp "ugt" %62, %54 : f32
      %64 = llvm.select %63, %62, %54 : i1, f32
      %65 = llvm.fcmp "uno" %54, %54 : f32
      %66 = llvm.select %65, %54, %64 : i1, f32
      %67 = llvm.fcmp "ugt" %66, %56 : f32
      %68 = llvm.select %67, %66, %56 : i1, f32
      %69 = llvm.fcmp "uno" %56, %56 : f32
      %70 = llvm.select %69, %56, %68 : i1, f32
      %71 = llvm.fcmp "ugt" %70, %58 : f32
      %72 = llvm.select %71, %70, %58 : i1, f32
      %73 = llvm.fcmp "uno" %58, %58 : f32
      %74 = llvm.select %73, %58, %72 : i1, f32
      %75 = llvm.add %41, %5  : i32
      llvm.br ^bb2(%75, %74 : i32, f32)
    ^bb4:  // pred: ^bb2
      llvm.br ^bb5(%39, %42 : i32, f32)
    ^bb5(%76: i32, %77: f32):  // 2 preds: ^bb4, ^bb6
      %78 = llvm.icmp "slt" %76, %arg1 : i32
      llvm.cond_br %78, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %79 = llvm.add %40, %76  : i32
      %80 = llvm.getelementptr %arg0[%79] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %81 = llvm.load %80 : !llvm.ptr<f32>
      %82 = llvm.fcmp "ugt" %77, %81 : f32
      %83 = llvm.select %82, %77, %81 : i1, f32
      %84 = llvm.fcmp "uno" %81, %81 : f32
      %85 = llvm.select %84, %81, %83 : i1, f32
      %86 = llvm.add %76, %1  : i32
      llvm.br ^bb5(%86, %85 : i32, f32)
    ^bb7:  // pred: ^bb5
      %87 = llvm.sub %8, %8  : i32
      %88 = llvm.lshr %0, %87  : i32
      %89 = llvm.sub %8, %7  : i32
      %90 = nvvm.shfl.sync  bfly %88, %77, %7, %89 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %91 = llvm.extractvalue %90[0] : !llvm.struct<(f32, i1)> 
      %92 = llvm.fcmp "ugt" %77, %91 : f32
      %93 = llvm.select %92, %77, %91 : i1, f32
      %94 = llvm.fcmp "uno" %91, %91 : f32
      %95 = llvm.select %94, %91, %93 : i1, f32
      %96 = llvm.sub %8, %8  : i32
      %97 = llvm.lshr %0, %96  : i32
      %98 = llvm.sub %8, %7  : i32
      %99 = nvvm.shfl.sync  bfly %97, %95, %9, %98 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %100 = llvm.extractvalue %99[0] : !llvm.struct<(f32, i1)> 
      %101 = llvm.fcmp "ugt" %95, %100 : f32
      %102 = llvm.select %101, %95, %100 : i1, f32
      %103 = llvm.fcmp "uno" %100, %100 : f32
      %104 = llvm.select %103, %100, %102 : i1, f32
      %105 = llvm.sub %8, %8  : i32
      %106 = llvm.lshr %0, %105  : i32
      %107 = llvm.sub %8, %7  : i32
      %108 = nvvm.shfl.sync  bfly %106, %104, %10, %107 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %109 = llvm.extractvalue %108[0] : !llvm.struct<(f32, i1)> 
      %110 = llvm.fcmp "ugt" %104, %109 : f32
      %111 = llvm.select %110, %104, %109 : i1, f32
      %112 = llvm.fcmp "uno" %109, %109 : f32
      %113 = llvm.select %112, %109, %111 : i1, f32
      %114 = llvm.sub %8, %8  : i32
      %115 = llvm.lshr %0, %114  : i32
      %116 = llvm.sub %8, %7  : i32
      %117 = nvvm.shfl.sync  bfly %115, %113, %11, %116 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %118 = llvm.extractvalue %117[0] : !llvm.struct<(f32, i1)> 
      %119 = llvm.fcmp "ugt" %113, %118 : f32
      %120 = llvm.select %119, %113, %118 : i1, f32
      %121 = llvm.fcmp "uno" %118, %118 : f32
      %122 = llvm.select %121, %118, %120 : i1, f32
      %123 = llvm.sub %8, %8  : i32
      %124 = llvm.lshr %0, %123  : i32
      %125 = llvm.sub %8, %7  : i32
      %126 = nvvm.shfl.sync  bfly %124, %122, %12, %125 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %127 = llvm.extractvalue %126[0] : !llvm.struct<(f32, i1)> 
      %128 = llvm.fcmp "ugt" %122, %127 : f32
      %129 = llvm.select %128, %122, %127 : i1, f32
      %130 = llvm.fcmp "uno" %127, %127 : f32
      %131 = llvm.select %130, %127, %129 : i1, f32
      %132 = llvm.icmp "eq" %29, %17 : i32
      llvm.cond_br %132, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %133 = llvm.getelementptr %21[%28] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %131, %133 : !llvm.ptr<f32, 3>
      llvm.br ^bb9
    ^bb9:  // 2 preds: ^bb7, ^bb8
      nvvm.barrier0
      %134 = llvm.icmp "slt" %27, %16 : i32
      llvm.cond_br %134, ^bb10, ^bb17
    ^bb10:  // pred: ^bb9
      %135 = llvm.icmp "slt" %29, %14 : i32
      llvm.cond_br %135, ^bb11, ^bb12
    ^bb11:  // pred: ^bb10
      %136 = llvm.getelementptr %21[%29] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %137 = llvm.load %136 : !llvm.ptr<f32, 3>
      llvm.br ^bb13(%137 : f32)
    ^bb12:  // pred: ^bb10
      llvm.br ^bb13(%6 : f32)
    ^bb13(%138: f32):  // 2 preds: ^bb11, ^bb12
      llvm.br ^bb14
    ^bb14:  // pred: ^bb13
      %139 = llvm.sub %8, %11  : i32
      %140 = llvm.lshr %0, %139  : i32
      %141 = llvm.sub %11, %7  : i32
      %142 = nvvm.shfl.sync  bfly %140, %138, %7, %141 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %143 = llvm.extractvalue %142[0] : !llvm.struct<(f32, i1)> 
      %144 = llvm.fcmp "ugt" %138, %143 : f32
      %145 = llvm.select %144, %138, %143 : i1, f32
      %146 = llvm.fcmp "uno" %143, %143 : f32
      %147 = llvm.select %146, %143, %145 : i1, f32
      %148 = llvm.sub %8, %11  : i32
      %149 = llvm.lshr %0, %148  : i32
      %150 = llvm.sub %11, %7  : i32
      %151 = nvvm.shfl.sync  bfly %149, %147, %9, %150 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %152 = llvm.extractvalue %151[0] : !llvm.struct<(f32, i1)> 
      %153 = llvm.fcmp "ugt" %147, %152 : f32
      %154 = llvm.select %153, %147, %152 : i1, f32
      %155 = llvm.fcmp "uno" %152, %152 : f32
      %156 = llvm.select %155, %152, %154 : i1, f32
      %157 = llvm.sub %8, %11  : i32
      %158 = llvm.lshr %0, %157  : i32
      %159 = llvm.sub %11, %7  : i32
      %160 = nvvm.shfl.sync  bfly %158, %156, %10, %159 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %161 = llvm.extractvalue %160[0] : !llvm.struct<(f32, i1)> 
      %162 = llvm.fcmp "ugt" %156, %161 : f32
      %163 = llvm.select %162, %156, %161 : i1, f32
      %164 = llvm.fcmp "uno" %161, %161 : f32
      %165 = llvm.select %164, %161, %163 : i1, f32
      llvm.cond_br %132, ^bb15, ^bb16
    ^bb15:  // pred: ^bb14
      llvm.store %165, %19 : !llvm.ptr<f32, 3>
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb14, ^bb15
      llvm.br ^bb17
    ^bb17:  // 2 preds: ^bb9, ^bb16
      nvvm.barrier0
      %166 = llvm.load %19 : !llvm.ptr<f32, 3>
      llvm.br ^bb18(%27, %13 : i32, f32)
    ^bb18(%167: i32, %168: f32):  // 2 preds: ^bb17, ^bb19
      %169 = llvm.icmp "slt" %167, %39 : i32
      llvm.cond_br %169, ^bb19, ^bb20
    ^bb19:  // pred: ^bb18
      %170 = llvm.add %167, %1  : i32
      %171 = llvm.add %167, %3  : i32
      %172 = llvm.add %167, %4  : i32
      %173 = llvm.add %40, %167  : i32
      %174 = llvm.add %40, %170  : i32
      %175 = llvm.add %40, %171  : i32
      %176 = llvm.add %40, %172  : i32
      %177 = llvm.getelementptr %arg0[%173] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %178 = llvm.load %177 : !llvm.ptr<f32>
      %179 = llvm.getelementptr %arg0[%174] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %180 = llvm.load %179 : !llvm.ptr<f32>
      %181 = llvm.getelementptr %arg0[%175] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %182 = llvm.load %181 : !llvm.ptr<f32>
      %183 = llvm.getelementptr %arg0[%176] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %184 = llvm.load %183 : !llvm.ptr<f32>
      %185 = llvm.fsub %178, %166  : f32
      %186 = llvm.fsub %180, %166  : f32
      %187 = llvm.fsub %182, %166  : f32
      %188 = llvm.fsub %184, %166  : f32
      %189 = llvm.call @__nv_expf(%185) : (f32) -> f32
      %190 = llvm.call @__nv_expf(%186) : (f32) -> f32
      %191 = llvm.call @__nv_expf(%187) : (f32) -> f32
      %192 = llvm.call @__nv_expf(%188) : (f32) -> f32
      %193 = llvm.fadd %168, %189  : f32
      %194 = llvm.fadd %193, %190  : f32
      %195 = llvm.fadd %194, %191  : f32
      %196 = llvm.fadd %195, %192  : f32
      %197 = llvm.add %167, %5  : i32
      llvm.br ^bb18(%197, %196 : i32, f32)
    ^bb20:  // pred: ^bb18
      %198 = llvm.load %19 : !llvm.ptr<f32, 3>
      llvm.br ^bb21(%39, %168 : i32, f32)
    ^bb21(%199: i32, %200: f32):  // 2 preds: ^bb20, ^bb22
      %201 = llvm.icmp "slt" %199, %arg1 : i32
      llvm.cond_br %201, ^bb22, ^bb23
    ^bb22:  // pred: ^bb21
      %202 = llvm.add %40, %199  : i32
      %203 = llvm.getelementptr %arg0[%202] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %204 = llvm.load %203 : !llvm.ptr<f32>
      %205 = llvm.fsub %204, %198  : f32
      %206 = llvm.call @__nv_expf(%205) : (f32) -> f32
      %207 = llvm.fadd %200, %206  : f32
      %208 = llvm.add %199, %1  : i32
      llvm.br ^bb21(%208, %207 : i32, f32)
    ^bb23:  // pred: ^bb21
      %209 = llvm.sub %8, %8  : i32
      %210 = llvm.lshr %0, %209  : i32
      %211 = llvm.sub %8, %7  : i32
      %212 = nvvm.shfl.sync  bfly %210, %200, %7, %211 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %213 = llvm.extractvalue %212[0] : !llvm.struct<(f32, i1)> 
      %214 = llvm.fadd %200, %213  : f32
      %215 = llvm.sub %8, %8  : i32
      %216 = llvm.lshr %0, %215  : i32
      %217 = llvm.sub %8, %7  : i32
      %218 = nvvm.shfl.sync  bfly %216, %214, %9, %217 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %219 = llvm.extractvalue %218[0] : !llvm.struct<(f32, i1)> 
      %220 = llvm.fadd %214, %219  : f32
      %221 = llvm.sub %8, %8  : i32
      %222 = llvm.lshr %0, %221  : i32
      %223 = llvm.sub %8, %7  : i32
      %224 = nvvm.shfl.sync  bfly %222, %220, %10, %223 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %225 = llvm.extractvalue %224[0] : !llvm.struct<(f32, i1)> 
      %226 = llvm.fadd %220, %225  : f32
      %227 = llvm.sub %8, %8  : i32
      %228 = llvm.lshr %0, %227  : i32
      %229 = llvm.sub %8, %7  : i32
      %230 = nvvm.shfl.sync  bfly %228, %226, %11, %229 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %231 = llvm.extractvalue %230[0] : !llvm.struct<(f32, i1)> 
      %232 = llvm.fadd %226, %231  : f32
      %233 = llvm.sub %8, %8  : i32
      %234 = llvm.lshr %0, %233  : i32
      %235 = llvm.sub %8, %7  : i32
      %236 = nvvm.shfl.sync  bfly %234, %232, %12, %235 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %237 = llvm.extractvalue %236[0] : !llvm.struct<(f32, i1)> 
      %238 = llvm.fadd %232, %237  : f32
      llvm.cond_br %132, ^bb24, ^bb25
    ^bb24:  // pred: ^bb23
      %239 = llvm.getelementptr %25[%28] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %238, %239 : !llvm.ptr<f32, 3>
      llvm.br ^bb25
    ^bb25:  // 2 preds: ^bb23, ^bb24
      nvvm.barrier0
      llvm.cond_br %134, ^bb26, ^bb33
    ^bb26:  // pred: ^bb25
      %240 = llvm.icmp "slt" %29, %14 : i32
      llvm.cond_br %240, ^bb27, ^bb28
    ^bb27:  // pred: ^bb26
      %241 = llvm.getelementptr %25[%29] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %242 = llvm.load %241 : !llvm.ptr<f32, 3>
      llvm.br ^bb29(%242 : f32)
    ^bb28:  // pred: ^bb26
      llvm.br ^bb29(%13 : f32)
    ^bb29(%243: f32):  // 2 preds: ^bb27, ^bb28
      llvm.br ^bb30
    ^bb30:  // pred: ^bb29
      %244 = llvm.sub %8, %11  : i32
      %245 = llvm.lshr %0, %244  : i32
      %246 = llvm.sub %11, %7  : i32
      %247 = nvvm.shfl.sync  bfly %245, %243, %7, %246 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %248 = llvm.extractvalue %247[0] : !llvm.struct<(f32, i1)> 
      %249 = llvm.fadd %243, %248  : f32
      %250 = llvm.sub %8, %11  : i32
      %251 = llvm.lshr %0, %250  : i32
      %252 = llvm.sub %11, %7  : i32
      %253 = nvvm.shfl.sync  bfly %251, %249, %9, %252 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %254 = llvm.extractvalue %253[0] : !llvm.struct<(f32, i1)> 
      %255 = llvm.fadd %249, %254  : f32
      %256 = llvm.sub %8, %11  : i32
      %257 = llvm.lshr %0, %256  : i32
      %258 = llvm.sub %11, %7  : i32
      %259 = nvvm.shfl.sync  bfly %257, %255, %10, %258 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %260 = llvm.extractvalue %259[0] : !llvm.struct<(f32, i1)> 
      %261 = llvm.fadd %255, %260  : f32
      llvm.cond_br %132, ^bb31, ^bb32
    ^bb31:  // pred: ^bb30
      llvm.store %261, %23 : !llvm.ptr<f32, 3>
      llvm.br ^bb32
    ^bb32:  // 2 preds: ^bb30, ^bb31
      llvm.br ^bb33
    ^bb33:  // 2 preds: ^bb25, ^bb32
      nvvm.barrier0
      %262 = llvm.load %19 : !llvm.ptr<f32, 3>
      %263 = llvm.load %23 : !llvm.ptr<f32, 3>
      llvm.br ^bb34(%27 : i32)
    ^bb34(%264: i32):  // 2 preds: ^bb33, ^bb35
      %265 = llvm.icmp "slt" %264, %39 : i32
      llvm.cond_br %265, ^bb35, ^bb36
    ^bb35:  // pred: ^bb34
      %266 = llvm.add %264, %1  : i32
      %267 = llvm.add %264, %3  : i32
      %268 = llvm.add %264, %4  : i32
      %269 = llvm.add %40, %264  : i32
      %270 = llvm.add %40, %266  : i32
      %271 = llvm.add %40, %267  : i32
      %272 = llvm.add %40, %268  : i32
      %273 = llvm.getelementptr %arg0[%269] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %274 = llvm.load %273 : !llvm.ptr<f32>
      %275 = llvm.getelementptr %arg0[%270] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %276 = llvm.load %275 : !llvm.ptr<f32>
      %277 = llvm.getelementptr %arg0[%271] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %278 = llvm.load %277 : !llvm.ptr<f32>
      %279 = llvm.getelementptr %arg0[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %280 = llvm.load %279 : !llvm.ptr<f32>
      %281 = llvm.fsub %274, %262  : f32
      %282 = llvm.fsub %276, %262  : f32
      %283 = llvm.fsub %278, %262  : f32
      %284 = llvm.fsub %280, %262  : f32
      %285 = llvm.call @__nv_expf(%281) : (f32) -> f32
      %286 = llvm.call @__nv_expf(%282) : (f32) -> f32
      %287 = llvm.call @__nv_expf(%283) : (f32) -> f32
      %288 = llvm.call @__nv_expf(%284) : (f32) -> f32
      %289 = llvm.fdiv %285, %263  : f32
      %290 = llvm.fdiv %286, %263  : f32
      %291 = llvm.fdiv %287, %263  : f32
      %292 = llvm.fdiv %288, %263  : f32
      %293 = llvm.getelementptr %arg2[%269] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %289, %293 : !llvm.ptr<f32>
      %294 = llvm.getelementptr %arg2[%270] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %290, %294 : !llvm.ptr<f32>
      %295 = llvm.getelementptr %arg2[%271] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %291, %295 : !llvm.ptr<f32>
      %296 = llvm.getelementptr %arg2[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %292, %296 : !llvm.ptr<f32>
      %297 = llvm.add %264, %5  : i32
      llvm.br ^bb34(%297 : i32)
    ^bb36:  // pred: ^bb34
      %298 = llvm.load %19 : !llvm.ptr<f32, 3>
      %299 = llvm.load %23 : !llvm.ptr<f32, 3>
      llvm.br ^bb37(%39 : i32)
    ^bb37(%300: i32):  // 2 preds: ^bb36, ^bb38
      %301 = llvm.icmp "slt" %300, %arg1 : i32
      llvm.cond_br %301, ^bb38, ^bb39
    ^bb38:  // pred: ^bb37
      %302 = llvm.add %40, %300  : i32
      %303 = llvm.getelementptr %arg0[%302] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %304 = llvm.load %303 : !llvm.ptr<f32>
      %305 = llvm.fsub %304, %298  : f32
      %306 = llvm.call @__nv_expf(%305) : (f32) -> f32
      %307 = llvm.fdiv %306, %299  : f32
      %308 = llvm.getelementptr %arg2[%302] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %307, %308 : !llvm.ptr<f32>
      %309 = llvm.add %300, %1  : i32
      llvm.br ^bb37(%309 : i32)
    ^bb39:  // pred: ^bb37
      llvm.return
    }
  }
  gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\A0 \00\00\00\00\00\00\02\00\01\01@\00\00\00` \00\00\00\00\00\00^ \00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(K\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80J\08\00\11G\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F5)_shndx\00.nv.info\00.text.main_kStitch_divide__13_1_0___1w1r2\00\0F,\00\15oshared.\00\15\9Fconstant01\00\12\FD\01debug_frame\00.rel\11\00\1Aa\12\00!nv&\00oaction%\01 \0F\94\00\0F\0FH\01q\1F$$\00\0F\FF\16$__cuda_sm70_shflsync_bfly_p\00$____wg_G\00\0F\00\09\00/261\00\19_1__28\EA\01\1Fo_param\F1\01.\0F\01\00\06\8CU\00\00\00\03\00\0A\00\01\00\11\AA\18\00,\0B\00\01\00\A1\D8\00\00\00\22\00\0A\00@=\0F\001\00@\01\07\000\00z\010\00,\09\00\01\00\11\B2\18\00,\04\00\01\00\11\E2\18\00,\07\00\01\00f2\00\00\00\12\10\90\00!\80>\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\130\C0\00\10\04\9B\00R\04<\00\00\00E\005\04\08\0F<\00&8\00H\00\04p\00L\84\80\80(o\00\10\08\15\00\81\16\85\80\80(\09\05\0C2\00\068\00\22\18\00\01\00\22p\00\01\00\048\01\1308\011\04/\08\06\01\10&\0C\00R#\08\00\03\00\01\00'\04\12\0C\00\17\11\0C\00\12#0\00\04$\00\17\07$\00\05\0C\00!7\044\05q\015\00\00\04\0A\08~\01\A2`\01 \00\03\19 \00\04\17\A2\00u\04\00\18\00\00\F0!\10\00u\03\00\10\00\00\F0\11\10\009\02\00\0C\10\009\01\00\08\10\00\01\01\00\F1_\F0!\00\03\1B\FF\00\04(P\00\C0\12\00\00\00\13\00\00@\13\00\00\80\13\00\00\C0\13\00\00\10+\00\000+\00\00P+\00\00p+\00\00\90+\00\00\00:\00\00p:\00\00\E0:\00\00P;\00\00\C0;\00\00\10<\00\00P<\00\00\90<\00\00\D0<\00\00 =\00\00\04\1C\0C\00\F0+\00\00`6\00\00\C09\00\00\04\1E\04s\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\D8\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00\C8\00\01\001\02\00\00\C8\01*H\00\10\00\0F\01\00\FF\D6@$v\01\FF\8F\05\B1\FF\00\8E\07\00\C4\0F\00\19y\0D\18\00 \00!y\05\A1\0E\00\B9z\04\00\00F\00\00\B3\05\90\E2\0F\00Ey\00\00\10\14!\00 \80\03\10\00c$t\0B\FF\1F\00@\00\10\E2@\00\03:\04\B1%\00\00\00b\0E\00$t\1A\FF\01\00\04 \00\F1\04x\0A\FF\05\00\00\00\0D\16\01\00\00\E4\1F\00\12x\0D\0D@\00\B0\C0\8E\07\00\C6\0F\00$x\0C\03\A9\02 \0A\020\00\D0/\00\0Cz\00\0D\00[\00\00pR\F2p\00g\04$x\0A\0A\04p\00P\0Cr\00\0D\FF\F0\04`R\F4\03\00\E4\0F0\00\81\0C\00\\\00\00pb\F6\10\00@\12r\00\FF\D2\00\A2\FF3\8E\07\00\D6\0F\00G94\04\01\C0\00p\EA\0F\00\10z\02\00`\00B\FF\E0\FF\07\E0\00H\01\00\E0\0C\E0\00R\13\FF\00\00\80\C0\00\93\E4\0F\00\10x\02\02 \000\00\80\C8\0F\00\12x\02\02\80\E0\00!\C0\8E\10\00a\07r\02\02\FF\00@\00\11\00 \001r\0F\02\80\00\12\FC \00\00\B0\00\00\F2\05\E6pb\F0\03\00\DA\0F\00G\09\00\00`\0C\90\00@$x\02\0F\AB\05!\0D\0A\E0\00v\10Ey\02\00\F0\06\90\001\1Cx\00\01\00?p\F0\F0\A0\00\02\B2\0Cx\00\02\80\01\00\00pB\F8 \000r\11\FF\90\00\10\0D \00\96\D8\0F\00G\C9\00\00\90\06p\00\05P\002\E1\F0\03\E0\00a\10\0F\80\FE\FF\FF\E0\00\10\C4`\00%\16\FFp\01p\E4\0F\00$z\09\0C0\01\22\11\02\D0\00\90%v\02\09\00X\00\00\16\10\00p\CA\0F\00\81y\0E\020\00\C5\00\19\1E\0C\00\A2\00\00\10x\05\090\01\84\E4\0F\04\10x\07\09@\10\00\10\C6@\00*\04\05@\00%\15\04@\00u\E2\02\00%v\06\07 \00\86\E2\0F\08\10x\09\09`P\00g\00\10x\17\11\80\10\00E\81y\12\06@\00fb\09\00%v\08\A0\00\10\C8\C0\00\11\1B\C0\003\17\02\8E0\00%\17\080\00\10d0\00&\02\1Bp\00h\1F\04\10x\19\1B\F0\016\81y\14\E0\00 b\01\C0\00\16\190\00y/\00\10x\1D\1B@0\00\16\19\D0\00 b\03\D0\00\16\1D0\00j\0F\09\10x\1B\1B\D0\00F\1D\11\00\01\10\00<\81y\18\D0\00\06\A0\00\01\D0\00\11\1F\D0\00\12\1DP\00F\00\81y\1B\D0\00\11f\D0\00\1A\1F\D0\00*\1D\1F\D0\00\1C\1C\D0\00\1A\1D\D0\00*!\1F\D0\00\1C\1D\D0\00\1A!\D0\00*\1F\1F\D0\00F!\11\80\01\10\00<\81y\1E\D0\00\06\A0\00\01\D0\00\11!\D0\00\15!\D0\00\17\1F\D0\00\00p\00*\02!\D0\00%#!\D0\00\01@\02\17\02\D0\00\01`\02\1A#\D0\009#!@0\00*\05\040\00\17\060\00y\0F\01\10x!!`0\00\16\06\C0\00\10\220\00\18\08\00\01`\00\0Br\00\0E\0E \00\B1\80\F8\03\00\E4O\08\0Br\00\13\10\000\C0\FA\03\80\04A\08\C2\0E\0E\7F\042\00\80\06\10\01\16\13\E0\00 \A2\0E@\00!\15\15\1F\00P\FA\03\00\E4\8F@\00%\12\12P\00 \0F\02`\00\01 \00\B1\C0\FC\03\00\D2\0F\00\08\D2\15\15`\00!\00\00`\01U\0Br\00\17\17@\00\01\10\00$\15\120\00\10\C8\80\00!\12\12@\00\060\00&\14\14`\00\00\10\00(\12\170\001\D2\17\17@\00\060\00*\19\19`\00)\17\14`\00!\14\14@\00\060\00*\18\18`\00)\14\19`\00!\19\19@\00\060\00*\1B\1B`\00)\19\18`\00!\18\18@\00\060\00*\1C\1C`\00)\18\1B`\00!\1B\1B@\00\060\00*\1D\1D`\00)\1B\1C`\00!\1C\1C@\00\060\00*\1E\1E`\00)\1C\1D`\00!\1D\1D@\00\060\00*\1F\1F`\00)\1D\1E`\00!\1E\1E@\00\060\00*\02\02`\00)\1E\1F`\00!\1F\1F@\00\060\00*\05\05`\00)\1F\02`\00!\02\02@\00\060\00&\06\06`\00\10\01p\00\19\05`\00!\05\05@\00\030\00c\10x\11\11\00\02\F0\02\03p\00\19\06`\00!\06\06@\00\030\00P\0Cr\00\11\10@\06$b\FC\90\00%\13\13\A0\00\01\00\03\11\06\10\00\22\C0\F8\00\031\D2\13\13P\000\00\00\060\03`G\E9\00\00\90\F9A\07\11\83\90\06#Ay\8E\00\0B\10\073\11\0A\8E\A0\07C\02\00p\03 \00\13\E8\F0\06\13\00\F0\06\12\DA\E0\06'@\03@\00+t\03\C0\06\1B\0E\C0\06 \04\0E\C0\033\03\02\8E\E0\03*\02\04\C0\06%\06\0E`\04\01\C0\06*\08\0E\C0\06*\06\06@\00*\19\06\C0\06%\08\08 \00\01\C0\06/\14\0E\C0\06\0B*\0E\08 \05%\14\14@\00\01 \05\1B\10\C0\06%\15\140\00\10d\90\00'\16\10p\00V\04\10x\04\10 \05k\1F\00\81y\16\16\F0\04\16\040\00\00 \05*\04\10 \05*\07\060\00\17\040\00\00\C0\00'\08\10 \05[\01\81y\04\04 \05\08\00\01\07\F0\02\15\F0 \05\18\02 \021\82\02\020\02/\00\00 \05\05%\19\19\D0\02&\8F\08p\05\14\F0 \05$\02\19p\05\00 \051\C2\19\19`\00\03p\05\06`\05\14\F8\B0\02\15\19\A0\05\00\E0\02J\82\0E\0E\190\00&\16\16`\00\16\00\80\05\030\00\14\C2\80\05\050\00*\07\07`\00)\15\16`\00:\16\16\150\00&\04\04`\00\00\A0\03)\16\07`\004\07\07\160\00\11\C80\00)\07\04P\007\04\04\07P\00\0E\D0\09\19\11P\08\06\A0\03\14\F8\80\01$\04\13P\00\00\80\01D\C2\13\13\04P\00\1E\C4\90\03\00\F0\03\01\C0\0A1\16p\00p\03X\89\00\00\A0\01p\03\1B\09p\03\1B\08p\03\11\02\10\02\16\09\A0\02/\08 \C0\07\05%\A2\0Ep\02\15\09p\02/\06\08\C0\07\06%\E2\0E\90\03\060\00?\08\08`\C0\07\0B\06\90\00\10\CC \00(\09\08\F0\02\06\B0\02\1B\F8\B0\02*\FA\03\90\05Q\F0\03\00\D2\8F\80\05\02\C0\02/\80\06\80\05\0B\04\C0\01\04\80\05\050\00*\09\09\D0\02\03\80\05\1B\FA\80\05\05\10\02$\06\09@\03\01P\00!\09\090\00\00@\03\10\CA\00\0C\11\13\00\0C\13\09\E0\0DIAy\01\00\D0\01Sz\00\0F\00[\90\0C\03\00\0D#\90\04 \00\12\D8\A0\0C)p\040\0D\010\005\FF\E1\FF\90\05#`\02 \00\0C\A0\0C\03\90\0C\1B`\A0\05& \02P\00c$v\0E\FF\00[0\02\1B\E2\C0\02\10\C8\C0\01H\0E\0E\A0\FF\90\0C\1B\10`\02\11\02`\02\16\0F`\02\10\02\D0\013\10\02\8E\F0\01\1C\08\90\0C\15\0Fp\02\02\90\0C\18\0F\D0\051$z\05P\001\05\02\8E \001$z\07\10\00\12\07\10\00\05\B0\0C\15\10p\00%\06\07\10\00\1E\E4\90\02W\10x\09\0F``\00\09\80\02\16$\C0\00\01\80\02*\C8\1F\C0\00\020\05+\08\08\80\02\19\08\B0\03(\08\08p\02K\81y\13\020\05*\05\05P\0A$\08\05\D0\07\0F\A0\02\00E\10x\0F\0F\90\06\12\CE \08\01p\00!\00\00\F0\00\0F\10\08\10/\0F\0E\10\08,.\10\FE\10\08\0F\80\02\09#0\01 \00\12\E6p\02\1B p\02(\00\01\A0\04\1F\04@\02\12\15\04@\02\19\06P\01\09@\02\1B\CA0\02\16\C8 \02\16\04\C0\01%\06\06\D0\03\02\C0\01\18\06\00\01\22\82\06\10\01$\00\00\C0\01\18\04p\00\09\10\03\01\D0\05\16\0F\B0\02\09`\01\1B\F8`\01\06\D0\05\01p\00\0F\D0\05\05\04\00\04\07\D0\05.\80\00@\09\0F0\01\0D\15\030\01\16\040\01+\A4\0E\E0\06\02\00\01\19\04\00\01\12\04\80\06\0B\B0\04\1F\04\B0\04\04aGy\00\7F\12'\10\00\10\0B\10\00\FB\00\89\7F\02\13\00\1F \0C\00\00\0E\00\00$\0E`\08\1F\1F`\08\0B\10\CA@\00X\03\02\00\1F@@\00*\03\03@\00)\02\03@\00!\03\03P\00\05@\00X\04\03\00\1F\80@\00*\04\04@\00\1C\03\F0\00\01P\00\05@\00g\05\04\00\1F\00\0D@\00*\05\05@\00)\04\05@\00!\05\05P\00\05@\00a\06\05\00\1F\00\0E@\00ab\00\00$r\02@\011\05\00\8Ep\04(\0B\A2\E0\03,/\08\A0\05T\0B\A2\00\02\060\02\00@\08#\A8\00 \00 p\02\10\02\00\A0\02\14\02@\02q\CA\0F\00\88\A3\00\0A0\00\00\86\16i\E4\03\00Ay\00\A0\01\22Hy\88\18\04\10\066\1D{\00\01\00\12\EC@\14\11P!\0E\02 \00\00\90\13/0\17\90\13\0B&\B0\07\E0\03\0F\80\13\1F\16\03\80\13\010\13\01\1F\00#\80\FF \01\00\90\13\11\03\B0\04\11\F8\00\01@\0Cz\00\03\80\00\01\10\07\000\14VI\00\00 \07\A0\00\09@\00\00\10\03&r\04`\13\01\A0\0B\11\18 \03\01\00\04\1B\E4@\13\10\C6\C0\00%\11\18\C0\00w\E2\0F\04%v\12\18@\10X\04\10x\10\18\90\06G%v\0E\11 \00V\08\81y\06\12 \04\10\22`\06\15\18`\06\01\A0\06*\14\100\00%\05\0E0\00 \E6\10 \0C\17\09 \00\00\E0\0B\06@\0C h\090\07\18\16\10\121\06{\19\90\01\C1\00\90 \00\000\0E\00\08s\19\00 \0B\00\F5\0D $\1E\A0\07Q\19\FE\FF\FF\0F\80\00`\CC\1F\00\05s\0F\8A\02\01\AC\19 \00d\90\025\0E\FF\FF@\04\15\1F\E0\03\10\0F\D0\0Dp\C8/\00$z\13\13`\00\22\FF\02@\01@'r\0F\0F\A0\03\22\0E\00\90\09T'r\12\0F\18@\00\16\C8 \04%\12\0Ap\01\01@\00\13\18p\01V'r\14\0F\11p\01P\01\0Cz\00\18 \00$p`\10\15\02\90\00\15\14@\00%\11\0E\A0\0A\01@\006\0E\0F\10@\00\00\10\02\12\11@\00 \FC\03\10\000\10J\18P\00\11\80\F0\00\16\E2\90\00%\0E\0A0\00\03p\00\12\FAp\00!z\10\A0\00\04\D0\07T\10H\12\12\01\B0\01\10\E2p\007\16\0F\09p\00E\10j\11\11`\00\02\90\02\1A\10\C0\00\11\18p\00\13\16\C0\00'\10XP\00\18\E4\B0\00\04\80\00%\0F\18`\08\00\80\005h\14\140\00\00@\00;J\10\10p\00\15\0F\F0\00\10\CA`\00\070\00\09\90\00\020\04I\10H\0E\0EP\00;j\0F\0FP\00\01\E0\09\22pP`\032\12z\09\F0\09\133\F0\00\17X@\00\09\80\00\02`\00V\07r\12\09\12 \07g\04\07r\14\09\14\10\00H\10h\16\16P\01T$x\0F\12\04P\01\10\E2@\00!\0E\09\90\02\03`\08W$x\10\14\04p\01@\12x\0F\0F\C0\11\22\FF\E2\00\02,\10XP\00(\0E\0EP\00H\12x\10\100\00W\07r\09\09\16`\00T\84y\0F\0F\00 \05u\A2\0E\00\12x\0E\0E0\00\01\E0\18(\09\09P\00E\84y\10\100\00\10\E80\00%\09\090\00\01 \00%\0E\0E \00\10j\10\00%\09\09\10\00!\22\0E\00\18S\9D\98\BB;\FF0\07\84$t\15\FF\00\00|C`\00W\10x\04\04\80\A0\01E\0Cr\00\04\F0\04\84\E2\0F\00!r\0F\06\0F\F3\07P\C8O\00#t\0F\00B\00?\13 \F0\19T!r\10\05\10 \00 \C6\8F \00t\06\01\00@K\15@ \004\07\07\0E \00u\E2\0F\02#t\05\10@\00\B3\E4\0F\08!t\0E\06\7F\00@\CB \00X\00#t\12\07`\00G#t\05\05P\00e\08!r\08\08\09P\00\C4\1F\00#x\0E\0F;\AA\B8?\0E\080\00)\12\12\80\00:t\11\05`\00)\09\08`\00\94x\0E\0F`p\A52\0E\000\00)\14\120\000x\11\10`\00\15\11`\00%\0F\09`\00\10\C4 \00 \14\07 \00&\14\080\00\00P\00\22\11\00\10\00W\08s\0E\00\0E\90\01J!t\09\0F`\00 \14\070\00\01\B4\02\10\C6\10\00 \07\08P\00\22\09\08\10\00T\08s\10\00\11@\00@b\0E\00#\B0\0ECp\A52\07@\00@$x\07\06\0F\01\13\FF\10\05W\08s\09\00\14`\02E$x\05\05 \00\00p\00Cr\02\07\0E\C8\1D\00\90\05X\08s\11\00\080\02:x\12\120\004\05\05\100\00!\E2/p\13\05 \00\11\C6 \00!\12\09P\08\03 \02Dr\02\02\11\10\00\22\E2\1F`\0B\09\C0\13\0A\B0\09\01P\0E\13\0E\10\00\13\E2P\0E\18\0E\F0\07,\04\03\D0\0B/\A0\07P\0E\0A\1C\04P\0E(`\07P\0E\12\04`\04\0FP\0E\06(\04\04P\0EV\10x\09\03 \F0\03F\04$t\18\B0\07\01@\035\05\03@ \00Q\1F\04$z\12\E0\07\01\F0\0A\01 \0E\16\09\C0\0D\01\80\07C\0E\12\00X\90\06\01@\00%\11\03\A0\07\01\D0\07'\14\09 \00F\08\81y\07\A0\07%\A6\00\E0\13\12\05 \00F\04\81y\08\A0\07 \22\07\B0\06\18\0C\B0\06\04\F0\13\22\18\02\10\06G%v\18\11`\00\00\C0\0C\06\D0\07 h#\A0\0E\18\18\E0\106\06{\13\E0\07*b\0E\B0\07\84\E2\1F\00\10x\03\03\80\C0\00\01\80\02\10\13w!\03\00\08l.\00\10x\1B\13\00\08\14\1B\00\08\10\E4P\00\17\15\F0\07j\8F\00$z\15\15\F0\079\16\0F\15\F0\074\0E\16\12\80\00\00\A0\07&r\0F`\07\01\A0\078\13\16\10 \00!z\120\06$\12\02 \00 \0F\16\B0\02\07\C0\07\1A\12@\07\16\14\90\00%\E4\0F\A0\00%\13\0A\E0\1C\18\14\D0\01E$z\10\15\C0\07\02P\00\1A\09\10\08(\12\12\F0\06-\10H\D0\06\15\12\D0\06\01\F0\074\12\16\11\A0\00\1B\E2\E0\07\16\C6\A0\00\22\12\0A \00I\10j\09\09`\005h\0F\0F`\00\11\E2\C0\08\18\14\10\026$t\14\00\06\1F\E4`\07\02\15\09\90\00\1F\C4\00\08\02\15\11\E0\00\00@\00;H\13\13@\00\06\D0\07\01 \08/\0F\0F \08\0A\0B\D0\086\12z\10\00\08\00 \00\1Ah\C0\08A\07r\0E\10\1F\04!\00\00\B0\0ET\07r\0F\10\0F\10\00\010\08)\13\13\00\01)x\09\A0\07\0D\00\098x\0F\0F \00A\07r\13\10\C0\02\02@\08\0Ap\07\1B\E4\10\08\01@\00%\13\13@\00\11\E4\90\07$\09\00P\05*\A8\0E\80\09\0C\00\08\10&p\007\12\10\12\D0\00I\12x\10\13`\00\08\C0\07\00\C0\00\030\0E\04\A0\07\00\B0\05\0A\80\00\01o\19\02`\00\10fP\00%\12\12P\00\10\CC \00\02/\1C\05\90\06Ir\0E\07\0E\E0\07*\07\0E\E0\074\08\08\0F \00p\C6\0F\01#t\07\07\00\07!\14@ \00H\08#t\0FP\07d\08!r\11\06\110\00y\E4\0F\02!t\09\07\F0\068t\06\0F@\00 $x\0F\17\07p\06:t\10\11P\009\05\05\12\F0\07%\09\0E \07\01\B0\07\1B\0F@\08*\10\10\A0\00'\12\05P\00\01@\00\00@\07$\09\00\10\00\11\0Fp\07$\0F\08P\00*\0E\10P\00'\12\12P\00Y\00$x\06\06\B0\00!x\0F\90\07%\0F\00P\08\17\11\90\08I\04!t\08P\08\00\A0\07\14\09\80\01!$\0E0\00\08\80\08@#x\08\05@\00\01\D5\13\12\C8\10\00\00 \00\15\08@\007\0F\00\0F\10\08E$x\05\10\90\00\1B\CEp\08\85\A2\0E\00#r\07\07\09\C0\07\10\1F\C0\07\06\E0\07\01\B0\057\08\00\08\10\021#r\06\E1\09\013\07!\C8/\00\08\11\0E\9D\02\06\E0\07\1B\08\E0\07.\D0\F8\90\13\0F\C0\07\09)0\04\90\13\1D\04\90\13\17\04@\00'x\05\80\07h\1F\00$t\08\FF\F0\02\04\10\07\06p\07\16\11 \07\01\00\07\11\06\00\07\22\08\02P\0EE%v\08\11\10\00\01\00\07\16\05p\15 \A8\00\A0\12\18\08\A0\1B6\06{\12\00\07!\22\0F\F0\06\18@\00\15Es\12\00\12\F0\06\00 \15&\0E\12\F0\06*\0F\01\F0\0E\1A\09@\074\E4\0F\01 \0E\02\00\07.\0F\02\F0\0E\16\13\F0\0E\00\D0\06&t\0E\10\04\01\D0\064\06\13\10P\00\10\C8@\0F\11\09P\00\10\06P\00u\E4/\00'r\07\13P\06\01\10\07%\10\09\B0\06\00@\01!r\080\00#\07\0A\10\00)t\0F \0C\06\B0\05#\FC\03P\06\15\08P\06\00\F0\0C&z\08\B0\05\1B\C8`\05\10\CE\E0\05\0A@\06E\10h\06\06\10\05\1B\E4\E0\06\00 \00?Z\11\11 \0E\03\02p\06\06`\00\12\F0\A0\06\10X\90\04\06p\15'\10H`\00\01\F0\11!\06\08\BC\02\03\80\16'\10\080\00\1B\E4\00\0A\03\E0\03\14\040\01\01\10\06!\07\08\0C\03!\00\00P\00I\12x\06\06\A0\055x\07\070\00\10\CA \00(\08\07 \00\22\84y_\13\04\10\06E\84y\09\08\10\00\10\E2\80\05\10\050\17\08\80\05\01\B0\04$\0E \80\059\09\04\09`\0D\00\C0\00D@K\0F@\A0\04%\0E\090\00\10\C6\C0\04\18\04p\05X\04#t\0E\0E0\00\09p\05\10\E4`\04\11\04p\04$\04\08\00\05%\06\0E@\00\12\C6 \00\00\80\04\22\04\00 \00@#x\06\090\00$\06\08P\00%\05\0EP\00\12\C6 \00\000\00\13\060\00T\08s\04\00\04\F0\00\01 \127\06\00\06\C0\04\00\90\0C\14\04\A0\04 \C8\1F\10\00$\05\06\10\00/\C6/\90\16\01\1C\03\90\16&@\02 \00\0E\C0\17\16\03 \04\01\00\0B%\04\03\80\179\CC\1F\00\90\1Ef\A2\00\00\06{\09\F0\03\10\E2\A0\0A4\06\FF\FFP\00'\E2/\90\02\03@\08&t\050\03&\CA\1F\E0\05\01\10\04 \8E\00\D0&\0A\00\0B4\07\00\07\10\04%$\0E\C0\0A\01\80\03\00\C0\03:z\0F\0F\00\041\06\07\0F\80\05\02\B0\05T'r\06\06\03\90\00\16\C8\C0\03#\06\0A\10\007z\03\08\10\05\010\05\050\04\17\C6@\01!`\F0p\05I\10\0A\03\03\A0\03\1D\08\A0\03\15\03\A0\03\1B\DA`\03\00\10\09)\DA\06\00\04\00 \03\14\07\B0)\1B\CAP\03\10\CA\10\034\03\06\00\00\02\10\A4\00\03\10\03p\10\07\00\03\01\7F\103?\08 \00\070t\04\04\C0\02\22\05@\10\00E!t\08\04\A0\02\02 \07\1B\030\07\18\030\07E$x\03\04\A0\02\1B\CA\F0\06\11$\80\02'\03\08\10\0F\0FP\18\01(\E2\10P\18%\03\02P\18\02\C0\00\04\F0\17C\00\00\CAO\F0\17\17@ \00\11\04\B0\09\06 \00&\05\04\10\18\10\1E\00\04\02\C0\17\01 \00\14\1F\B0\17\03\F0\17Q.\00!r\06\10\03\06 \00%\07\06\D0\17`d\00\00!\A2\07\01\10\03 \00\11/\80\17/\07\80\80\17';M9\00\C0\07\11\00\10\03$\FF\E0\C0\07H\00\00@\0Ap\174\00\00 \10\02\01\00\02\00\FD\16\08p\17\00\FC\01\010\00\03p\17%\00\00p\17\02p\12\11\0D\80\05$b\F2\E0\05\01p\00\05`\17V\19\00\00\C0\09\90\00\04p\1D\11\0D\E0\02\11op\17\05`\02\11\C6\A0\07\15\07\90\00\01@\17\11\0E\80\1D$\02\02\00\04%\10\03\10\00\01\00\08\16\04`\0F \A8\02\E0\16\18\10P\0FY\10x\09\07@\10\1E&\05\07\B0\0F\00\90\1D*\12\09`\00*\0A\05`\00\07\90\17zj\01\00\81y\0A\0A@\17\17\14`\08\0C`\0F\00p#4\0D\0D\80p\00\01\B0\099\14\00\14`\17,\15\14`\0F\19\15`\0F\02\E0+\06`\0F\01`\07\07`\04\11\12\10\17\07`\0F$\12\07\80\00\01`\04\1B\10`\0F)\0F\12\80\048z\0B\10\00\1F\040\00\15\0F0\004\11\12\090\00\02P\08\12\0BP\04\12\F6\D0\0E8z\10\10\B0\04W'r\12\12\05\D0\01\06 \08\19\F2\A0\0F$\11\0Ap\00\16\16\10\0F\00\C0\049:\0B\0B\D0\04\1D8\E0\0E\15\0B\D0\04\11\E2\C0\00\06\D0\0F\18\E2p\08\12\F6`\08\1A\1A\C0\08\06@\00*\FA\03\C0\08\13\F4P\00'\10\16\A0\11:\00\10\18\00\0F6\12z\140\05\00\A0\00\08\10\10\18\C4P\00\02\10\09R\07r\0E\14\0E@\03\11\050\00\1BZ\E0\00\17(`\00\0F\90\16\00\06\B0\00\13\F2P\004\0F\14\0FP\00\01p\0E\16\13\80\16\1B\E2`\0A\1B\E2\00\10\0F0\0F\00H\10X\11\11\80\00T\84y\15\13\00`\05\1B\E2 \01\0F@\0F\00f\84y\0B\13\00\80\80\0C\09\00\19\00\80\01\1C\18`\00%\17\0F`\00\10\E6\D0\004\11\14\11\D0\00\04`\0F\04P\00Wd\0E\00\10(P\00\000\025x\11\11\C0\00\01\10\014\12\14\12@\00\01\B0\06\16\10P\0F\11\E2@\0F\15\11\B0\00\1B\C8\E0\17\01p\00%\19\12\90\00\1Bh\80\0F\22\E8\0E\C0\17\16\80\10\01 \0Bx\C56\94\80~\00B\FC\03\00\C4\1F\10\002\00\00\E2\A0\01B\0Bx\00\0F \00\000\01#/\04\10\00A\00\00\E2\F6P\0BB h\0B\0B\866\11@P\05! \98\10\00\11K\10\00u\E2\0F\00 (\0F\0F \00\01\F0\06\10\0Ba\00\020\04\00 \008\B8\0F\0F0\00Q!r\15\04\15a\00\00 \00\11O@\09\08\00\0CV!r\17\08\17\D0\0FW\01#t\13\15\A0\0AX\08\84y\08\12\C0\01E#t\1B\17 \00\00`\0A\10t\90\114@K\040\175t\1B\1B\10\00\10\E4\D0\07%\14\13\D0\07\01p\007\19\06\190\18E!t\16\1B \00\01\B0\0A(\14\15p\17 $x\CF $\80\00\B0\00@#x\16\17 \00&\16\080\00\05`\17\01`\00%\11\0A\A0\10|\E2\8F\00$x\1B\1B@\00\000\00\13\160\00X\08s\15\00\14\B0\0A8t\17\19\00\01H#t\13\11\80\0BE#t\17\17\E0\00\02 \00\06\00\01\10\E4P\007\16\00\16\A0\0FI!t\0A\17\00\01)t\04\10\01E$x\17\17\A0\00\01\D0\00 \0A\19\E0\00\22\0A\08 \00H#x\04\11\C0\0B\00\90\12\08\D0\00R r\06\06\15\EA7\01\C0\17\00@\00\00\E0\00&\0A\00@\00\08\E0\0B\00\F0\00\15\0F0\02e\02\00 h\06\06P\02\02\C0\02\12\08\A0\02\14\F8\C0\02\15\10\D0\02\01p\005\16\1B\16p\00\14O0\002\00\00\E20\0BH \98\06\06\80\021\08s\0A;4\04@\02K (\16\16p\00\12\10@\00\13\F4`\007\0F\0B\06\D0\00\01`%\16^p\07H \B8\16\16`\00\09`\08\00 \008H\08\08`\00\09\A0\0Cxb\0E\00 h\10\10 \00F r\15\15\D0\00h\8F\00 \D8\08\08`\001\86y\000\11\CA\04\19\10\0C\00\E6\05\00 \A8\10\10\E0\00\03`\19\00P\01\84\22\07\00 r\0A\17\0AP\00u\C8\1F\00 H\0A\0Ap\00\12\C6\F0\19\15\100\00D\0E\00 rp)\02\F0\00H \D8\0A\0A`\00\01\C02\04\00\01z\E4\8F\00 h\13\13\C0\00%\11\11p\00z\C6\0F\01 \A8\13\13@\00%\0A\03@\00\01\C0\08(\02\05P\01T\86y\00\0A\15\E0\00E\E2\05\00  \15\00P\00\10\1F \00$\08\11 \00 \E8\05\10\00'\02\130\00hG\99\00\00@\F6 \12\01=\00\05\D0\099M\09\00\10\009\06{\04\80\0D\00p\0A#0\03 \00\09\A0\07\03P\06\09\80\10\16O\D0\01\00@\01\10\A4\80\0D&\02\04 \09uO\00\05s\03\00\02 \09\15\04\10#\02`\03QO\00$r\05\10\00\10\03P\08 \C8\9F\C0%\1A\050\091\09\03\050\0C\02\10\00\00\E06\15\04@\00\00\00\08\11\04\A0\0A\16\00P\01\01\A0*3\0B\02\8Ep\0E\19\02p(9'r\05\F0\1D\09P\0B\01\F0\17\11\07\90\00\13\05\D0\116$t\0A\A0\05\01p\00*\06\07P\22\1C\07\F0\0D\1A\06\F0\0D)\06\06@\088\08\05\05 \07\060\00\12\F20\00\17\18 \00\01\D0\0B\12\05 (#\00\000\07(\05\05\E0\0DE\12x\03\05 \07\84\CA\1F\00\84y\05\03\00`\03\10\E8 \07F\06\03\00\80\80\0DB\0Bx\00\06@\04\00\C0\1B#\1F\04\10\00\02\10\07W\D6\0F\00 \08\80\04\1B\C8@\04&\CC\0FP\10#\10\00\E0\051\02\02\05A\00\04P\0E\01?+\11?\15\0C\00 \0E\03P\1E\22\0A@\10\008!t\07 \1E\19\040\1D\01\D0\10 \07\02`\05#\07\08@\005x\07\02\80\1D\00\E0(\22s\02\D0\0F\03\D0\00$ r\B0+\11@ \15\01\00\02\13^\00\02\10\C6\D0\00(\05\05\A0\03\09\F0\0C\00 \005\98\05\05\E0\00\00\F0\00#r\05@)\02\D0\07\00`\03\15\05`\03\11\01\00\11/p\FD`\03\08\0B\10\00\04\F0\02\13\13p\02\81\02x\04\00\10:\00\00\F5\05\12\E2\D0'\14\01\D0\01E\C4\0F\00D\80\03\11\C0@\00\00P(\14\07p&\00\C0\17Gt\03\FF\02\C0\02\06\E0.\01\90\1DR\04\08r\13\070\04\02\90!\00p\00\14\80p\00\00\10\07$r\02 \00\14\00p\00)\C0\02p\00\12\02p\00\15\F0p\00\1F\04p\00\02\00\90\06\02P\00\01?\00\00F\16\03p\00\1B\F0p\00\03 \00\14\02p\00\1FPp\00\0C\1F\08p\00\05\01\E0\00\09p\00\01P\01#`;p\00\1F\C4p\00\03/\E0\01p\00\0B\1F\10p\00\17\01\E0\00/\D0;\E0\00\0B\17pp\00\04\D0\12\13\07`\00fGy\00\00\F0\D7@\02\09\00\02\03 \02& <\D0\00\00P\00\15 P\00\11?p\03\01\A0\00\16\00@\02\1B\02@\00\1B`@\00&\E0\00\90\00\0F@\00\01\1B\04@\00\1B\A0@\00\1F\A0@\00\0C\1B\08@\00\1B\E0@\00\19`@\00\1D\06@\00\1B\10@\00&0=@\00\04@\03\17\06 \03\17\10P\00\00P\01(`\EEP\01\00 \14\06P\00fHs\00\00\1A\00\A040\89s\07@*A\0C\0B\00\0E@\04`Py\00\04\80\C2@\00R\C3\03\00\EA\1F\A0\01\13\FFP\00f\C0\0F\00\18y\00\01\00\0F\10\00\D0\0F\01\00-\00\AB\02.\03\00\01\00\22@\00\01\00=%\01\000\00\08\01\00\1F\0B@\00\04\13e)\00\1F\F1@\00\0C\00\DC\04.\02\00\01\00\13XU\00$\C0\00\AD\03\00T\02\02G\06\00\01\00\03\948/\00\E6T\00\00\00\01\00\02\A5!\02\00E\0F\80\00\09\11)\06\00\05\EFC\05\01\00\13\F0@\00\11T\06\00\06\E4\00*\04\00\01\00/[\00@\00\03\12D\95\066\00\DC\00@\00\1F\0A@\00\00\00-'\1F\0B@\00\00\11 \B5\07\0E\C0\00\1B\08\08\00\11\F3\9C2\0E\01\00\13\F8@\00& \00\80\00\17\048\00\02\94\033\00\00\B5\14\01\0C\84\01\13\18\F5\03\00|<\04\01\00\0F\C0\00\01\132@\00\15\06R\00\03\01\00\03\95\00&\80>\80\00j\07\00\00&\80\00\01\00\13\87\94\00*\03\00\01\00\040J\01]\00\0F\80\00\08\13\06l\1D\04hJ\0C\01\00\1B\A8\08\00\17\08W\00\17\05\E8\00\0C\01\00*h@\08\00\088\00\18\06\A0\00\0F\01\00\05\00)\00\B0\00\00\00\08\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1w1r_0() {addr_space = 3 : i32} : !llvm.array<32 x f32>
    llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1w1r_1() {addr_space = 3 : i32} : !llvm.array<32 x f32>
    llvm.func @__nv_expf(f32) -> f32
    llvm.func @main_kStitch_divide__13_1_0___1w1r(%arg0: !llvm.ptr<f32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: !llvm.ptr<f32>) attributes {disc.elimargs = [0 : index, 2 : index, 3 : index, 6 : index, 7 : index, 8 : index, 10 : index, 12 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(-1 : i32) : i32
      %1 = llvm.mlir.constant(8 : index) : i32
      %2 = llvm.mlir.constant(4 : index) : i32
      %3 = llvm.mlir.constant(64 : index) : i32
      %4 = llvm.mlir.constant(96 : index) : i32
      %5 = llvm.mlir.constant(128 : index) : i32
      %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %7 = llvm.mlir.constant(1 : i32) : i32
      %8 = llvm.mlir.constant(32 : i32) : i32
      %9 = llvm.mlir.constant(2 : i32) : i32
      %10 = llvm.mlir.constant(4 : i32) : i32
      %11 = llvm.mlir.constant(8 : i32) : i32
      %12 = llvm.mlir.constant(16 : i32) : i32
      %13 = llvm.mlir.constant(-0.000000e+00 : f32) : f32
      %14 = llvm.mlir.constant(1 : index) : i32
      %15 = llvm.mlir.constant(32 : index) : i32
      %16 = llvm.mlir.constant(0 : index) : i32
      %17 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1w1r_0 : !llvm.ptr<array<32 x f32>, 3>
      %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %19 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1w1r_1 : !llvm.ptr<array<32 x f32>, 3>
      %20 = llvm.getelementptr %19[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %21 = nvvm.read.ptx.sreg.ctaid.x : i32
      %22 = nvvm.read.ptx.sreg.tid.x : i32
      %23 = llvm.udiv %22, %15  : i32
      %24 = llvm.urem %22, %15  : i32
      %25 = llvm.mul %21, %1  : i32
      %26 = llvm.add %25, %23  : i32
      %27 = llvm.icmp "slt" %26, %arg3 : i32
      llvm.cond_br %27, ^bb2, ^bb11
    ^bb2:  // pred: ^bb1
      %28 = llvm.sub %arg2, %24  : i32
      %29 = llvm.icmp "eq" %28, %16 : i32
      %30 = llvm.sub %28, %14  : i32
      %31 = llvm.udiv %30, %15  : i32
      %32 = llvm.add %31, %14  : i32
      %33 = llvm.select %29, %16, %32 : i1, i32
      %34 = llvm.srem %33, %2  : i32
      %35 = llvm.sub %33, %34  : i32
      %36 = llvm.mul %35, %15  : i32
      %37 = llvm.add %24, %36  : i32
      %38 = llvm.mul %26, %arg2  : i32
      llvm.br ^bb3(%24, %6 : i32, f32)
    ^bb3(%39: i32, %40: f32):  // 2 preds: ^bb2, ^bb4
      %41 = llvm.icmp "slt" %39, %37 : i32
      llvm.cond_br %41, ^bb4, ^bb5
    ^bb4:  // pred: ^bb3
      %42 = llvm.add %39, %15  : i32
      %43 = llvm.add %39, %3  : i32
      %44 = llvm.add %39, %4  : i32
      %45 = llvm.add %38, %39  : i32
      %46 = llvm.add %38, %42  : i32
      %47 = llvm.add %38, %43  : i32
      %48 = llvm.add %38, %44  : i32
      %49 = llvm.getelementptr %arg0[%45] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %50 = llvm.load %49 : !llvm.ptr<f32>
      %51 = llvm.getelementptr %arg0[%46] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %52 = llvm.load %51 : !llvm.ptr<f32>
      %53 = llvm.getelementptr %arg0[%47] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %54 = llvm.load %53 : !llvm.ptr<f32>
      %55 = llvm.getelementptr %arg0[%48] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %56 = llvm.load %55 : !llvm.ptr<f32>
      %57 = llvm.fcmp "ugt" %40, %50 : f32
      %58 = llvm.select %57, %40, %50 : i1, f32
      %59 = llvm.fcmp "uno" %50, %50 : f32
      %60 = llvm.select %59, %50, %58 : i1, f32
      %61 = llvm.fcmp "ugt" %60, %52 : f32
      %62 = llvm.select %61, %60, %52 : i1, f32
      %63 = llvm.fcmp "uno" %52, %52 : f32
      %64 = llvm.select %63, %52, %62 : i1, f32
      %65 = llvm.fcmp "ugt" %64, %54 : f32
      %66 = llvm.select %65, %64, %54 : i1, f32
      %67 = llvm.fcmp "uno" %54, %54 : f32
      %68 = llvm.select %67, %54, %66 : i1, f32
      %69 = llvm.fcmp "ugt" %68, %56 : f32
      %70 = llvm.select %69, %68, %56 : i1, f32
      %71 = llvm.fcmp "uno" %56, %56 : f32
      %72 = llvm.select %71, %56, %70 : i1, f32
      %73 = llvm.add %39, %5  : i32
      llvm.br ^bb3(%73, %72 : i32, f32)
    ^bb5:  // pred: ^bb3
      llvm.br ^bb6(%37, %40 : i32, f32)
    ^bb6(%74: i32, %75: f32):  // 2 preds: ^bb5, ^bb7
      %76 = llvm.icmp "slt" %74, %arg2 : i32
      llvm.cond_br %76, ^bb7, ^bb8
    ^bb7:  // pred: ^bb6
      %77 = llvm.add %38, %74  : i32
      %78 = llvm.getelementptr %arg0[%77] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %79 = llvm.load %78 : !llvm.ptr<f32>
      %80 = llvm.fcmp "ugt" %75, %79 : f32
      %81 = llvm.select %80, %75, %79 : i1, f32
      %82 = llvm.fcmp "uno" %79, %79 : f32
      %83 = llvm.select %82, %79, %81 : i1, f32
      %84 = llvm.add %74, %15  : i32
      llvm.br ^bb6(%84, %83 : i32, f32)
    ^bb8:  // pred: ^bb6
      %85 = llvm.sub %8, %8  : i32
      %86 = llvm.lshr %0, %85  : i32
      %87 = llvm.sub %8, %7  : i32
      %88 = nvvm.shfl.sync  bfly %86, %75, %7, %87 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %89 = llvm.extractvalue %88[0] : !llvm.struct<(f32, i1)> 
      %90 = llvm.fcmp "ugt" %75, %89 : f32
      %91 = llvm.select %90, %75, %89 : i1, f32
      %92 = llvm.fcmp "uno" %89, %89 : f32
      %93 = llvm.select %92, %89, %91 : i1, f32
      %94 = llvm.sub %8, %8  : i32
      %95 = llvm.lshr %0, %94  : i32
      %96 = llvm.sub %8, %7  : i32
      %97 = nvvm.shfl.sync  bfly %95, %93, %9, %96 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %98 = llvm.extractvalue %97[0] : !llvm.struct<(f32, i1)> 
      %99 = llvm.fcmp "ugt" %93, %98 : f32
      %100 = llvm.select %99, %93, %98 : i1, f32
      %101 = llvm.fcmp "uno" %98, %98 : f32
      %102 = llvm.select %101, %98, %100 : i1, f32
      %103 = llvm.sub %8, %8  : i32
      %104 = llvm.lshr %0, %103  : i32
      %105 = llvm.sub %8, %7  : i32
      %106 = nvvm.shfl.sync  bfly %104, %102, %10, %105 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %107 = llvm.extractvalue %106[0] : !llvm.struct<(f32, i1)> 
      %108 = llvm.fcmp "ugt" %102, %107 : f32
      %109 = llvm.select %108, %102, %107 : i1, f32
      %110 = llvm.fcmp "uno" %107, %107 : f32
      %111 = llvm.select %110, %107, %109 : i1, f32
      %112 = llvm.sub %8, %8  : i32
      %113 = llvm.lshr %0, %112  : i32
      %114 = llvm.sub %8, %7  : i32
      %115 = nvvm.shfl.sync  bfly %113, %111, %11, %114 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %116 = llvm.extractvalue %115[0] : !llvm.struct<(f32, i1)> 
      %117 = llvm.fcmp "ugt" %111, %116 : f32
      %118 = llvm.select %117, %111, %116 : i1, f32
      %119 = llvm.fcmp "uno" %116, %116 : f32
      %120 = llvm.select %119, %116, %118 : i1, f32
      %121 = llvm.sub %8, %8  : i32
      %122 = llvm.lshr %0, %121  : i32
      %123 = llvm.sub %8, %7  : i32
      %124 = nvvm.shfl.sync  bfly %122, %120, %12, %123 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %125 = llvm.extractvalue %124[0] : !llvm.struct<(f32, i1)> 
      %126 = llvm.fcmp "ugt" %120, %125 : f32
      %127 = llvm.select %126, %120, %125 : i1, f32
      %128 = llvm.fcmp "uno" %125, %125 : f32
      %129 = llvm.select %128, %125, %127 : i1, f32
      %130 = llvm.icmp "eq" %24, %16 : i32
      llvm.cond_br %130, ^bb9, ^bb10
    ^bb9:  // pred: ^bb8
      %131 = llvm.getelementptr %18[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %129, %131 : !llvm.ptr<f32, 3>
      llvm.br ^bb10
    ^bb10:  // 2 preds: ^bb8, ^bb9
      llvm.br ^bb11
    ^bb11:  // 2 preds: ^bb1, ^bb10
      nvvm.barrier0
      llvm.cond_br %27, ^bb12, ^bb21
    ^bb12:  // pred: ^bb11
      %132 = llvm.sub %arg2, %24  : i32
      %133 = llvm.icmp "eq" %132, %16 : i32
      %134 = llvm.sub %132, %14  : i32
      %135 = llvm.udiv %134, %15  : i32
      %136 = llvm.add %135, %14  : i32
      %137 = llvm.select %133, %16, %136 : i1, i32
      %138 = llvm.srem %137, %2  : i32
      %139 = llvm.sub %137, %138  : i32
      %140 = llvm.mul %139, %15  : i32
      %141 = llvm.add %24, %140  : i32
      %142 = llvm.mul %26, %arg2  : i32
      llvm.br ^bb13(%24, %13 : i32, f32)
    ^bb13(%143: i32, %144: f32):  // 2 preds: ^bb12, ^bb14
      %145 = llvm.icmp "slt" %143, %141 : i32
      llvm.cond_br %145, ^bb14, ^bb15
    ^bb14:  // pred: ^bb13
      %146 = llvm.add %143, %15  : i32
      %147 = llvm.add %143, %3  : i32
      %148 = llvm.add %143, %4  : i32
      %149 = llvm.add %142, %143  : i32
      %150 = llvm.add %142, %146  : i32
      %151 = llvm.add %142, %147  : i32
      %152 = llvm.add %142, %148  : i32
      %153 = llvm.udiv %149, %arg2  : i32
      %154 = llvm.urem %153, %arg1  : i32
      %155 = llvm.udiv %153, %arg1  : i32
      %156 = llvm.udiv %150, %arg2  : i32
      %157 = llvm.urem %156, %arg1  : i32
      %158 = llvm.udiv %156, %arg1  : i32
      %159 = llvm.udiv %151, %arg2  : i32
      %160 = llvm.urem %159, %arg1  : i32
      %161 = llvm.udiv %159, %arg1  : i32
      %162 = llvm.udiv %152, %arg2  : i32
      %163 = llvm.urem %162, %arg1  : i32
      %164 = llvm.udiv %162, %arg1  : i32
      %165 = llvm.getelementptr %arg0[%149] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %166 = llvm.load %165 : !llvm.ptr<f32>
      %167 = llvm.getelementptr %arg0[%150] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %168 = llvm.load %167 : !llvm.ptr<f32>
      %169 = llvm.getelementptr %arg0[%151] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %170 = llvm.load %169 : !llvm.ptr<f32>
      %171 = llvm.getelementptr %arg0[%152] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %172 = llvm.load %171 : !llvm.ptr<f32>
      %173 = llvm.mul %155, %arg1  : i32
      %174 = llvm.add %173, %154  : i32
      %175 = llvm.mul %158, %arg1  : i32
      %176 = llvm.add %175, %157  : i32
      %177 = llvm.mul %161, %arg1  : i32
      %178 = llvm.add %177, %160  : i32
      %179 = llvm.mul %164, %arg1  : i32
      %180 = llvm.add %179, %163  : i32
      %181 = llvm.urem %174, %1  : i32
      %182 = llvm.urem %176, %1  : i32
      %183 = llvm.urem %178, %1  : i32
      %184 = llvm.urem %180, %1  : i32
      %185 = llvm.getelementptr %18[%181] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %186 = llvm.load %185 : !llvm.ptr<f32, 3>
      %187 = llvm.getelementptr %18[%182] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %188 = llvm.load %187 : !llvm.ptr<f32, 3>
      %189 = llvm.getelementptr %18[%183] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %190 = llvm.load %189 : !llvm.ptr<f32, 3>
      %191 = llvm.getelementptr %18[%184] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %192 = llvm.load %191 : !llvm.ptr<f32, 3>
      %193 = llvm.fsub %166, %186  : f32
      %194 = llvm.fsub %168, %188  : f32
      %195 = llvm.fsub %170, %190  : f32
      %196 = llvm.fsub %172, %192  : f32
      %197 = llvm.call @__nv_expf(%193) : (f32) -> f32
      %198 = llvm.call @__nv_expf(%194) : (f32) -> f32
      %199 = llvm.call @__nv_expf(%195) : (f32) -> f32
      %200 = llvm.call @__nv_expf(%196) : (f32) -> f32
      %201 = llvm.fadd %144, %197  : f32
      %202 = llvm.fadd %201, %198  : f32
      %203 = llvm.fadd %202, %199  : f32
      %204 = llvm.fadd %203, %200  : f32
      %205 = llvm.add %143, %5  : i32
      llvm.br ^bb13(%205, %204 : i32, f32)
    ^bb15:  // pred: ^bb13
      llvm.br ^bb16(%141, %144 : i32, f32)
    ^bb16(%206: i32, %207: f32):  // 2 preds: ^bb15, ^bb17
      %208 = llvm.icmp "slt" %206, %arg2 : i32
      llvm.cond_br %208, ^bb17, ^bb18
    ^bb17:  // pred: ^bb16
      %209 = llvm.add %142, %206  : i32
      %210 = llvm.udiv %209, %arg2  : i32
      %211 = llvm.urem %210, %arg1  : i32
      %212 = llvm.udiv %210, %arg1  : i32
      %213 = llvm.getelementptr %arg0[%209] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %214 = llvm.load %213 : !llvm.ptr<f32>
      %215 = llvm.mul %212, %arg1  : i32
      %216 = llvm.add %215, %211  : i32
      %217 = llvm.urem %216, %1  : i32
      %218 = llvm.getelementptr %18[%217] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %219 = llvm.load %218 : !llvm.ptr<f32, 3>
      %220 = llvm.fsub %214, %219  : f32
      %221 = llvm.call @__nv_expf(%220) : (f32) -> f32
      %222 = llvm.fadd %207, %221  : f32
      %223 = llvm.add %206, %15  : i32
      llvm.br ^bb16(%223, %222 : i32, f32)
    ^bb18:  // pred: ^bb16
      %224 = llvm.sub %8, %8  : i32
      %225 = llvm.lshr %0, %224  : i32
      %226 = llvm.sub %8, %7  : i32
      %227 = nvvm.shfl.sync  bfly %225, %207, %7, %226 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %228 = llvm.extractvalue %227[0] : !llvm.struct<(f32, i1)> 
      %229 = llvm.fadd %207, %228  : f32
      %230 = llvm.sub %8, %8  : i32
      %231 = llvm.lshr %0, %230  : i32
      %232 = llvm.sub %8, %7  : i32
      %233 = nvvm.shfl.sync  bfly %231, %229, %9, %232 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %234 = llvm.extractvalue %233[0] : !llvm.struct<(f32, i1)> 
      %235 = llvm.fadd %229, %234  : f32
      %236 = llvm.sub %8, %8  : i32
      %237 = llvm.lshr %0, %236  : i32
      %238 = llvm.sub %8, %7  : i32
      %239 = nvvm.shfl.sync  bfly %237, %235, %10, %238 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %240 = llvm.extractvalue %239[0] : !llvm.struct<(f32, i1)> 
      %241 = llvm.fadd %235, %240  : f32
      %242 = llvm.sub %8, %8  : i32
      %243 = llvm.lshr %0, %242  : i32
      %244 = llvm.sub %8, %7  : i32
      %245 = nvvm.shfl.sync  bfly %243, %241, %11, %244 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %246 = llvm.extractvalue %245[0] : !llvm.struct<(f32, i1)> 
      %247 = llvm.fadd %241, %246  : f32
      %248 = llvm.sub %8, %8  : i32
      %249 = llvm.lshr %0, %248  : i32
      %250 = llvm.sub %8, %7  : i32
      %251 = nvvm.shfl.sync  bfly %249, %247, %12, %250 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %252 = llvm.extractvalue %251[0] : !llvm.struct<(f32, i1)> 
      %253 = llvm.fadd %247, %252  : f32
      %254 = llvm.icmp "eq" %24, %16 : i32
      llvm.cond_br %254, ^bb19, ^bb20
    ^bb19:  // pred: ^bb18
      %255 = llvm.getelementptr %20[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %253, %255 : !llvm.ptr<f32, 3>
      llvm.br ^bb20
    ^bb20:  // 2 preds: ^bb18, ^bb19
      llvm.br ^bb21
    ^bb21:  // 2 preds: ^bb11, ^bb20
      nvvm.barrier0
      llvm.cond_br %27, ^bb22, ^bb29
    ^bb22:  // pred: ^bb21
      %256 = llvm.sub %arg2, %24  : i32
      %257 = llvm.icmp "eq" %256, %16 : i32
      %258 = llvm.sub %256, %14  : i32
      %259 = llvm.udiv %258, %15  : i32
      %260 = llvm.add %259, %14  : i32
      %261 = llvm.select %257, %16, %260 : i1, i32
      %262 = llvm.srem %261, %2  : i32
      %263 = llvm.sub %261, %262  : i32
      %264 = llvm.mul %263, %15  : i32
      %265 = llvm.add %24, %264  : i32
      %266 = llvm.mul %26, %arg2  : i32
      llvm.br ^bb23(%24 : i32)
    ^bb23(%267: i32):  // 2 preds: ^bb22, ^bb24
      %268 = llvm.icmp "slt" %267, %265 : i32
      llvm.cond_br %268, ^bb24, ^bb25
    ^bb24:  // pred: ^bb23
      %269 = llvm.add %267, %15  : i32
      %270 = llvm.add %267, %3  : i32
      %271 = llvm.add %267, %4  : i32
      %272 = llvm.add %266, %267  : i32
      %273 = llvm.add %266, %269  : i32
      %274 = llvm.add %266, %270  : i32
      %275 = llvm.add %266, %271  : i32
      %276 = llvm.udiv %272, %arg2  : i32
      %277 = llvm.urem %276, %arg1  : i32
      %278 = llvm.udiv %276, %arg1  : i32
      %279 = llvm.udiv %273, %arg2  : i32
      %280 = llvm.urem %279, %arg1  : i32
      %281 = llvm.udiv %279, %arg1  : i32
      %282 = llvm.udiv %274, %arg2  : i32
      %283 = llvm.urem %282, %arg1  : i32
      %284 = llvm.udiv %282, %arg1  : i32
      %285 = llvm.udiv %275, %arg2  : i32
      %286 = llvm.urem %285, %arg1  : i32
      %287 = llvm.udiv %285, %arg1  : i32
      %288 = llvm.getelementptr %arg0[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %289 = llvm.load %288 : !llvm.ptr<f32>
      %290 = llvm.getelementptr %arg0[%273] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %291 = llvm.load %290 : !llvm.ptr<f32>
      %292 = llvm.getelementptr %arg0[%274] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %293 = llvm.load %292 : !llvm.ptr<f32>
      %294 = llvm.getelementptr %arg0[%275] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %295 = llvm.load %294 : !llvm.ptr<f32>
      %296 = llvm.mul %278, %arg1  : i32
      %297 = llvm.add %296, %277  : i32
      %298 = llvm.mul %281, %arg1  : i32
      %299 = llvm.add %298, %280  : i32
      %300 = llvm.mul %284, %arg1  : i32
      %301 = llvm.add %300, %283  : i32
      %302 = llvm.mul %287, %arg1  : i32
      %303 = llvm.add %302, %286  : i32
      %304 = llvm.urem %297, %1  : i32
      %305 = llvm.urem %299, %1  : i32
      %306 = llvm.urem %301, %1  : i32
      %307 = llvm.urem %303, %1  : i32
      %308 = llvm.getelementptr %18[%304] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %309 = llvm.load %308 : !llvm.ptr<f32, 3>
      %310 = llvm.getelementptr %18[%305] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %311 = llvm.load %310 : !llvm.ptr<f32, 3>
      %312 = llvm.getelementptr %18[%306] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %313 = llvm.load %312 : !llvm.ptr<f32, 3>
      %314 = llvm.getelementptr %18[%307] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %315 = llvm.load %314 : !llvm.ptr<f32, 3>
      %316 = llvm.fsub %289, %309  : f32
      %317 = llvm.fsub %291, %311  : f32
      %318 = llvm.fsub %293, %313  : f32
      %319 = llvm.fsub %295, %315  : f32
      %320 = llvm.call @__nv_expf(%316) : (f32) -> f32
      %321 = llvm.call @__nv_expf(%317) : (f32) -> f32
      %322 = llvm.call @__nv_expf(%318) : (f32) -> f32
      %323 = llvm.call @__nv_expf(%319) : (f32) -> f32
      %324 = llvm.getelementptr %20[%304] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %325 = llvm.load %324 : !llvm.ptr<f32, 3>
      %326 = llvm.getelementptr %20[%305] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %327 = llvm.load %326 : !llvm.ptr<f32, 3>
      %328 = llvm.getelementptr %20[%306] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %329 = llvm.load %328 : !llvm.ptr<f32, 3>
      %330 = llvm.getelementptr %20[%307] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %331 = llvm.load %330 : !llvm.ptr<f32, 3>
      %332 = llvm.fdiv %320, %325  : f32
      %333 = llvm.fdiv %321, %327  : f32
      %334 = llvm.fdiv %322, %329  : f32
      %335 = llvm.fdiv %323, %331  : f32
      %336 = llvm.getelementptr %arg4[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %332, %336 : !llvm.ptr<f32>
      %337 = llvm.getelementptr %arg4[%273] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %333, %337 : !llvm.ptr<f32>
      %338 = llvm.getelementptr %arg4[%274] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %334, %338 : !llvm.ptr<f32>
      %339 = llvm.getelementptr %arg4[%275] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %335, %339 : !llvm.ptr<f32>
      %340 = llvm.add %267, %5  : i32
      llvm.br ^bb23(%340 : i32)
    ^bb25:  // pred: ^bb23
      llvm.br ^bb26(%265 : i32)
    ^bb26(%341: i32):  // 2 preds: ^bb25, ^bb27
      %342 = llvm.icmp "slt" %341, %arg2 : i32
      llvm.cond_br %342, ^bb27, ^bb28
    ^bb27:  // pred: ^bb26
      %343 = llvm.add %266, %341  : i32
      %344 = llvm.udiv %343, %arg2  : i32
      %345 = llvm.urem %344, %arg1  : i32
      %346 = llvm.udiv %344, %arg1  : i32
      %347 = llvm.getelementptr %arg0[%343] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %348 = llvm.load %347 : !llvm.ptr<f32>
      %349 = llvm.mul %346, %arg1  : i32
      %350 = llvm.add %349, %345  : i32
      %351 = llvm.urem %350, %1  : i32
      %352 = llvm.getelementptr %18[%351] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %353 = llvm.load %352 : !llvm.ptr<f32, 3>
      %354 = llvm.fsub %348, %353  : f32
      %355 = llvm.call @__nv_expf(%354) : (f32) -> f32
      %356 = llvm.getelementptr %20[%351] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %357 = llvm.load %356 : !llvm.ptr<f32, 3>
      %358 = llvm.fdiv %355, %357  : f32
      %359 = llvm.getelementptr %arg4[%343] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %358, %359 : !llvm.ptr<f32>
      %360 = llvm.add %341, %15  : i32
      llvm.br ^bb26(%360 : i32)
    ^bb28:  // pred: ^bb26
      llvm.br ^bb29
    ^bb29:  // 2 preds: ^bb21, ^bb28
      llvm.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -1 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S0} : () -> index
    %2 = "disc_shape.dim"() {name = @S1} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = dense<[1, 2]> : vector<2xi32>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscStripShapeConstraintOpsPass (disc-strip-shape-constraint-ops) //----- //
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c8 = arith.constant 8 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c1024 = arith.constant 1024 : index
    %c6 = arith.constant 6 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %0 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "cpu", call_target_name = "ral_recv_input", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
    %2 = memref.dim %0, %c0 : memref<?x?x?xf32, "gpu">
    %3 = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
    %4 = arith.index_cast %2 : index to i32
    %5 = arith.index_cast %1 : index to i32
    %6 = arith.muli %4, %5 : i32
    %7 = arith.index_cast %6 : i32 to index
    %8 = memref.alloc(%2, %1, %3) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    %9 = arith.cmpi slt, %7, %c3 : index
    %10 = arith.cmpi sge, %3, %c1024 : index
    %11 = arith.cmpi slt, %7, %c6 : index
    %12 = arith.andi %11, %10 : i1
    %13 = arith.cmpi sge, %3, %c512 : index
    %14 = arith.cmpi sge, %7, %c6 : index
    %15 = arith.andi %14, %13 : i1
    %16 = arith.ori %9, %12 : i1
    %17 = arith.ori %16, %15 : i1
    cf.cond_br %17, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    gpu.launch_func  @main_kernel::@main_kStitch_divide__13_1_0___1b1r blocks in (%7, %c1, %c1) threads in (%c256, %c1, %c1) args(%0 : memref<?x?x?xf32, "gpu">, %8 : memref<?x?x?xf32, "gpu">)
    cf.br ^bb3
  ^bb2:  // pred: ^bb0
    %18 = arith.cmpi eq, %7, %c0 : index
    %19 = arith.subi %7, %c1 : index
    %20 = arith.divui %19, %c8 : index
    %21 = arith.addi %20, %c1 : index
    %22 = arith.select %18, %c0, %21 : index
    gpu.launch_func  @main_kernel_0::@main_kStitch_divide__13_1_0___1w1r blocks in (%22, %c1, %c1) threads in (%c256, %c1, %c1) args(%0 : memref<?x?x?xf32, "gpu">, %7 : index, %8 : memref<?x?x?xf32, "gpu">)
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    "disc_ral.dispatch"(%arg0, %c0, %8) {backend_config = "cpu", call_target_name = "ral_send_output", has_side_effect = false} : (!disc_ral.context, index, memref<?x?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\00\E8\19\00\00\00\00\00\00\02\00\01\01@\00\00\00\A8\19\00\00\00\00\00\00\A2\19\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8:\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\002@:\00\01\00\117\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0D\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F5)_shndx\00.nv.info\00.text.main_kStitch_divide__13_1_0___1b1r2\00\0F,\00\15oshared.\00\15\9Fconstant21\00\1D\1F01\00\12\FD\01debug_frame\00.rel\11\00\1Aa\12\00!nv&\00oactionV\01 \0F\94\00\0F\0Fy\01\A2a__ocg_3\00/\00$0\00\0F\FF\16$__cuda_sm70_shflsync_bfly_p\00$____wg_G\00\0F@_0_$\0D\00/265\00\19_1__281\00\19\122f\00/305\00\19_3__32\91\02\1Fo_param\98\02.\0F\01\00\06\8CU\00\00\00\03\00\0B\00\01\00\11\AA\18\00,\0C\00\01\00\11\D8\18\00,\09\00\01\00\A1\15\01\00\00\22\00\0B\00`,\0F\001\00 \01\07\000\00!\020\00,\0A\00\01\00\11Y\18\00,\04\00\01\00\11\89\18\00,\07\00\01\00f2\00\00\00\12\10\A8\00!\80-\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13P\C0\00\10\04\9B\00R\044\00\00\00E\005\04\D4\0A<\00&8\00H\00\04p\00L\88\80\80(o\00\10\08\15\00x\16\89\80\80(\09\05J\01\004\00\22\18\00\01\00\22p\00\01\00\048\01\13\108\01\90\04/\08\00\08\00\00\00$\0C\00%#\08*\01'\04\12\0C\00\17\11\0C\00\12#0\00\04$\00\17\08$\00\05\0C\00!7\04$\06\90\015\00\00\04\0A\08\00\05x\00\A3\01\18\00\03\19\18\00\04\17\0CH\06U\10\00\00\F0!\10\00\10\01>\00%\F0\11\10\00\01\01\00\F09\F0!\00\03\1B\FF\00\04(X\00 \12\00\00p\12\00\00\B0\12\00\00\00\13\00\00`\13\00\00\90\14\00\00\D0\14\00\00\10\15\00\000!\00\00`!\00\00\80!\00\00\A0!\00\00\C0!\00\00P\22\00\00p\22\00\00\90\08\00\F4\16*\00\00\E0*\00\00`+\00\00\B0+\00\00\F0+\00\000,\00\00\04\1C\08\00@(\00\00@*\00\00\04\1E\E6\00C\00\00K\00\01\00\94\02\02\08\10\0A/\22\00\00\DF\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\E0\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00@\00\00\00\14\FB\01\22\C8\00\01\00\13\02\D1\00*H\00\10\00/\04\00\01\00\FF|@$v\01\FF\D7\04\C1\FF\00\8E\07\00\C4\0F\00\19y\02\00\01\00\F1\00!\00\00\00\22\0E\00\B9z\04\00\00F\00\00\FB\04\90\E2\0F\00Ey\00\00\10\0D!\00 \80\03\10\00\81$t\0C\FF\00\00\80\FF@\00\F3\05\E2\0F\00\10z\00\02\00Z\00\00\FF\E1\FF\07\00\E4\1F\04\0C\10\00\A0pR\F0\03\00\E4\0F\00\10xl\00\F0\07\00\00\FF\E0\FF\07\00\C8\0F\00\12x\03\00\00\FC\FF\FF\FF\C0\8E\07 \002\19y\00\01\00\10%\9D\00\83\0E\00\07r\03\03\FF\00\01\00\000\000r\03\03\A2\001\FF\FC\8E@\00P\0Cr\00\02\03\80\04\D3b\F2\03\00\DA\0F\00G\19\00\00`\0C\A0\00a\EA\0F\00\10r\04 \00\01\90\00\01\C0\00F\01\00\F0\06\C0\001\1Cx\00\01\00?p\F0\F0\D0\00\02@\0Cx\00\04?\062pB\F4 \00Ar\0E\FF\FF\D0\02\C6\8E\07\00\D8\0F\00G\A9\00\00\90\06p\00\05P\00\14\E1\F0\00a\0D\03\00\F4\FF\FF\F0\00b\C4\0F\00\02x\10\04\03\12\0FP\01@$z\0B\00 \01 \0E\02\D0\00\B0\1F\00%v\04\0B\00X\00\00\10\10\00a\CA\0F\00\81y\0F\C3\05\E3\00\19\1E\0C\00\A2\00\00\10x\07\0B\00\01@\01\93\E4\0F\04\10x\09\0B\00\02\10\00z\C6\0F\00%v\06\07@\00%\12\06@\00u\E2\02\00%v\08\09 \00\95\E2\0F\08\10x\0B\0B\00\03P\00v\00\10x\13\0E\00\04\10\00E\81y\11\08@\00fb\09\00%v\0A\A0\00\10\C8\C0\00\11\15\C0\00\12\13P\00U\00\81y\14\0A0\00\10f0\00&\04\15p\00e\1F\04\10x\17\15\C0\00\10\C80\00\16\13\E0\00 b\01\C0\00\16\170\00\88/\00\10x\1D\15\00\020\00\16\16\D0\00zb\03\00%v\1C\1D\D0\00*\1F\15\D0\00E\09\0E\00\08\10\00Z\01\81y\15\1C\D0\00%\1E\1F@\00\01\D0\00\11\19\D0\00\15\09\D0\00%\18\1E0\00\10f`\01' \19p\00\00\80\01\1A\19\D0\00&\17 `\00\1C\0Fp\01Z\00\10x\0B\19\D0\00\1F\1A`\01\07\01\D0\00&\05\19\D0\00\10\1F\00\024\0E\00\0C\10\00j/\00\81y\19\0A@\01%\04\05@\00\01\D0\00\11\1B\D0\00\15\07\D0\00\16\1Cp\01z&\03\01%v\06\1B\D0\00%\1D\1B\D0\00\01@\02\16\07p\01\11\22\D0\00\1A\1D\D0\00H\1D\1B\00\020\00*\08\080\00\17\0A0\00\00\C0\00H\1B\1B\00\030\00*\0B\0A0\00(\04\1B\00\02Q\0Br\00\0F\0F_\03\A1\F4\03\00\E4O\08\0Br\00\0C\10\00\B2\C0\F6\03\00\D6\0F\00\08\A2\0F\0F\D2\07\22\80\05\B0\02\16\0C\E0\00 \A2\0E@\00%\12\12@\00\10\8F@\00!\11\11\10\00\10\F6\A0\03\10\02`\00\01 \00P\C0\F8\03\00\D2P\00!\12\12`\000\00\00\06P\00U\0Br\00\14\14@\00\01\10\00$\12\110\00q\C8\0F\00\08\B2\11\11@\00\060\00&\13\13`\00\00\10\00(\11\140\001\A2\14\14@\00\060\00*\16\16`\00)\14\13`\00!\13\13@\00\060\00*\15\15`\00)\13\16`\00!\16\16@\00\060\00*\18\18`\00)\16\15`\00!\15\15@\00\060\00*\17\17`\00)\15\18`\00!\18\18@\00\060\00*\1A\1A`\00)\18\17`\00!\17\17@\00\060\00*\19\19`\00)\17\1A`\00!\1A\1A@\00\060\00&\1C\1C`\00\10\01p\00\19\19`\00!\19\19@\00\060\00*\07\07`\00)\19\1C`\00!\1C\1C@\00\060\00*\08\08\C0\00)\1C\07`\00!\07\07@\00\060\00*\0B\0B`\00)\07\08`\00!\08\08@\00\030\00f\10x\0E\0E\00\10\90\04\00p\00\19\0B`\00!\0B\0B@\00\030\00Q\0Cr\00\0E\0D\D0\06\14\F8`\00*\0C\0C\00\03)\0B\0C\00\03!\0C\0CP\00\00\00\03\90\E2\0F\00G\C9\00\00\90\F9\9A\0B\11\83\90\06#Ay\89\0B\03\A0\06@$x\04\03\12\005\0E\0A\8E\10\07#p\03 \00\13\E8\F0\06\13\04\F0\06\12\DA\E0\06'@\03@\00St\14\FF\04\000\08\10\E4`\04\1C\0A\C0\06\10\0A\C0\033\14\02\8E\E0\03\1B\0D\C0\06%\06\0A`\04\01\C0\06+\08\0A\C0\06\1A\06@\00\1C\10\C0\06\15\08 \00\01 \05&\0A\0A \05\00p\01\1B\11\C0\06\1C\0F`\05\06\A0\00\01 \05\11\11\C0\00\15\11 \05\16\12\90\04\10f`\06&\04\11p\00\00\C0\06*\13\11 \05*\05\04\F0\05&\06\130\00!/\000\00\09 \05+\06\06 \06\070\00j\0F\01\10x\11\11 \05\1C\09P\05\07\90\00\01 \02\12\0D\8F\09\15\F0 \05\01\10\00\22\C0\F4 \021\82\0D\0D0\02$\00\00 \05\1B\0A \05%\10\10@\00*\8F\08p\05\01 \05$\0D\10p\02\00 \05\22\82\10\FF\09\03p\05\06`\05#\F0\03\10\00$\10\0F0\00\12\C8\A0\05\1A\100\00&\05\05`\00\16\00\80\05\030\00\14\82\80\05\050\00*\06\06`\00)\12\05`\00:\05\05\120\00*\09\090\04)\05\06`\004\06\06\050\00\11\C80\00\11\060\00#\C0\F0P\00!\09\090\00\00\AB\0C\1F\E4\D0\09\01\18\0EP\08\00@\00%\0C\0C\10\04\01\80\01(\09\0C\80\011\B2\0C\0C`\00\00\80\01\1E\C4\90\03\00\F0\03\00_\03Ap\16p\00p\03V\89\00\00\A0\01 \000\02x\0B\CF\02\070\0A\0Fp\03\03\15\0B\C0\07\07`\03\0C\A0\02%\A2\0E`\03\030\00\19\00\80\03\0C\A0\02%\E2\0Ep\03\060\00\07p\03\0F\A0\02\03\06\90\00\10\CC \00(\0A\0A\F0\02\06\00\02\06\B0\02\05\D0\01\09\F0\01\BF\F4\03\00\D2\8F\00\08\82\05\05\0C\E0\01\02\1F\F0\E0\01\03\17\A2\E0\01\020\00*\0A\0A\D0\02(\06\090\00\14\82\F0\01\05\10\02)\09\0A\10\02!\0A\0A0\00\00\10\02\10\CA\00\0C\11\0C\00\0C\13\0A0\0D\14A\B0\0C\04\D0\010z\00\03\A0\01Dpb\F4\030\0D#\B0\04 \00\10\E2\F0\0C0\05\02\1F@\05\00\F0\0C\96\D6\0F\00G)\00\00\80\04@\001$r\07`\00'\03\00\A0\05\16\02@\00\09\B0\0C\10\C6p\0D%\04\07p\0D\13\C8\C0\05\10\03\C0\05\12\F6\C0\05V\B9\00\00 \02`\00\10\02\D0\0D\13ZP\02\0F\E0\02\02F\04\04\00\FD\B0\0C;$t\06\F0\05\16\08P\0A%\C8\1F \023\06\02\8E\10\02*\0D\08\F0\05%\0B\07\90\02\01\F0\05(\0F\07\F0\05\04\00\0D\010\02a\E4\0F\04$z\0F\10\00\12\0F\10\00\05\10\0B\13\06\B0\05E%v\0E\0F\10\00\1B\E4`\02 \E2\0E\00\066\07\00\03`\00E\81y\0F\0E \00\16$\C0\00\1F\11\C0\00\01\09P\05\05\D0\03$\0C\0D\B0\07\01\D0\03\05P\05\12\06\80\00+\0C\08P\05\03p\02\14\F8P\05\12\0DP\02\11\FA`\01\06`\05\01\F0\02U\01\10x\07\07\B0\06q\CE\0F\00\08\C2\0A\0Ap\00Q\00\80\06\00\C80\00$\0A\0F@\00\00\90\02D\B2\0F\0F\0A \00\020\08!\07\040\08*\FA\03\90\04\14\F8\C0\00$\0F\0C\F0\02\00\C0\00\22\C2\0C@\0B\040\08_\D9\00\00\10\FE0\08\07\09`\02\03\90\02\140\A0\04\13\E6p\02\1A\01p\02(\00\010\08\1F\0A@\02\12\1F\0A@\02\02\1A\0E@\02\1B\CA0\02\16\C8 \02\1D\0A\C0\01\1B\F0\C0\01\1B\F6\10\07/\80\05\10\07\01\0E\10\03\16\07\B0\02\09`\01\05 \02)\0D\0C \02\12\0Cp\00\00 \02\0F\F0\05\01!z\00P\01\08\F0\05(\80\00\F0\05\1B\09\F0\05\1B\060\01\11\06\C0\05\15\090\01\16\09\B0\05+\A4\0E \05\02\00\01\19\09\00\014\09\09\0C\00\01\07\D0\04\1F\09\D0\04\04\F1\00\89\7F\07\0C\00\1F \0C\00\00\0E\00\00b\0E\D0\04&`\03\90\04\06p\0B\00p\00\11/p\00\19\07p\00(\07\07p\00r\89\7F\04\07\00\1F@P\00\11d\B0\00*\04\04@\00)\07\04@\009\04\04\07@\00X\09\04\00\1F\80@\00*\09\09@\00\1C\04\F0\00\17\04\F0\01@\19x\04\FF \06@\02\16\01\00 \04\87\89\7F\06\09\00\1F\00\0DP\00*\06\06P\00)\09\06P\007\06\06\09P\00@\0Cr\00\05@\01!pR\D0\03\0DP\12\81\89\7F\0B\06\00\1F\00\0E`\00ft\0E\00$\B4\07\10\03\00\80\0A\10\B6\EF\02\12\80\E0\01\00 \03\12t\00\06#\FF\00\D0\02\14\B2p\0C\12\F8\90\00G\B2\00\06\0BP\043\1C\B8\00p\00Ap\02\00\E4\90\03\10\050\01 p@0\00`\0F\04\19x\05\05\FF\05\10\FF\C5\00\01\C0\094\0B\0B\06@\04\12\E2\E0\05\17\07p\001\0Cx\00\A0\061p@\FA`\06#\88\B3\B0\0C\00\A5\0Cf\E8\0F\00\1D{\00\01\00\89\EC\0F\00GY\00\00@\B0\08\00\9E\04$\80\FF\C0\021\82x\06\F0\13\01\01\00\93\CA\0F\00\84\C9\0A\05\00\04P\00\81\A2\02\00Gy\00\06\C2Q\0E\22\80\0Bp\02C\09\0A\00\07p\02/\A4N\D0\02\01\02\90\07%\C0\FC\E0\01\02\10\05\11\00\10\03c\89\7F\08\09\00\07`\02'\A4\0E\E0\0D\05@\00)\09\08@\00!\08\08P\00\05@\00C\0C\08\00\07`\02a\E2\04\00$r\0B\90\011\08\00\8EP\06$\0B\B2\00\04\11\FC\E0\05\09\F0\01\0A\90\01\01\10\00\00\C0\01\14\0Cp\00\10\DAp\00\04\B0\0D\02p\00c\88\B3\00\FF\0C\00\10\01-\E4\07\80\03\22Hyh\19\01\10\00\1E\E4\80\01T\84y\08\FF\00@\00\22\A2Np\08\07\A0\03!\02x\DE\00\14\80\90\01\00 \15&\80\03`\00B$t\09\FF\B1\01\03\80\02&r\0A\E0\14\10\C6@\00\1B\13\90\04\02\80\05\15\0A\90\04\11\0C\80\05\02\00\14[\8F\04\10x\0E\A0\14\19\10\A0\14J\81y\0D\0C`\0A&\0E\0E@\00\00p\05\16\12\90\14\01\10\0E%\10\10 \00<\E4\0F\08\D0\07G%v\12\12 \00\00\B0\14\15\10 \00\10h\10\00%\13\12\10\00\B4\A2N\00$t\16\FF\9D\98\BB;\F0\02\01\D0\0A\04\80\07\01\10\03a\15\FF\00\00|C \00\11\C6\E0\03!\0A\03`\07\02\D0\03c!r\14\08\0D\00\E4\06`\C8\8F\00#t\0B\9F\112?\16  \01T!r\0D\08\0F \00\D2\C6\0F\01#t\0B\0B\01\00@K\15@ \00E#t\0C\0D0\00\D4\C6\0F\00!t\0F\0B\7F\00@\CB\00\00@\004\11\08\11@\00u\E2\0F\02#t\0C\0C@\00\D4\E4\0F\08#x\0F\14;\AA\B8?\0F\08P\00'\10\11\80\00e\08!r\13\08\13@\00hO\00!t\0E\0C`\00\00@\00e`p\A52\0F\00@\00\17\10\A0\00W\08#t\14\13P\00P\00#x\0E\0Dp\00\12\0Ep\000\04$x\FF\02\15\800\04I!t\12\10`\007t\14\14P\00P\00#x\0D\0Dp\00\13\0E \00T\08s\0F\00\0F\B0\02p\22\0E\00#x\12\11`\00#\12\08\00\018t\0E\14P\00E$x\10\10p\00!\C4\0F0\00\00P\00$\12\00`\00%\0E\13\A0\00`\E4\0F\04\08s\0D\CD\05\02`\00\10\A4`\00(\13\13\80\00T\19x\0E\0B\17\10\05\10\CA\90\004\11\00\120\00\81\E2\0E\00#r\0F\0E\0F\10\07\00\10\00e\1F\00\19x\09\140\00\10\CC0\008\16\00\13\C0\00Cr\0D\0C\0D\1F\03q\00\C8O\00#r\10\91\01#\0D\00 \020r\09\09`\13\13\10P\00\01\E0\0D\18\FCp\09\0Ap\07\00\C0\03&\90\07\C0\03\00 \0C&p\07 \00\04\90\04\08 \0C&\10\040\00\0E \0C+\0A\0B \0C\12\0A \0C\02\90\04XG\E9\00\00\C0\10\041v\0A\FF0\00\03\E0\01\09\10\09\13\C80\03\07 \0C;\02x\12 \04\16\0C\A0\09k\E2\9F\00\10x\0F\10\04\16\11\10\04p\E2\0F\04%v\0C\0C\C0\03$\12\02\90\03\19\13\00\04\0B\10\0C\1C\04@\04\09\C0\11\16\E4 \0C\22\12\02`\07E%v\10\11\10\00\0F@\04\001$z\13@\00\0F@\04\02\10d@\00%\12\13@\00\1F\CCP\04\11\11\0B\B1\0F\03\E0\00L\00$t\17P\04!\0B\0AP\04/\FC\03P\04\03\16\0CP\044\C8\0F\00\00\04$\17@ \04\19\15`\04:!t\0F\E0\03:t\0D\15\10\04\0AP\04E\19x\0C\0C\D0\02\01@\03\09P\04W\04#t\0D\0Dp\00U\08#t\0E\11P\00\03\80\04\08@\04\0Ep\04'\10\0D\90\00X\04#t\0E\0EP\00H$x\0D\0D@\04\0Ep\04 \10\15\90\00\01~\1Ex\E2\0F\04!t\12\0E\E0\00\0D@\04\03p\04&\17@@\00\00\A0\00\02p\03\1A\0F`\04\10\C4`\04\18\16`\04F\04\19x\14\E0\03\1E\E2`\04T\08s\10\00\10p\00\10\A2P\04\11\16p\04#\16\08\A0\01!x\16`\04\22\16\00\D0\05;\08s\12P\04(\0C\0CP\04\10$\90\1A\04\00\01\03P\04\19\16P\04\11\0Cp\13\15\0CP\045\09\09\12\10\00\01P\04'\14\16P\00\00\C0\03/p\FC\C0\0D\09\07\00\04\040\04\19\02\C0\0D\010\12\08\10\04'\E0\01@\00\08\C0\03\01\D0\02\1B\0E\D0\0D\07\F0\03/\E4\9F\C0\0F\00\02\E0\03\1B\0E\C0\0F\15\0Ep\03\07\10\08*\E8\0E\B0\0F\10\22p\00\19\13`\03 \02x\1F\03!|C\05\02\1B\E4\A0\04\13\C4\80\03\06`\04G\00!r\0A`\03\10\CF\F0\06\00\7F\034?\13 \C0\02\1B\12\B0\07\02\10\07\14\14P\025t\13\120\00\01\B0\07*\11\10\E0\02(\13\130\00\09\B0\06\10\E4 \02 \11\0A0\02\22\11\08 \00E!t\0D\13@\00\12\C6 \00\00@\02\22\11\00 \00@#x\0D\120\00$\0D\08P\00%\0A\13P\00\12\C6 \00\000\00\13\0D0\00\00\C0\06\14\11@\0290\0E\00\00\07\00\B0\02Dr\09\10\110\02 \C8\1F\10\00$\0A\0D\10\00/\C6O\A0\0E\01\1C\0B\A0\0E\17\F0 \00?\02x\0D\90\14\02\15\0B\10\0A\02\00\14\15\0D0\05\0A\E0\1B6$t\0D\D0\01\00@\02&t\0F \05\01\90\015\08\08\0B\B0\01PO\01#t\0C\CA\005?\0D  \05\01\10\05\22\0F@\80\03J!t\0D\0C\D0\03\07\E0\04\020\01\15\08P\01\12\C8\10\00\05@\01*\CC\0F0\01\11$0\01\15\0C \01-\CC\1F\A0\07\01`\0C\04\10\0F\03p\0B&\80\01\80\07%!rP\0C@\00\00\CAO\00\0E6\08\00\1F\90\0C2!r\0B\E0\00\05 \00%\0A\0B\D0\0E\01 \00\11\0A \06\06 \00%\0D\0A\A0\0E\01 \00\13\0C \12\00 \00\10\CF \00\15\0C`\0E\00 \00\12\B2\90\08\04@\00c\88\B3\00\04\0D(\89#\0F\C0\0D\04\17\B0\E0\006$t\04\10\0C\0F\C0\0D\02C\04\05\00(0\01!\E2\04\C0\0D(2\09\C0\0D%\05\04\C0\0D\93d\EE\00!r\05\05\04\00\01\00 \CA/p\0F\15\05\A0\0D\11d\F0\008\05\06\00 \00%\07\0A\80\0D\8A\A4\02\00!\B2\07\07\0A\D0\003\FF\07$\80\00/\E4\050\0D 4\05\FF\00@\00!b.\C0\01&0\05\10\05!\84y\00\01\05\C0\05\000\0D(\00\05 \01\08\10\15Q\1F\00$z\0D\00\03\15\02\00\03 \0E\0D\00\03\15\06\90\17%\07\0D@\05 \E4O0 \15\0D\A0\04\0C\A0\14\01\A0\17'\10\07@\00\00\D0\04\18\0D\10\09E%v\12\09 \00\01\D0\08*\11\10\10\0D'\14\0B \00\0A\B0\08\01\10\0D%\15\14\10\00p\E2\8E\00\02x\16\00\90\03\05\D0\03\08\B0\08\00\80\10\10x\FE\00\D0\80\00\00\E2\F2\03\00\E2/\04 x\18\10\00A>\00\00@p\03\03 \004~\00B\80\05C\02\02\00\04\B0\00`\E4\0F\00\08r\181\00\02\01\00P\CE\0F\00 \98\8F\1E#\80K@\00\00\D0\02&\04\0F\F0\03X\00#t\08\0C\10\08T!r\0F\04\11 \00\01\A0\05(\08\08\F0\07V!r\13\04\13 \00W\02!t\11\08\10\04W\00#t\0A\0FP\00Z\08#t\12\13\D0\08)\11\0C\A0\05Dr\15\04\15P\00\10\E2p\04%\0A\0Ap\00\010\00)\0E\0C\B0\05:t\0C\120\09\18\11\80\09\00\10\09\18\0A\A0\04\10$@\0B\04\D0\05\01 \00\1B\12\C0\09'\14\11P\00\00\D0\08\17\0F\10\09@\00\08s\0E\1A\03\060\08:x\12\13@\0D\09\E0\08\01@\00\08\10\09\0E\00\0A*\12\13P\0D%\16\15\E0\08\01P\05(\0F\080\00\00p\06\14\10\80\00xb\0E\00$x\14\14\D0\00\00@\00\00P\00\22\16\000\00\01\00#\13\\\90\02\01p\0D7\13\00\12\A0\06R r\0E\0F\0E\EA/\01\A0\06E \08\0E\0E`\02\10\C60\00\19\15 \09T r\0A\0A\110\00u\E2/\00 \98\0E\0EP\02z\C6\0F\00 \08\0A\0A@\00C\17\00\18\00\81\0D\10b@\005\0C\0C\13@\00jO\00 \98\0A\0A@\00%\0C\0C@\00\84\E2\0F\00 r\14\14\150\00u\C6\1F\00 \98\0C\0C0\00\00 \008\08\14\140\00\06\90&\02\80\0DI \98\14\140\00#r\15\82*!@\00P\035r\11\17\00\01\010\01%\0C\0D0\01\010\006\13\17\0A \00d\04 r\17\17\14\10\00\02\F0\0C\17\070\00P\08\86y\00\0C \01a\04\19\10\0C\00\E6P%\18\0BP\00T\86y\00\0E\13 \00 \E8\01\10\00*\08\15\10\00$\06\17\10\00!\E2\01\00\08(\00\FB`\0A\0Ap\059M)\00\10\00\0E0\04/\02\050\04\03\22\F4\03\90\0AF\00\00\B0\01\80\050\08r\021\00\02\82\03\010\04%\02\02@\01\00\00\19 s\0C?\05\02\D0\01/f\100\17\00\11\02\90\05\15\03\90\05%\06\020\17*\CCO\D0$k\A2\0E\00$t\08`\0A\00\9E\07\06`\0Ac\10x\03\03\00\01\D0\04\18\C8\E0\1B\13\F0\F0\075\05\04\070\04\12\CFp\00\13\\p\00\00\80\08 t\08\CF\003?\08 \80\08\03\C0\04\22\0B@\10\00J!t\0A\08 \04%\08\08P\03\01\C0\08 \0A\05\90\03#\0A\08@\000x\0A\05`\03\01\14\02\01\10\017\05\00\0A\C0\081 r\05q\00\05P\03&(\05\80\01\10\C8P\01(\05\05P\01E r\05\0C0\00!\CA/\00\02\1B\05\00\02/\90\FE\00\02\08\0B\10\00\00\E0\09\00\0B\01\06P\01C\08\00\80*\A0\01T\C4\0F\00Dy`\0CP\C0\03\00\EA\7F\A0\15\15\0A \15\00 \0EGt\0D\FF\02@\00\06p\19\01`\15Q\04\08r\0A\0C\F0\00\00@\150\E4\0F\04`\00\14\F0`\00\01p\06\05 \00\05p\00\15pp\00\17\0Fp\00\01P\00\00\10\16\06\D0\15\10\C4@\006\0B\0C\0A\00 \19\04\F0\0A\12\E4 \00\03\C0\09\00@\00\01\E0\00#p+ \00\16\C6@\15\13\0B\B0\12\00\80\00&\F0\00\80\00\10G\F0\02\16\E9P\01\040\00\11\04\F0\00\11\8F`\0B\17\01p\00\00`\00*\C0+@\01&\A0\00@\01c!r\0A\04\0C\00\01\00\01\C0\07\0A@\01\00\80\01*\00,@\00\17`\90\002!r\0A\10\01\07@\00\1B\04@\00\1B@@\00\17 @\002\02r\07\9C\0E\12\0F@\00\00\E0\00&@\F6\E0\00\03\80\16\05 \00AHs\00\000\03\000\02Q\E8\0F\00\89s\D0\0AA\0C\06\00\0E\C0\02`Py\00\08`\D3@\00Q\C3\03\00\EA\1FP\00#\F0\FFP\00f\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-\11\01\89\02\0E\01\00\22@\00\01\00=V\01\000\00\08\01\00\1F\0B@\00\04\13\96)\00?\98\02\00@\00\0A\00L\06.\02\00\01\00\1105\03\02h3\13\00$\00\13\07\A7\04\00\01\00\03\14(>\00\17\01T\00\00\01\00*\08\05@\00\0F\80\00\05\1A)\7F2\05\01\00\13\E0@\00\11T\06\00\06\E4\00*\04\00\01\00\1F[@\00\0414\06\00\01\00&\C0\00@\00\1F\0B@\00\00!G\01\18\00\0D@\00/\F8\06\C0\00\03\1B\08\08\00#$\01l\0E\0B\01\00\12\D0%\03\03l\03\02\80\00\17\048\00\22\10\00\01\00\13\B5\14\01\0C\84\01*\F0\07h\00\1F\00\C0\00\04\1F\E6@\00\04\13\F8@\00?x\01\00@\00\0A\132@\00\15\06\92\00\02\01\00*\80\09\E04\12\03\8C\00:$\80\00\01\00\13\87\A4\00+\03\00\01\00\03\F09/H\00\80\00\0B\13\06\9F\0D\05(:\0B\01\00*\A8\00\08\00\04\10\01\13\018\00\04(\01\0C\01\00*\10/\08\00\088\00\17\06\A0\00\0F\01\00\05\04\B8\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_0() {addr_space = 3 : i32} : !llvm.array<32 x f32>
    llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_1() {addr_space = 3 : i32} : !llvm.array<8 x f32>
    llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_2() {addr_space = 3 : i32} : !llvm.array<32 x f32>
    llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1b1r_3() {addr_space = 3 : i32} : !llvm.array<8 x f32>
    llvm.func @__nv_expf(f32) -> f32
    llvm.func @main_kStitch_divide__13_1_0___1b1r(%arg0: !llvm.ptr<f32>, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [0 : index, 2 : index, 3 : index, 4 : index, 6 : index, 7 : index, 8 : index, 9 : index, 11 : index, 12 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(-1 : i32) : i32
      %1 = llvm.mlir.constant(256 : index) : i32
      %2 = llvm.mlir.constant(4 : index) : i32
      %3 = llvm.mlir.constant(512 : index) : i32
      %4 = llvm.mlir.constant(768 : index) : i32
      %5 = llvm.mlir.constant(1024 : index) : i32
      %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %7 = llvm.mlir.constant(1 : i32) : i32
      %8 = llvm.mlir.constant(32 : i32) : i32
      %9 = llvm.mlir.constant(2 : i32) : i32
      %10 = llvm.mlir.constant(4 : i32) : i32
      %11 = llvm.mlir.constant(8 : i32) : i32
      %12 = llvm.mlir.constant(16 : i32) : i32
      %13 = llvm.mlir.constant(-0.000000e+00 : f32) : f32
      %14 = llvm.mlir.constant(8 : index) : i32
      %15 = llvm.mlir.constant(1 : index) : i32
      %16 = llvm.mlir.constant(32 : index) : i32
      %17 = llvm.mlir.constant(0 : index) : i32
      %18 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_0 : !llvm.ptr<array<32 x f32>, 3>
      %19 = llvm.getelementptr %18[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %20 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_1 : !llvm.ptr<array<8 x f32>, 3>
      %21 = llvm.getelementptr %20[0, 0] : (!llvm.ptr<array<8 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %22 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_2 : !llvm.ptr<array<32 x f32>, 3>
      %23 = llvm.getelementptr %22[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %24 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1b1r_3 : !llvm.ptr<array<8 x f32>, 3>
      %25 = llvm.getelementptr %24[0, 0] : (!llvm.ptr<array<8 x f32>, 3>) -> !llvm.ptr<f32, 3>
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %26 = nvvm.read.ptx.sreg.ctaid.x : i32
      %27 = nvvm.read.ptx.sreg.tid.x : i32
      %28 = llvm.udiv %27, %16  : i32
      %29 = llvm.urem %27, %16  : i32
      %30 = llvm.sub %arg1, %27  : i32
      %31 = llvm.icmp "eq" %30, %17 : i32
      %32 = llvm.sub %30, %15  : i32
      %33 = llvm.udiv %32, %1  : i32
      %34 = llvm.add %33, %15  : i32
      %35 = llvm.select %31, %17, %34 : i1, i32
      %36 = llvm.srem %35, %2  : i32
      %37 = llvm.sub %35, %36  : i32
      %38 = llvm.mul %37, %1  : i32
      %39 = llvm.add %27, %38  : i32
      %40 = llvm.mul %26, %arg1  : i32
      llvm.br ^bb2(%27, %6 : i32, f32)
    ^bb2(%41: i32, %42: f32):  // 2 preds: ^bb1, ^bb3
      %43 = llvm.icmp "slt" %41, %39 : i32
      llvm.cond_br %43, ^bb3, ^bb4
    ^bb3:  // pred: ^bb2
      %44 = llvm.add %41, %1  : i32
      %45 = llvm.add %41, %3  : i32
      %46 = llvm.add %41, %4  : i32
      %47 = llvm.add %40, %41  : i32
      %48 = llvm.add %40, %44  : i32
      %49 = llvm.add %40, %45  : i32
      %50 = llvm.add %40, %46  : i32
      %51 = llvm.getelementptr %arg0[%47] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %52 = llvm.load %51 : !llvm.ptr<f32>
      %53 = llvm.getelementptr %arg0[%48] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %54 = llvm.load %53 : !llvm.ptr<f32>
      %55 = llvm.getelementptr %arg0[%49] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %56 = llvm.load %55 : !llvm.ptr<f32>
      %57 = llvm.getelementptr %arg0[%50] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %58 = llvm.load %57 : !llvm.ptr<f32>
      %59 = llvm.fcmp "ugt" %42, %52 : f32
      %60 = llvm.select %59, %42, %52 : i1, f32
      %61 = llvm.fcmp "uno" %52, %52 : f32
      %62 = llvm.select %61, %52, %60 : i1, f32
      %63 = llvm.fcmp "ugt" %62, %54 : f32
      %64 = llvm.select %63, %62, %54 : i1, f32
      %65 = llvm.fcmp "uno" %54, %54 : f32
      %66 = llvm.select %65, %54, %64 : i1, f32
      %67 = llvm.fcmp "ugt" %66, %56 : f32
      %68 = llvm.select %67, %66, %56 : i1, f32
      %69 = llvm.fcmp "uno" %56, %56 : f32
      %70 = llvm.select %69, %56, %68 : i1, f32
      %71 = llvm.fcmp "ugt" %70, %58 : f32
      %72 = llvm.select %71, %70, %58 : i1, f32
      %73 = llvm.fcmp "uno" %58, %58 : f32
      %74 = llvm.select %73, %58, %72 : i1, f32
      %75 = llvm.add %41, %5  : i32
      llvm.br ^bb2(%75, %74 : i32, f32)
    ^bb4:  // pred: ^bb2
      llvm.br ^bb5(%39, %42 : i32, f32)
    ^bb5(%76: i32, %77: f32):  // 2 preds: ^bb4, ^bb6
      %78 = llvm.icmp "slt" %76, %arg1 : i32
      llvm.cond_br %78, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %79 = llvm.add %40, %76  : i32
      %80 = llvm.getelementptr %arg0[%79] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %81 = llvm.load %80 : !llvm.ptr<f32>
      %82 = llvm.fcmp "ugt" %77, %81 : f32
      %83 = llvm.select %82, %77, %81 : i1, f32
      %84 = llvm.fcmp "uno" %81, %81 : f32
      %85 = llvm.select %84, %81, %83 : i1, f32
      %86 = llvm.add %76, %1  : i32
      llvm.br ^bb5(%86, %85 : i32, f32)
    ^bb7:  // pred: ^bb5
      %87 = llvm.sub %8, %8  : i32
      %88 = llvm.lshr %0, %87  : i32
      %89 = llvm.sub %8, %7  : i32
      %90 = nvvm.shfl.sync  bfly %88, %77, %7, %89 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %91 = llvm.extractvalue %90[0] : !llvm.struct<(f32, i1)> 
      %92 = llvm.fcmp "ugt" %77, %91 : f32
      %93 = llvm.select %92, %77, %91 : i1, f32
      %94 = llvm.fcmp "uno" %91, %91 : f32
      %95 = llvm.select %94, %91, %93 : i1, f32
      %96 = llvm.sub %8, %8  : i32
      %97 = llvm.lshr %0, %96  : i32
      %98 = llvm.sub %8, %7  : i32
      %99 = nvvm.shfl.sync  bfly %97, %95, %9, %98 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %100 = llvm.extractvalue %99[0] : !llvm.struct<(f32, i1)> 
      %101 = llvm.fcmp "ugt" %95, %100 : f32
      %102 = llvm.select %101, %95, %100 : i1, f32
      %103 = llvm.fcmp "uno" %100, %100 : f32
      %104 = llvm.select %103, %100, %102 : i1, f32
      %105 = llvm.sub %8, %8  : i32
      %106 = llvm.lshr %0, %105  : i32
      %107 = llvm.sub %8, %7  : i32
      %108 = nvvm.shfl.sync  bfly %106, %104, %10, %107 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %109 = llvm.extractvalue %108[0] : !llvm.struct<(f32, i1)> 
      %110 = llvm.fcmp "ugt" %104, %109 : f32
      %111 = llvm.select %110, %104, %109 : i1, f32
      %112 = llvm.fcmp "uno" %109, %109 : f32
      %113 = llvm.select %112, %109, %111 : i1, f32
      %114 = llvm.sub %8, %8  : i32
      %115 = llvm.lshr %0, %114  : i32
      %116 = llvm.sub %8, %7  : i32
      %117 = nvvm.shfl.sync  bfly %115, %113, %11, %116 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %118 = llvm.extractvalue %117[0] : !llvm.struct<(f32, i1)> 
      %119 = llvm.fcmp "ugt" %113, %118 : f32
      %120 = llvm.select %119, %113, %118 : i1, f32
      %121 = llvm.fcmp "uno" %118, %118 : f32
      %122 = llvm.select %121, %118, %120 : i1, f32
      %123 = llvm.sub %8, %8  : i32
      %124 = llvm.lshr %0, %123  : i32
      %125 = llvm.sub %8, %7  : i32
      %126 = nvvm.shfl.sync  bfly %124, %122, %12, %125 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %127 = llvm.extractvalue %126[0] : !llvm.struct<(f32, i1)> 
      %128 = llvm.fcmp "ugt" %122, %127 : f32
      %129 = llvm.select %128, %122, %127 : i1, f32
      %130 = llvm.fcmp "uno" %127, %127 : f32
      %131 = llvm.select %130, %127, %129 : i1, f32
      %132 = llvm.icmp "eq" %29, %17 : i32
      llvm.cond_br %132, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %133 = llvm.getelementptr %21[%28] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %131, %133 : !llvm.ptr<f32, 3>
      llvm.br ^bb9
    ^bb9:  // 2 preds: ^bb7, ^bb8
      nvvm.barrier0
      %134 = llvm.icmp "slt" %27, %16 : i32
      llvm.cond_br %134, ^bb10, ^bb17
    ^bb10:  // pred: ^bb9
      %135 = llvm.icmp "slt" %29, %14 : i32
      llvm.cond_br %135, ^bb11, ^bb12
    ^bb11:  // pred: ^bb10
      %136 = llvm.getelementptr %21[%29] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %137 = llvm.load %136 : !llvm.ptr<f32, 3>
      llvm.br ^bb13(%137 : f32)
    ^bb12:  // pred: ^bb10
      llvm.br ^bb13(%6 : f32)
    ^bb13(%138: f32):  // 2 preds: ^bb11, ^bb12
      llvm.br ^bb14
    ^bb14:  // pred: ^bb13
      %139 = llvm.sub %8, %11  : i32
      %140 = llvm.lshr %0, %139  : i32
      %141 = llvm.sub %11, %7  : i32
      %142 = nvvm.shfl.sync  bfly %140, %138, %7, %141 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %143 = llvm.extractvalue %142[0] : !llvm.struct<(f32, i1)> 
      %144 = llvm.fcmp "ugt" %138, %143 : f32
      %145 = llvm.select %144, %138, %143 : i1, f32
      %146 = llvm.fcmp "uno" %143, %143 : f32
      %147 = llvm.select %146, %143, %145 : i1, f32
      %148 = llvm.sub %8, %11  : i32
      %149 = llvm.lshr %0, %148  : i32
      %150 = llvm.sub %11, %7  : i32
      %151 = nvvm.shfl.sync  bfly %149, %147, %9, %150 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %152 = llvm.extractvalue %151[0] : !llvm.struct<(f32, i1)> 
      %153 = llvm.fcmp "ugt" %147, %152 : f32
      %154 = llvm.select %153, %147, %152 : i1, f32
      %155 = llvm.fcmp "uno" %152, %152 : f32
      %156 = llvm.select %155, %152, %154 : i1, f32
      %157 = llvm.sub %8, %11  : i32
      %158 = llvm.lshr %0, %157  : i32
      %159 = llvm.sub %11, %7  : i32
      %160 = nvvm.shfl.sync  bfly %158, %156, %10, %159 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %161 = llvm.extractvalue %160[0] : !llvm.struct<(f32, i1)> 
      %162 = llvm.fcmp "ugt" %156, %161 : f32
      %163 = llvm.select %162, %156, %161 : i1, f32
      %164 = llvm.fcmp "uno" %161, %161 : f32
      %165 = llvm.select %164, %161, %163 : i1, f32
      llvm.cond_br %132, ^bb15, ^bb16
    ^bb15:  // pred: ^bb14
      llvm.store %165, %19 : !llvm.ptr<f32, 3>
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb14, ^bb15
      llvm.br ^bb17
    ^bb17:  // 2 preds: ^bb9, ^bb16
      nvvm.barrier0
      %166 = llvm.load %19 : !llvm.ptr<f32, 3>
      llvm.br ^bb18(%27, %13 : i32, f32)
    ^bb18(%167: i32, %168: f32):  // 2 preds: ^bb17, ^bb19
      %169 = llvm.icmp "slt" %167, %39 : i32
      llvm.cond_br %169, ^bb19, ^bb20
    ^bb19:  // pred: ^bb18
      %170 = llvm.add %167, %1  : i32
      %171 = llvm.add %167, %3  : i32
      %172 = llvm.add %167, %4  : i32
      %173 = llvm.add %40, %167  : i32
      %174 = llvm.add %40, %170  : i32
      %175 = llvm.add %40, %171  : i32
      %176 = llvm.add %40, %172  : i32
      %177 = llvm.getelementptr %arg0[%173] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %178 = llvm.load %177 : !llvm.ptr<f32>
      %179 = llvm.getelementptr %arg0[%174] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %180 = llvm.load %179 : !llvm.ptr<f32>
      %181 = llvm.getelementptr %arg0[%175] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %182 = llvm.load %181 : !llvm.ptr<f32>
      %183 = llvm.getelementptr %arg0[%176] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %184 = llvm.load %183 : !llvm.ptr<f32>
      %185 = llvm.fsub %178, %166  : f32
      %186 = llvm.fsub %180, %166  : f32
      %187 = llvm.fsub %182, %166  : f32
      %188 = llvm.fsub %184, %166  : f32
      %189 = llvm.call @__nv_expf(%185) : (f32) -> f32
      %190 = llvm.call @__nv_expf(%186) : (f32) -> f32
      %191 = llvm.call @__nv_expf(%187) : (f32) -> f32
      %192 = llvm.call @__nv_expf(%188) : (f32) -> f32
      %193 = llvm.fadd %168, %189  : f32
      %194 = llvm.fadd %193, %190  : f32
      %195 = llvm.fadd %194, %191  : f32
      %196 = llvm.fadd %195, %192  : f32
      %197 = llvm.add %167, %5  : i32
      llvm.br ^bb18(%197, %196 : i32, f32)
    ^bb20:  // pred: ^bb18
      %198 = llvm.load %19 : !llvm.ptr<f32, 3>
      llvm.br ^bb21(%39, %168 : i32, f32)
    ^bb21(%199: i32, %200: f32):  // 2 preds: ^bb20, ^bb22
      %201 = llvm.icmp "slt" %199, %arg1 : i32
      llvm.cond_br %201, ^bb22, ^bb23
    ^bb22:  // pred: ^bb21
      %202 = llvm.add %40, %199  : i32
      %203 = llvm.getelementptr %arg0[%202] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %204 = llvm.load %203 : !llvm.ptr<f32>
      %205 = llvm.fsub %204, %198  : f32
      %206 = llvm.call @__nv_expf(%205) : (f32) -> f32
      %207 = llvm.fadd %200, %206  : f32
      %208 = llvm.add %199, %1  : i32
      llvm.br ^bb21(%208, %207 : i32, f32)
    ^bb23:  // pred: ^bb21
      %209 = llvm.sub %8, %8  : i32
      %210 = llvm.lshr %0, %209  : i32
      %211 = llvm.sub %8, %7  : i32
      %212 = nvvm.shfl.sync  bfly %210, %200, %7, %211 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %213 = llvm.extractvalue %212[0] : !llvm.struct<(f32, i1)> 
      %214 = llvm.fadd %200, %213  : f32
      %215 = llvm.sub %8, %8  : i32
      %216 = llvm.lshr %0, %215  : i32
      %217 = llvm.sub %8, %7  : i32
      %218 = nvvm.shfl.sync  bfly %216, %214, %9, %217 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %219 = llvm.extractvalue %218[0] : !llvm.struct<(f32, i1)> 
      %220 = llvm.fadd %214, %219  : f32
      %221 = llvm.sub %8, %8  : i32
      %222 = llvm.lshr %0, %221  : i32
      %223 = llvm.sub %8, %7  : i32
      %224 = nvvm.shfl.sync  bfly %222, %220, %10, %223 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %225 = llvm.extractvalue %224[0] : !llvm.struct<(f32, i1)> 
      %226 = llvm.fadd %220, %225  : f32
      %227 = llvm.sub %8, %8  : i32
      %228 = llvm.lshr %0, %227  : i32
      %229 = llvm.sub %8, %7  : i32
      %230 = nvvm.shfl.sync  bfly %228, %226, %11, %229 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %231 = llvm.extractvalue %230[0] : !llvm.struct<(f32, i1)> 
      %232 = llvm.fadd %226, %231  : f32
      %233 = llvm.sub %8, %8  : i32
      %234 = llvm.lshr %0, %233  : i32
      %235 = llvm.sub %8, %7  : i32
      %236 = nvvm.shfl.sync  bfly %234, %232, %12, %235 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %237 = llvm.extractvalue %236[0] : !llvm.struct<(f32, i1)> 
      %238 = llvm.fadd %232, %237  : f32
      llvm.cond_br %132, ^bb24, ^bb25
    ^bb24:  // pred: ^bb23
      %239 = llvm.getelementptr %25[%28] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %238, %239 : !llvm.ptr<f32, 3>
      llvm.br ^bb25
    ^bb25:  // 2 preds: ^bb23, ^bb24
      nvvm.barrier0
      llvm.cond_br %134, ^bb26, ^bb33
    ^bb26:  // pred: ^bb25
      %240 = llvm.icmp "slt" %29, %14 : i32
      llvm.cond_br %240, ^bb27, ^bb28
    ^bb27:  // pred: ^bb26
      %241 = llvm.getelementptr %25[%29] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %242 = llvm.load %241 : !llvm.ptr<f32, 3>
      llvm.br ^bb29(%242 : f32)
    ^bb28:  // pred: ^bb26
      llvm.br ^bb29(%13 : f32)
    ^bb29(%243: f32):  // 2 preds: ^bb27, ^bb28
      llvm.br ^bb30
    ^bb30:  // pred: ^bb29
      %244 = llvm.sub %8, %11  : i32
      %245 = llvm.lshr %0, %244  : i32
      %246 = llvm.sub %11, %7  : i32
      %247 = nvvm.shfl.sync  bfly %245, %243, %7, %246 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %248 = llvm.extractvalue %247[0] : !llvm.struct<(f32, i1)> 
      %249 = llvm.fadd %243, %248  : f32
      %250 = llvm.sub %8, %11  : i32
      %251 = llvm.lshr %0, %250  : i32
      %252 = llvm.sub %11, %7  : i32
      %253 = nvvm.shfl.sync  bfly %251, %249, %9, %252 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %254 = llvm.extractvalue %253[0] : !llvm.struct<(f32, i1)> 
      %255 = llvm.fadd %249, %254  : f32
      %256 = llvm.sub %8, %11  : i32
      %257 = llvm.lshr %0, %256  : i32
      %258 = llvm.sub %11, %7  : i32
      %259 = nvvm.shfl.sync  bfly %257, %255, %10, %258 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %260 = llvm.extractvalue %259[0] : !llvm.struct<(f32, i1)> 
      %261 = llvm.fadd %255, %260  : f32
      llvm.cond_br %132, ^bb31, ^bb32
    ^bb31:  // pred: ^bb30
      llvm.store %261, %23 : !llvm.ptr<f32, 3>
      llvm.br ^bb32
    ^bb32:  // 2 preds: ^bb30, ^bb31
      llvm.br ^bb33
    ^bb33:  // 2 preds: ^bb25, ^bb32
      nvvm.barrier0
      %262 = llvm.load %19 : !llvm.ptr<f32, 3>
      %263 = llvm.load %23 : !llvm.ptr<f32, 3>
      llvm.br ^bb34(%27 : i32)
    ^bb34(%264: i32):  // 2 preds: ^bb33, ^bb35
      %265 = llvm.icmp "slt" %264, %39 : i32
      llvm.cond_br %265, ^bb35, ^bb36
    ^bb35:  // pred: ^bb34
      %266 = llvm.add %264, %1  : i32
      %267 = llvm.add %264, %3  : i32
      %268 = llvm.add %264, %4  : i32
      %269 = llvm.add %40, %264  : i32
      %270 = llvm.add %40, %266  : i32
      %271 = llvm.add %40, %267  : i32
      %272 = llvm.add %40, %268  : i32
      %273 = llvm.getelementptr %arg0[%269] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %274 = llvm.load %273 : !llvm.ptr<f32>
      %275 = llvm.getelementptr %arg0[%270] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %276 = llvm.load %275 : !llvm.ptr<f32>
      %277 = llvm.getelementptr %arg0[%271] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %278 = llvm.load %277 : !llvm.ptr<f32>
      %279 = llvm.getelementptr %arg0[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %280 = llvm.load %279 : !llvm.ptr<f32>
      %281 = llvm.fsub %274, %262  : f32
      %282 = llvm.fsub %276, %262  : f32
      %283 = llvm.fsub %278, %262  : f32
      %284 = llvm.fsub %280, %262  : f32
      %285 = llvm.call @__nv_expf(%281) : (f32) -> f32
      %286 = llvm.call @__nv_expf(%282) : (f32) -> f32
      %287 = llvm.call @__nv_expf(%283) : (f32) -> f32
      %288 = llvm.call @__nv_expf(%284) : (f32) -> f32
      %289 = llvm.fdiv %285, %263  : f32
      %290 = llvm.fdiv %286, %263  : f32
      %291 = llvm.fdiv %287, %263  : f32
      %292 = llvm.fdiv %288, %263  : f32
      %293 = llvm.getelementptr %arg2[%269] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %289, %293 : !llvm.ptr<f32>
      %294 = llvm.getelementptr %arg2[%270] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %290, %294 : !llvm.ptr<f32>
      %295 = llvm.getelementptr %arg2[%271] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %291, %295 : !llvm.ptr<f32>
      %296 = llvm.getelementptr %arg2[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %292, %296 : !llvm.ptr<f32>
      %297 = llvm.add %264, %5  : i32
      llvm.br ^bb34(%297 : i32)
    ^bb36:  // pred: ^bb34
      %298 = llvm.load %19 : !llvm.ptr<f32, 3>
      %299 = llvm.load %23 : !llvm.ptr<f32, 3>
      llvm.br ^bb37(%39 : i32)
    ^bb37(%300: i32):  // 2 preds: ^bb36, ^bb38
      %301 = llvm.icmp "slt" %300, %arg1 : i32
      llvm.cond_br %301, ^bb38, ^bb39
    ^bb38:  // pred: ^bb37
      %302 = llvm.add %40, %300  : i32
      %303 = llvm.getelementptr %arg0[%302] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %304 = llvm.load %303 : !llvm.ptr<f32>
      %305 = llvm.fsub %304, %298  : f32
      %306 = llvm.call @__nv_expf(%305) : (f32) -> f32
      %307 = llvm.fdiv %306, %299  : f32
      %308 = llvm.getelementptr %arg2[%302] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %307, %308 : !llvm.ptr<f32>
      %309 = llvm.add %300, %1  : i32
      llvm.br ^bb37(%309 : i32)
    ^bb39:  // pred: ^bb37
      llvm.return
    }
  }
  gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\A0 \00\00\00\00\00\00\02\00\01\01@\00\00\00` \00\00\00\00\00\00^ \00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(K\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80J\08\00\11G\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F5)_shndx\00.nv.info\00.text.main_kStitch_divide__13_1_0___1w1r2\00\0F,\00\15oshared.\00\15\9Fconstant01\00\12\FD\01debug_frame\00.rel\11\00\1Aa\12\00!nv&\00oaction%\01 \0F\94\00\0F\0FH\01q\1F$$\00\0F\FF\16$__cuda_sm70_shflsync_bfly_p\00$____wg_G\00\0F\00\09\00/261\00\19_1__28\EA\01\1Fo_param\F1\01.\0F\01\00\06\8CU\00\00\00\03\00\0A\00\01\00\11\AA\18\00,\0B\00\01\00\A1\D8\00\00\00\22\00\0A\00@=\0F\001\00@\01\07\000\00z\010\00,\09\00\01\00\11\B2\18\00,\04\00\01\00\11\E2\18\00,\07\00\01\00f2\00\00\00\12\10\90\00!\80>\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\130\C0\00\10\04\9B\00R\04<\00\00\00E\005\04\08\0F<\00&8\00H\00\04p\00L\84\80\80(o\00\10\08\15\00\81\16\85\80\80(\09\05\0C2\00\068\00\22\18\00\01\00\22p\00\01\00\048\01\1308\011\04/\08\06\01\10&\0C\00R#\08\00\03\00\01\00'\04\12\0C\00\17\11\0C\00\12#0\00\04$\00\17\07$\00\05\0C\00!7\044\05q\015\00\00\04\0A\08~\01\A2`\01 \00\03\19 \00\04\17\A2\00u\04\00\18\00\00\F0!\10\00u\03\00\10\00\00\F0\11\10\009\02\00\0C\10\009\01\00\08\10\00\01\01\00\F1_\F0!\00\03\1B\FF\00\04(P\00\C0\12\00\00\00\13\00\00@\13\00\00\80\13\00\00\C0\13\00\00\10+\00\000+\00\00P+\00\00p+\00\00\90+\00\00\00:\00\00p:\00\00\E0:\00\00P;\00\00\C0;\00\00\10<\00\00P<\00\00\90<\00\00\D0<\00\00 =\00\00\04\1C\0C\00\F0+\00\00`6\00\00\C09\00\00\04\1E\04s\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\D8\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00\C8\00\01\001\02\00\00\C8\01*H\00\10\00\0F\01\00\FF\D6@$v\01\FF\8F\05\B1\FF\00\8E\07\00\C4\0F\00\19y\0D\18\00 \00!y\05\A1\0E\00\B9z\04\00\00F\00\00\B3\05\90\E2\0F\00Ey\00\00\10\14!\00 \80\03\10\00c$t\0B\FF\1F\00@\00\10\E2@\00\03:\04\B1%\00\00\00b\0E\00$t\1A\FF\01\00\04 \00\F1\04x\0A\FF\05\00\00\00\0D\16\01\00\00\E4\1F\00\12x\0D\0D@\00\B0\C0\8E\07\00\C6\0F\00$x\0C\03\A9\02 \0A\020\00\D0/\00\0Cz\00\0D\00[\00\00pR\F2p\00g\04$x\0A\0A\04p\00P\0Cr\00\0D\FF\F0\04`R\F4\03\00\E4\0F0\00\81\0C\00\\\00\00pb\F6\10\00@\12r\00\FF\D2\00\A2\FF3\8E\07\00\D6\0F\00G94\04\01\C0\00p\EA\0F\00\10z\02\00`\00B\FF\E0\FF\07\E0\00H\01\00\E0\0C\E0\00R\13\FF\00\00\80\C0\00\93\E4\0F\00\10x\02\02 \000\00\80\C8\0F\00\12x\02\02\80\E0\00!\C0\8E\10\00a\07r\02\02\FF\00@\00\11\00 \001r\0F\02\80\00\12\FC \00\00\B0\00\00\F2\05\E6pb\F0\03\00\DA\0F\00G\09\00\00`\0C\90\00@$x\02\0F\AB\05!\0D\0A\E0\00v\10Ey\02\00\F0\06\90\001\1Cx\00\01\00?p\F0\F0\A0\00\02\B2\0Cx\00\02\80\01\00\00pB\F8 \000r\11\FF\90\00\10\0D \00\96\D8\0F\00G\C9\00\00\90\06p\00\05P\002\E1\F0\03\E0\00a\10\0F\80\FE\FF\FF\E0\00\10\C4`\00%\16\FFp\01p\E4\0F\00$z\09\0C0\01\22\11\02\D0\00\90%v\02\09\00X\00\00\16\10\00p\CA\0F\00\81y\0E\020\00\C5\00\19\1E\0C\00\A2\00\00\10x\05\090\01\84\E4\0F\04\10x\07\09@\10\00\10\C6@\00*\04\05@\00%\15\04@\00u\E2\02\00%v\06\07 \00\86\E2\0F\08\10x\09\09`P\00g\00\10x\17\11\80\10\00E\81y\12\06@\00fb\09\00%v\08\A0\00\10\C8\C0\00\11\1B\C0\003\17\02\8E0\00%\17\080\00\10d0\00&\02\1Bp\00h\1F\04\10x\19\1B\F0\016\81y\14\E0\00 b\01\C0\00\16\190\00y/\00\10x\1D\1B@0\00\16\19\D0\00 b\03\D0\00\16\1D0\00j\0F\09\10x\1B\1B\D0\00F\1D\11\00\01\10\00<\81y\18\D0\00\06\A0\00\01\D0\00\11\1F\D0\00\12\1DP\00F\00\81y\1B\D0\00\11f\D0\00\1A\1F\D0\00*\1D\1F\D0\00\1C\1C\D0\00\1A\1D\D0\00*!\1F\D0\00\1C\1D\D0\00\1A!\D0\00*\1F\1F\D0\00F!\11\80\01\10\00<\81y\1E\D0\00\06\A0\00\01\D0\00\11!\D0\00\15!\D0\00\17\1F\D0\00\00p\00*\02!\D0\00%#!\D0\00\01@\02\17\02\D0\00\01`\02\1A#\D0\009#!@0\00*\05\040\00\17\060\00y\0F\01\10x!!`0\00\16\06\C0\00\10\220\00\18\08\00\01`\00\0Br\00\0E\0E \00\B1\80\F8\03\00\E4O\08\0Br\00\13\10\000\C0\FA\03\80\04A\08\C2\0E\0E\7F\042\00\80\06\10\01\16\13\E0\00 \A2\0E@\00!\15\15\1F\00P\FA\03\00\E4\8F@\00%\12\12P\00 \0F\02`\00\01 \00\B1\C0\FC\03\00\D2\0F\00\08\D2\15\15`\00!\00\00`\01U\0Br\00\17\17@\00\01\10\00$\15\120\00\10\C8\80\00!\12\12@\00\060\00&\14\14`\00\00\10\00(\12\170\001\D2\17\17@\00\060\00*\19\19`\00)\17\14`\00!\14\14@\00\060\00*\18\18`\00)\14\19`\00!\19\19@\00\060\00*\1B\1B`\00)\19\18`\00!\18\18@\00\060\00*\1C\1C`\00)\18\1B`\00!\1B\1B@\00\060\00*\1D\1D`\00)\1B\1C`\00!\1C\1C@\00\060\00*\1E\1E`\00)\1C\1D`\00!\1D\1D@\00\060\00*\1F\1F`\00)\1D\1E`\00!\1E\1E@\00\060\00*\02\02`\00)\1E\1F`\00!\1F\1F@\00\060\00*\05\05`\00)\1F\02`\00!\02\02@\00\060\00&\06\06`\00\10\01p\00\19\05`\00!\05\05@\00\030\00c\10x\11\11\00\02\F0\02\03p\00\19\06`\00!\06\06@\00\030\00P\0Cr\00\11\10@\06$b\FC\90\00%\13\13\A0\00\01\00\03\11\06\10\00\22\C0\F8\00\031\D2\13\13P\000\00\00\060\03`G\E9\00\00\90\F9A\07\11\83\90\06#Ay\8E\00\0B\10\073\11\0A\8E\A0\07C\02\00p\03 \00\13\E8\F0\06\13\00\F0\06\12\DA\E0\06'@\03@\00+t\03\C0\06\1B\0E\C0\06 \04\0E\C0\033\03\02\8E\E0\03*\02\04\C0\06%\06\0E`\04\01\C0\06*\08\0E\C0\06*\06\06@\00*\19\06\C0\06%\08\08 \00\01\C0\06/\14\0E\C0\06\0B*\0E\08 \05%\14\14@\00\01 \05\1B\10\C0\06%\15\140\00\10d\90\00'\16\10p\00V\04\10x\04\10 \05k\1F\00\81y\16\16\F0\04\16\040\00\00 \05*\04\10 \05*\07\060\00\17\040\00\00\C0\00'\08\10 \05[\01\81y\04\04 \05\08\00\01\07\F0\02\15\F0 \05\18\02 \021\82\02\020\02/\00\00 \05\05%\19\19\D0\02&\8F\08p\05\14\F0 \05$\02\19p\05\00 \051\C2\19\19`\00\03p\05\06`\05\14\F8\B0\02\15\19\A0\05\00\E0\02J\82\0E\0E\190\00&\16\16`\00\16\00\80\05\030\00\14\C2\80\05\050\00*\07\07`\00)\15\16`\00:\16\16\150\00&\04\04`\00\00\A0\03)\16\07`\004\07\07\160\00\11\C80\00)\07\04P\007\04\04\07P\00\0E\D0\09\19\11P\08\06\A0\03\14\F8\80\01$\04\13P\00\00\80\01D\C2\13\13\04P\00\1E\C4\90\03\00\F0\03\01\C0\0A1\16p\00p\03X\89\00\00\A0\01p\03\1B\09p\03\1B\08p\03\11\02\10\02\16\09\A0\02/\08 \C0\07\05%\A2\0Ep\02\15\09p\02/\06\08\C0\07\06%\E2\0E\90\03\060\00?\08\08`\C0\07\0B\06\90\00\10\CC \00(\09\08\F0\02\06\B0\02\1B\F8\B0\02*\FA\03\90\05Q\F0\03\00\D2\8F\80\05\02\C0\02/\80\06\80\05\0B\04\C0\01\04\80\05\050\00*\09\09\D0\02\03\80\05\1B\FA\80\05\05\10\02$\06\09@\03\01P\00!\09\090\00\00@\03\10\CA\00\0C\11\13\00\0C\13\09\E0\0DIAy\01\00\D0\01Sz\00\0F\00[\90\0C\03\00\0D#\90\04 \00\12\D8\A0\0C)p\040\0D\010\005\FF\E1\FF\90\05#`\02 \00\0C\A0\0C\03\90\0C\1B`\A0\05& \02P\00c$v\0E\FF\00[0\02\1B\E2\C0\02\10\C8\C0\01H\0E\0E\A0\FF\90\0C\1B\10`\02\11\02`\02\16\0F`\02\10\02\D0\013\10\02\8E\F0\01\1C\08\90\0C\15\0Fp\02\02\90\0C\18\0F\D0\051$z\05P\001\05\02\8E \001$z\07\10\00\12\07\10\00\05\B0\0C\15\10p\00%\06\07\10\00\1E\E4\90\02W\10x\09\0F``\00\09\80\02\16$\C0\00\01\80\02*\C8\1F\C0\00\020\05+\08\08\80\02\19\08\B0\03(\08\08p\02K\81y\13\020\05*\05\05P\0A$\08\05\D0\07\0F\A0\02\00E\10x\0F\0F\90\06\12\CE \08\01p\00!\00\00\F0\00\0F\10\08\10/\0F\0E\10\08,.\10\FE\10\08\0F\80\02\09#0\01 \00\12\E6p\02\1B p\02(\00\01\A0\04\1F\04@\02\12\15\04@\02\19\06P\01\09@\02\1B\CA0\02\16\C8 \02\16\04\C0\01%\06\06\D0\03\02\C0\01\18\06\00\01\22\82\06\10\01$\00\00\C0\01\18\04p\00\09\10\03\01\D0\05\16\0F\B0\02\09`\01\1B\F8`\01\06\D0\05\01p\00\0F\D0\05\05\04\00\04\07\D0\05.\80\00@\09\0F0\01\0D\15\030\01\16\040\01+\A4\0E\E0\06\02\00\01\19\04\00\01\12\04\80\06\0B\B0\04\1F\04\B0\04\04aGy\00\7F\12'\10\00\10\0B\10\00\FB\00\89\7F\02\13\00\1F \0C\00\00\0E\00\00$\0E`\08\1F\1F`\08\0B\10\CA@\00X\03\02\00\1F@@\00*\03\03@\00)\02\03@\00!\03\03P\00\05@\00X\04\03\00\1F\80@\00*\04\04@\00\1C\03\F0\00\01P\00\05@\00g\05\04\00\1F\00\0D@\00*\05\05@\00)\04\05@\00!\05\05P\00\05@\00a\06\05\00\1F\00\0E@\00ab\00\00$r\02@\011\05\00\8Ep\04(\0B\A2\E0\03,/\08\A0\05T\0B\A2\00\02\060\02\00@\08#\A8\00 \00 p\02\10\02\00\A0\02\14\02@\02q\CA\0F\00\88\A3\00\0A0\00\00\86\16i\E4\03\00Ay\00\A0\01\22Hy\88\18\04\10\066\1D{\00\01\00\12\EC@\14\11P!\0E\02 \00\00\90\13/0\17\90\13\0B&\B0\07\E0\03\0F\80\13\1F\16\03\80\13\010\13\01\1F\00#\80\FF \01\00\90\13\11\03\B0\04\11\F8\00\01@\0Cz\00\03\80\00\01\10\07\000\14VI\00\00 \07\A0\00\09@\00\00\10\03&r\04`\13\01\A0\0B\11\18 \03\01\00\04\1B\E4@\13\10\C6\C0\00%\11\18\C0\00w\E2\0F\04%v\12\18@\10X\04\10x\10\18\90\06G%v\0E\11 \00V\08\81y\06\12 \04\10\22`\06\15\18`\06\01\A0\06*\14\100\00%\05\0E0\00 \E6\10 \0C\17\09 \00\00\E0\0B\06@\0C h\090\07\18\16\10\121\06{\19\90\01\C1\00\90 \00\000\0E\00\08s\19\00 \0B\00\F5\0D $\1E\A0\07Q\19\FE\FF\FF\0F\80\00`\CC\1F\00\05s\0F\8A\02\01\AC\19 \00d\90\025\0E\FF\FF@\04\15\1F\E0\03\10\0F\D0\0Dp\C8/\00$z\13\13`\00\22\FF\02@\01@'r\0F\0F\A0\03\22\0E\00\90\09T'r\12\0F\18@\00\16\C8 \04%\12\0Ap\01\01@\00\13\18p\01V'r\14\0F\11p\01P\01\0Cz\00\18 \00$p`\10\15\02\90\00\15\14@\00%\11\0E\A0\0A\01@\006\0E\0F\10@\00\00\10\02\12\11@\00 \FC\03\10\000\10J\18P\00\11\80\F0\00\16\E2\90\00%\0E\0A0\00\03p\00\12\FAp\00!z\10\A0\00\04\D0\07T\10H\12\12\01\B0\01\10\E2p\007\16\0F\09p\00E\10j\11\11`\00\02\90\02\1A\10\C0\00\11\18p\00\13\16\C0\00'\10XP\00\18\E4\B0\00\04\80\00%\0F\18`\08\00\80\005h\14\140\00\00@\00;J\10\10p\00\15\0F\F0\00\10\CA`\00\070\00\09\90\00\020\04I\10H\0E\0EP\00;j\0F\0FP\00\01\E0\09\22pP`\032\12z\09\F0\09\133\F0\00\17X@\00\09\80\00\02`\00V\07r\12\09\12 \07g\04\07r\14\09\14\10\00H\10h\16\16P\01T$x\0F\12\04P\01\10\E2@\00!\0E\09\90\02\03`\08W$x\10\14\04p\01@\12x\0F\0F\C0\11\22\FF\E2\00\02,\10XP\00(\0E\0EP\00H\12x\10\100\00W\07r\09\09\16`\00T\84y\0F\0F\00 \05u\A2\0E\00\12x\0E\0E0\00\01\E0\18(\09\09P\00E\84y\10\100\00\10\E80\00%\09\090\00\01 \00%\0E\0E \00\10j\10\00%\09\09\10\00!\22\0E\00\18S\9D\98\BB;\FF0\07\84$t\15\FF\00\00|C`\00W\10x\04\04\80\A0\01E\0Cr\00\04\F0\04\84\E2\0F\00!r\0F\06\0F\F3\07P\C8O\00#t\0F\00B\00?\13 \F0\19T!r\10\05\10 \00 \C6\8F \00t\06\01\00@K\15@ \004\07\07\0E \00u\E2\0F\02#t\05\10@\00\B3\E4\0F\08!t\0E\06\7F\00@\CB \00X\00#t\12\07`\00G#t\05\05P\00e\08!r\08\08\09P\00\C4\1F\00#x\0E\0F;\AA\B8?\0E\080\00)\12\12\80\00:t\11\05`\00)\09\08`\00\94x\0E\0F`p\A52\0E\000\00)\14\120\000x\11\10`\00\15\11`\00%\0F\09`\00\10\C4 \00 \14\07 \00&\14\080\00\00P\00\22\11\00\10\00W\08s\0E\00\0E\90\01J!t\09\0F`\00 \14\070\00\01\B4\02\10\C6\10\00 \07\08P\00\22\09\08\10\00T\08s\10\00\11@\00@b\0E\00#\B0\0ECp\A52\07@\00@$x\07\06\0F\01\13\FF\10\05W\08s\09\00\14`\02E$x\05\05 \00\00p\00Cr\02\07\0E\C8\1D\00\90\05X\08s\11\00\080\02:x\12\120\004\05\05\100\00!\E2/p\13\05 \00\11\C6 \00!\12\09P\08\03 \02Dr\02\02\11\10\00\22\E2\1F`\0B\09\C0\13\0A\B0\09\01P\0E\13\0E\10\00\13\E2P\0E\18\0E\F0\07,\04\03\D0\0B/\A0\07P\0E\0A\1C\04P\0E(`\07P\0E\12\04`\04\0FP\0E\06(\04\04P\0EV\10x\09\03 \F0\03F\04$t\18\B0\07\01@\035\05\03@ \00Q\1F\04$z\12\E0\07\01\F0\0A\01 \0E\16\09\C0\0D\01\80\07C\0E\12\00X\90\06\01@\00%\11\03\A0\07\01\D0\07'\14\09 \00F\08\81y\07\A0\07%\A6\00\E0\13\12\05 \00F\04\81y\08\A0\07 \22\07\B0\06\18\0C\B0\06\04\F0\13\22\18\02\10\06G%v\18\11`\00\00\C0\0C\06\D0\07 h#\A0\0E\18\18\E0\106\06{\13\E0\07*b\0E\B0\07\84\E2\1F\00\10x\03\03\80\C0\00\01\80\02\10\13w!\03\00\08l.\00\10x\1B\13\00\08\14\1B\00\08\10\E4P\00\17\15\F0\07j\8F\00$z\15\15\F0\079\16\0F\15\F0\074\0E\16\12\80\00\00\A0\07&r\0F`\07\01\A0\078\13\16\10 \00!z\120\06$\12\02 \00 \0F\16\B0\02\07\C0\07\1A\12@\07\16\14\90\00%\E4\0F\A0\00%\13\0A\E0\1C\18\14\D0\01E$z\10\15\C0\07\02P\00\1A\09\10\08(\12\12\F0\06-\10H\D0\06\15\12\D0\06\01\F0\074\12\16\11\A0\00\1B\E2\E0\07\16\C6\A0\00\22\12\0A \00I\10j\09\09`\005h\0F\0F`\00\11\E2\C0\08\18\14\10\026$t\14\00\06\1F\E4`\07\02\15\09\90\00\1F\C4\00\08\02\15\11\E0\00\00@\00;H\13\13@\00\06\D0\07\01 \08/\0F\0F \08\0A\0B\D0\086\12z\10\00\08\00 \00\1Ah\C0\08A\07r\0E\10\1F\04!\00\00\B0\0ET\07r\0F\10\0F\10\00\010\08)\13\13\00\01)x\09\A0\07\0D\00\098x\0F\0F \00A\07r\13\10\C0\02\02@\08\0Ap\07\1B\E4\10\08\01@\00%\13\13@\00\11\E4\90\07$\09\00P\05*\A8\0E\80\09\0C\00\08\10&p\007\12\10\12\D0\00I\12x\10\13`\00\08\C0\07\00\C0\00\030\0E\04\A0\07\00\B0\05\0A\80\00\01o\19\02`\00\10fP\00%\12\12P\00\10\CC \00\02/\1C\05\90\06Ir\0E\07\0E\E0\07*\07\0E\E0\074\08\08\0F \00p\C6\0F\01#t\07\07\00\07!\14@ \00H\08#t\0FP\07d\08!r\11\06\110\00y\E4\0F\02!t\09\07\F0\068t\06\0F@\00 $x\0F\17\07p\06:t\10\11P\009\05\05\12\F0\07%\09\0E \07\01\B0\07\1B\0F@\08*\10\10\A0\00'\12\05P\00\01@\00\00@\07$\09\00\10\00\11\0Fp\07$\0F\08P\00*\0E\10P\00'\12\12P\00Y\00$x\06\06\B0\00!x\0F\90\07%\0F\00P\08\17\11\90\08I\04!t\08P\08\00\A0\07\14\09\80\01!$\0E0\00\08\80\08@#x\08\05@\00\01\D5\13\12\C8\10\00\00 \00\15\08@\007\0F\00\0F\10\08E$x\05\10\90\00\1B\CEp\08\85\A2\0E\00#r\07\07\09\C0\07\10\1F\C0\07\06\E0\07\01\B0\057\08\00\08\10\021#r\06\E1\09\013\07!\C8/\00\08\11\0E\9D\02\06\E0\07\1B\08\E0\07.\D0\F8\90\13\0F\C0\07\09)0\04\90\13\1D\04\90\13\17\04@\00'x\05\80\07h\1F\00$t\08\FF\F0\02\04\10\07\06p\07\16\11 \07\01\00\07\11\06\00\07\22\08\02P\0EE%v\08\11\10\00\01\00\07\16\05p\15 \A8\00\A0\12\18\08\A0\1B6\06{\12\00\07!\22\0F\F0\06\18@\00\15Es\12\00\12\F0\06\00 \15&\0E\12\F0\06*\0F\01\F0\0E\1A\09@\074\E4\0F\01 \0E\02\00\07.\0F\02\F0\0E\16\13\F0\0E\00\D0\06&t\0E\10\04\01\D0\064\06\13\10P\00\10\C8@\0F\11\09P\00\10\06P\00u\E4/\00'r\07\13P\06\01\10\07%\10\09\B0\06\00@\01!r\080\00#\07\0A\10\00)t\0F \0C\06\B0\05#\FC\03P\06\15\08P\06\00\F0\0C&z\08\B0\05\1B\C8`\05\10\CE\E0\05\0A@\06E\10h\06\06\10\05\1B\E4\E0\06\00 \00?Z\11\11 \0E\03\02p\06\06`\00\12\F0\A0\06\10X\90\04\06p\15'\10H`\00\01\F0\11!\06\08\BC\02\03\80\16'\10\080\00\1B\E4\00\0A\03\E0\03\14\040\01\01\10\06!\07\08\0C\03!\00\00P\00I\12x\06\06\A0\055x\07\070\00\10\CA \00(\08\07 \00\22\84y_\13\04\10\06E\84y\09\08\10\00\10\E2\80\05\10\050\17\08\80\05\01\B0\04$\0E \80\059\09\04\09`\0D\00\C0\00D@K\0F@\A0\04%\0E\090\00\10\C6\C0\04\18\04p\05X\04#t\0E\0E0\00\09p\05\10\E4`\04\11\04p\04$\04\08\00\05%\06\0E@\00\12\C6 \00\00\80\04\22\04\00 \00@#x\06\090\00$\06\08P\00%\05\0EP\00\12\C6 \00\000\00\13\060\00T\08s\04\00\04\F0\00\01 \127\06\00\06\C0\04\00\90\0C\14\04\A0\04 \C8\1F\10\00$\05\06\10\00/\C6/\90\16\01\1C\03\90\16&@\02 \00\0E\C0\17\16\03 \04\01\00\0B%\04\03\80\179\CC\1F\00\90\1Ef\A2\00\00\06{\09\F0\03\10\E2\A0\0A4\06\FF\FFP\00'\E2/\90\02\03@\08&t\050\03&\CA\1F\E0\05\01\10\04 \8E\00\D0&\0A\00\0B4\07\00\07\10\04%$\0E\C0\0A\01\80\03\00\C0\03:z\0F\0F\00\041\06\07\0F\80\05\02\B0\05T'r\06\06\03\90\00\16\C8\C0\03#\06\0A\10\007z\03\08\10\05\010\05\050\04\17\C6@\01!`\F0p\05I\10\0A\03\03\A0\03\1D\08\A0\03\15\03\A0\03\1B\DA`\03\00\10\09)\DA\06\00\04\00 \03\14\07\B0)\1B\CAP\03\10\CA\10\034\03\06\00\00\02\10\A4\00\03\10\03p\10\07\00\03\01\7F\103?\08 \00\070t\04\04\C0\02\22\05@\10\00E!t\08\04\A0\02\02 \07\1B\030\07\18\030\07E$x\03\04\A0\02\1B\CA\F0\06\11$\80\02'\03\08\10\0F\0FP\18\01(\E2\10P\18%\03\02P\18\02\C0\00\04\F0\17C\00\00\CAO\F0\17\17@ \00\11\04\B0\09\06 \00&\05\04\10\18\10\1E\00\04\02\C0\17\01 \00\14\1F\B0\17\03\F0\17Q.\00!r\06\10\03\06 \00%\07\06\D0\17`d\00\00!\A2\07\01\10\03 \00\11/\80\17/\07\80\80\17';M9\00\C0\07\11\00\10\03$\FF\E0\C0\07H\00\00@\0Ap\174\00\00 \10\02\01\00\02\00\FD\16\08p\17\00\FC\01\010\00\03p\17%\00\00p\17\02p\12\11\0D\80\05$b\F2\E0\05\01p\00\05`\17V\19\00\00\C0\09\90\00\04p\1D\11\0D\E0\02\11op\17\05`\02\11\C6\A0\07\15\07\90\00\01@\17\11\0E\80\1D$\02\02\00\04%\10\03\10\00\01\00\08\16\04`\0F \A8\02\E0\16\18\10P\0FY\10x\09\07@\10\1E&\05\07\B0\0F\00\90\1D*\12\09`\00*\0A\05`\00\07\90\17zj\01\00\81y\0A\0A@\17\17\14`\08\0C`\0F\00p#4\0D\0D\80p\00\01\B0\099\14\00\14`\17,\15\14`\0F\19\15`\0F\02\E0+\06`\0F\01`\07\07`\04\11\12\10\17\07`\0F$\12\07\80\00\01`\04\1B\10`\0F)\0F\12\80\048z\0B\10\00\1F\040\00\15\0F0\004\11\12\090\00\02P\08\12\0BP\04\12\F6\D0\0E8z\10\10\B0\04W'r\12\12\05\D0\01\06 \08\19\F2\A0\0F$\11\0Ap\00\16\16\10\0F\00\C0\049:\0B\0B\D0\04\1D8\E0\0E\15\0B\D0\04\11\E2\C0\00\06\D0\0F\18\E2p\08\12\F6`\08\1A\1A\C0\08\06@\00*\FA\03\C0\08\13\F4P\00'\10\16\A0\11:\00\10\18\00\0F6\12z\140\05\00\A0\00\08\10\10\18\C4P\00\02\10\09R\07r\0E\14\0E@\03\11\050\00\1BZ\E0\00\17(`\00\0F\90\16\00\06\B0\00\13\F2P\004\0F\14\0FP\00\01p\0E\16\13\80\16\1B\E2`\0A\1B\E2\00\10\0F0\0F\00H\10X\11\11\80\00T\84y\15\13\00`\05\1B\E2 \01\0F@\0F\00f\84y\0B\13\00\80\80\0C\09\00\19\00\80\01\1C\18`\00%\17\0F`\00\10\E6\D0\004\11\14\11\D0\00\04`\0F\04P\00Wd\0E\00\10(P\00\000\025x\11\11\C0\00\01\10\014\12\14\12@\00\01\B0\06\16\10P\0F\11\E2@\0F\15\11\B0\00\1B\C8\E0\17\01p\00%\19\12\90\00\1Bh\80\0F\22\E8\0E\C0\17\16\80\10\01 \0Bx\C56\94\80~\00B\FC\03\00\C4\1F\10\002\00\00\E2\A0\01B\0Bx\00\0F \00\000\01#/\04\10\00A\00\00\E2\F6P\0BB h\0B\0B\866\11@P\05! \98\10\00\11K\10\00u\E2\0F\00 (\0F\0F \00\01\F0\06\10\0Ba\00\020\04\00 \008\B8\0F\0F0\00Q!r\15\04\15a\00\00 \00\11O@\09\08\00\0CV!r\17\08\17\D0\0FW\01#t\13\15\A0\0AX\08\84y\08\12\C0\01E#t\1B\17 \00\00`\0A\10t\90\114@K\040\175t\1B\1B\10\00\10\E4\D0\07%\14\13\D0\07\01p\007\19\06\190\18E!t\16\1B \00\01\B0\0A(\14\15p\17 $x\CF $\80\00\B0\00@#x\16\17 \00&\16\080\00\05`\17\01`\00%\11\0A\A0\10|\E2\8F\00$x\1B\1B@\00\000\00\13\160\00X\08s\15\00\14\B0\0A8t\17\19\00\01H#t\13\11\80\0BE#t\17\17\E0\00\02 \00\06\00\01\10\E4P\007\16\00\16\A0\0FI!t\0A\17\00\01)t\04\10\01E$x\17\17\A0\00\01\D0\00 \0A\19\E0\00\22\0A\08 \00H#x\04\11\C0\0B\00\90\12\08\D0\00R r\06\06\15\EA7\01\C0\17\00@\00\00\E0\00&\0A\00@\00\08\E0\0B\00\F0\00\15\0F0\02e\02\00 h\06\06P\02\02\C0\02\12\08\A0\02\14\F8\C0\02\15\10\D0\02\01p\005\16\1B\16p\00\14O0\002\00\00\E20\0BH \98\06\06\80\021\08s\0A;4\04@\02K (\16\16p\00\12\10@\00\13\F4`\007\0F\0B\06\D0\00\01`%\16^p\07H \B8\16\16`\00\09`\08\00 \008H\08\08`\00\09\A0\0Cxb\0E\00 h\10\10 \00F r\15\15\D0\00h\8F\00 \D8\08\08`\001\86y\000\11\CA\04\19\10\0C\00\E6\05\00 \A8\10\10\E0\00\03`\19\00P\01\84\22\07\00 r\0A\17\0AP\00u\C8\1F\00 H\0A\0Ap\00\12\C6\F0\19\15\100\00D\0E\00 rp)\02\F0\00H \D8\0A\0A`\00\01\C02\04\00\01z\E4\8F\00 h\13\13\C0\00%\11\11p\00z\C6\0F\01 \A8\13\13@\00%\0A\03@\00\01\C0\08(\02\05P\01T\86y\00\0A\15\E0\00E\E2\05\00  \15\00P\00\10\1F \00$\08\11 \00 \E8\05\10\00'\02\130\00hG\99\00\00@\F6 \12\01=\00\05\D0\099M\09\00\10\009\06{\04\80\0D\00p\0A#0\03 \00\09\A0\07\03P\06\09\80\10\16O\D0\01\00@\01\10\A4\80\0D&\02\04 \09uO\00\05s\03\00\02 \09\15\04\10#\02`\03QO\00$r\05\10\00\10\03P\08 \C8\9F\C0%\1A\050\091\09\03\050\0C\02\10\00\00\E06\15\04@\00\00\00\08\11\04\A0\0A\16\00P\01\01\A0*3\0B\02\8Ep\0E\19\02p(9'r\05\F0\1D\09P\0B\01\F0\17\11\07\90\00\13\05\D0\116$t\0A\A0\05\01p\00*\06\07P\22\1C\07\F0\0D\1A\06\F0\0D)\06\06@\088\08\05\05 \07\060\00\12\F20\00\17\18 \00\01\D0\0B\12\05 (#\00\000\07(\05\05\E0\0DE\12x\03\05 \07\84\CA\1F\00\84y\05\03\00`\03\10\E8 \07F\06\03\00\80\80\0DB\0Bx\00\06@\04\00\C0\1B#\1F\04\10\00\02\10\07W\D6\0F\00 \08\80\04\1B\C8@\04&\CC\0FP\10#\10\00\E0\051\02\02\05A\00\04P\0E\01?+\11?\15\0C\00 \0E\03P\1E\22\0A@\10\008!t\07 \1E\19\040\1D\01\D0\10 \07\02`\05#\07\08@\005x\07\02\80\1D\00\E0(\22s\02\D0\0F\03\D0\00$ r\B0+\11@ \15\01\00\02\13^\00\02\10\C6\D0\00(\05\05\A0\03\09\F0\0C\00 \005\98\05\05\E0\00\00\F0\00#r\05@)\02\D0\07\00`\03\15\05`\03\11\01\00\11/p\FD`\03\08\0B\10\00\04\F0\02\13\13p\02\81\02x\04\00\10:\00\00\F5\05\12\E2\D0'\14\01\D0\01E\C4\0F\00D\80\03\11\C0@\00\00P(\14\07p&\00\C0\17Gt\03\FF\02\C0\02\06\E0.\01\90\1DR\04\08r\13\070\04\02\90!\00p\00\14\80p\00\00\10\07$r\02 \00\14\00p\00)\C0\02p\00\12\02p\00\15\F0p\00\1F\04p\00\02\00\90\06\02P\00\01?\00\00F\16\03p\00\1B\F0p\00\03 \00\14\02p\00\1FPp\00\0C\1F\08p\00\05\01\E0\00\09p\00\01P\01#`;p\00\1F\C4p\00\03/\E0\01p\00\0B\1F\10p\00\17\01\E0\00/\D0;\E0\00\0B\17pp\00\04\D0\12\13\07`\00fGy\00\00\F0\D7@\02\09\00\02\03 \02& <\D0\00\00P\00\15 P\00\11?p\03\01\A0\00\16\00@\02\1B\02@\00\1B`@\00&\E0\00\90\00\0F@\00\01\1B\04@\00\1B\A0@\00\1F\A0@\00\0C\1B\08@\00\1B\E0@\00\19`@\00\1D\06@\00\1B\10@\00&0=@\00\04@\03\17\06 \03\17\10P\00\00P\01(`\EEP\01\00 \14\06P\00fHs\00\00\1A\00\A040\89s\07@*A\0C\0B\00\0E@\04`Py\00\04\80\C2@\00R\C3\03\00\EA\1F\A0\01\13\FFP\00f\C0\0F\00\18y\00\01\00\0F\10\00\D0\0F\01\00-\00\AB\02.\03\00\01\00\22@\00\01\00=%\01\000\00\08\01\00\1F\0B@\00\04\13e)\00\1F\F1@\00\0C\00\DC\04.\02\00\01\00\13XU\00$\C0\00\AD\03\00T\02\02G\06\00\01\00\03\948/\00\E6T\00\00\00\01\00\02\A5!\02\00E\0F\80\00\09\11)\06\00\05\EFC\05\01\00\13\F0@\00\11T\06\00\06\E4\00*\04\00\01\00/[\00@\00\03\12D\95\066\00\DC\00@\00\1F\0A@\00\00\00-'\1F\0B@\00\00\11 \B5\07\0E\C0\00\1B\08\08\00\11\F3\9C2\0E\01\00\13\F8@\00& \00\80\00\17\048\00\02\94\033\00\00\B5\14\01\0C\84\01\13\18\F5\03\00|<\04\01\00\0F\C0\00\01\132@\00\15\06R\00\03\01\00\03\95\00&\80>\80\00j\07\00\00&\80\00\01\00\13\87\94\00*\03\00\01\00\040J\01]\00\0F\80\00\08\13\06l\1D\04hJ\0C\01\00\1B\A8\08\00\17\08W\00\17\05\E8\00\0C\01\00*h@\08\00\088\00\18\06\A0\00\0F\01\00\05\00)\00\B0\00\00\00\08\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1w1r_0() {addr_space = 3 : i32} : !llvm.array<32 x f32>
    llvm.mlir.global internal @__wg_main_kStitch_divide__13_1_0___1w1r_1() {addr_space = 3 : i32} : !llvm.array<32 x f32>
    llvm.func @__nv_expf(f32) -> f32
    llvm.func @main_kStitch_divide__13_1_0___1w1r(%arg0: !llvm.ptr<f32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: !llvm.ptr<f32>) attributes {disc.elimargs = [0 : index, 2 : index, 3 : index, 6 : index, 7 : index, 8 : index, 10 : index, 12 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(-1 : i32) : i32
      %1 = llvm.mlir.constant(8 : index) : i32
      %2 = llvm.mlir.constant(4 : index) : i32
      %3 = llvm.mlir.constant(64 : index) : i32
      %4 = llvm.mlir.constant(96 : index) : i32
      %5 = llvm.mlir.constant(128 : index) : i32
      %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %7 = llvm.mlir.constant(1 : i32) : i32
      %8 = llvm.mlir.constant(32 : i32) : i32
      %9 = llvm.mlir.constant(2 : i32) : i32
      %10 = llvm.mlir.constant(4 : i32) : i32
      %11 = llvm.mlir.constant(8 : i32) : i32
      %12 = llvm.mlir.constant(16 : i32) : i32
      %13 = llvm.mlir.constant(-0.000000e+00 : f32) : f32
      %14 = llvm.mlir.constant(1 : index) : i32
      %15 = llvm.mlir.constant(32 : index) : i32
      %16 = llvm.mlir.constant(0 : index) : i32
      %17 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1w1r_0 : !llvm.ptr<array<32 x f32>, 3>
      %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %19 = llvm.mlir.addressof @__wg_main_kStitch_divide__13_1_0___1w1r_1 : !llvm.ptr<array<32 x f32>, 3>
      %20 = llvm.getelementptr %19[0, 0] : (!llvm.ptr<array<32 x f32>, 3>) -> !llvm.ptr<f32, 3>
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %21 = nvvm.read.ptx.sreg.ctaid.x : i32
      %22 = nvvm.read.ptx.sreg.tid.x : i32
      %23 = llvm.udiv %22, %15  : i32
      %24 = llvm.urem %22, %15  : i32
      %25 = llvm.mul %21, %1  : i32
      %26 = llvm.add %25, %23  : i32
      %27 = llvm.icmp "slt" %26, %arg3 : i32
      llvm.cond_br %27, ^bb2, ^bb11
    ^bb2:  // pred: ^bb1
      %28 = llvm.sub %arg2, %24  : i32
      %29 = llvm.icmp "eq" %28, %16 : i32
      %30 = llvm.sub %28, %14  : i32
      %31 = llvm.udiv %30, %15  : i32
      %32 = llvm.add %31, %14  : i32
      %33 = llvm.select %29, %16, %32 : i1, i32
      %34 = llvm.srem %33, %2  : i32
      %35 = llvm.sub %33, %34  : i32
      %36 = llvm.mul %35, %15  : i32
      %37 = llvm.add %24, %36  : i32
      %38 = llvm.mul %26, %arg2  : i32
      llvm.br ^bb3(%24, %6 : i32, f32)
    ^bb3(%39: i32, %40: f32):  // 2 preds: ^bb2, ^bb4
      %41 = llvm.icmp "slt" %39, %37 : i32
      llvm.cond_br %41, ^bb4, ^bb5
    ^bb4:  // pred: ^bb3
      %42 = llvm.add %39, %15  : i32
      %43 = llvm.add %39, %3  : i32
      %44 = llvm.add %39, %4  : i32
      %45 = llvm.add %38, %39  : i32
      %46 = llvm.add %38, %42  : i32
      %47 = llvm.add %38, %43  : i32
      %48 = llvm.add %38, %44  : i32
      %49 = llvm.getelementptr %arg0[%45] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %50 = llvm.load %49 : !llvm.ptr<f32>
      %51 = llvm.getelementptr %arg0[%46] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %52 = llvm.load %51 : !llvm.ptr<f32>
      %53 = llvm.getelementptr %arg0[%47] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %54 = llvm.load %53 : !llvm.ptr<f32>
      %55 = llvm.getelementptr %arg0[%48] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %56 = llvm.load %55 : !llvm.ptr<f32>
      %57 = llvm.fcmp "ugt" %40, %50 : f32
      %58 = llvm.select %57, %40, %50 : i1, f32
      %59 = llvm.fcmp "uno" %50, %50 : f32
      %60 = llvm.select %59, %50, %58 : i1, f32
      %61 = llvm.fcmp "ugt" %60, %52 : f32
      %62 = llvm.select %61, %60, %52 : i1, f32
      %63 = llvm.fcmp "uno" %52, %52 : f32
      %64 = llvm.select %63, %52, %62 : i1, f32
      %65 = llvm.fcmp "ugt" %64, %54 : f32
      %66 = llvm.select %65, %64, %54 : i1, f32
      %67 = llvm.fcmp "uno" %54, %54 : f32
      %68 = llvm.select %67, %54, %66 : i1, f32
      %69 = llvm.fcmp "ugt" %68, %56 : f32
      %70 = llvm.select %69, %68, %56 : i1, f32
      %71 = llvm.fcmp "uno" %56, %56 : f32
      %72 = llvm.select %71, %56, %70 : i1, f32
      %73 = llvm.add %39, %5  : i32
      llvm.br ^bb3(%73, %72 : i32, f32)
    ^bb5:  // pred: ^bb3
      llvm.br ^bb6(%37, %40 : i32, f32)
    ^bb6(%74: i32, %75: f32):  // 2 preds: ^bb5, ^bb7
      %76 = llvm.icmp "slt" %74, %arg2 : i32
      llvm.cond_br %76, ^bb7, ^bb8
    ^bb7:  // pred: ^bb6
      %77 = llvm.add %38, %74  : i32
      %78 = llvm.getelementptr %arg0[%77] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %79 = llvm.load %78 : !llvm.ptr<f32>
      %80 = llvm.fcmp "ugt" %75, %79 : f32
      %81 = llvm.select %80, %75, %79 : i1, f32
      %82 = llvm.fcmp "uno" %79, %79 : f32
      %83 = llvm.select %82, %79, %81 : i1, f32
      %84 = llvm.add %74, %15  : i32
      llvm.br ^bb6(%84, %83 : i32, f32)
    ^bb8:  // pred: ^bb6
      %85 = llvm.sub %8, %8  : i32
      %86 = llvm.lshr %0, %85  : i32
      %87 = llvm.sub %8, %7  : i32
      %88 = nvvm.shfl.sync  bfly %86, %75, %7, %87 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %89 = llvm.extractvalue %88[0] : !llvm.struct<(f32, i1)> 
      %90 = llvm.fcmp "ugt" %75, %89 : f32
      %91 = llvm.select %90, %75, %89 : i1, f32
      %92 = llvm.fcmp "uno" %89, %89 : f32
      %93 = llvm.select %92, %89, %91 : i1, f32
      %94 = llvm.sub %8, %8  : i32
      %95 = llvm.lshr %0, %94  : i32
      %96 = llvm.sub %8, %7  : i32
      %97 = nvvm.shfl.sync  bfly %95, %93, %9, %96 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %98 = llvm.extractvalue %97[0] : !llvm.struct<(f32, i1)> 
      %99 = llvm.fcmp "ugt" %93, %98 : f32
      %100 = llvm.select %99, %93, %98 : i1, f32
      %101 = llvm.fcmp "uno" %98, %98 : f32
      %102 = llvm.select %101, %98, %100 : i1, f32
      %103 = llvm.sub %8, %8  : i32
      %104 = llvm.lshr %0, %103  : i32
      %105 = llvm.sub %8, %7  : i32
      %106 = nvvm.shfl.sync  bfly %104, %102, %10, %105 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %107 = llvm.extractvalue %106[0] : !llvm.struct<(f32, i1)> 
      %108 = llvm.fcmp "ugt" %102, %107 : f32
      %109 = llvm.select %108, %102, %107 : i1, f32
      %110 = llvm.fcmp "uno" %107, %107 : f32
      %111 = llvm.select %110, %107, %109 : i1, f32
      %112 = llvm.sub %8, %8  : i32
      %113 = llvm.lshr %0, %112  : i32
      %114 = llvm.sub %8, %7  : i32
      %115 = nvvm.shfl.sync  bfly %113, %111, %11, %114 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %116 = llvm.extractvalue %115[0] : !llvm.struct<(f32, i1)> 
      %117 = llvm.fcmp "ugt" %111, %116 : f32
      %118 = llvm.select %117, %111, %116 : i1, f32
      %119 = llvm.fcmp "uno" %116, %116 : f32
      %120 = llvm.select %119, %116, %118 : i1, f32
      %121 = llvm.sub %8, %8  : i32
      %122 = llvm.lshr %0, %121  : i32
      %123 = llvm.sub %8, %7  : i32
      %124 = nvvm.shfl.sync  bfly %122, %120, %12, %123 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %125 = llvm.extractvalue %124[0] : !llvm.struct<(f32, i1)> 
      %126 = llvm.fcmp "ugt" %120, %125 : f32
      %127 = llvm.select %126, %120, %125 : i1, f32
      %128 = llvm.fcmp "uno" %125, %125 : f32
      %129 = llvm.select %128, %125, %127 : i1, f32
      %130 = llvm.icmp "eq" %24, %16 : i32
      llvm.cond_br %130, ^bb9, ^bb10
    ^bb9:  // pred: ^bb8
      %131 = llvm.getelementptr %18[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %129, %131 : !llvm.ptr<f32, 3>
      llvm.br ^bb10
    ^bb10:  // 2 preds: ^bb8, ^bb9
      llvm.br ^bb11
    ^bb11:  // 2 preds: ^bb1, ^bb10
      nvvm.barrier0
      llvm.cond_br %27, ^bb12, ^bb21
    ^bb12:  // pred: ^bb11
      %132 = llvm.sub %arg2, %24  : i32
      %133 = llvm.icmp "eq" %132, %16 : i32
      %134 = llvm.sub %132, %14  : i32
      %135 = llvm.udiv %134, %15  : i32
      %136 = llvm.add %135, %14  : i32
      %137 = llvm.select %133, %16, %136 : i1, i32
      %138 = llvm.srem %137, %2  : i32
      %139 = llvm.sub %137, %138  : i32
      %140 = llvm.mul %139, %15  : i32
      %141 = llvm.add %24, %140  : i32
      %142 = llvm.mul %26, %arg2  : i32
      llvm.br ^bb13(%24, %13 : i32, f32)
    ^bb13(%143: i32, %144: f32):  // 2 preds: ^bb12, ^bb14
      %145 = llvm.icmp "slt" %143, %141 : i32
      llvm.cond_br %145, ^bb14, ^bb15
    ^bb14:  // pred: ^bb13
      %146 = llvm.add %143, %15  : i32
      %147 = llvm.add %143, %3  : i32
      %148 = llvm.add %143, %4  : i32
      %149 = llvm.add %142, %143  : i32
      %150 = llvm.add %142, %146  : i32
      %151 = llvm.add %142, %147  : i32
      %152 = llvm.add %142, %148  : i32
      %153 = llvm.udiv %149, %arg2  : i32
      %154 = llvm.urem %153, %arg1  : i32
      %155 = llvm.udiv %153, %arg1  : i32
      %156 = llvm.udiv %150, %arg2  : i32
      %157 = llvm.urem %156, %arg1  : i32
      %158 = llvm.udiv %156, %arg1  : i32
      %159 = llvm.udiv %151, %arg2  : i32
      %160 = llvm.urem %159, %arg1  : i32
      %161 = llvm.udiv %159, %arg1  : i32
      %162 = llvm.udiv %152, %arg2  : i32
      %163 = llvm.urem %162, %arg1  : i32
      %164 = llvm.udiv %162, %arg1  : i32
      %165 = llvm.getelementptr %arg0[%149] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %166 = llvm.load %165 : !llvm.ptr<f32>
      %167 = llvm.getelementptr %arg0[%150] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %168 = llvm.load %167 : !llvm.ptr<f32>
      %169 = llvm.getelementptr %arg0[%151] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %170 = llvm.load %169 : !llvm.ptr<f32>
      %171 = llvm.getelementptr %arg0[%152] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %172 = llvm.load %171 : !llvm.ptr<f32>
      %173 = llvm.mul %155, %arg1  : i32
      %174 = llvm.add %173, %154  : i32
      %175 = llvm.mul %158, %arg1  : i32
      %176 = llvm.add %175, %157  : i32
      %177 = llvm.mul %161, %arg1  : i32
      %178 = llvm.add %177, %160  : i32
      %179 = llvm.mul %164, %arg1  : i32
      %180 = llvm.add %179, %163  : i32
      %181 = llvm.urem %174, %1  : i32
      %182 = llvm.urem %176, %1  : i32
      %183 = llvm.urem %178, %1  : i32
      %184 = llvm.urem %180, %1  : i32
      %185 = llvm.getelementptr %18[%181] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %186 = llvm.load %185 : !llvm.ptr<f32, 3>
      %187 = llvm.getelementptr %18[%182] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %188 = llvm.load %187 : !llvm.ptr<f32, 3>
      %189 = llvm.getelementptr %18[%183] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %190 = llvm.load %189 : !llvm.ptr<f32, 3>
      %191 = llvm.getelementptr %18[%184] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %192 = llvm.load %191 : !llvm.ptr<f32, 3>
      %193 = llvm.fsub %166, %186  : f32
      %194 = llvm.fsub %168, %188  : f32
      %195 = llvm.fsub %170, %190  : f32
      %196 = llvm.fsub %172, %192  : f32
      %197 = llvm.call @__nv_expf(%193) : (f32) -> f32
      %198 = llvm.call @__nv_expf(%194) : (f32) -> f32
      %199 = llvm.call @__nv_expf(%195) : (f32) -> f32
      %200 = llvm.call @__nv_expf(%196) : (f32) -> f32
      %201 = llvm.fadd %144, %197  : f32
      %202 = llvm.fadd %201, %198  : f32
      %203 = llvm.fadd %202, %199  : f32
      %204 = llvm.fadd %203, %200  : f32
      %205 = llvm.add %143, %5  : i32
      llvm.br ^bb13(%205, %204 : i32, f32)
    ^bb15:  // pred: ^bb13
      llvm.br ^bb16(%141, %144 : i32, f32)
    ^bb16(%206: i32, %207: f32):  // 2 preds: ^bb15, ^bb17
      %208 = llvm.icmp "slt" %206, %arg2 : i32
      llvm.cond_br %208, ^bb17, ^bb18
    ^bb17:  // pred: ^bb16
      %209 = llvm.add %142, %206  : i32
      %210 = llvm.udiv %209, %arg2  : i32
      %211 = llvm.urem %210, %arg1  : i32
      %212 = llvm.udiv %210, %arg1  : i32
      %213 = llvm.getelementptr %arg0[%209] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %214 = llvm.load %213 : !llvm.ptr<f32>
      %215 = llvm.mul %212, %arg1  : i32
      %216 = llvm.add %215, %211  : i32
      %217 = llvm.urem %216, %1  : i32
      %218 = llvm.getelementptr %18[%217] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %219 = llvm.load %218 : !llvm.ptr<f32, 3>
      %220 = llvm.fsub %214, %219  : f32
      %221 = llvm.call @__nv_expf(%220) : (f32) -> f32
      %222 = llvm.fadd %207, %221  : f32
      %223 = llvm.add %206, %15  : i32
      llvm.br ^bb16(%223, %222 : i32, f32)
    ^bb18:  // pred: ^bb16
      %224 = llvm.sub %8, %8  : i32
      %225 = llvm.lshr %0, %224  : i32
      %226 = llvm.sub %8, %7  : i32
      %227 = nvvm.shfl.sync  bfly %225, %207, %7, %226 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %228 = llvm.extractvalue %227[0] : !llvm.struct<(f32, i1)> 
      %229 = llvm.fadd %207, %228  : f32
      %230 = llvm.sub %8, %8  : i32
      %231 = llvm.lshr %0, %230  : i32
      %232 = llvm.sub %8, %7  : i32
      %233 = nvvm.shfl.sync  bfly %231, %229, %9, %232 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %234 = llvm.extractvalue %233[0] : !llvm.struct<(f32, i1)> 
      %235 = llvm.fadd %229, %234  : f32
      %236 = llvm.sub %8, %8  : i32
      %237 = llvm.lshr %0, %236  : i32
      %238 = llvm.sub %8, %7  : i32
      %239 = nvvm.shfl.sync  bfly %237, %235, %10, %238 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %240 = llvm.extractvalue %239[0] : !llvm.struct<(f32, i1)> 
      %241 = llvm.fadd %235, %240  : f32
      %242 = llvm.sub %8, %8  : i32
      %243 = llvm.lshr %0, %242  : i32
      %244 = llvm.sub %8, %7  : i32
      %245 = nvvm.shfl.sync  bfly %243, %241, %11, %244 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %246 = llvm.extractvalue %245[0] : !llvm.struct<(f32, i1)> 
      %247 = llvm.fadd %241, %246  : f32
      %248 = llvm.sub %8, %8  : i32
      %249 = llvm.lshr %0, %248  : i32
      %250 = llvm.sub %8, %7  : i32
      %251 = nvvm.shfl.sync  bfly %249, %247, %12, %250 {return_value_and_is_valid} : f32 -> !llvm.struct<(f32, i1)>
      %252 = llvm.extractvalue %251[0] : !llvm.struct<(f32, i1)> 
      %253 = llvm.fadd %247, %252  : f32
      %254 = llvm.icmp "eq" %24, %16 : i32
      llvm.cond_br %254, ^bb19, ^bb20
    ^bb19:  // pred: ^bb18
      %255 = llvm.getelementptr %20[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %253, %255 : !llvm.ptr<f32, 3>
      llvm.br ^bb20
    ^bb20:  // 2 preds: ^bb18, ^bb19
      llvm.br ^bb21
    ^bb21:  // 2 preds: ^bb11, ^bb20
      nvvm.barrier0
      llvm.cond_br %27, ^bb22, ^bb29
    ^bb22:  // pred: ^bb21
      %256 = llvm.sub %arg2, %24  : i32
      %257 = llvm.icmp "eq" %256, %16 : i32
      %258 = llvm.sub %256, %14  : i32
      %259 = llvm.udiv %258, %15  : i32
      %260 = llvm.add %259, %14  : i32
      %261 = llvm.select %257, %16, %260 : i1, i32
      %262 = llvm.srem %261, %2  : i32
      %263 = llvm.sub %261, %262  : i32
      %264 = llvm.mul %263, %15  : i32
      %265 = llvm.add %24, %264  : i32
      %266 = llvm.mul %26, %arg2  : i32
      llvm.br ^bb23(%24 : i32)
    ^bb23(%267: i32):  // 2 preds: ^bb22, ^bb24
      %268 = llvm.icmp "slt" %267, %265 : i32
      llvm.cond_br %268, ^bb24, ^bb25
    ^bb24:  // pred: ^bb23
      %269 = llvm.add %267, %15  : i32
      %270 = llvm.add %267, %3  : i32
      %271 = llvm.add %267, %4  : i32
      %272 = llvm.add %266, %267  : i32
      %273 = llvm.add %266, %269  : i32
      %274 = llvm.add %266, %270  : i32
      %275 = llvm.add %266, %271  : i32
      %276 = llvm.udiv %272, %arg2  : i32
      %277 = llvm.urem %276, %arg1  : i32
      %278 = llvm.udiv %276, %arg1  : i32
      %279 = llvm.udiv %273, %arg2  : i32
      %280 = llvm.urem %279, %arg1  : i32
      %281 = llvm.udiv %279, %arg1  : i32
      %282 = llvm.udiv %274, %arg2  : i32
      %283 = llvm.urem %282, %arg1  : i32
      %284 = llvm.udiv %282, %arg1  : i32
      %285 = llvm.udiv %275, %arg2  : i32
      %286 = llvm.urem %285, %arg1  : i32
      %287 = llvm.udiv %285, %arg1  : i32
      %288 = llvm.getelementptr %arg0[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %289 = llvm.load %288 : !llvm.ptr<f32>
      %290 = llvm.getelementptr %arg0[%273] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %291 = llvm.load %290 : !llvm.ptr<f32>
      %292 = llvm.getelementptr %arg0[%274] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %293 = llvm.load %292 : !llvm.ptr<f32>
      %294 = llvm.getelementptr %arg0[%275] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %295 = llvm.load %294 : !llvm.ptr<f32>
      %296 = llvm.mul %278, %arg1  : i32
      %297 = llvm.add %296, %277  : i32
      %298 = llvm.mul %281, %arg1  : i32
      %299 = llvm.add %298, %280  : i32
      %300 = llvm.mul %284, %arg1  : i32
      %301 = llvm.add %300, %283  : i32
      %302 = llvm.mul %287, %arg1  : i32
      %303 = llvm.add %302, %286  : i32
      %304 = llvm.urem %297, %1  : i32
      %305 = llvm.urem %299, %1  : i32
      %306 = llvm.urem %301, %1  : i32
      %307 = llvm.urem %303, %1  : i32
      %308 = llvm.getelementptr %18[%304] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %309 = llvm.load %308 : !llvm.ptr<f32, 3>
      %310 = llvm.getelementptr %18[%305] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %311 = llvm.load %310 : !llvm.ptr<f32, 3>
      %312 = llvm.getelementptr %18[%306] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %313 = llvm.load %312 : !llvm.ptr<f32, 3>
      %314 = llvm.getelementptr %18[%307] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %315 = llvm.load %314 : !llvm.ptr<f32, 3>
      %316 = llvm.fsub %289, %309  : f32
      %317 = llvm.fsub %291, %311  : f32
      %318 = llvm.fsub %293, %313  : f32
      %319 = llvm.fsub %295, %315  : f32
      %320 = llvm.call @__nv_expf(%316) : (f32) -> f32
      %321 = llvm.call @__nv_expf(%317) : (f32) -> f32
      %322 = llvm.call @__nv_expf(%318) : (f32) -> f32
      %323 = llvm.call @__nv_expf(%319) : (f32) -> f32
      %324 = llvm.getelementptr %20[%304] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %325 = llvm.load %324 : !llvm.ptr<f32, 3>
      %326 = llvm.getelementptr %20[%305] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %327 = llvm.load %326 : !llvm.ptr<f32, 3>
      %328 = llvm.getelementptr %20[%306] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %329 = llvm.load %328 : !llvm.ptr<f32, 3>
      %330 = llvm.getelementptr %20[%307] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %331 = llvm.load %330 : !llvm.ptr<f32, 3>
      %332 = llvm.fdiv %320, %325  : f32
      %333 = llvm.fdiv %321, %327  : f32
      %334 = llvm.fdiv %322, %329  : f32
      %335 = llvm.fdiv %323, %331  : f32
      %336 = llvm.getelementptr %arg4[%272] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %332, %336 : !llvm.ptr<f32>
      %337 = llvm.getelementptr %arg4[%273] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %333, %337 : !llvm.ptr<f32>
      %338 = llvm.getelementptr %arg4[%274] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %334, %338 : !llvm.ptr<f32>
      %339 = llvm.getelementptr %arg4[%275] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %335, %339 : !llvm.ptr<f32>
      %340 = llvm.add %267, %5  : i32
      llvm.br ^bb23(%340 : i32)
    ^bb25:  // pred: ^bb23
      llvm.br ^bb26(%265 : i32)
    ^bb26(%341: i32):  // 2 preds: ^bb25, ^bb27
      %342 = llvm.icmp "slt" %341, %arg2 : i32
      llvm.cond_br %342, ^bb27, ^bb28
    ^bb27:  // pred: ^bb26
      %343 = llvm.add %266, %341  : i32
      %344 = llvm.udiv %343, %arg2  : i32
      %345 = llvm.urem %344, %arg1  : i32
      %346 = llvm.udiv %344, %arg1  : i32
      %347 = llvm.getelementptr %arg0[%343] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %348 = llvm.load %347 : !llvm.ptr<f32>
      %349 = llvm.mul %346, %arg1  : i32
      %350 = llvm.add %349, %345  : i32
      %351 = llvm.urem %350, %1  : i32
      %352 = llvm.getelementptr %18[%351] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %353 = llvm.load %352 : !llvm.ptr<f32, 3>
      %354 = llvm.fsub %348, %353  : f32
      %355 = llvm.call @__nv_expf(%354) : (f32) -> f32
      %356 = llvm.getelementptr %20[%351] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %357 = llvm.load %356 : !llvm.ptr<f32, 3>
      %358 = llvm.fdiv %355, %357  : f32
      %359 = llvm.getelementptr %arg4[%343] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %358, %359 : !llvm.ptr<f32>
      %360 = llvm.add %341, %15  : i32
      llvm.br ^bb26(%360 : i32)
    ^bb28:  // pred: ^bb26
      llvm.br ^bb29
    ^bb29:  // 2 preds: ^bb21, ^bb28
      llvm.return
    }
  }
}


// -----// IR Dump After DiscToLLVMPass (disc-to-llvm) //----- //
module attributes {gpu.container_module} {
  llvm.mlir.global internal constant @main_kernel_main_kStitch_divide__13_1_0___1b1r_kernel_name("main_kStitch_divide__13_1_0___1b1r\00")
  llvm.mlir.global internal constant @main_kernel_blob_gpu.binary("P\EDU\BA\01\00\10\00\E8\19\00\00\00\00\00\00\02\00\01\01@\00\00\00\A8\19\00\00\00\00\00\00\A2\19\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8:\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\002@:\00\01\00\117\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0D\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F5)_shndx\00.nv.info\00.text.main_kStitch_divide__13_1_0___1b1r2\00\0F,\00\15oshared.\00\15\9Fconstant21\00\1D\1F01\00\12\FD\01debug_frame\00.rel\11\00\1Aa\12\00!nv&\00oactionV\01 \0F\94\00\0F\0Fy\01\A2a__ocg_3\00/\00$0\00\0F\FF\16$__cuda_sm70_shflsync_bfly_p\00$____wg_G\00\0F@_0_$\0D\00/265\00\19_1__281\00\19\122f\00/305\00\19_3__32\91\02\1Fo_param\98\02.\0F\01\00\06\8CU\00\00\00\03\00\0B\00\01\00\11\AA\18\00,\0C\00\01\00\11\D8\18\00,\09\00\01\00\A1\15\01\00\00\22\00\0B\00`,\0F\001\00 \01\07\000\00!\020\00,\0A\00\01\00\11Y\18\00,\04\00\01\00\11\89\18\00,\07\00\01\00f2\00\00\00\12\10\A8\00!\80-\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13P\C0\00\10\04\9B\00R\044\00\00\00E\005\04\D4\0A<\00&8\00H\00\04p\00L\88\80\80(o\00\10\08\15\00x\16\89\80\80(\09\05J\01\004\00\22\18\00\01\00\22p\00\01\00\048\01\13\108\01\90\04/\08\00\08\00\00\00$\0C\00%#\08*\01'\04\12\0C\00\17\11\0C\00\12#0\00\04$\00\17\08$\00\05\0C\00!7\04$\06\90\015\00\00\04\0A\08\00\05x\00\A3\01\18\00\03\19\18\00\04\17\0CH\06U\10\00\00\F0!\10\00\10\01>\00%\F0\11\10\00\01\01\00\F09\F0!\00\03\1B\FF\00\04(X\00 \12\00\00p\12\00\00\B0\12\00\00\00\13\00\00`\13\00\00\90\14\00\00\D0\14\00\00\10\15\00\000!\00\00`!\00\00\80!\00\00\A0!\00\00\C0!\00\00P\22\00\00p\22\00\00\90\08\00\F4\16*\00\00\E0*\00\00`+\00\00\B0+\00\00\F0+\00\000,\00\00\04\1C\08\00@(\00\00@*\00\00\04\1E\E6\00C\00\00K\00\01\00\94\02\02\08\10\0A/\22\00\00\DF\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\E0\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00@\00\00\00\14\FB\01\22\C8\00\01\00\13\02\D1\00*H\00\10\00/\04\00\01\00\FF|@$v\01\FF\D7\04\C1\FF\00\8E\07\00\C4\0F\00\19y\02\00\01\00\F1\00!\00\00\00\22\0E\00\B9z\04\00\00F\00\00\FB\04\90\E2\0F\00Ey\00\00\10\0D!\00 \80\03\10\00\81$t\0C\FF\00\00\80\FF@\00\F3\05\E2\0F\00\10z\00\02\00Z\00\00\FF\E1\FF\07\00\E4\1F\04\0C\10\00\A0pR\F0\03\00\E4\0F\00\10xl\00\F0\07\00\00\FF\E0\FF\07\00\C8\0F\00\12x\03\00\00\FC\FF\FF\FF\C0\8E\07 \002\19y\00\01\00\10%\9D\00\83\0E\00\07r\03\03\FF\00\01\00\000\000r\03\03\A2\001\FF\FC\8E@\00P\0Cr\00\02\03\80\04\D3b\F2\03\00\DA\0F\00G\19\00\00`\0C\A0\00a\EA\0F\00\10r\04 \00\01\90\00\01\C0\00F\01\00\F0\06\C0\001\1Cx\00\01\00?p\F0\F0\D0\00\02@\0Cx\00\04?\062pB\F4 \00Ar\0E\FF\FF\D0\02\C6\8E\07\00\D8\0F\00G\A9\00\00\90\06p\00\05P\00\14\E1\F0\00a\0D\03\00\F4\FF\FF\F0\00b\C4\0F\00\02x\10\04\03\12\0FP\01@$z\0B\00 \01 \0E\02\D0\00\B0\1F\00%v\04\0B\00X\00\00\10\10\00a\CA\0F\00\81y\0F\C3\05\E3\00\19\1E\0C\00\A2\00\00\10x\07\0B\00\01@\01\93\E4\0F\04\10x\09\0B\00\02\10\00z\C6\0F\00%v\06\07@\00%\12\06@\00u\E2\02\00%v\08\09 \00\95\E2\0F\08\10x\0B\0B\00\03P\00v\00\10x\13\0E\00\04\10\00E\81y\11\08@\00fb\09\00%v\0A\A0\00\10\C8\C0\00\11\15\C0\00\12\13P\00U\00\81y\14\0A0\00\10f0\00&\04\15p\00e\1F\04\10x\17\15\C0\00\10\C80\00\16\13\E0\00 b\01\C0\00\16\170\00\88/\00\10x\1D\15\00\020\00\16\16\D0\00zb\03\00%v\1C\1D\D0\00*\1F\15\D0\00E\09\0E\00\08\10\00Z\01\81y\15\1C\D0\00%\1E\1F@\00\01\D0\00\11\19\D0\00\15\09\D0\00%\18\1E0\00\10f`\01' \19p\00\00\80\01\1A\19\D0\00&\17 `\00\1C\0Fp\01Z\00\10x\0B\19\D0\00\1F\1A`\01\07\01\D0\00&\05\19\D0\00\10\1F\00\024\0E\00\0C\10\00j/\00\81y\19\0A@\01%\04\05@\00\01\D0\00\11\1B\D0\00\15\07\D0\00\16\1Cp\01z&\03\01%v\06\1B\D0\00%\1D\1B\D0\00\01@\02\16\07p\01\11\22\D0\00\1A\1D\D0\00H\1D\1B\00\020\00*\08\080\00\17\0A0\00\00\C0\00H\1B\1B\00\030\00*\0B\0A0\00(\04\1B\00\02Q\0Br\00\0F\0F_\03\A1\F4\03\00\E4O\08\0Br\00\0C\10\00\B2\C0\F6\03\00\D6\0F\00\08\A2\0F\0F\D2\07\22\80\05\B0\02\16\0C\E0\00 \A2\0E@\00%\12\12@\00\10\8F@\00!\11\11\10\00\10\F6\A0\03\10\02`\00\01 \00P\C0\F8\03\00\D2P\00!\12\12`\000\00\00\06P\00U\0Br\00\14\14@\00\01\10\00$\12\110\00q\C8\0F\00\08\B2\11\11@\00\060\00&\13\13`\00\00\10\00(\11\140\001\A2\14\14@\00\060\00*\16\16`\00)\14\13`\00!\13\13@\00\060\00*\15\15`\00)\13\16`\00!\16\16@\00\060\00*\18\18`\00)\16\15`\00!\15\15@\00\060\00*\17\17`\00)\15\18`\00!\18\18@\00\060\00*\1A\1A`\00)\18\17`\00!\17\17@\00\060\00*\19\19`\00)\17\1A`\00!\1A\1A@\00\060\00&\1C\1C`\00\10\01p\00\19\19`\00!\19\19@\00\060\00*\07\07`\00)\19\1C`\00!\1C\1C@\00\060\00*\08\08\C0\00)\1C\07`\00!\07\07@\00\060\00*\0B\0B`\00)\07\08`\00!\08\08@\00\030\00f\10x\0E\0E\00\10\90\04\00p\00\19\0B`\00!\0B\0B@\00\030\00Q\0Cr\00\0E\0D\D0\06\14\F8`\00*\0C\0C\00\03)\0B\0C\00\03!\0C\0CP\00\00\00\03\90\E2\0F\00G\C9\00\00\90\F9\9A\0B\11\83\90\06#Ay\89\0B\03\A0\06@$x\04\03\12\005\0E\0A\8E\10\07#p\03 \00\13\E8\F0\06\13\04\F0\06\12\DA\E0\06'@\03@\00St\14\FF\04\000\08\10\E4`\04\1C\0A\C0\06\10\0A\C0\033\14\02\8E\E0\03\1B\0D\C0\06%\06\0A`\04\01\C0\06+\08\0A\C0\06\1A\06@\00\1C\10\C0\06\15\08 \00\01 \05&\0A\0A \05\00p\01\1B\11\C0\06\1C\0F`\05\06\A0\00\01 \05\11\11\C0\00\15\11 \05\16\12\90\04\10f`\06&\04\11p\00\00\C0\06*\13\11 \05*\05\04\F0\05&\06\130\00!/\000\00\09 \05+\06\06 \06\070\00j\0F\01\10x\11\11 \05\1C\09P\05\07\90\00\01 \02\12\0D\8F\09\15\F0 \05\01\10\00\22\C0\F4 \021\82\0D\0D0\02$\00\00 \05\1B\0A \05%\10\10@\00*\8F\08p\05\01 \05$\0D\10p\02\00 \05\22\82\10\FF\09\03p\05\06`\05#\F0\03\10\00$\10\0F0\00\12\C8\A0\05\1A\100\00&\05\05`\00\16\00\80\05\030\00\14\82\80\05\050\00*\06\06`\00)\12\05`\00:\05\05\120\00*\09\090\04)\05\06`\004\06\06\050\00\11\C80\00\11\060\00#\C0\F0P\00!\09\090\00\00\AB\0C\1F\E4\D0\09\01\18\0EP\08\00@\00%\0C\0C\10\04\01\80\01(\09\0C\80\011\B2\0C\0C`\00\00\80\01\1E\C4\90\03\00\F0\03\00_\03Ap\16p\00p\03V\89\00\00\A0\01 \000\02x\0B\CF\02\070\0A\0Fp\03\03\15\0B\C0\07\07`\03\0C\A0\02%\A2\0E`\03\030\00\19\00\80\03\0C\A0\02%\E2\0Ep\03\060\00\07p\03\0F\A0\02\03\06\90\00\10\CC \00(\0A\0A\F0\02\06\00\02\06\B0\02\05\D0\01\09\F0\01\BF\F4\03\00\D2\8F\00\08\82\05\05\0C\E0\01\02\1F\F0\E0\01\03\17\A2\E0\01\020\00*\0A\0A\D0\02(\06\090\00\14\82\F0\01\05\10\02)\09\0A\10\02!\0A\0A0\00\00\10\02\10\CA\00\0C\11\0C\00\0C\13\0A0\0D\14A\B0\0C\04\D0\010z\00\03\A0\01Dpb\F4\030\0D#\B0\04 \00\10\E2\F0\0C0\05\02\1F@\05\00\F0\0C\96\D6\0F\00G)\00\00\80\04@\001$r\07`\00'\03\00\A0\05\16\02@\00\09\B0\0C\10\C6p\0D%\04\07p\0D\13\C8\C0\05\10\03\C0\05\12\F6\C0\05V\B9\00\00 \02`\00\10\02\D0\0D\13ZP\02\0F\E0\02\02F\04\04\00\FD\B0\0C;$t\06\F0\05\16\08P\0A%\C8\1F \023\06\02\8E\10\02*\0D\08\F0\05%\0B\07\90\02\01\F0\05(\0F\07\F0\05\04\00\0D\010\02a\E4\0F\04$z\0F\10\00\12\0F\10\00\05\10\0B\13\06\B0\05E%v\0E\0F\10\00\1B\E4`\02 \E2\0E\00\066\07\00\03`\00E\81y\0F\0E \00\16$\C0\00\1F\11\C0\00\01\09P\05\05\D0\03$\0C\0D\B0\07\01\D0\03\05P\05\12\06\80\00+\0C\08P\05\03p\02\14\F8P\05\12\0DP\02\11\FA`\01\06`\05\01\F0\02U\01\10x\07\07\B0\06q\CE\0F\00\08\C2\0A\0Ap\00Q\00\80\06\00\C80\00$\0A\0F@\00\00\90\02D\B2\0F\0F\0A \00\020\08!\07\040\08*\FA\03\90\04\14\F8\C0\00$\0F\0C\F0\02\00\C0\00\22\C2\0C@\0B\040\08_\D9\00\00\10\FE0\08\07\09`\02\03\90\02\140\A0\04\13\E6p\02\1A\01p\02(\00\010\08\1F\0A@\02\12\1F\0A@\02\02\1A\0E@\02\1B\CA0\02\16\C8 \02\1D\0A\C0\01\1B\F0\C0\01\1B\F6\10\07/\80\05\10\07\01\0E\10\03\16\07\B0\02\09`\01\05 \02)\0D\0C \02\12\0Cp\00\00 \02\0F\F0\05\01!z\00P\01\08\F0\05(\80\00\F0\05\1B\09\F0\05\1B\060\01\11\06\C0\05\15\090\01\16\09\B0\05+\A4\0E \05\02\00\01\19\09\00\014\09\09\0C\00\01\07\D0\04\1F\09\D0\04\04\F1\00\89\7F\07\0C\00\1F \0C\00\00\0E\00\00b\0E\D0\04&`\03\90\04\06p\0B\00p\00\11/p\00\19\07p\00(\07\07p\00r\89\7F\04\07\00\1F@P\00\11d\B0\00*\04\04@\00)\07\04@\009\04\04\07@\00X\09\04\00\1F\80@\00*\09\09@\00\1C\04\F0\00\17\04\F0\01@\19x\04\FF \06@\02\16\01\00 \04\87\89\7F\06\09\00\1F\00\0DP\00*\06\06P\00)\09\06P\007\06\06\09P\00@\0Cr\00\05@\01!pR\D0\03\0DP\12\81\89\7F\0B\06\00\1F\00\0E`\00ft\0E\00$\B4\07\10\03\00\80\0A\10\B6\EF\02\12\80\E0\01\00 \03\12t\00\06#\FF\00\D0\02\14\B2p\0C\12\F8\90\00G\B2\00\06\0BP\043\1C\B8\00p\00Ap\02\00\E4\90\03\10\050\01 p@0\00`\0F\04\19x\05\05\FF\05\10\FF\C5\00\01\C0\094\0B\0B\06@\04\12\E2\E0\05\17\07p\001\0Cx\00\A0\061p@\FA`\06#\88\B3\B0\0C\00\A5\0Cf\E8\0F\00\1D{\00\01\00\89\EC\0F\00GY\00\00@\B0\08\00\9E\04$\80\FF\C0\021\82x\06\F0\13\01\01\00\93\CA\0F\00\84\C9\0A\05\00\04P\00\81\A2\02\00Gy\00\06\C2Q\0E\22\80\0Bp\02C\09\0A\00\07p\02/\A4N\D0\02\01\02\90\07%\C0\FC\E0\01\02\10\05\11\00\10\03c\89\7F\08\09\00\07`\02'\A4\0E\E0\0D\05@\00)\09\08@\00!\08\08P\00\05@\00C\0C\08\00\07`\02a\E2\04\00$r\0B\90\011\08\00\8EP\06$\0B\B2\00\04\11\FC\E0\05\09\F0\01\0A\90\01\01\10\00\00\C0\01\14\0Cp\00\10\DAp\00\04\B0\0D\02p\00c\88\B3\00\FF\0C\00\10\01-\E4\07\80\03\22Hyh\19\01\10\00\1E\E4\80\01T\84y\08\FF\00@\00\22\A2Np\08\07\A0\03!\02x\DE\00\14\80\90\01\00 \15&\80\03`\00B$t\09\FF\B1\01\03\80\02&r\0A\E0\14\10\C6@\00\1B\13\90\04\02\80\05\15\0A\90\04\11\0C\80\05\02\00\14[\8F\04\10x\0E\A0\14\19\10\A0\14J\81y\0D\0C`\0A&\0E\0E@\00\00p\05\16\12\90\14\01\10\0E%\10\10 \00<\E4\0F\08\D0\07G%v\12\12 \00\00\B0\14\15\10 \00\10h\10\00%\13\12\10\00\B4\A2N\00$t\16\FF\9D\98\BB;\F0\02\01\D0\0A\04\80\07\01\10\03a\15\FF\00\00|C \00\11\C6\E0\03!\0A\03`\07\02\D0\03c!r\14\08\0D\00\E4\06`\C8\8F\00#t\0B\9F\112?\16  \01T!r\0D\08\0F \00\D2\C6\0F\01#t\0B\0B\01\00@K\15@ \00E#t\0C\0D0\00\D4\C6\0F\00!t\0F\0B\7F\00@\CB\00\00@\004\11\08\11@\00u\E2\0F\02#t\0C\0C@\00\D4\E4\0F\08#x\0F\14;\AA\B8?\0F\08P\00'\10\11\80\00e\08!r\13\08\13@\00hO\00!t\0E\0C`\00\00@\00e`p\A52\0F\00@\00\17\10\A0\00W\08#t\14\13P\00P\00#x\0E\0Dp\00\12\0Ep\000\04$x\FF\02\15\800\04I!t\12\10`\007t\14\14P\00P\00#x\0D\0Dp\00\13\0E \00T\08s\0F\00\0F\B0\02p\22\0E\00#x\12\11`\00#\12\08\00\018t\0E\14P\00E$x\10\10p\00!\C4\0F0\00\00P\00$\12\00`\00%\0E\13\A0\00`\E4\0F\04\08s\0D\CD\05\02`\00\10\A4`\00(\13\13\80\00T\19x\0E\0B\17\10\05\10\CA\90\004\11\00\120\00\81\E2\0E\00#r\0F\0E\0F\10\07\00\10\00e\1F\00\19x\09\140\00\10\CC0\008\16\00\13\C0\00Cr\0D\0C\0D\1F\03q\00\C8O\00#r\10\91\01#\0D\00 \020r\09\09`\13\13\10P\00\01\E0\0D\18\FCp\09\0Ap\07\00\C0\03&\90\07\C0\03\00 \0C&p\07 \00\04\90\04\08 \0C&\10\040\00\0E \0C+\0A\0B \0C\12\0A \0C\02\90\04XG\E9\00\00\C0\10\041v\0A\FF0\00\03\E0\01\09\10\09\13\C80\03\07 \0C;\02x\12 \04\16\0C\A0\09k\E2\9F\00\10x\0F\10\04\16\11\10\04p\E2\0F\04%v\0C\0C\C0\03$\12\02\90\03\19\13\00\04\0B\10\0C\1C\04@\04\09\C0\11\16\E4 \0C\22\12\02`\07E%v\10\11\10\00\0F@\04\001$z\13@\00\0F@\04\02\10d@\00%\12\13@\00\1F\CCP\04\11\11\0B\B1\0F\03\E0\00L\00$t\17P\04!\0B\0AP\04/\FC\03P\04\03\16\0CP\044\C8\0F\00\00\04$\17@ \04\19\15`\04:!t\0F\E0\03:t\0D\15\10\04\0AP\04E\19x\0C\0C\D0\02\01@\03\09P\04W\04#t\0D\0Dp\00U\08#t\0E\11P\00\03\80\04\08@\04\0Ep\04'\10\0D\90\00X\04#t\0E\0EP\00H$x\0D\0D@\04\0Ep\04 \10\15\90\00\01~\1Ex\E2\0F\04!t\12\0E\E0\00\0D@\04\03p\04&\17@@\00\00\A0\00\02p\03\1A\0F`\04\10\C4`\04\18\16`\04F\04\19x\14\E0\03\1E\E2`\04T\08s\10\00\10p\00\10\A2P\04\11\16p\04#\16\08\A0\01!x\16`\04\22\16\00\D0\05;\08s\12P\04(\0C\0CP\04\10$\90\1A\04\00\01\03P\04\19\16P\04\11\0Cp\13\15\0CP\045\09\09\12\10\00\01P\04'\14\16P\00\00\C0\03/p\FC\C0\0D\09\07\00\04\040\04\19\02\C0\0D\010\12\08\10\04'\E0\01@\00\08\C0\03\01\D0\02\1B\0E\D0\0D\07\F0\03/\E4\9F\C0\0F\00\02\E0\03\1B\0E\C0\0F\15\0Ep\03\07\10\08*\E8\0E\B0\0F\10\22p\00\19\13`\03 \02x\1F\03!|C\05\02\1B\E4\A0\04\13\C4\80\03\06`\04G\00!r\0A`\03\10\CF\F0\06\00\7F\034?\13 \C0\02\1B\12\B0\07\02\10\07\14\14P\025t\13\120\00\01\B0\07*\11\10\E0\02(\13\130\00\09\B0\06\10\E4 \02 \11\0A0\02\22\11\08 \00E!t\0D\13@\00\12\C6 \00\00@\02\22\11\00 \00@#x\0D\120\00$\0D\08P\00%\0A\13P\00\12\C6 \00\000\00\13\0D0\00\00\C0\06\14\11@\0290\0E\00\00\07\00\B0\02Dr\09\10\110\02 \C8\1F\10\00$\0A\0D\10\00/\C6O\A0\0E\01\1C\0B\A0\0E\17\F0 \00?\02x\0D\90\14\02\15\0B\10\0A\02\00\14\15\0D0\05\0A\E0\1B6$t\0D\D0\01\00@\02&t\0F \05\01\90\015\08\08\0B\B0\01PO\01#t\0C\CA\005?\0D  \05\01\10\05\22\0F@\80\03J!t\0D\0C\D0\03\07\E0\04\020\01\15\08P\01\12\C8\10\00\05@\01*\CC\0F0\01\11$0\01\15\0C \01-\CC\1F\A0\07\01`\0C\04\10\0F\03p\0B&\80\01\80\07%!rP\0C@\00\00\CAO\00\0E6\08\00\1F\90\0C2!r\0B\E0\00\05 \00%\0A\0B\D0\0E\01 \00\11\0A \06\06 \00%\0D\0A\A0\0E\01 \00\13\0C \12\00 \00\10\CF \00\15\0C`\0E\00 \00\12\B2\90\08\04@\00c\88\B3\00\04\0D(\89#\0F\C0\0D\04\17\B0\E0\006$t\04\10\0C\0F\C0\0D\02C\04\05\00(0\01!\E2\04\C0\0D(2\09\C0\0D%\05\04\C0\0D\93d\EE\00!r\05\05\04\00\01\00 \CA/p\0F\15\05\A0\0D\11d\F0\008\05\06\00 \00%\07\0A\80\0D\8A\A4\02\00!\B2\07\07\0A\D0\003\FF\07$\80\00/\E4\050\0D 4\05\FF\00@\00!b.\C0\01&0\05\10\05!\84y\00\01\05\C0\05\000\0D(\00\05 \01\08\10\15Q\1F\00$z\0D\00\03\15\02\00\03 \0E\0D\00\03\15\06\90\17%\07\0D@\05 \E4O0 \15\0D\A0\04\0C\A0\14\01\A0\17'\10\07@\00\00\D0\04\18\0D\10\09E%v\12\09 \00\01\D0\08*\11\10\10\0D'\14\0B \00\0A\B0\08\01\10\0D%\15\14\10\00p\E2\8E\00\02x\16\00\90\03\05\D0\03\08\B0\08\00\80\10\10x\FE\00\D0\80\00\00\E2\F2\03\00\E2/\04 x\18\10\00A>\00\00@p\03\03 \004~\00B\80\05C\02\02\00\04\B0\00`\E4\0F\00\08r\181\00\02\01\00P\CE\0F\00 \98\8F\1E#\80K@\00\00\D0\02&\04\0F\F0\03X\00#t\08\0C\10\08T!r\0F\04\11 \00\01\A0\05(\08\08\F0\07V!r\13\04\13 \00W\02!t\11\08\10\04W\00#t\0A\0FP\00Z\08#t\12\13\D0\08)\11\0C\A0\05Dr\15\04\15P\00\10\E2p\04%\0A\0Ap\00\010\00)\0E\0C\B0\05:t\0C\120\09\18\11\80\09\00\10\09\18\0A\A0\04\10$@\0B\04\D0\05\01 \00\1B\12\C0\09'\14\11P\00\00\D0\08\17\0F\10\09@\00\08s\0E\1A\03\060\08:x\12\13@\0D\09\E0\08\01@\00\08\10\09\0E\00\0A*\12\13P\0D%\16\15\E0\08\01P\05(\0F\080\00\00p\06\14\10\80\00xb\0E\00$x\14\14\D0\00\00@\00\00P\00\22\16\000\00\01\00#\13\\\90\02\01p\0D7\13\00\12\A0\06R r\0E\0F\0E\EA/\01\A0\06E \08\0E\0E`\02\10\C60\00\19\15 \09T r\0A\0A\110\00u\E2/\00 \98\0E\0EP\02z\C6\0F\00 \08\0A\0A@\00C\17\00\18\00\81\0D\10b@\005\0C\0C\13@\00jO\00 \98\0A\0A@\00%\0C\0C@\00\84\E2\0F\00 r\14\14\150\00u\C6\1F\00 \98\0C\0C0\00\00 \008\08\14\140\00\06\90&\02\80\0DI \98\14\140\00#r\15\82*!@\00P\035r\11\17\00\01\010\01%\0C\0D0\01\010\006\13\17\0A \00d\04 r\17\17\14\10\00\02\F0\0C\17\070\00P\08\86y\00\0C \01a\04\19\10\0C\00\E6P%\18\0BP\00T\86y\00\0E\13 \00 \E8\01\10\00*\08\15\10\00$\06\17\10\00!\E2\01\00\08(\00\FB`\0A\0Ap\059M)\00\10\00\0E0\04/\02\050\04\03\22\F4\03\90\0AF\00\00\B0\01\80\050\08r\021\00\02\82\03\010\04%\02\02@\01\00\00\19 s\0C?\05\02\D0\01/f\100\17\00\11\02\90\05\15\03\90\05%\06\020\17*\CCO\D0$k\A2\0E\00$t\08`\0A\00\9E\07\06`\0Ac\10x\03\03\00\01\D0\04\18\C8\E0\1B\13\F0\F0\075\05\04\070\04\12\CFp\00\13\\p\00\00\80\08 t\08\CF\003?\08 \80\08\03\C0\04\22\0B@\10\00J!t\0A\08 \04%\08\08P\03\01\C0\08 \0A\05\90\03#\0A\08@\000x\0A\05`\03\01\14\02\01\10\017\05\00\0A\C0\081 r\05q\00\05P\03&(\05\80\01\10\C8P\01(\05\05P\01E r\05\0C0\00!\CA/\00\02\1B\05\00\02/\90\FE\00\02\08\0B\10\00\00\E0\09\00\0B\01\06P\01C\08\00\80*\A0\01T\C4\0F\00Dy`\0CP\C0\03\00\EA\7F\A0\15\15\0A \15\00 \0EGt\0D\FF\02@\00\06p\19\01`\15Q\04\08r\0A\0C\F0\00\00@\150\E4\0F\04`\00\14\F0`\00\01p\06\05 \00\05p\00\15pp\00\17\0Fp\00\01P\00\00\10\16\06\D0\15\10\C4@\006\0B\0C\0A\00 \19\04\F0\0A\12\E4 \00\03\C0\09\00@\00\01\E0\00#p+ \00\16\C6@\15\13\0B\B0\12\00\80\00&\F0\00\80\00\10G\F0\02\16\E9P\01\040\00\11\04\F0\00\11\8F`\0B\17\01p\00\00`\00*\C0+@\01&\A0\00@\01c!r\0A\04\0C\00\01\00\01\C0\07\0A@\01\00\80\01*\00,@\00\17`\90\002!r\0A\10\01\07@\00\1B\04@\00\1B@@\00\17 @\002\02r\07\9C\0E\12\0F@\00\00\E0\00&@\F6\E0\00\03\80\16\05 \00AHs\00\000\03\000\02Q\E8\0F\00\89s\D0\0AA\0C\06\00\0E\C0\02`Py\00\08`\D3@\00Q\C3\03\00\EA\1FP\00#\F0\FFP\00f\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-\11\01\89\02\0E\01\00\22@\00\01\00=V\01\000\00\08\01\00\1F\0B@\00\04\13\96)\00?\98\02\00@\00\0A\00L\06.\02\00\01\00\1105\03\02h3\13\00$\00\13\07\A7\04\00\01\00\03\14(>\00\17\01T\00\00\01\00*\08\05@\00\0F\80\00\05\1A)\7F2\05\01\00\13\E0@\00\11T\06\00\06\E4\00*\04\00\01\00\1F[@\00\0414\06\00\01\00&\C0\00@\00\1F\0B@\00\00!G\01\18\00\0D@\00/\F8\06\C0\00\03\1B\08\08\00#$\01l\0E\0B\01\00\12\D0%\03\03l\03\02\80\00\17\048\00\22\10\00\01\00\13\B5\14\01\0C\84\01*\F0\07h\00\1F\00\C0\00\04\1F\E6@\00\04\13\F8@\00?x\01\00@\00\0A\132@\00\15\06\92\00\02\01\00*\80\09\E04\12\03\8C\00:$\80\00\01\00\13\87\A4\00+\03\00\01\00\03\F09/H\00\80\00\0B\13\06\9F\0D\05(:\0B\01\00*\A8\00\08\00\04\10\01\13\018\00\04(\01\0C\01\00*\10/\08\00\088\00\17\06\A0\00\0F\01\00\05\04\B8\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00")
  llvm.mlir.global internal constant @ral_send_output___cpu___pvoid_i64_m3df32___void("ral_send_output___cpu___pvoid_i64_m3df32___void\00")
  llvm.mlir.global internal constant @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void("ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void\00")
  llvm.mlir.global internal constant @main_kernel_0_main_kStitch_divide__13_1_0___1w1r_kernel_name("main_kStitch_divide__13_1_0___1w1r\00")
  llvm.mlir.global internal constant @main_kernel_0_blob_gpu.binary("P\EDU\BA\01\00\10\00\A0 \00\00\00\00\00\00\02\00\01\01@\00\00\00` \00\00\00\00\00\00^ \00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(K\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80J\08\00\11G\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F5)_shndx\00.nv.info\00.text.main_kStitch_divide__13_1_0___1w1r2\00\0F,\00\15oshared.\00\15\9Fconstant01\00\12\FD\01debug_frame\00.rel\11\00\1Aa\12\00!nv&\00oaction%\01 \0F\94\00\0F\0FH\01q\1F$$\00\0F\FF\16$__cuda_sm70_shflsync_bfly_p\00$____wg_G\00\0F\00\09\00/261\00\19_1__28\EA\01\1Fo_param\F1\01.\0F\01\00\06\8CU\00\00\00\03\00\0A\00\01\00\11\AA\18\00,\0B\00\01\00\A1\D8\00\00\00\22\00\0A\00@=\0F\001\00@\01\07\000\00z\010\00,\09\00\01\00\11\B2\18\00,\04\00\01\00\11\E2\18\00,\07\00\01\00f2\00\00\00\12\10\90\00!\80>\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\130\C0\00\10\04\9B\00R\04<\00\00\00E\005\04\08\0F<\00&8\00H\00\04p\00L\84\80\80(o\00\10\08\15\00\81\16\85\80\80(\09\05\0C2\00\068\00\22\18\00\01\00\22p\00\01\00\048\01\1308\011\04/\08\06\01\10&\0C\00R#\08\00\03\00\01\00'\04\12\0C\00\17\11\0C\00\12#0\00\04$\00\17\07$\00\05\0C\00!7\044\05q\015\00\00\04\0A\08~\01\A2`\01 \00\03\19 \00\04\17\A2\00u\04\00\18\00\00\F0!\10\00u\03\00\10\00\00\F0\11\10\009\02\00\0C\10\009\01\00\08\10\00\01\01\00\F1_\F0!\00\03\1B\FF\00\04(P\00\C0\12\00\00\00\13\00\00@\13\00\00\80\13\00\00\C0\13\00\00\10+\00\000+\00\00P+\00\00p+\00\00\90+\00\00\00:\00\00p:\00\00\E0:\00\00P;\00\00\C0;\00\00\10<\00\00P<\00\00\90<\00\00\D0<\00\00 =\00\00\04\1C\0C\00\F0+\00\00`6\00\00\C09\00\00\04\1E\04s\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\D8\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00\C8\00\01\001\02\00\00\C8\01*H\00\10\00\0F\01\00\FF\D6@$v\01\FF\8F\05\B1\FF\00\8E\07\00\C4\0F\00\19y\0D\18\00 \00!y\05\A1\0E\00\B9z\04\00\00F\00\00\B3\05\90\E2\0F\00Ey\00\00\10\14!\00 \80\03\10\00c$t\0B\FF\1F\00@\00\10\E2@\00\03:\04\B1%\00\00\00b\0E\00$t\1A\FF\01\00\04 \00\F1\04x\0A\FF\05\00\00\00\0D\16\01\00\00\E4\1F\00\12x\0D\0D@\00\B0\C0\8E\07\00\C6\0F\00$x\0C\03\A9\02 \0A\020\00\D0/\00\0Cz\00\0D\00[\00\00pR\F2p\00g\04$x\0A\0A\04p\00P\0Cr\00\0D\FF\F0\04`R\F4\03\00\E4\0F0\00\81\0C\00\\\00\00pb\F6\10\00@\12r\00\FF\D2\00\A2\FF3\8E\07\00\D6\0F\00G94\04\01\C0\00p\EA\0F\00\10z\02\00`\00B\FF\E0\FF\07\E0\00H\01\00\E0\0C\E0\00R\13\FF\00\00\80\C0\00\93\E4\0F\00\10x\02\02 \000\00\80\C8\0F\00\12x\02\02\80\E0\00!\C0\8E\10\00a\07r\02\02\FF\00@\00\11\00 \001r\0F\02\80\00\12\FC \00\00\B0\00\00\F2\05\E6pb\F0\03\00\DA\0F\00G\09\00\00`\0C\90\00@$x\02\0F\AB\05!\0D\0A\E0\00v\10Ey\02\00\F0\06\90\001\1Cx\00\01\00?p\F0\F0\A0\00\02\B2\0Cx\00\02\80\01\00\00pB\F8 \000r\11\FF\90\00\10\0D \00\96\D8\0F\00G\C9\00\00\90\06p\00\05P\002\E1\F0\03\E0\00a\10\0F\80\FE\FF\FF\E0\00\10\C4`\00%\16\FFp\01p\E4\0F\00$z\09\0C0\01\22\11\02\D0\00\90%v\02\09\00X\00\00\16\10\00p\CA\0F\00\81y\0E\020\00\C5\00\19\1E\0C\00\A2\00\00\10x\05\090\01\84\E4\0F\04\10x\07\09@\10\00\10\C6@\00*\04\05@\00%\15\04@\00u\E2\02\00%v\06\07 \00\86\E2\0F\08\10x\09\09`P\00g\00\10x\17\11\80\10\00E\81y\12\06@\00fb\09\00%v\08\A0\00\10\C8\C0\00\11\1B\C0\003\17\02\8E0\00%\17\080\00\10d0\00&\02\1Bp\00h\1F\04\10x\19\1B\F0\016\81y\14\E0\00 b\01\C0\00\16\190\00y/\00\10x\1D\1B@0\00\16\19\D0\00 b\03\D0\00\16\1D0\00j\0F\09\10x\1B\1B\D0\00F\1D\11\00\01\10\00<\81y\18\D0\00\06\A0\00\01\D0\00\11\1F\D0\00\12\1DP\00F\00\81y\1B\D0\00\11f\D0\00\1A\1F\D0\00*\1D\1F\D0\00\1C\1C\D0\00\1A\1D\D0\00*!\1F\D0\00\1C\1D\D0\00\1A!\D0\00*\1F\1F\D0\00F!\11\80\01\10\00<\81y\1E\D0\00\06\A0\00\01\D0\00\11!\D0\00\15!\D0\00\17\1F\D0\00\00p\00*\02!\D0\00%#!\D0\00\01@\02\17\02\D0\00\01`\02\1A#\D0\009#!@0\00*\05\040\00\17\060\00y\0F\01\10x!!`0\00\16\06\C0\00\10\220\00\18\08\00\01`\00\0Br\00\0E\0E \00\B1\80\F8\03\00\E4O\08\0Br\00\13\10\000\C0\FA\03\80\04A\08\C2\0E\0E\7F\042\00\80\06\10\01\16\13\E0\00 \A2\0E@\00!\15\15\1F\00P\FA\03\00\E4\8F@\00%\12\12P\00 \0F\02`\00\01 \00\B1\C0\FC\03\00\D2\0F\00\08\D2\15\15`\00!\00\00`\01U\0Br\00\17\17@\00\01\10\00$\15\120\00\10\C8\80\00!\12\12@\00\060\00&\14\14`\00\00\10\00(\12\170\001\D2\17\17@\00\060\00*\19\19`\00)\17\14`\00!\14\14@\00\060\00*\18\18`\00)\14\19`\00!\19\19@\00\060\00*\1B\1B`\00)\19\18`\00!\18\18@\00\060\00*\1C\1C`\00)\18\1B`\00!\1B\1B@\00\060\00*\1D\1D`\00)\1B\1C`\00!\1C\1C@\00\060\00*\1E\1E`\00)\1C\1D`\00!\1D\1D@\00\060\00*\1F\1F`\00)\1D\1E`\00!\1E\1E@\00\060\00*\02\02`\00)\1E\1F`\00!\1F\1F@\00\060\00*\05\05`\00)\1F\02`\00!\02\02@\00\060\00&\06\06`\00\10\01p\00\19\05`\00!\05\05@\00\030\00c\10x\11\11\00\02\F0\02\03p\00\19\06`\00!\06\06@\00\030\00P\0Cr\00\11\10@\06$b\FC\90\00%\13\13\A0\00\01\00\03\11\06\10\00\22\C0\F8\00\031\D2\13\13P\000\00\00\060\03`G\E9\00\00\90\F9A\07\11\83\90\06#Ay\8E\00\0B\10\073\11\0A\8E\A0\07C\02\00p\03 \00\13\E8\F0\06\13\00\F0\06\12\DA\E0\06'@\03@\00+t\03\C0\06\1B\0E\C0\06 \04\0E\C0\033\03\02\8E\E0\03*\02\04\C0\06%\06\0E`\04\01\C0\06*\08\0E\C0\06*\06\06@\00*\19\06\C0\06%\08\08 \00\01\C0\06/\14\0E\C0\06\0B*\0E\08 \05%\14\14@\00\01 \05\1B\10\C0\06%\15\140\00\10d\90\00'\16\10p\00V\04\10x\04\10 \05k\1F\00\81y\16\16\F0\04\16\040\00\00 \05*\04\10 \05*\07\060\00\17\040\00\00\C0\00'\08\10 \05[\01\81y\04\04 \05\08\00\01\07\F0\02\15\F0 \05\18\02 \021\82\02\020\02/\00\00 \05\05%\19\19\D0\02&\8F\08p\05\14\F0 \05$\02\19p\05\00 \051\C2\19\19`\00\03p\05\06`\05\14\F8\B0\02\15\19\A0\05\00\E0\02J\82\0E\0E\190\00&\16\16`\00\16\00\80\05\030\00\14\C2\80\05\050\00*\07\07`\00)\15\16`\00:\16\16\150\00&\04\04`\00\00\A0\03)\16\07`\004\07\07\160\00\11\C80\00)\07\04P\007\04\04\07P\00\0E\D0\09\19\11P\08\06\A0\03\14\F8\80\01$\04\13P\00\00\80\01D\C2\13\13\04P\00\1E\C4\90\03\00\F0\03\01\C0\0A1\16p\00p\03X\89\00\00\A0\01p\03\1B\09p\03\1B\08p\03\11\02\10\02\16\09\A0\02/\08 \C0\07\05%\A2\0Ep\02\15\09p\02/\06\08\C0\07\06%\E2\0E\90\03\060\00?\08\08`\C0\07\0B\06\90\00\10\CC \00(\09\08\F0\02\06\B0\02\1B\F8\B0\02*\FA\03\90\05Q\F0\03\00\D2\8F\80\05\02\C0\02/\80\06\80\05\0B\04\C0\01\04\80\05\050\00*\09\09\D0\02\03\80\05\1B\FA\80\05\05\10\02$\06\09@\03\01P\00!\09\090\00\00@\03\10\CA\00\0C\11\13\00\0C\13\09\E0\0DIAy\01\00\D0\01Sz\00\0F\00[\90\0C\03\00\0D#\90\04 \00\12\D8\A0\0C)p\040\0D\010\005\FF\E1\FF\90\05#`\02 \00\0C\A0\0C\03\90\0C\1B`\A0\05& \02P\00c$v\0E\FF\00[0\02\1B\E2\C0\02\10\C8\C0\01H\0E\0E\A0\FF\90\0C\1B\10`\02\11\02`\02\16\0F`\02\10\02\D0\013\10\02\8E\F0\01\1C\08\90\0C\15\0Fp\02\02\90\0C\18\0F\D0\051$z\05P\001\05\02\8E \001$z\07\10\00\12\07\10\00\05\B0\0C\15\10p\00%\06\07\10\00\1E\E4\90\02W\10x\09\0F``\00\09\80\02\16$\C0\00\01\80\02*\C8\1F\C0\00\020\05+\08\08\80\02\19\08\B0\03(\08\08p\02K\81y\13\020\05*\05\05P\0A$\08\05\D0\07\0F\A0\02\00E\10x\0F\0F\90\06\12\CE \08\01p\00!\00\00\F0\00\0F\10\08\10/\0F\0E\10\08,.\10\FE\10\08\0F\80\02\09#0\01 \00\12\E6p\02\1B p\02(\00\01\A0\04\1F\04@\02\12\15\04@\02\19\06P\01\09@\02\1B\CA0\02\16\C8 \02\16\04\C0\01%\06\06\D0\03\02\C0\01\18\06\00\01\22\82\06\10\01$\00\00\C0\01\18\04p\00\09\10\03\01\D0\05\16\0F\B0\02\09`\01\1B\F8`\01\06\D0\05\01p\00\0F\D0\05\05\04\00\04\07\D0\05.\80\00@\09\0F0\01\0D\15\030\01\16\040\01+\A4\0E\E0\06\02\00\01\19\04\00\01\12\04\80\06\0B\B0\04\1F\04\B0\04\04aGy\00\7F\12'\10\00\10\0B\10\00\FB\00\89\7F\02\13\00\1F \0C\00\00\0E\00\00$\0E`\08\1F\1F`\08\0B\10\CA@\00X\03\02\00\1F@@\00*\03\03@\00)\02\03@\00!\03\03P\00\05@\00X\04\03\00\1F\80@\00*\04\04@\00\1C\03\F0\00\01P\00\05@\00g\05\04\00\1F\00\0D@\00*\05\05@\00)\04\05@\00!\05\05P\00\05@\00a\06\05\00\1F\00\0E@\00ab\00\00$r\02@\011\05\00\8Ep\04(\0B\A2\E0\03,/\08\A0\05T\0B\A2\00\02\060\02\00@\08#\A8\00 \00 p\02\10\02\00\A0\02\14\02@\02q\CA\0F\00\88\A3\00\0A0\00\00\86\16i\E4\03\00Ay\00\A0\01\22Hy\88\18\04\10\066\1D{\00\01\00\12\EC@\14\11P!\0E\02 \00\00\90\13/0\17\90\13\0B&\B0\07\E0\03\0F\80\13\1F\16\03\80\13\010\13\01\1F\00#\80\FF \01\00\90\13\11\03\B0\04\11\F8\00\01@\0Cz\00\03\80\00\01\10\07\000\14VI\00\00 \07\A0\00\09@\00\00\10\03&r\04`\13\01\A0\0B\11\18 \03\01\00\04\1B\E4@\13\10\C6\C0\00%\11\18\C0\00w\E2\0F\04%v\12\18@\10X\04\10x\10\18\90\06G%v\0E\11 \00V\08\81y\06\12 \04\10\22`\06\15\18`\06\01\A0\06*\14\100\00%\05\0E0\00 \E6\10 \0C\17\09 \00\00\E0\0B\06@\0C h\090\07\18\16\10\121\06{\19\90\01\C1\00\90 \00\000\0E\00\08s\19\00 \0B\00\F5\0D $\1E\A0\07Q\19\FE\FF\FF\0F\80\00`\CC\1F\00\05s\0F\8A\02\01\AC\19 \00d\90\025\0E\FF\FF@\04\15\1F\E0\03\10\0F\D0\0Dp\C8/\00$z\13\13`\00\22\FF\02@\01@'r\0F\0F\A0\03\22\0E\00\90\09T'r\12\0F\18@\00\16\C8 \04%\12\0Ap\01\01@\00\13\18p\01V'r\14\0F\11p\01P\01\0Cz\00\18 \00$p`\10\15\02\90\00\15\14@\00%\11\0E\A0\0A\01@\006\0E\0F\10@\00\00\10\02\12\11@\00 \FC\03\10\000\10J\18P\00\11\80\F0\00\16\E2\90\00%\0E\0A0\00\03p\00\12\FAp\00!z\10\A0\00\04\D0\07T\10H\12\12\01\B0\01\10\E2p\007\16\0F\09p\00E\10j\11\11`\00\02\90\02\1A\10\C0\00\11\18p\00\13\16\C0\00'\10XP\00\18\E4\B0\00\04\80\00%\0F\18`\08\00\80\005h\14\140\00\00@\00;J\10\10p\00\15\0F\F0\00\10\CA`\00\070\00\09\90\00\020\04I\10H\0E\0EP\00;j\0F\0FP\00\01\E0\09\22pP`\032\12z\09\F0\09\133\F0\00\17X@\00\09\80\00\02`\00V\07r\12\09\12 \07g\04\07r\14\09\14\10\00H\10h\16\16P\01T$x\0F\12\04P\01\10\E2@\00!\0E\09\90\02\03`\08W$x\10\14\04p\01@\12x\0F\0F\C0\11\22\FF\E2\00\02,\10XP\00(\0E\0EP\00H\12x\10\100\00W\07r\09\09\16`\00T\84y\0F\0F\00 \05u\A2\0E\00\12x\0E\0E0\00\01\E0\18(\09\09P\00E\84y\10\100\00\10\E80\00%\09\090\00\01 \00%\0E\0E \00\10j\10\00%\09\09\10\00!\22\0E\00\18S\9D\98\BB;\FF0\07\84$t\15\FF\00\00|C`\00W\10x\04\04\80\A0\01E\0Cr\00\04\F0\04\84\E2\0F\00!r\0F\06\0F\F3\07P\C8O\00#t\0F\00B\00?\13 \F0\19T!r\10\05\10 \00 \C6\8F \00t\06\01\00@K\15@ \004\07\07\0E \00u\E2\0F\02#t\05\10@\00\B3\E4\0F\08!t\0E\06\7F\00@\CB \00X\00#t\12\07`\00G#t\05\05P\00e\08!r\08\08\09P\00\C4\1F\00#x\0E\0F;\AA\B8?\0E\080\00)\12\12\80\00:t\11\05`\00)\09\08`\00\94x\0E\0F`p\A52\0E\000\00)\14\120\000x\11\10`\00\15\11`\00%\0F\09`\00\10\C4 \00 \14\07 \00&\14\080\00\00P\00\22\11\00\10\00W\08s\0E\00\0E\90\01J!t\09\0F`\00 \14\070\00\01\B4\02\10\C6\10\00 \07\08P\00\22\09\08\10\00T\08s\10\00\11@\00@b\0E\00#\B0\0ECp\A52\07@\00@$x\07\06\0F\01\13\FF\10\05W\08s\09\00\14`\02E$x\05\05 \00\00p\00Cr\02\07\0E\C8\1D\00\90\05X\08s\11\00\080\02:x\12\120\004\05\05\100\00!\E2/p\13\05 \00\11\C6 \00!\12\09P\08\03 \02Dr\02\02\11\10\00\22\E2\1F`\0B\09\C0\13\0A\B0\09\01P\0E\13\0E\10\00\13\E2P\0E\18\0E\F0\07,\04\03\D0\0B/\A0\07P\0E\0A\1C\04P\0E(`\07P\0E\12\04`\04\0FP\0E\06(\04\04P\0EV\10x\09\03 \F0\03F\04$t\18\B0\07\01@\035\05\03@ \00Q\1F\04$z\12\E0\07\01\F0\0A\01 \0E\16\09\C0\0D\01\80\07C\0E\12\00X\90\06\01@\00%\11\03\A0\07\01\D0\07'\14\09 \00F\08\81y\07\A0\07%\A6\00\E0\13\12\05 \00F\04\81y\08\A0\07 \22\07\B0\06\18\0C\B0\06\04\F0\13\22\18\02\10\06G%v\18\11`\00\00\C0\0C\06\D0\07 h#\A0\0E\18\18\E0\106\06{\13\E0\07*b\0E\B0\07\84\E2\1F\00\10x\03\03\80\C0\00\01\80\02\10\13w!\03\00\08l.\00\10x\1B\13\00\08\14\1B\00\08\10\E4P\00\17\15\F0\07j\8F\00$z\15\15\F0\079\16\0F\15\F0\074\0E\16\12\80\00\00\A0\07&r\0F`\07\01\A0\078\13\16\10 \00!z\120\06$\12\02 \00 \0F\16\B0\02\07\C0\07\1A\12@\07\16\14\90\00%\E4\0F\A0\00%\13\0A\E0\1C\18\14\D0\01E$z\10\15\C0\07\02P\00\1A\09\10\08(\12\12\F0\06-\10H\D0\06\15\12\D0\06\01\F0\074\12\16\11\A0\00\1B\E2\E0\07\16\C6\A0\00\22\12\0A \00I\10j\09\09`\005h\0F\0F`\00\11\E2\C0\08\18\14\10\026$t\14\00\06\1F\E4`\07\02\15\09\90\00\1F\C4\00\08\02\15\11\E0\00\00@\00;H\13\13@\00\06\D0\07\01 \08/\0F\0F \08\0A\0B\D0\086\12z\10\00\08\00 \00\1Ah\C0\08A\07r\0E\10\1F\04!\00\00\B0\0ET\07r\0F\10\0F\10\00\010\08)\13\13\00\01)x\09\A0\07\0D\00\098x\0F\0F \00A\07r\13\10\C0\02\02@\08\0Ap\07\1B\E4\10\08\01@\00%\13\13@\00\11\E4\90\07$\09\00P\05*\A8\0E\80\09\0C\00\08\10&p\007\12\10\12\D0\00I\12x\10\13`\00\08\C0\07\00\C0\00\030\0E\04\A0\07\00\B0\05\0A\80\00\01o\19\02`\00\10fP\00%\12\12P\00\10\CC \00\02/\1C\05\90\06Ir\0E\07\0E\E0\07*\07\0E\E0\074\08\08\0F \00p\C6\0F\01#t\07\07\00\07!\14@ \00H\08#t\0FP\07d\08!r\11\06\110\00y\E4\0F\02!t\09\07\F0\068t\06\0F@\00 $x\0F\17\07p\06:t\10\11P\009\05\05\12\F0\07%\09\0E \07\01\B0\07\1B\0F@\08*\10\10\A0\00'\12\05P\00\01@\00\00@\07$\09\00\10\00\11\0Fp\07$\0F\08P\00*\0E\10P\00'\12\12P\00Y\00$x\06\06\B0\00!x\0F\90\07%\0F\00P\08\17\11\90\08I\04!t\08P\08\00\A0\07\14\09\80\01!$\0E0\00\08\80\08@#x\08\05@\00\01\D5\13\12\C8\10\00\00 \00\15\08@\007\0F\00\0F\10\08E$x\05\10\90\00\1B\CEp\08\85\A2\0E\00#r\07\07\09\C0\07\10\1F\C0\07\06\E0\07\01\B0\057\08\00\08\10\021#r\06\E1\09\013\07!\C8/\00\08\11\0E\9D\02\06\E0\07\1B\08\E0\07.\D0\F8\90\13\0F\C0\07\09)0\04\90\13\1D\04\90\13\17\04@\00'x\05\80\07h\1F\00$t\08\FF\F0\02\04\10\07\06p\07\16\11 \07\01\00\07\11\06\00\07\22\08\02P\0EE%v\08\11\10\00\01\00\07\16\05p\15 \A8\00\A0\12\18\08\A0\1B6\06{\12\00\07!\22\0F\F0\06\18@\00\15Es\12\00\12\F0\06\00 \15&\0E\12\F0\06*\0F\01\F0\0E\1A\09@\074\E4\0F\01 \0E\02\00\07.\0F\02\F0\0E\16\13\F0\0E\00\D0\06&t\0E\10\04\01\D0\064\06\13\10P\00\10\C8@\0F\11\09P\00\10\06P\00u\E4/\00'r\07\13P\06\01\10\07%\10\09\B0\06\00@\01!r\080\00#\07\0A\10\00)t\0F \0C\06\B0\05#\FC\03P\06\15\08P\06\00\F0\0C&z\08\B0\05\1B\C8`\05\10\CE\E0\05\0A@\06E\10h\06\06\10\05\1B\E4\E0\06\00 \00?Z\11\11 \0E\03\02p\06\06`\00\12\F0\A0\06\10X\90\04\06p\15'\10H`\00\01\F0\11!\06\08\BC\02\03\80\16'\10\080\00\1B\E4\00\0A\03\E0\03\14\040\01\01\10\06!\07\08\0C\03!\00\00P\00I\12x\06\06\A0\055x\07\070\00\10\CA \00(\08\07 \00\22\84y_\13\04\10\06E\84y\09\08\10\00\10\E2\80\05\10\050\17\08\80\05\01\B0\04$\0E \80\059\09\04\09`\0D\00\C0\00D@K\0F@\A0\04%\0E\090\00\10\C6\C0\04\18\04p\05X\04#t\0E\0E0\00\09p\05\10\E4`\04\11\04p\04$\04\08\00\05%\06\0E@\00\12\C6 \00\00\80\04\22\04\00 \00@#x\06\090\00$\06\08P\00%\05\0EP\00\12\C6 \00\000\00\13\060\00T\08s\04\00\04\F0\00\01 \127\06\00\06\C0\04\00\90\0C\14\04\A0\04 \C8\1F\10\00$\05\06\10\00/\C6/\90\16\01\1C\03\90\16&@\02 \00\0E\C0\17\16\03 \04\01\00\0B%\04\03\80\179\CC\1F\00\90\1Ef\A2\00\00\06{\09\F0\03\10\E2\A0\0A4\06\FF\FFP\00'\E2/\90\02\03@\08&t\050\03&\CA\1F\E0\05\01\10\04 \8E\00\D0&\0A\00\0B4\07\00\07\10\04%$\0E\C0\0A\01\80\03\00\C0\03:z\0F\0F\00\041\06\07\0F\80\05\02\B0\05T'r\06\06\03\90\00\16\C8\C0\03#\06\0A\10\007z\03\08\10\05\010\05\050\04\17\C6@\01!`\F0p\05I\10\0A\03\03\A0\03\1D\08\A0\03\15\03\A0\03\1B\DA`\03\00\10\09)\DA\06\00\04\00 \03\14\07\B0)\1B\CAP\03\10\CA\10\034\03\06\00\00\02\10\A4\00\03\10\03p\10\07\00\03\01\7F\103?\08 \00\070t\04\04\C0\02\22\05@\10\00E!t\08\04\A0\02\02 \07\1B\030\07\18\030\07E$x\03\04\A0\02\1B\CA\F0\06\11$\80\02'\03\08\10\0F\0FP\18\01(\E2\10P\18%\03\02P\18\02\C0\00\04\F0\17C\00\00\CAO\F0\17\17@ \00\11\04\B0\09\06 \00&\05\04\10\18\10\1E\00\04\02\C0\17\01 \00\14\1F\B0\17\03\F0\17Q.\00!r\06\10\03\06 \00%\07\06\D0\17`d\00\00!\A2\07\01\10\03 \00\11/\80\17/\07\80\80\17';M9\00\C0\07\11\00\10\03$\FF\E0\C0\07H\00\00@\0Ap\174\00\00 \10\02\01\00\02\00\FD\16\08p\17\00\FC\01\010\00\03p\17%\00\00p\17\02p\12\11\0D\80\05$b\F2\E0\05\01p\00\05`\17V\19\00\00\C0\09\90\00\04p\1D\11\0D\E0\02\11op\17\05`\02\11\C6\A0\07\15\07\90\00\01@\17\11\0E\80\1D$\02\02\00\04%\10\03\10\00\01\00\08\16\04`\0F \A8\02\E0\16\18\10P\0FY\10x\09\07@\10\1E&\05\07\B0\0F\00\90\1D*\12\09`\00*\0A\05`\00\07\90\17zj\01\00\81y\0A\0A@\17\17\14`\08\0C`\0F\00p#4\0D\0D\80p\00\01\B0\099\14\00\14`\17,\15\14`\0F\19\15`\0F\02\E0+\06`\0F\01`\07\07`\04\11\12\10\17\07`\0F$\12\07\80\00\01`\04\1B\10`\0F)\0F\12\80\048z\0B\10\00\1F\040\00\15\0F0\004\11\12\090\00\02P\08\12\0BP\04\12\F6\D0\0E8z\10\10\B0\04W'r\12\12\05\D0\01\06 \08\19\F2\A0\0F$\11\0Ap\00\16\16\10\0F\00\C0\049:\0B\0B\D0\04\1D8\E0\0E\15\0B\D0\04\11\E2\C0\00\06\D0\0F\18\E2p\08\12\F6`\08\1A\1A\C0\08\06@\00*\FA\03\C0\08\13\F4P\00'\10\16\A0\11:\00\10\18\00\0F6\12z\140\05\00\A0\00\08\10\10\18\C4P\00\02\10\09R\07r\0E\14\0E@\03\11\050\00\1BZ\E0\00\17(`\00\0F\90\16\00\06\B0\00\13\F2P\004\0F\14\0FP\00\01p\0E\16\13\80\16\1B\E2`\0A\1B\E2\00\10\0F0\0F\00H\10X\11\11\80\00T\84y\15\13\00`\05\1B\E2 \01\0F@\0F\00f\84y\0B\13\00\80\80\0C\09\00\19\00\80\01\1C\18`\00%\17\0F`\00\10\E6\D0\004\11\14\11\D0\00\04`\0F\04P\00Wd\0E\00\10(P\00\000\025x\11\11\C0\00\01\10\014\12\14\12@\00\01\B0\06\16\10P\0F\11\E2@\0F\15\11\B0\00\1B\C8\E0\17\01p\00%\19\12\90\00\1Bh\80\0F\22\E8\0E\C0\17\16\80\10\01 \0Bx\C56\94\80~\00B\FC\03\00\C4\1F\10\002\00\00\E2\A0\01B\0Bx\00\0F \00\000\01#/\04\10\00A\00\00\E2\F6P\0BB h\0B\0B\866\11@P\05! \98\10\00\11K\10\00u\E2\0F\00 (\0F\0F \00\01\F0\06\10\0Ba\00\020\04\00 \008\B8\0F\0F0\00Q!r\15\04\15a\00\00 \00\11O@\09\08\00\0CV!r\17\08\17\D0\0FW\01#t\13\15\A0\0AX\08\84y\08\12\C0\01E#t\1B\17 \00\00`\0A\10t\90\114@K\040\175t\1B\1B\10\00\10\E4\D0\07%\14\13\D0\07\01p\007\19\06\190\18E!t\16\1B \00\01\B0\0A(\14\15p\17 $x\CF $\80\00\B0\00@#x\16\17 \00&\16\080\00\05`\17\01`\00%\11\0A\A0\10|\E2\8F\00$x\1B\1B@\00\000\00\13\160\00X\08s\15\00\14\B0\0A8t\17\19\00\01H#t\13\11\80\0BE#t\17\17\E0\00\02 \00\06\00\01\10\E4P\007\16\00\16\A0\0FI!t\0A\17\00\01)t\04\10\01E$x\17\17\A0\00\01\D0\00 \0A\19\E0\00\22\0A\08 \00H#x\04\11\C0\0B\00\90\12\08\D0\00R r\06\06\15\EA7\01\C0\17\00@\00\00\E0\00&\0A\00@\00\08\E0\0B\00\F0\00\15\0F0\02e\02\00 h\06\06P\02\02\C0\02\12\08\A0\02\14\F8\C0\02\15\10\D0\02\01p\005\16\1B\16p\00\14O0\002\00\00\E20\0BH \98\06\06\80\021\08s\0A;4\04@\02K (\16\16p\00\12\10@\00\13\F4`\007\0F\0B\06\D0\00\01`%\16^p\07H \B8\16\16`\00\09`\08\00 \008H\08\08`\00\09\A0\0Cxb\0E\00 h\10\10 \00F r\15\15\D0\00h\8F\00 \D8\08\08`\001\86y\000\11\CA\04\19\10\0C\00\E6\05\00 \A8\10\10\E0\00\03`\19\00P\01\84\22\07\00 r\0A\17\0AP\00u\C8\1F\00 H\0A\0Ap\00\12\C6\F0\19\15\100\00D\0E\00 rp)\02\F0\00H \D8\0A\0A`\00\01\C02\04\00\01z\E4\8F\00 h\13\13\C0\00%\11\11p\00z\C6\0F\01 \A8\13\13@\00%\0A\03@\00\01\C0\08(\02\05P\01T\86y\00\0A\15\E0\00E\E2\05\00  \15\00P\00\10\1F \00$\08\11 \00 \E8\05\10\00'\02\130\00hG\99\00\00@\F6 \12\01=\00\05\D0\099M\09\00\10\009\06{\04\80\0D\00p\0A#0\03 \00\09\A0\07\03P\06\09\80\10\16O\D0\01\00@\01\10\A4\80\0D&\02\04 \09uO\00\05s\03\00\02 \09\15\04\10#\02`\03QO\00$r\05\10\00\10\03P\08 \C8\9F\C0%\1A\050\091\09\03\050\0C\02\10\00\00\E06\15\04@\00\00\00\08\11\04\A0\0A\16\00P\01\01\A0*3\0B\02\8Ep\0E\19\02p(9'r\05\F0\1D\09P\0B\01\F0\17\11\07\90\00\13\05\D0\116$t\0A\A0\05\01p\00*\06\07P\22\1C\07\F0\0D\1A\06\F0\0D)\06\06@\088\08\05\05 \07\060\00\12\F20\00\17\18 \00\01\D0\0B\12\05 (#\00\000\07(\05\05\E0\0DE\12x\03\05 \07\84\CA\1F\00\84y\05\03\00`\03\10\E8 \07F\06\03\00\80\80\0DB\0Bx\00\06@\04\00\C0\1B#\1F\04\10\00\02\10\07W\D6\0F\00 \08\80\04\1B\C8@\04&\CC\0FP\10#\10\00\E0\051\02\02\05A\00\04P\0E\01?+\11?\15\0C\00 \0E\03P\1E\22\0A@\10\008!t\07 \1E\19\040\1D\01\D0\10 \07\02`\05#\07\08@\005x\07\02\80\1D\00\E0(\22s\02\D0\0F\03\D0\00$ r\B0+\11@ \15\01\00\02\13^\00\02\10\C6\D0\00(\05\05\A0\03\09\F0\0C\00 \005\98\05\05\E0\00\00\F0\00#r\05@)\02\D0\07\00`\03\15\05`\03\11\01\00\11/p\FD`\03\08\0B\10\00\04\F0\02\13\13p\02\81\02x\04\00\10:\00\00\F5\05\12\E2\D0'\14\01\D0\01E\C4\0F\00D\80\03\11\C0@\00\00P(\14\07p&\00\C0\17Gt\03\FF\02\C0\02\06\E0.\01\90\1DR\04\08r\13\070\04\02\90!\00p\00\14\80p\00\00\10\07$r\02 \00\14\00p\00)\C0\02p\00\12\02p\00\15\F0p\00\1F\04p\00\02\00\90\06\02P\00\01?\00\00F\16\03p\00\1B\F0p\00\03 \00\14\02p\00\1FPp\00\0C\1F\08p\00\05\01\E0\00\09p\00\01P\01#`;p\00\1F\C4p\00\03/\E0\01p\00\0B\1F\10p\00\17\01\E0\00/\D0;\E0\00\0B\17pp\00\04\D0\12\13\07`\00fGy\00\00\F0\D7@\02\09\00\02\03 \02& <\D0\00\00P\00\15 P\00\11?p\03\01\A0\00\16\00@\02\1B\02@\00\1B`@\00&\E0\00\90\00\0F@\00\01\1B\04@\00\1B\A0@\00\1F\A0@\00\0C\1B\08@\00\1B\E0@\00\19`@\00\1D\06@\00\1B\10@\00&0=@\00\04@\03\17\06 \03\17\10P\00\00P\01(`\EEP\01\00 \14\06P\00fHs\00\00\1A\00\A040\89s\07@*A\0C\0B\00\0E@\04`Py\00\04\80\C2@\00R\C3\03\00\EA\1F\A0\01\13\FFP\00f\C0\0F\00\18y\00\01\00\0F\10\00\D0\0F\01\00-\00\AB\02.\03\00\01\00\22@\00\01\00=%\01\000\00\08\01\00\1F\0B@\00\04\13e)\00\1F\F1@\00\0C\00\DC\04.\02\00\01\00\13XU\00$\C0\00\AD\03\00T\02\02G\06\00\01\00\03\948/\00\E6T\00\00\00\01\00\02\A5!\02\00E\0F\80\00\09\11)\06\00\05\EFC\05\01\00\13\F0@\00\11T\06\00\06\E4\00*\04\00\01\00/[\00@\00\03\12D\95\066\00\DC\00@\00\1F\0A@\00\00\00-'\1F\0B@\00\00\11 \B5\07\0E\C0\00\1B\08\08\00\11\F3\9C2\0E\01\00\13\F8@\00& \00\80\00\17\048\00\02\94\033\00\00\B5\14\01\0C\84\01\13\18\F5\03\00|<\04\01\00\0F\C0\00\01\132@\00\15\06R\00\03\01\00\03\95\00&\80>\80\00j\07\00\00&\80\00\01\00\13\87\94\00*\03\00\01\00\040J\01]\00\0F\80\00\08\13\06l\1D\04hJ\0C\01\00\1B\A8\08\00\17\08W\00\17\05\E8\00\0C\01\00*h@\08\00\088\00\18\06\A0\00\0F\01\00\05\00)\00\B0\00\00\00\08\00\00\00\00\00\00\00\00\00")
  llvm.mlir.global internal constant @alloc___gpu___pvoid_i64___pvoid("alloc___gpu___pvoid_i64___pvoid\00")
  llvm.mlir.global internal constant @ral_recv_input___cpu___pvoid_i64___m3df32("ral_recv_input___cpu___pvoid_i64___m3df32\00")
  llvm.func @disc_ral_call(!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>)
  llvm.func @main(%arg0: !llvm.ptr<i8>) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = llvm.mlir.constant(8 : index) : i64
    %1 = llvm.mlir.constant(256 : index) : i64
    %2 = llvm.mlir.constant(1 : index) : i64
    %3 = llvm.mlir.constant(2 : index) : i64
    %4 = llvm.mlir.constant(3 : index) : i64
    %5 = llvm.mlir.constant(1024 : index) : i64
    %6 = llvm.mlir.constant(6 : index) : i64
    %7 = llvm.mlir.constant(512 : index) : i64
    %8 = llvm.mlir.constant(0 : index) : i64
    %9 = llvm.mlir.constant(0 : i32) : i32
    %10 = llvm.mlir.constant(1 : i32) : i32
    %11 = llvm.alloca %10 x !llvm.struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)> : (i32) -> !llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)>>
    %12 = llvm.mlir.constant(3 : i32) : i32
    %13 = llvm.alloca %12 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %14 = llvm.mlir.constant(0 : i32) : i32
    %15 = llvm.getelementptr %11[%9, 0] : (!llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %15 : !llvm.ptr<ptr<i8>>
    %16 = llvm.getelementptr %13[%14] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %17 = llvm.bitcast %15 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %17, %16 : !llvm.ptr<ptr<i8>>
    %18 = llvm.mlir.constant(1 : i32) : i32
    %19 = llvm.getelementptr %11[%9, 1] : (!llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %8, %19 : !llvm.ptr<i64>
    %20 = llvm.getelementptr %13[%18] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %21 = llvm.bitcast %19 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %21, %20 : !llvm.ptr<ptr<i8>>
    %22 = llvm.mlir.constant(2 : i32) : i32
    %23 = llvm.getelementptr %11[%9, 2] : (!llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)>>, i32) -> !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>>
    %24 = llvm.getelementptr %13[%22] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %25 = llvm.bitcast %23 : !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>> to !llvm.ptr<i8>
    llvm.store %25, %24 : !llvm.ptr<ptr<i8>>
    %26 = llvm.mlir.addressof @ral_recv_input___cpu___pvoid_i64___m3df32 : !llvm.ptr<array<42 x i8>>
    %27 = llvm.mlir.constant(0 : index) : i64
    %28 = llvm.getelementptr %26[%27, %27] : (!llvm.ptr<array<42 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %28, %13) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %29 = llvm.load %23 : !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>>
    %30 = llvm.extractvalue %29[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %31 = llvm.extractvalue %29[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %32 = llvm.extractvalue %29[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %33 = llvm.trunc %31 : i64 to i32
    %34 = llvm.trunc %30 : i64 to i32
    %35 = llvm.mul %33, %34  : i32
    %36 = llvm.sext %35 : i32 to i64
    %37 = llvm.mlir.constant(1 : index) : i64
    %38 = llvm.mul %32, %30  : i64
    %39 = llvm.mul %38, %31  : i64
    %40 = llvm.mlir.null : !llvm.ptr<f32>
    %41 = llvm.getelementptr %40[%39] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
    %42 = llvm.ptrtoint %41 : !llvm.ptr<f32> to i64
    %43 = llvm.mlir.constant(0 : i32) : i32
    %44 = llvm.mlir.constant(1 : i32) : i32
    %45 = llvm.alloca %44 x !llvm.struct<".1", (ptr<i8>, i64, ptr<i8>)> : (i32) -> !llvm.ptr<struct<".1", (ptr<i8>, i64, ptr<i8>)>>
    %46 = llvm.mlir.constant(3 : i32) : i32
    %47 = llvm.alloca %46 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %48 = llvm.mlir.constant(0 : i32) : i32
    %49 = llvm.getelementptr %45[%43, 0] : (!llvm.ptr<struct<".1", (ptr<i8>, i64, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %49 : !llvm.ptr<ptr<i8>>
    %50 = llvm.getelementptr %47[%48] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %51 = llvm.bitcast %49 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %51, %50 : !llvm.ptr<ptr<i8>>
    %52 = llvm.mlir.constant(1 : i32) : i32
    %53 = llvm.getelementptr %45[%43, 1] : (!llvm.ptr<struct<".1", (ptr<i8>, i64, ptr<i8>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %42, %53 : !llvm.ptr<i64>
    %54 = llvm.getelementptr %47[%52] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %55 = llvm.bitcast %53 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %55, %54 : !llvm.ptr<ptr<i8>>
    %56 = llvm.mlir.constant(2 : i32) : i32
    %57 = llvm.getelementptr %45[%43, 2] : (!llvm.ptr<struct<".1", (ptr<i8>, i64, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    %58 = llvm.getelementptr %47[%56] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %59 = llvm.bitcast %57 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %59, %58 : !llvm.ptr<ptr<i8>>
    %60 = llvm.mlir.addressof @alloc___gpu___pvoid_i64___pvoid : !llvm.ptr<array<32 x i8>>
    %61 = llvm.mlir.constant(0 : index) : i64
    %62 = llvm.getelementptr %60[%61, %61] : (!llvm.ptr<array<32 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %62, %47) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %63 = llvm.load %57 : !llvm.ptr<ptr<i8>>
    %64 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>
    %65 = llvm.bitcast %63 : !llvm.ptr<i8> to !llvm.ptr<f32>
    %66 = llvm.insertvalue %65, %64[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %67 = llvm.insertvalue %65, %66[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %68 = llvm.mlir.constant(0 : index) : i64
    %69 = llvm.insertvalue %68, %67[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %70 = llvm.mlir.constant(1 : index) : i64
    %71 = llvm.insertvalue %32, %69[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %72 = llvm.insertvalue %70, %71[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %73 = llvm.mul %70, %32  : i64
    %74 = llvm.insertvalue %30, %72[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %75 = llvm.insertvalue %73, %74[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %76 = llvm.mul %73, %30  : i64
    %77 = llvm.insertvalue %31, %75[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %78 = llvm.insertvalue %76, %77[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %79 = llvm.icmp "slt" %36, %4 : i64
    %80 = llvm.icmp "sge" %32, %5 : i64
    %81 = llvm.icmp "slt" %36, %6 : i64
    %82 = llvm.and %81, %80  : i1
    %83 = llvm.icmp "sge" %32, %7 : i64
    %84 = llvm.icmp "sge" %36, %6 : i64
    %85 = llvm.and %84, %83  : i1
    %86 = llvm.or %79, %82  : i1
    %87 = llvm.or %86, %85  : i1
    llvm.cond_br %87, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %88 = llvm.mlir.addressof @main_kernel_blob_gpu.binary : !llvm.ptr<array<6648 x i8>>
    %89 = llvm.mlir.constant(0 : index) : i64
    %90 = llvm.getelementptr %88[%89, %89] : (!llvm.ptr<array<6648 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %91 = llvm.mlir.constant(1 : i32) : i32
    %92 = llvm.alloca %91 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %93 = llvm.mlir.constant(0 : i32) : i32
    %94 = llvm.getelementptr %92[%93] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %90, %94 : !llvm.ptr<ptr<i8>>
    %95 = llvm.mlir.constant(1 : i64) : i64
    %96 = llvm.mlir.addressof @main_kernel_main_kStitch_divide__13_1_0___1b1r_kernel_name : !llvm.ptr<array<35 x i8>>
    %97 = llvm.mlir.constant(0 : index) : i64
    %98 = llvm.getelementptr %96[%97, %97] : (!llvm.ptr<array<35 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %99 = llvm.extractvalue %29[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %100 = llvm.extractvalue %29[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %101 = llvm.extractvalue %29[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %102 = llvm.extractvalue %29[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %103 = llvm.extractvalue %29[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %104 = llvm.extractvalue %29[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %105 = llvm.extractvalue %29[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %106 = llvm.extractvalue %29[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %107 = llvm.extractvalue %29[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %108 = llvm.extractvalue %78[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %109 = llvm.extractvalue %78[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %110 = llvm.extractvalue %78[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %111 = llvm.extractvalue %78[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %112 = llvm.extractvalue %78[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %113 = llvm.extractvalue %78[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %114 = llvm.extractvalue %78[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %115 = llvm.extractvalue %78[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %116 = llvm.extractvalue %78[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %117 = llvm.mlir.constant(1 : i32) : i32
    %118 = llvm.alloca %117 x !llvm.struct<".5", (ptr<f32>, i64, ptr<f32>)> : (i32) -> !llvm.ptr<struct<".5", (ptr<f32>, i64, ptr<f32>)>>
    %119 = llvm.mlir.constant(3 : i32) : i32
    %120 = llvm.alloca %119 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %121 = llvm.mlir.constant(0 : i32) : i32
    %122 = llvm.mlir.constant(0 : i32) : i32
    %123 = llvm.getelementptr %118[%121, 0] : (!llvm.ptr<struct<".5", (ptr<f32>, i64, ptr<f32>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %100, %123 : !llvm.ptr<ptr<f32>>
    %124 = llvm.getelementptr %120[%122] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %125 = llvm.bitcast %123 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %125, %124 : !llvm.ptr<ptr<i8>>
    %126 = llvm.mlir.constant(1 : i32) : i32
    %127 = llvm.getelementptr %118[%121, 1] : (!llvm.ptr<struct<".5", (ptr<f32>, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %104, %127 : !llvm.ptr<i64>
    %128 = llvm.getelementptr %120[%126] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %129 = llvm.bitcast %127 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %129, %128 : !llvm.ptr<ptr<i8>>
    %130 = llvm.mlir.constant(2 : i32) : i32
    %131 = llvm.getelementptr %118[%121, 2] : (!llvm.ptr<struct<".5", (ptr<f32>, i64, ptr<f32>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %109, %131 : !llvm.ptr<ptr<f32>>
    %132 = llvm.getelementptr %120[%130] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %133 = llvm.bitcast %131 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %133, %132 : !llvm.ptr<ptr<i8>>
    %134 = llvm.mlir.constant(0 : i32) : i32
    %135 = llvm.mlir.constant(3 : i32) : i32
    %136 = llvm.inttoptr %134 : i32 to !llvm.ptr<i8>
    %137 = llvm.mlir.constant(0 : i32) : i32
    %138 = llvm.mlir.constant(1 : i32) : i32
    %139 = llvm.alloca %138 x !llvm.struct<".6", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".6", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %140 = llvm.mlir.constant(14 : i32) : i32
    %141 = llvm.alloca %140 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %142 = llvm.mlir.constant(0 : i32) : i32
    %143 = llvm.getelementptr %139[%137, 0] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %143 : !llvm.ptr<ptr<i8>>
    %144 = llvm.getelementptr %141[%142] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %145 = llvm.bitcast %143 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %145, %144 : !llvm.ptr<ptr<i8>>
    %146 = llvm.mlir.constant(1 : i32) : i32
    %147 = llvm.getelementptr %139[%137, 1] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %92, %147 : !llvm.ptr<ptr<ptr<i8>>>
    %148 = llvm.getelementptr %141[%146] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %149 = llvm.bitcast %147 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %149, %148 : !llvm.ptr<ptr<i8>>
    %150 = llvm.mlir.constant(2 : i32) : i32
    %151 = llvm.getelementptr %139[%137, 2] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %95, %151 : !llvm.ptr<i64>
    %152 = llvm.getelementptr %141[%150] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %153 = llvm.bitcast %151 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %153, %152 : !llvm.ptr<ptr<i8>>
    %154 = llvm.mlir.constant(3 : i32) : i32
    %155 = llvm.getelementptr %139[%137, 3] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %98, %155 : !llvm.ptr<ptr<i8>>
    %156 = llvm.getelementptr %141[%154] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %157 = llvm.bitcast %155 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %157, %156 : !llvm.ptr<ptr<i8>>
    %158 = llvm.mlir.constant(4 : i32) : i32
    %159 = llvm.getelementptr %139[%137, 4] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %36, %159 : !llvm.ptr<i64>
    %160 = llvm.getelementptr %141[%158] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %161 = llvm.bitcast %159 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %161, %160 : !llvm.ptr<ptr<i8>>
    %162 = llvm.mlir.constant(5 : i32) : i32
    %163 = llvm.getelementptr %139[%137, 5] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %163 : !llvm.ptr<i64>
    %164 = llvm.getelementptr %141[%162] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %165 = llvm.bitcast %163 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %165, %164 : !llvm.ptr<ptr<i8>>
    %166 = llvm.mlir.constant(6 : i32) : i32
    %167 = llvm.getelementptr %139[%137, 6] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %167 : !llvm.ptr<i64>
    %168 = llvm.getelementptr %141[%166] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %169 = llvm.bitcast %167 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %169, %168 : !llvm.ptr<ptr<i8>>
    %170 = llvm.mlir.constant(7 : i32) : i32
    %171 = llvm.getelementptr %139[%137, 7] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1, %171 : !llvm.ptr<i64>
    %172 = llvm.getelementptr %141[%170] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %173 = llvm.bitcast %171 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %173, %172 : !llvm.ptr<ptr<i8>>
    %174 = llvm.mlir.constant(8 : i32) : i32
    %175 = llvm.getelementptr %139[%137, 8] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %175 : !llvm.ptr<i64>
    %176 = llvm.getelementptr %141[%174] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %177 = llvm.bitcast %175 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %177, %176 : !llvm.ptr<ptr<i8>>
    %178 = llvm.mlir.constant(9 : i32) : i32
    %179 = llvm.getelementptr %139[%137, 9] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %179 : !llvm.ptr<i64>
    %180 = llvm.getelementptr %141[%178] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %181 = llvm.bitcast %179 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %181, %180 : !llvm.ptr<ptr<i8>>
    %182 = llvm.mlir.constant(10 : i32) : i32
    %183 = llvm.getelementptr %139[%137, 10] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %134, %183 : !llvm.ptr<i32>
    %184 = llvm.getelementptr %141[%182] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %185 = llvm.bitcast %183 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %185, %184 : !llvm.ptr<ptr<i8>>
    %186 = llvm.mlir.constant(11 : i32) : i32
    %187 = llvm.getelementptr %139[%137, 11] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %136, %187 : !llvm.ptr<ptr<i8>>
    %188 = llvm.getelementptr %141[%186] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %189 = llvm.bitcast %187 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %189, %188 : !llvm.ptr<ptr<i8>>
    %190 = llvm.mlir.constant(12 : i32) : i32
    %191 = llvm.getelementptr %139[%137, 12] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %135, %191 : !llvm.ptr<i32>
    %192 = llvm.getelementptr %141[%190] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %193 = llvm.bitcast %191 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %193, %192 : !llvm.ptr<ptr<i8>>
    %194 = llvm.mlir.constant(13 : i32) : i32
    %195 = llvm.getelementptr %139[%137, 13] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %120, %195 : !llvm.ptr<ptr<ptr<i8>>>
    %196 = llvm.getelementptr %141[%194] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %197 = llvm.bitcast %195 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %197, %196 : !llvm.ptr<ptr<i8>>
    %198 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %199 = llvm.mlir.constant(0 : index) : i64
    %200 = llvm.getelementptr %198[%199, %199] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %200, %141) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    llvm.br ^bb3
  ^bb2:  // pred: ^bb0
    %201 = llvm.icmp "eq" %36, %8 : i64
    %202 = llvm.sub %36, %2  : i64
    %203 = llvm.udiv %202, %0  : i64
    %204 = llvm.add %203, %2  : i64
    %205 = llvm.select %201, %8, %204 : i1, i64
    %206 = llvm.mlir.addressof @main_kernel_0_blob_gpu.binary : !llvm.ptr<array<8368 x i8>>
    %207 = llvm.mlir.constant(0 : index) : i64
    %208 = llvm.getelementptr %206[%207, %207] : (!llvm.ptr<array<8368 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %209 = llvm.mlir.constant(1 : i32) : i32
    %210 = llvm.alloca %209 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %211 = llvm.mlir.constant(0 : i32) : i32
    %212 = llvm.getelementptr %210[%211] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %208, %212 : !llvm.ptr<ptr<i8>>
    %213 = llvm.mlir.constant(1 : i64) : i64
    %214 = llvm.mlir.addressof @main_kernel_0_main_kStitch_divide__13_1_0___1w1r_kernel_name : !llvm.ptr<array<35 x i8>>
    %215 = llvm.mlir.constant(0 : index) : i64
    %216 = llvm.getelementptr %214[%215, %215] : (!llvm.ptr<array<35 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %217 = llvm.extractvalue %29[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %218 = llvm.extractvalue %29[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %219 = llvm.extractvalue %29[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %220 = llvm.extractvalue %29[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %221 = llvm.extractvalue %29[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %222 = llvm.extractvalue %29[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %223 = llvm.extractvalue %29[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %224 = llvm.extractvalue %29[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %225 = llvm.extractvalue %29[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %226 = llvm.extractvalue %78[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %227 = llvm.extractvalue %78[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %228 = llvm.extractvalue %78[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %229 = llvm.extractvalue %78[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %230 = llvm.extractvalue %78[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %231 = llvm.extractvalue %78[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %232 = llvm.extractvalue %78[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %233 = llvm.extractvalue %78[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %234 = llvm.extractvalue %78[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %235 = llvm.mlir.constant(1 : i32) : i32
    %236 = llvm.alloca %235 x !llvm.struct<".2", (ptr<f32>, i64, i64, i64, ptr<f32>)> : (i32) -> !llvm.ptr<struct<".2", (ptr<f32>, i64, i64, i64, ptr<f32>)>>
    %237 = llvm.mlir.constant(5 : i32) : i32
    %238 = llvm.alloca %237 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %239 = llvm.mlir.constant(0 : i32) : i32
    %240 = llvm.mlir.constant(0 : i32) : i32
    %241 = llvm.getelementptr %236[%239, 0] : (!llvm.ptr<struct<".2", (ptr<f32>, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %218, %241 : !llvm.ptr<ptr<f32>>
    %242 = llvm.getelementptr %238[%240] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %243 = llvm.bitcast %241 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %243, %242 : !llvm.ptr<ptr<i8>>
    %244 = llvm.mlir.constant(1 : i32) : i32
    %245 = llvm.getelementptr %236[%239, 1] : (!llvm.ptr<struct<".2", (ptr<f32>, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %221, %245 : !llvm.ptr<i64>
    %246 = llvm.getelementptr %238[%244] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %247 = llvm.bitcast %245 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %247, %246 : !llvm.ptr<ptr<i8>>
    %248 = llvm.mlir.constant(2 : i32) : i32
    %249 = llvm.getelementptr %236[%239, 2] : (!llvm.ptr<struct<".2", (ptr<f32>, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %222, %249 : !llvm.ptr<i64>
    %250 = llvm.getelementptr %238[%248] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %251 = llvm.bitcast %249 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %251, %250 : !llvm.ptr<ptr<i8>>
    %252 = llvm.mlir.constant(3 : i32) : i32
    %253 = llvm.getelementptr %236[%239, 3] : (!llvm.ptr<struct<".2", (ptr<f32>, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %36, %253 : !llvm.ptr<i64>
    %254 = llvm.getelementptr %238[%252] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %255 = llvm.bitcast %253 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %255, %254 : !llvm.ptr<ptr<i8>>
    %256 = llvm.mlir.constant(4 : i32) : i32
    %257 = llvm.getelementptr %236[%239, 4] : (!llvm.ptr<struct<".2", (ptr<f32>, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %227, %257 : !llvm.ptr<ptr<f32>>
    %258 = llvm.getelementptr %238[%256] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %259 = llvm.bitcast %257 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %259, %258 : !llvm.ptr<ptr<i8>>
    %260 = llvm.mlir.constant(0 : i32) : i32
    %261 = llvm.mlir.constant(5 : i32) : i32
    %262 = llvm.inttoptr %260 : i32 to !llvm.ptr<i8>
    %263 = llvm.mlir.constant(0 : i32) : i32
    %264 = llvm.mlir.constant(1 : i32) : i32
    %265 = llvm.alloca %264 x !llvm.struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %266 = llvm.mlir.constant(14 : i32) : i32
    %267 = llvm.alloca %266 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %268 = llvm.mlir.constant(0 : i32) : i32
    %269 = llvm.getelementptr %265[%263, 0] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %269 : !llvm.ptr<ptr<i8>>
    %270 = llvm.getelementptr %267[%268] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %271 = llvm.bitcast %269 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %271, %270 : !llvm.ptr<ptr<i8>>
    %272 = llvm.mlir.constant(1 : i32) : i32
    %273 = llvm.getelementptr %265[%263, 1] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %210, %273 : !llvm.ptr<ptr<ptr<i8>>>
    %274 = llvm.getelementptr %267[%272] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %275 = llvm.bitcast %273 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %275, %274 : !llvm.ptr<ptr<i8>>
    %276 = llvm.mlir.constant(2 : i32) : i32
    %277 = llvm.getelementptr %265[%263, 2] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %213, %277 : !llvm.ptr<i64>
    %278 = llvm.getelementptr %267[%276] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %279 = llvm.bitcast %277 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %279, %278 : !llvm.ptr<ptr<i8>>
    %280 = llvm.mlir.constant(3 : i32) : i32
    %281 = llvm.getelementptr %265[%263, 3] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %216, %281 : !llvm.ptr<ptr<i8>>
    %282 = llvm.getelementptr %267[%280] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %283 = llvm.bitcast %281 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %283, %282 : !llvm.ptr<ptr<i8>>
    %284 = llvm.mlir.constant(4 : i32) : i32
    %285 = llvm.getelementptr %265[%263, 4] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %205, %285 : !llvm.ptr<i64>
    %286 = llvm.getelementptr %267[%284] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %287 = llvm.bitcast %285 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %287, %286 : !llvm.ptr<ptr<i8>>
    %288 = llvm.mlir.constant(5 : i32) : i32
    %289 = llvm.getelementptr %265[%263, 5] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %289 : !llvm.ptr<i64>
    %290 = llvm.getelementptr %267[%288] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %291 = llvm.bitcast %289 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %291, %290 : !llvm.ptr<ptr<i8>>
    %292 = llvm.mlir.constant(6 : i32) : i32
    %293 = llvm.getelementptr %265[%263, 6] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %293 : !llvm.ptr<i64>
    %294 = llvm.getelementptr %267[%292] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %295 = llvm.bitcast %293 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %295, %294 : !llvm.ptr<ptr<i8>>
    %296 = llvm.mlir.constant(7 : i32) : i32
    %297 = llvm.getelementptr %265[%263, 7] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1, %297 : !llvm.ptr<i64>
    %298 = llvm.getelementptr %267[%296] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %299 = llvm.bitcast %297 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %299, %298 : !llvm.ptr<ptr<i8>>
    %300 = llvm.mlir.constant(8 : i32) : i32
    %301 = llvm.getelementptr %265[%263, 8] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %301 : !llvm.ptr<i64>
    %302 = llvm.getelementptr %267[%300] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %303 = llvm.bitcast %301 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %303, %302 : !llvm.ptr<ptr<i8>>
    %304 = llvm.mlir.constant(9 : i32) : i32
    %305 = llvm.getelementptr %265[%263, 9] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %305 : !llvm.ptr<i64>
    %306 = llvm.getelementptr %267[%304] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %307 = llvm.bitcast %305 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %307, %306 : !llvm.ptr<ptr<i8>>
    %308 = llvm.mlir.constant(10 : i32) : i32
    %309 = llvm.getelementptr %265[%263, 10] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %260, %309 : !llvm.ptr<i32>
    %310 = llvm.getelementptr %267[%308] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %311 = llvm.bitcast %309 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %311, %310 : !llvm.ptr<ptr<i8>>
    %312 = llvm.mlir.constant(11 : i32) : i32
    %313 = llvm.getelementptr %265[%263, 11] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %262, %313 : !llvm.ptr<ptr<i8>>
    %314 = llvm.getelementptr %267[%312] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %315 = llvm.bitcast %313 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %315, %314 : !llvm.ptr<ptr<i8>>
    %316 = llvm.mlir.constant(12 : i32) : i32
    %317 = llvm.getelementptr %265[%263, 12] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %261, %317 : !llvm.ptr<i32>
    %318 = llvm.getelementptr %267[%316] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %319 = llvm.bitcast %317 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %319, %318 : !llvm.ptr<ptr<i8>>
    %320 = llvm.mlir.constant(13 : i32) : i32
    %321 = llvm.getelementptr %265[%263, 13] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %238, %321 : !llvm.ptr<ptr<ptr<i8>>>
    %322 = llvm.getelementptr %267[%320] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %323 = llvm.bitcast %321 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %323, %322 : !llvm.ptr<ptr<i8>>
    %324 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %325 = llvm.mlir.constant(0 : index) : i64
    %326 = llvm.getelementptr %324[%325, %325] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %326, %267) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    %327 = llvm.mlir.constant(0 : i32) : i32
    %328 = llvm.mlir.constant(1 : i32) : i32
    %329 = llvm.extractvalue %78[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %330 = llvm.extractvalue %78[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %331 = llvm.extractvalue %78[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %332 = llvm.extractvalue %78[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %333 = llvm.extractvalue %78[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %334 = llvm.extractvalue %78[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %335 = llvm.extractvalue %78[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %336 = llvm.extractvalue %78[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %337 = llvm.extractvalue %78[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %338 = llvm.alloca %328 x !llvm.struct<".4", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64, i64, i64)> : (i32) -> !llvm.ptr<struct<".4", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64, i64, i64)>>
    %339 = llvm.mlir.constant(11 : i32) : i32
    %340 = llvm.alloca %339 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %341 = llvm.mlir.constant(0 : i32) : i32
    %342 = llvm.getelementptr %338[%327, 0] : (!llvm.ptr<struct<".4", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %342 : !llvm.ptr<ptr<i8>>
    %343 = llvm.getelementptr %340[%341] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %344 = llvm.bitcast %342 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %344, %343 : !llvm.ptr<ptr<i8>>
    %345 = llvm.mlir.constant(1 : i32) : i32
    %346 = llvm.getelementptr %338[%327, 1] : (!llvm.ptr<struct<".4", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %8, %346 : !llvm.ptr<i64>
    %347 = llvm.getelementptr %340[%345] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %348 = llvm.bitcast %346 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %348, %347 : !llvm.ptr<ptr<i8>>
    %349 = llvm.mlir.constant(2 : i32) : i32
    %350 = llvm.getelementptr %338[%327, 2] : (!llvm.ptr<struct<".4", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %329, %350 : !llvm.ptr<ptr<f32>>
    %351 = llvm.getelementptr %340[%349] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %352 = llvm.bitcast %350 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %352, %351 : !llvm.ptr<ptr<i8>>
    %353 = llvm.mlir.constant(3 : i32) : i32
    %354 = llvm.getelementptr %338[%327, 3] : (!llvm.ptr<struct<".4", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %330, %354 : !llvm.ptr<ptr<f32>>
    %355 = llvm.getelementptr %340[%353] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %356 = llvm.bitcast %354 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %356, %355 : !llvm.ptr<ptr<i8>>
    %357 = llvm.mlir.constant(4 : i32) : i32
    %358 = llvm.getelementptr %338[%327, 4] : (!llvm.ptr<struct<".4", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %331, %358 : !llvm.ptr<i64>
    %359 = llvm.getelementptr %340[%357] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %360 = llvm.bitcast %358 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %360, %359 : !llvm.ptr<ptr<i8>>
    %361 = llvm.mlir.constant(5 : i32) : i32
    %362 = llvm.getelementptr %338[%327, 5] : (!llvm.ptr<struct<".4", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %332, %362 : !llvm.ptr<i64>
    %363 = llvm.getelementptr %340[%361] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %364 = llvm.bitcast %362 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %364, %363 : !llvm.ptr<ptr<i8>>
    %365 = llvm.mlir.constant(6 : i32) : i32
    %366 = llvm.getelementptr %338[%327, 6] : (!llvm.ptr<struct<".4", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %333, %366 : !llvm.ptr<i64>
    %367 = llvm.getelementptr %340[%365] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %368 = llvm.bitcast %366 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %368, %367 : !llvm.ptr<ptr<i8>>
    %369 = llvm.mlir.constant(7 : i32) : i32
    %370 = llvm.getelementptr %338[%327, 7] : (!llvm.ptr<struct<".4", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %334, %370 : !llvm.ptr<i64>
    %371 = llvm.getelementptr %340[%369] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %372 = llvm.bitcast %370 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %372, %371 : !llvm.ptr<ptr<i8>>
    %373 = llvm.mlir.constant(8 : i32) : i32
    %374 = llvm.getelementptr %338[%327, 8] : (!llvm.ptr<struct<".4", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %335, %374 : !llvm.ptr<i64>
    %375 = llvm.getelementptr %340[%373] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %376 = llvm.bitcast %374 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %376, %375 : !llvm.ptr<ptr<i8>>
    %377 = llvm.mlir.constant(9 : i32) : i32
    %378 = llvm.getelementptr %338[%327, 9] : (!llvm.ptr<struct<".4", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %336, %378 : !llvm.ptr<i64>
    %379 = llvm.getelementptr %340[%377] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %380 = llvm.bitcast %378 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %380, %379 : !llvm.ptr<ptr<i8>>
    %381 = llvm.mlir.constant(10 : i32) : i32
    %382 = llvm.getelementptr %338[%327, 10] : (!llvm.ptr<struct<".4", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %337, %382 : !llvm.ptr<i64>
    %383 = llvm.getelementptr %340[%381] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %384 = llvm.bitcast %382 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %384, %383 : !llvm.ptr<ptr<i8>>
    %385 = llvm.mlir.addressof @ral_send_output___cpu___pvoid_i64_m3df32___void : !llvm.ptr<array<48 x i8>>
    %386 = llvm.mlir.constant(0 : index) : i64
    %387 = llvm.getelementptr %385[%386, %386] : (!llvm.ptr<array<48 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %387, %340) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    llvm.return
  }
}


===-------------------------------------------------------------------------===
                         ... Execution time report ...
===-------------------------------------------------------------------------===
  Total Execution Time: 0.7124 seconds

  ----Wall Time----  ----Name----
    0.0004 (  0.1%)  Inliner
    0.0000 (  0.0%)    (A) CallGraph
    0.0002 (  0.0%)    'func.func' Pipeline
    0.0002 (  0.0%)      Canonicalizer
    0.0027 (  0.4%)  'func.func' Pipeline
    0.0007 (  0.1%)    RemoveShapeConstraintsPass
    0.0006 (  0.1%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0012 (  0.2%)    ConvertShapeToStandardPass
    0.0032 (  0.5%)  DiscShapeOptimizationPass
    0.0011 (  0.2%)    'builtin.func' Pipeline
    0.0011 (  0.2%)      CSE
    0.0000 (  0.0%)        (A) DominanceInfo
    0.0009 (  0.1%)  'func.func' Pipeline
    0.0001 (  0.0%)    ConvertTensorToStandardPass
    0.0000 (  0.0%)    ConvertHloToStandardPass
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0005 (  0.1%)    DiscAlgebraSimplifierPass
    0.0000 (  0.0%)    SplitLargeOpsPass
    0.0000 (  0.0%)    DotRewriterPass
    0.0014 (  0.2%)  DiscShapeOptimizationPass
    0.0001 (  0.0%)    'builtin.func' Pipeline
    0.0001 (  0.0%)      CSE
    0.0000 (  0.0%)        (A) DominanceInfo
    0.0001 (  0.0%)  'func.func' Pipeline
    0.0001 (  0.0%)    DiscDotMergePass
    0.0014 (  0.2%)  DiscShapeOptimizationPass
    0.0001 (  0.0%)    'builtin.func' Pipeline
    0.0001 (  0.0%)      CSE
    0.0000 (  0.0%)        (A) DominanceInfo
    0.0010 (  0.1%)  'func.func' Pipeline
    0.0010 (  0.1%)    HloCanonicalizeReductionPass
    0.0033 (  0.5%)  DiscShapeOptimizationPass
    0.0013 (  0.2%)    'builtin.func' Pipeline
    0.0013 (  0.2%)      CSE
    0.0000 (  0.0%)        (A) DominanceInfo
    0.0009 (  0.1%)  DiscMarkShapeCalculationPass
    0.0009 (  0.1%)  PlaceOpsPass
    0.0003 (  0.0%)  'func.func' Pipeline
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0001 (  0.0%)    ElementTypeConverterPass
    0.0035 (  0.5%)  DiscShapeOptimizationPass
    0.0016 (  0.2%)    'builtin.func' Pipeline
    0.0016 (  0.2%)      CSE
    0.0000 (  0.0%)        (A) DominanceInfo
    0.0001 (  0.0%)  'func.func' Pipeline
    0.0001 (  0.0%)    ConvRewriterPass
    0.0035 (  0.5%)  DiscShapeOptimizationPass
    0.0016 (  0.2%)    'builtin.func' Pipeline
    0.0016 (  0.2%)      CSE
    0.0000 (  0.0%)        (A) DominanceInfo
    0.0017 (  0.2%)  'func.func' Pipeline
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0014 (  0.2%)    TransposeSimplifierPass
    0.0002 (  0.0%)      'builtin.func' Pipeline
    0.0002 (  0.0%)        CSE
    0.0000 (  0.0%)          (A) DominanceInfo
    0.0000 (  0.0%)    GpuConvPaddingLegalizationPass
    0.0035 (  0.5%)  DiscShapeOptimizationPass
    0.0016 (  0.2%)    'builtin.func' Pipeline
    0.0016 (  0.2%)      CSE
    0.0000 (  0.0%)        (A) DominanceInfo
    0.0001 (  0.0%)  'func.func' Pipeline
    0.0001 (  0.0%)    DiscAlgebraSimplifierPass
    0.0039 (  0.5%)  DiscShapeOptimizationPass
    0.0016 (  0.2%)    'builtin.func' Pipeline
    0.0016 (  0.2%)      CSE
    0.0000 (  0.0%)        (A) DominanceInfo
    0.0015 (  0.2%)  'func.func' Pipeline
    0.0012 (  0.2%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0002 (  0.0%)    Canonicalizer
    0.0014 (  0.2%)  FuncBufferize
    0.0022 (  0.3%)  DiscHloLegalizeToLhloPass
    0.0038 (  0.5%)  HloLegalizeToLhloPass
    0.0079 (  1.1%)  'func.func' Pipeline
    0.0021 (  0.3%)    Canonicalizer
    0.0001 (  0.0%)    ConvertShapeToStandardPass
    0.0002 (  0.0%)    Canonicalizer
    0.0018 (  0.2%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0002 (  0.0%)    Canonicalizer
    0.0018 (  0.3%)    LegalizeToTensorOpPass
    0.0017 (  0.2%)    Canonicalizer
    0.0001 (  0.0%)    StdBufferizePass
    0.0001 (  0.0%)  ArithmeticBufferize
    0.0081 (  1.1%)  'func.func' Pipeline
    0.0018 (  0.2%)    TensorBufferize
    0.0016 (  0.2%)    FinalizingBufferize
    0.0017 (  0.2%)    Canonicalizer
    0.0016 (  0.2%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0002 (  0.0%)    Canonicalizer
    0.0012 (  0.2%)    DiscMemrefCanonicalizer
    0.0017 (  0.2%)  DiscAssignMemorySpacePass
    0.0119 (  1.7%)  'func.func' Pipeline
    0.0001 (  0.0%)    DiscDuplicateComputationForFusionPass
    0.0014 (  0.2%)    PromoteBuffersToStack
    0.0001 (  0.0%)    DiscMemRefLoadStoreSimplifierPass
    0.0020 (  0.3%)    DiscFusionPass
    0.0000 (  0.0%)    DiscFuseSplatConstPass
    0.0026 (  0.4%)    DiscSpecializeFusionWithSpeculationPass
    0.0025 (  0.4%)    Canonicalizer
    0.0001 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0002 (  0.0%)    Canonicalizer
    0.0002 (  0.0%)    Canonicalizer
    0.0001 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0002 (  0.0%)    Canonicalizer
    0.0025 (  0.4%)    BufferDeallocation
    0.0028 (  0.4%)  RalInjectExecutionContextPass
    0.0027 (  0.4%)  'func.func' Pipeline
    0.0027 (  0.4%)    DiscLowerToLibraryCallPass
    0.0003 (  0.0%)  DiscConstToRALPass
    0.1263 ( 17.7%)  'func.func' Pipeline
    0.0002 (  0.0%)    DiscMemRefLoadStoreSimplifierPass
    0.0071 (  1.0%)    DiscLhloLegalizeRootsToParallelLoopsPass
    0.0003 (  0.0%)    ExpandOps
    0.0004 (  0.1%)    UnhandledAtomicRMWConverterPass
    0.0004 (  0.1%)    InputInlineFusionPass
    0.0096 (  1.3%)    ForLoopUnrollInterleave
    0.0105 (  1.5%)    ArithmeticExpandOps
    0.0108 (  1.5%)    FoldSubViewOps
    0.0216 (  3.0%)    DiscFlattenMemrefAccessPass
    0.0134 (  1.9%)    Canonicalizer
    0.0092 (  1.3%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0009 (  0.1%)    Canonicalizer
    0.0015 (  0.2%)    DiscMemRefCSEPass
    0.0010 (  0.1%)      'func' Pipeline
    0.0010 (  0.1%)        CSE
    0.0000 (  0.0%)          (A) DominanceInfo
    0.0081 (  1.1%)    ConvertShapeToStandardPass
    0.0079 (  1.1%)    Canonicalizer
    0.0075 (  1.1%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0009 (  0.1%)    Canonicalizer
    0.0004 (  0.1%)    ParallelLoopCollapsing
    0.0004 (  0.1%)    SCFParallelLoopTiling
    0.0073 (  1.0%)    GpuMapParallelLoopsPass
    0.0079 (  1.1%)    ConvertParallelLoopToGpu
    0.0083 (  1.2%)  GpuLaunchSinkIndexComputations
    0.0088 (  1.2%)  GpuKernelOutlining
    0.0084 (  1.2%)  AssignKernelNamePass
    0.0008 (  0.1%)  'func.func' Pipeline
    0.0008 (  0.1%)    LhloFusionInlinerPass
    0.0083 (  1.2%)  ReviseGpuKernelOutliningPass
    0.3620 ( 50.8%)  Pipeline Collection : ['func.func', 'gpu.module']
    0.0010 (  0.1%)    'func.func' Pipeline
    0.0008 (  0.1%)      Canonicalizer
    0.0000 (  0.0%)      CSE
    0.0000 (  0.0%)        (A) DominanceInfo
    0.0001 (  0.0%)      Canonicalizer
    0.0000 (  0.0%)      RemoveDeadBufferPass
    0.3611 ( 50.7%)    'gpu.module' Pipeline
    0.0073 (  1.0%)      LoopInvariantCodeMotion
    0.0076 (  1.1%)      'gpu.func' Pipeline
    0.0076 (  1.1%)        SideEffectLoopInvariantCodeMotionPass
    0.0005 (  0.1%)      LoopInvariantCodeMotion
    0.0065 (  0.9%)      CSE
    0.0000 (  0.0%)        (A) DominanceInfo
    0.0070 (  1.0%)      SCFToControlFlow
    0.0005 (  0.1%)      ConvertAffineToStandard
    0.0068 (  1.0%)      StripDebugInfo
    0.0197 (  2.8%)      DiscLowerGpuOpsToNVVMOpsPass
    0.0095 (  1.3%)      'llvm.func' Pipeline
    0.0095 (  1.3%)        LLVMInsertValueSimplifierPass
    0.0094 (  1.3%)      FunctionDeadArgumentEliminationPass
    0.2860 ( 40.1%)      GpuKernelToBlobPass
    0.0270 (  3.8%)  SCFToControlFlow
    0.0007 (  0.1%)  ConvertAffineToStandard
    0.0003 (  0.0%)  'func.func' Pipeline
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0274 (  3.8%)  StripDebugInfo
    0.0270 (  3.8%)  DiscStripShapeConstraintOpsPass
    0.0251 (  3.5%)  DiscToLLVMPass
    0.0046 (  0.6%)  Rest
    0.7124 (100.0%)  Total
before optimize llvm module:
; ModuleID = 'LLVMDialectModule'
source_filename = "LLVMDialectModule"

%0 = type { ptr, i64, { ptr, ptr, i64, [3 x i64], [3 x i64] } }
%.1 = type { ptr, i64, ptr }
%.5 = type { ptr, i64, ptr }
%.6 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.2 = type { ptr, i64, i64, i64, ptr }
%.3 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.4 = type { ptr, i64, ptr, ptr, i64, i64, i64, i64, i64, i64, i64 }

@main_kernel_main_kStitch_divide__13_1_0___1b1r_kernel_name = internal constant [35 x i8] c"main_kStitch_divide__13_1_0___1b1r\00"
@main_kernel_blob_gpu.binary = internal constant [6648 x i8] c"P\EDU\BA\01\00\10\00\E8\19\00\00\00\00\00\00\02\00\01\01@\00\00\00\A8\19\00\00\00\00\00\00\A2\19\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8:\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\002@:\00\01\00\117\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0D\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F5)_shndx\00.nv.info\00.text.main_kStitch_divide__13_1_0___1b1r2\00\0F,\00\15oshared.\00\15\9Fconstant21\00\1D\1F01\00\12\FD\01debug_frame\00.rel\11\00\1Aa\12\00!nv&\00oactionV\01 \0F\94\00\0F\0Fy\01\A2a__ocg_3\00/\00$0\00\0F\FF\16$__cuda_sm70_shflsync_bfly_p\00$____wg_G\00\0F@_0_$\0D\00/265\00\19_1__281\00\19\122f\00/305\00\19_3__32\91\02\1Fo_param\98\02.\0F\01\00\06\8CU\00\00\00\03\00\0B\00\01\00\11\AA\18\00,\0C\00\01\00\11\D8\18\00,\09\00\01\00\A1\15\01\00\00\22\00\0B\00`,\0F\001\00 \01\07\000\00!\020\00,\0A\00\01\00\11Y\18\00,\04\00\01\00\11\89\18\00,\07\00\01\00f2\00\00\00\12\10\A8\00!\80-\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13P\C0\00\10\04\9B\00R\044\00\00\00E\005\04\D4\0A<\00&8\00H\00\04p\00L\88\80\80(o\00\10\08\15\00x\16\89\80\80(\09\05J\01\004\00\22\18\00\01\00\22p\00\01\00\048\01\13\108\01\90\04/\08\00\08\00\00\00$\0C\00%#\08*\01'\04\12\0C\00\17\11\0C\00\12#0\00\04$\00\17\08$\00\05\0C\00!7\04$\06\90\015\00\00\04\0A\08\00\05x\00\A3\01\18\00\03\19\18\00\04\17\0CH\06U\10\00\00\F0!\10\00\10\01>\00%\F0\11\10\00\01\01\00\F09\F0!\00\03\1B\FF\00\04(X\00 \12\00\00p\12\00\00\B0\12\00\00\00\13\00\00`\13\00\00\90\14\00\00\D0\14\00\00\10\15\00\000!\00\00`!\00\00\80!\00\00\A0!\00\00\C0!\00\00P\22\00\00p\22\00\00\90\08\00\F4\16*\00\00\E0*\00\00`+\00\00\B0+\00\00\F0+\00\000,\00\00\04\1C\08\00@(\00\00@*\00\00\04\1E\E6\00C\00\00K\00\01\00\94\02\02\08\10\0A/\22\00\00\DF\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\E0\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00@\00\00\00\14\FB\01\22\C8\00\01\00\13\02\D1\00*H\00\10\00/\04\00\01\00\FF|@$v\01\FF\D7\04\C1\FF\00\8E\07\00\C4\0F\00\19y\02\00\01\00\F1\00!\00\00\00\22\0E\00\B9z\04\00\00F\00\00\FB\04\90\E2\0F\00Ey\00\00\10\0D!\00 \80\03\10\00\81$t\0C\FF\00\00\80\FF@\00\F3\05\E2\0F\00\10z\00\02\00Z\00\00\FF\E1\FF\07\00\E4\1F\04\0C\10\00\A0pR\F0\03\00\E4\0F\00\10xl\00\F0\07\00\00\FF\E0\FF\07\00\C8\0F\00\12x\03\00\00\FC\FF\FF\FF\C0\8E\07 \002\19y\00\01\00\10%\9D\00\83\0E\00\07r\03\03\FF\00\01\00\000\000r\03\03\A2\001\FF\FC\8E@\00P\0Cr\00\02\03\80\04\D3b\F2\03\00\DA\0F\00G\19\00\00`\0C\A0\00a\EA\0F\00\10r\04 \00\01\90\00\01\C0\00F\01\00\F0\06\C0\001\1Cx\00\01\00?p\F0\F0\D0\00\02@\0Cx\00\04?\062pB\F4 \00Ar\0E\FF\FF\D0\02\C6\8E\07\00\D8\0F\00G\A9\00\00\90\06p\00\05P\00\14\E1\F0\00a\0D\03\00\F4\FF\FF\F0\00b\C4\0F\00\02x\10\04\03\12\0FP\01@$z\0B\00 \01 \0E\02\D0\00\B0\1F\00%v\04\0B\00X\00\00\10\10\00a\CA\0F\00\81y\0F\C3\05\E3\00\19\1E\0C\00\A2\00\00\10x\07\0B\00\01@\01\93\E4\0F\04\10x\09\0B\00\02\10\00z\C6\0F\00%v\06\07@\00%\12\06@\00u\E2\02\00%v\08\09 \00\95\E2\0F\08\10x\0B\0B\00\03P\00v\00\10x\13\0E\00\04\10\00E\81y\11\08@\00fb\09\00%v\0A\A0\00\10\C8\C0\00\11\15\C0\00\12\13P\00U\00\81y\14\0A0\00\10f0\00&\04\15p\00e\1F\04\10x\17\15\C0\00\10\C80\00\16\13\E0\00 b\01\C0\00\16\170\00\88/\00\10x\1D\15\00\020\00\16\16\D0\00zb\03\00%v\1C\1D\D0\00*\1F\15\D0\00E\09\0E\00\08\10\00Z\01\81y\15\1C\D0\00%\1E\1F@\00\01\D0\00\11\19\D0\00\15\09\D0\00%\18\1E0\00\10f`\01' \19p\00\00\80\01\1A\19\D0\00&\17 `\00\1C\0Fp\01Z\00\10x\0B\19\D0\00\1F\1A`\01\07\01\D0\00&\05\19\D0\00\10\1F\00\024\0E\00\0C\10\00j/\00\81y\19\0A@\01%\04\05@\00\01\D0\00\11\1B\D0\00\15\07\D0\00\16\1Cp\01z&\03\01%v\06\1B\D0\00%\1D\1B\D0\00\01@\02\16\07p\01\11\22\D0\00\1A\1D\D0\00H\1D\1B\00\020\00*\08\080\00\17\0A0\00\00\C0\00H\1B\1B\00\030\00*\0B\0A0\00(\04\1B\00\02Q\0Br\00\0F\0F_\03\A1\F4\03\00\E4O\08\0Br\00\0C\10\00\B2\C0\F6\03\00\D6\0F\00\08\A2\0F\0F\D2\07\22\80\05\B0\02\16\0C\E0\00 \A2\0E@\00%\12\12@\00\10\8F@\00!\11\11\10\00\10\F6\A0\03\10\02`\00\01 \00P\C0\F8\03\00\D2P\00!\12\12`\000\00\00\06P\00U\0Br\00\14\14@\00\01\10\00$\12\110\00q\C8\0F\00\08\B2\11\11@\00\060\00&\13\13`\00\00\10\00(\11\140\001\A2\14\14@\00\060\00*\16\16`\00)\14\13`\00!\13\13@\00\060\00*\15\15`\00)\13\16`\00!\16\16@\00\060\00*\18\18`\00)\16\15`\00!\15\15@\00\060\00*\17\17`\00)\15\18`\00!\18\18@\00\060\00*\1A\1A`\00)\18\17`\00!\17\17@\00\060\00*\19\19`\00)\17\1A`\00!\1A\1A@\00\060\00&\1C\1C`\00\10\01p\00\19\19`\00!\19\19@\00\060\00*\07\07`\00)\19\1C`\00!\1C\1C@\00\060\00*\08\08\C0\00)\1C\07`\00!\07\07@\00\060\00*\0B\0B`\00)\07\08`\00!\08\08@\00\030\00f\10x\0E\0E\00\10\90\04\00p\00\19\0B`\00!\0B\0B@\00\030\00Q\0Cr\00\0E\0D\D0\06\14\F8`\00*\0C\0C\00\03)\0B\0C\00\03!\0C\0CP\00\00\00\03\90\E2\0F\00G\C9\00\00\90\F9\9A\0B\11\83\90\06#Ay\89\0B\03\A0\06@$x\04\03\12\005\0E\0A\8E\10\07#p\03 \00\13\E8\F0\06\13\04\F0\06\12\DA\E0\06'@\03@\00St\14\FF\04\000\08\10\E4`\04\1C\0A\C0\06\10\0A\C0\033\14\02\8E\E0\03\1B\0D\C0\06%\06\0A`\04\01\C0\06+\08\0A\C0\06\1A\06@\00\1C\10\C0\06\15\08 \00\01 \05&\0A\0A \05\00p\01\1B\11\C0\06\1C\0F`\05\06\A0\00\01 \05\11\11\C0\00\15\11 \05\16\12\90\04\10f`\06&\04\11p\00\00\C0\06*\13\11 \05*\05\04\F0\05&\06\130\00!/\000\00\09 \05+\06\06 \06\070\00j\0F\01\10x\11\11 \05\1C\09P\05\07\90\00\01 \02\12\0D\8F\09\15\F0 \05\01\10\00\22\C0\F4 \021\82\0D\0D0\02$\00\00 \05\1B\0A \05%\10\10@\00*\8F\08p\05\01 \05$\0D\10p\02\00 \05\22\82\10\FF\09\03p\05\06`\05#\F0\03\10\00$\10\0F0\00\12\C8\A0\05\1A\100\00&\05\05`\00\16\00\80\05\030\00\14\82\80\05\050\00*\06\06`\00)\12\05`\00:\05\05\120\00*\09\090\04)\05\06`\004\06\06\050\00\11\C80\00\11\060\00#\C0\F0P\00!\09\090\00\00\AB\0C\1F\E4\D0\09\01\18\0EP\08\00@\00%\0C\0C\10\04\01\80\01(\09\0C\80\011\B2\0C\0C`\00\00\80\01\1E\C4\90\03\00\F0\03\00_\03Ap\16p\00p\03V\89\00\00\A0\01 \000\02x\0B\CF\02\070\0A\0Fp\03\03\15\0B\C0\07\07`\03\0C\A0\02%\A2\0E`\03\030\00\19\00\80\03\0C\A0\02%\E2\0Ep\03\060\00\07p\03\0F\A0\02\03\06\90\00\10\CC \00(\0A\0A\F0\02\06\00\02\06\B0\02\05\D0\01\09\F0\01\BF\F4\03\00\D2\8F\00\08\82\05\05\0C\E0\01\02\1F\F0\E0\01\03\17\A2\E0\01\020\00*\0A\0A\D0\02(\06\090\00\14\82\F0\01\05\10\02)\09\0A\10\02!\0A\0A0\00\00\10\02\10\CA\00\0C\11\0C\00\0C\13\0A0\0D\14A\B0\0C\04\D0\010z\00\03\A0\01Dpb\F4\030\0D#\B0\04 \00\10\E2\F0\0C0\05\02\1F@\05\00\F0\0C\96\D6\0F\00G)\00\00\80\04@\001$r\07`\00'\03\00\A0\05\16\02@\00\09\B0\0C\10\C6p\0D%\04\07p\0D\13\C8\C0\05\10\03\C0\05\12\F6\C0\05V\B9\00\00 \02`\00\10\02\D0\0D\13ZP\02\0F\E0\02\02F\04\04\00\FD\B0\0C;$t\06\F0\05\16\08P\0A%\C8\1F \023\06\02\8E\10\02*\0D\08\F0\05%\0B\07\90\02\01\F0\05(\0F\07\F0\05\04\00\0D\010\02a\E4\0F\04$z\0F\10\00\12\0F\10\00\05\10\0B\13\06\B0\05E%v\0E\0F\10\00\1B\E4`\02 \E2\0E\00\066\07\00\03`\00E\81y\0F\0E \00\16$\C0\00\1F\11\C0\00\01\09P\05\05\D0\03$\0C\0D\B0\07\01\D0\03\05P\05\12\06\80\00+\0C\08P\05\03p\02\14\F8P\05\12\0DP\02\11\FA`\01\06`\05\01\F0\02U\01\10x\07\07\B0\06q\CE\0F\00\08\C2\0A\0Ap\00Q\00\80\06\00\C80\00$\0A\0F@\00\00\90\02D\B2\0F\0F\0A \00\020\08!\07\040\08*\FA\03\90\04\14\F8\C0\00$\0F\0C\F0\02\00\C0\00\22\C2\0C@\0B\040\08_\D9\00\00\10\FE0\08\07\09`\02\03\90\02\140\A0\04\13\E6p\02\1A\01p\02(\00\010\08\1F\0A@\02\12\1F\0A@\02\02\1A\0E@\02\1B\CA0\02\16\C8 \02\1D\0A\C0\01\1B\F0\C0\01\1B\F6\10\07/\80\05\10\07\01\0E\10\03\16\07\B0\02\09`\01\05 \02)\0D\0C \02\12\0Cp\00\00 \02\0F\F0\05\01!z\00P\01\08\F0\05(\80\00\F0\05\1B\09\F0\05\1B\060\01\11\06\C0\05\15\090\01\16\09\B0\05+\A4\0E \05\02\00\01\19\09\00\014\09\09\0C\00\01\07\D0\04\1F\09\D0\04\04\F1\00\89\7F\07\0C\00\1F \0C\00\00\0E\00\00b\0E\D0\04&`\03\90\04\06p\0B\00p\00\11/p\00\19\07p\00(\07\07p\00r\89\7F\04\07\00\1F@P\00\11d\B0\00*\04\04@\00)\07\04@\009\04\04\07@\00X\09\04\00\1F\80@\00*\09\09@\00\1C\04\F0\00\17\04\F0\01@\19x\04\FF \06@\02\16\01\00 \04\87\89\7F\06\09\00\1F\00\0DP\00*\06\06P\00)\09\06P\007\06\06\09P\00@\0Cr\00\05@\01!pR\D0\03\0DP\12\81\89\7F\0B\06\00\1F\00\0E`\00ft\0E\00$\B4\07\10\03\00\80\0A\10\B6\EF\02\12\80\E0\01\00 \03\12t\00\06#\FF\00\D0\02\14\B2p\0C\12\F8\90\00G\B2\00\06\0BP\043\1C\B8\00p\00Ap\02\00\E4\90\03\10\050\01 p@0\00`\0F\04\19x\05\05\FF\05\10\FF\C5\00\01\C0\094\0B\0B\06@\04\12\E2\E0\05\17\07p\001\0Cx\00\A0\061p@\FA`\06#\88\B3\B0\0C\00\A5\0Cf\E8\0F\00\1D{\00\01\00\89\EC\0F\00GY\00\00@\B0\08\00\9E\04$\80\FF\C0\021\82x\06\F0\13\01\01\00\93\CA\0F\00\84\C9\0A\05\00\04P\00\81\A2\02\00Gy\00\06\C2Q\0E\22\80\0Bp\02C\09\0A\00\07p\02/\A4N\D0\02\01\02\90\07%\C0\FC\E0\01\02\10\05\11\00\10\03c\89\7F\08\09\00\07`\02'\A4\0E\E0\0D\05@\00)\09\08@\00!\08\08P\00\05@\00C\0C\08\00\07`\02a\E2\04\00$r\0B\90\011\08\00\8EP\06$\0B\B2\00\04\11\FC\E0\05\09\F0\01\0A\90\01\01\10\00\00\C0\01\14\0Cp\00\10\DAp\00\04\B0\0D\02p\00c\88\B3\00\FF\0C\00\10\01-\E4\07\80\03\22Hyh\19\01\10\00\1E\E4\80\01T\84y\08\FF\00@\00\22\A2Np\08\07\A0\03!\02x\DE\00\14\80\90\01\00 \15&\80\03`\00B$t\09\FF\B1\01\03\80\02&r\0A\E0\14\10\C6@\00\1B\13\90\04\02\80\05\15\0A\90\04\11\0C\80\05\02\00\14[\8F\04\10x\0E\A0\14\19\10\A0\14J\81y\0D\0C`\0A&\0E\0E@\00\00p\05\16\12\90\14\01\10\0E%\10\10 \00<\E4\0F\08\D0\07G%v\12\12 \00\00\B0\14\15\10 \00\10h\10\00%\13\12\10\00\B4\A2N\00$t\16\FF\9D\98\BB;\F0\02\01\D0\0A\04\80\07\01\10\03a\15\FF\00\00|C \00\11\C6\E0\03!\0A\03`\07\02\D0\03c!r\14\08\0D\00\E4\06`\C8\8F\00#t\0B\9F\112?\16  \01T!r\0D\08\0F \00\D2\C6\0F\01#t\0B\0B\01\00@K\15@ \00E#t\0C\0D0\00\D4\C6\0F\00!t\0F\0B\7F\00@\CB\00\00@\004\11\08\11@\00u\E2\0F\02#t\0C\0C@\00\D4\E4\0F\08#x\0F\14;\AA\B8?\0F\08P\00'\10\11\80\00e\08!r\13\08\13@\00hO\00!t\0E\0C`\00\00@\00e`p\A52\0F\00@\00\17\10\A0\00W\08#t\14\13P\00P\00#x\0E\0Dp\00\12\0Ep\000\04$x\FF\02\15\800\04I!t\12\10`\007t\14\14P\00P\00#x\0D\0Dp\00\13\0E \00T\08s\0F\00\0F\B0\02p\22\0E\00#x\12\11`\00#\12\08\00\018t\0E\14P\00E$x\10\10p\00!\C4\0F0\00\00P\00$\12\00`\00%\0E\13\A0\00`\E4\0F\04\08s\0D\CD\05\02`\00\10\A4`\00(\13\13\80\00T\19x\0E\0B\17\10\05\10\CA\90\004\11\00\120\00\81\E2\0E\00#r\0F\0E\0F\10\07\00\10\00e\1F\00\19x\09\140\00\10\CC0\008\16\00\13\C0\00Cr\0D\0C\0D\1F\03q\00\C8O\00#r\10\91\01#\0D\00 \020r\09\09`\13\13\10P\00\01\E0\0D\18\FCp\09\0Ap\07\00\C0\03&\90\07\C0\03\00 \0C&p\07 \00\04\90\04\08 \0C&\10\040\00\0E \0C+\0A\0B \0C\12\0A \0C\02\90\04XG\E9\00\00\C0\10\041v\0A\FF0\00\03\E0\01\09\10\09\13\C80\03\07 \0C;\02x\12 \04\16\0C\A0\09k\E2\9F\00\10x\0F\10\04\16\11\10\04p\E2\0F\04%v\0C\0C\C0\03$\12\02\90\03\19\13\00\04\0B\10\0C\1C\04@\04\09\C0\11\16\E4 \0C\22\12\02`\07E%v\10\11\10\00\0F@\04\001$z\13@\00\0F@\04\02\10d@\00%\12\13@\00\1F\CCP\04\11\11\0B\B1\0F\03\E0\00L\00$t\17P\04!\0B\0AP\04/\FC\03P\04\03\16\0CP\044\C8\0F\00\00\04$\17@ \04\19\15`\04:!t\0F\E0\03:t\0D\15\10\04\0AP\04E\19x\0C\0C\D0\02\01@\03\09P\04W\04#t\0D\0Dp\00U\08#t\0E\11P\00\03\80\04\08@\04\0Ep\04'\10\0D\90\00X\04#t\0E\0EP\00H$x\0D\0D@\04\0Ep\04 \10\15\90\00\01~\1Ex\E2\0F\04!t\12\0E\E0\00\0D@\04\03p\04&\17@@\00\00\A0\00\02p\03\1A\0F`\04\10\C4`\04\18\16`\04F\04\19x\14\E0\03\1E\E2`\04T\08s\10\00\10p\00\10\A2P\04\11\16p\04#\16\08\A0\01!x\16`\04\22\16\00\D0\05;\08s\12P\04(\0C\0CP\04\10$\90\1A\04\00\01\03P\04\19\16P\04\11\0Cp\13\15\0CP\045\09\09\12\10\00\01P\04'\14\16P\00\00\C0\03/p\FC\C0\0D\09\07\00\04\040\04\19\02\C0\0D\010\12\08\10\04'\E0\01@\00\08\C0\03\01\D0\02\1B\0E\D0\0D\07\F0\03/\E4\9F\C0\0F\00\02\E0\03\1B\0E\C0\0F\15\0Ep\03\07\10\08*\E8\0E\B0\0F\10\22p\00\19\13`\03 \02x\1F\03!|C\05\02\1B\E4\A0\04\13\C4\80\03\06`\04G\00!r\0A`\03\10\CF\F0\06\00\7F\034?\13 \C0\02\1B\12\B0\07\02\10\07\14\14P\025t\13\120\00\01\B0\07*\11\10\E0\02(\13\130\00\09\B0\06\10\E4 \02 \11\0A0\02\22\11\08 \00E!t\0D\13@\00\12\C6 \00\00@\02\22\11\00 \00@#x\0D\120\00$\0D\08P\00%\0A\13P\00\12\C6 \00\000\00\13\0D0\00\00\C0\06\14\11@\0290\0E\00\00\07\00\B0\02Dr\09\10\110\02 \C8\1F\10\00$\0A\0D\10\00/\C6O\A0\0E\01\1C\0B\A0\0E\17\F0 \00?\02x\0D\90\14\02\15\0B\10\0A\02\00\14\15\0D0\05\0A\E0\1B6$t\0D\D0\01\00@\02&t\0F \05\01\90\015\08\08\0B\B0\01PO\01#t\0C\CA\005?\0D  \05\01\10\05\22\0F@\80\03J!t\0D\0C\D0\03\07\E0\04\020\01\15\08P\01\12\C8\10\00\05@\01*\CC\0F0\01\11$0\01\15\0C \01-\CC\1F\A0\07\01`\0C\04\10\0F\03p\0B&\80\01\80\07%!rP\0C@\00\00\CAO\00\0E6\08\00\1F\90\0C2!r\0B\E0\00\05 \00%\0A\0B\D0\0E\01 \00\11\0A \06\06 \00%\0D\0A\A0\0E\01 \00\13\0C \12\00 \00\10\CF \00\15\0C`\0E\00 \00\12\B2\90\08\04@\00c\88\B3\00\04\0D(\89#\0F\C0\0D\04\17\B0\E0\006$t\04\10\0C\0F\C0\0D\02C\04\05\00(0\01!\E2\04\C0\0D(2\09\C0\0D%\05\04\C0\0D\93d\EE\00!r\05\05\04\00\01\00 \CA/p\0F\15\05\A0\0D\11d\F0\008\05\06\00 \00%\07\0A\80\0D\8A\A4\02\00!\B2\07\07\0A\D0\003\FF\07$\80\00/\E4\050\0D 4\05\FF\00@\00!b.\C0\01&0\05\10\05!\84y\00\01\05\C0\05\000\0D(\00\05 \01\08\10\15Q\1F\00$z\0D\00\03\15\02\00\03 \0E\0D\00\03\15\06\90\17%\07\0D@\05 \E4O0 \15\0D\A0\04\0C\A0\14\01\A0\17'\10\07@\00\00\D0\04\18\0D\10\09E%v\12\09 \00\01\D0\08*\11\10\10\0D'\14\0B \00\0A\B0\08\01\10\0D%\15\14\10\00p\E2\8E\00\02x\16\00\90\03\05\D0\03\08\B0\08\00\80\10\10x\FE\00\D0\80\00\00\E2\F2\03\00\E2/\04 x\18\10\00A>\00\00@p\03\03 \004~\00B\80\05C\02\02\00\04\B0\00`\E4\0F\00\08r\181\00\02\01\00P\CE\0F\00 \98\8F\1E#\80K@\00\00\D0\02&\04\0F\F0\03X\00#t\08\0C\10\08T!r\0F\04\11 \00\01\A0\05(\08\08\F0\07V!r\13\04\13 \00W\02!t\11\08\10\04W\00#t\0A\0FP\00Z\08#t\12\13\D0\08)\11\0C\A0\05Dr\15\04\15P\00\10\E2p\04%\0A\0Ap\00\010\00)\0E\0C\B0\05:t\0C\120\09\18\11\80\09\00\10\09\18\0A\A0\04\10$@\0B\04\D0\05\01 \00\1B\12\C0\09'\14\11P\00\00\D0\08\17\0F\10\09@\00\08s\0E\1A\03\060\08:x\12\13@\0D\09\E0\08\01@\00\08\10\09\0E\00\0A*\12\13P\0D%\16\15\E0\08\01P\05(\0F\080\00\00p\06\14\10\80\00xb\0E\00$x\14\14\D0\00\00@\00\00P\00\22\16\000\00\01\00#\13\\\90\02\01p\0D7\13\00\12\A0\06R r\0E\0F\0E\EA/\01\A0\06E \08\0E\0E`\02\10\C60\00\19\15 \09T r\0A\0A\110\00u\E2/\00 \98\0E\0EP\02z\C6\0F\00 \08\0A\0A@\00C\17\00\18\00\81\0D\10b@\005\0C\0C\13@\00jO\00 \98\0A\0A@\00%\0C\0C@\00\84\E2\0F\00 r\14\14\150\00u\C6\1F\00 \98\0C\0C0\00\00 \008\08\14\140\00\06\90&\02\80\0DI \98\14\140\00#r\15\82*!@\00P\035r\11\17\00\01\010\01%\0C\0D0\01\010\006\13\17\0A \00d\04 r\17\17\14\10\00\02\F0\0C\17\070\00P\08\86y\00\0C \01a\04\19\10\0C\00\E6P%\18\0BP\00T\86y\00\0E\13 \00 \E8\01\10\00*\08\15\10\00$\06\17\10\00!\E2\01\00\08(\00\FB`\0A\0Ap\059M)\00\10\00\0E0\04/\02\050\04\03\22\F4\03\90\0AF\00\00\B0\01\80\050\08r\021\00\02\82\03\010\04%\02\02@\01\00\00\19 s\0C?\05\02\D0\01/f\100\17\00\11\02\90\05\15\03\90\05%\06\020\17*\CCO\D0$k\A2\0E\00$t\08`\0A\00\9E\07\06`\0Ac\10x\03\03\00\01\D0\04\18\C8\E0\1B\13\F0\F0\075\05\04\070\04\12\CFp\00\13\\p\00\00\80\08 t\08\CF\003?\08 \80\08\03\C0\04\22\0B@\10\00J!t\0A\08 \04%\08\08P\03\01\C0\08 \0A\05\90\03#\0A\08@\000x\0A\05`\03\01\14\02\01\10\017\05\00\0A\C0\081 r\05q\00\05P\03&(\05\80\01\10\C8P\01(\05\05P\01E r\05\0C0\00!\CA/\00\02\1B\05\00\02/\90\FE\00\02\08\0B\10\00\00\E0\09\00\0B\01\06P\01C\08\00\80*\A0\01T\C4\0F\00Dy`\0CP\C0\03\00\EA\7F\A0\15\15\0A \15\00 \0EGt\0D\FF\02@\00\06p\19\01`\15Q\04\08r\0A\0C\F0\00\00@\150\E4\0F\04`\00\14\F0`\00\01p\06\05 \00\05p\00\15pp\00\17\0Fp\00\01P\00\00\10\16\06\D0\15\10\C4@\006\0B\0C\0A\00 \19\04\F0\0A\12\E4 \00\03\C0\09\00@\00\01\E0\00#p+ \00\16\C6@\15\13\0B\B0\12\00\80\00&\F0\00\80\00\10G\F0\02\16\E9P\01\040\00\11\04\F0\00\11\8F`\0B\17\01p\00\00`\00*\C0+@\01&\A0\00@\01c!r\0A\04\0C\00\01\00\01\C0\07\0A@\01\00\80\01*\00,@\00\17`\90\002!r\0A\10\01\07@\00\1B\04@\00\1B@@\00\17 @\002\02r\07\9C\0E\12\0F@\00\00\E0\00&@\F6\E0\00\03\80\16\05 \00AHs\00\000\03\000\02Q\E8\0F\00\89s\D0\0AA\0C\06\00\0E\C0\02`Py\00\08`\D3@\00Q\C3\03\00\EA\1FP\00#\F0\FFP\00f\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-\11\01\89\02\0E\01\00\22@\00\01\00=V\01\000\00\08\01\00\1F\0B@\00\04\13\96)\00?\98\02\00@\00\0A\00L\06.\02\00\01\00\1105\03\02h3\13\00$\00\13\07\A7\04\00\01\00\03\14(>\00\17\01T\00\00\01\00*\08\05@\00\0F\80\00\05\1A)\7F2\05\01\00\13\E0@\00\11T\06\00\06\E4\00*\04\00\01\00\1F[@\00\0414\06\00\01\00&\C0\00@\00\1F\0B@\00\00!G\01\18\00\0D@\00/\F8\06\C0\00\03\1B\08\08\00#$\01l\0E\0B\01\00\12\D0%\03\03l\03\02\80\00\17\048\00\22\10\00\01\00\13\B5\14\01\0C\84\01*\F0\07h\00\1F\00\C0\00\04\1F\E6@\00\04\13\F8@\00?x\01\00@\00\0A\132@\00\15\06\92\00\02\01\00*\80\09\E04\12\03\8C\00:$\80\00\01\00\13\87\A4\00+\03\00\01\00\03\F09/H\00\80\00\0B\13\06\9F\0D\05(:\0B\01\00*\A8\00\08\00\04\10\01\13\018\00\04(\01\0C\01\00*\10/\08\00\088\00\17\06\A0\00\0F\01\00\05\04\B8\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00"
@ral_send_output___cpu___pvoid_i64_m3df32___void = internal constant [48 x i8] c"ral_send_output___cpu___pvoid_i64_m3df32___void\00"
@ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void = internal constant [101 x i8] c"ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void\00"
@main_kernel_0_main_kStitch_divide__13_1_0___1w1r_kernel_name = internal constant [35 x i8] c"main_kStitch_divide__13_1_0___1w1r\00"
@main_kernel_0_blob_gpu.binary = internal constant [8368 x i8] c"P\EDU\BA\01\00\10\00\A0 \00\00\00\00\00\00\02\00\01\01@\00\00\00` \00\00\00\00\00\00^ \00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(K\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80J\08\00\11G\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F5)_shndx\00.nv.info\00.text.main_kStitch_divide__13_1_0___1w1r2\00\0F,\00\15oshared.\00\15\9Fconstant01\00\12\FD\01debug_frame\00.rel\11\00\1Aa\12\00!nv&\00oaction%\01 \0F\94\00\0F\0FH\01q\1F$$\00\0F\FF\16$__cuda_sm70_shflsync_bfly_p\00$____wg_G\00\0F\00\09\00/261\00\19_1__28\EA\01\1Fo_param\F1\01.\0F\01\00\06\8CU\00\00\00\03\00\0A\00\01\00\11\AA\18\00,\0B\00\01\00\A1\D8\00\00\00\22\00\0A\00@=\0F\001\00@\01\07\000\00z\010\00,\09\00\01\00\11\B2\18\00,\04\00\01\00\11\E2\18\00,\07\00\01\00f2\00\00\00\12\10\90\00!\80>\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\130\C0\00\10\04\9B\00R\04<\00\00\00E\005\04\08\0F<\00&8\00H\00\04p\00L\84\80\80(o\00\10\08\15\00\81\16\85\80\80(\09\05\0C2\00\068\00\22\18\00\01\00\22p\00\01\00\048\01\1308\011\04/\08\06\01\10&\0C\00R#\08\00\03\00\01\00'\04\12\0C\00\17\11\0C\00\12#0\00\04$\00\17\07$\00\05\0C\00!7\044\05q\015\00\00\04\0A\08~\01\A2`\01 \00\03\19 \00\04\17\A2\00u\04\00\18\00\00\F0!\10\00u\03\00\10\00\00\F0\11\10\009\02\00\0C\10\009\01\00\08\10\00\01\01\00\F1_\F0!\00\03\1B\FF\00\04(P\00\C0\12\00\00\00\13\00\00@\13\00\00\80\13\00\00\C0\13\00\00\10+\00\000+\00\00P+\00\00p+\00\00\90+\00\00\00:\00\00p:\00\00\E0:\00\00P;\00\00\C0;\00\00\10<\00\00P<\00\00\90<\00\00\D0<\00\00 =\00\00\04\1C\0C\00\F0+\00\00`6\00\00\C09\00\00\04\1E\04s\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\D8\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00\C8\00\01\001\02\00\00\C8\01*H\00\10\00\0F\01\00\FF\D6@$v\01\FF\8F\05\B1\FF\00\8E\07\00\C4\0F\00\19y\0D\18\00 \00!y\05\A1\0E\00\B9z\04\00\00F\00\00\B3\05\90\E2\0F\00Ey\00\00\10\14!\00 \80\03\10\00c$t\0B\FF\1F\00@\00\10\E2@\00\03:\04\B1%\00\00\00b\0E\00$t\1A\FF\01\00\04 \00\F1\04x\0A\FF\05\00\00\00\0D\16\01\00\00\E4\1F\00\12x\0D\0D@\00\B0\C0\8E\07\00\C6\0F\00$x\0C\03\A9\02 \0A\020\00\D0/\00\0Cz\00\0D\00[\00\00pR\F2p\00g\04$x\0A\0A\04p\00P\0Cr\00\0D\FF\F0\04`R\F4\03\00\E4\0F0\00\81\0C\00\\\00\00pb\F6\10\00@\12r\00\FF\D2\00\A2\FF3\8E\07\00\D6\0F\00G94\04\01\C0\00p\EA\0F\00\10z\02\00`\00B\FF\E0\FF\07\E0\00H\01\00\E0\0C\E0\00R\13\FF\00\00\80\C0\00\93\E4\0F\00\10x\02\02 \000\00\80\C8\0F\00\12x\02\02\80\E0\00!\C0\8E\10\00a\07r\02\02\FF\00@\00\11\00 \001r\0F\02\80\00\12\FC \00\00\B0\00\00\F2\05\E6pb\F0\03\00\DA\0F\00G\09\00\00`\0C\90\00@$x\02\0F\AB\05!\0D\0A\E0\00v\10Ey\02\00\F0\06\90\001\1Cx\00\01\00?p\F0\F0\A0\00\02\B2\0Cx\00\02\80\01\00\00pB\F8 \000r\11\FF\90\00\10\0D \00\96\D8\0F\00G\C9\00\00\90\06p\00\05P\002\E1\F0\03\E0\00a\10\0F\80\FE\FF\FF\E0\00\10\C4`\00%\16\FFp\01p\E4\0F\00$z\09\0C0\01\22\11\02\D0\00\90%v\02\09\00X\00\00\16\10\00p\CA\0F\00\81y\0E\020\00\C5\00\19\1E\0C\00\A2\00\00\10x\05\090\01\84\E4\0F\04\10x\07\09@\10\00\10\C6@\00*\04\05@\00%\15\04@\00u\E2\02\00%v\06\07 \00\86\E2\0F\08\10x\09\09`P\00g\00\10x\17\11\80\10\00E\81y\12\06@\00fb\09\00%v\08\A0\00\10\C8\C0\00\11\1B\C0\003\17\02\8E0\00%\17\080\00\10d0\00&\02\1Bp\00h\1F\04\10x\19\1B\F0\016\81y\14\E0\00 b\01\C0\00\16\190\00y/\00\10x\1D\1B@0\00\16\19\D0\00 b\03\D0\00\16\1D0\00j\0F\09\10x\1B\1B\D0\00F\1D\11\00\01\10\00<\81y\18\D0\00\06\A0\00\01\D0\00\11\1F\D0\00\12\1DP\00F\00\81y\1B\D0\00\11f\D0\00\1A\1F\D0\00*\1D\1F\D0\00\1C\1C\D0\00\1A\1D\D0\00*!\1F\D0\00\1C\1D\D0\00\1A!\D0\00*\1F\1F\D0\00F!\11\80\01\10\00<\81y\1E\D0\00\06\A0\00\01\D0\00\11!\D0\00\15!\D0\00\17\1F\D0\00\00p\00*\02!\D0\00%#!\D0\00\01@\02\17\02\D0\00\01`\02\1A#\D0\009#!@0\00*\05\040\00\17\060\00y\0F\01\10x!!`0\00\16\06\C0\00\10\220\00\18\08\00\01`\00\0Br\00\0E\0E \00\B1\80\F8\03\00\E4O\08\0Br\00\13\10\000\C0\FA\03\80\04A\08\C2\0E\0E\7F\042\00\80\06\10\01\16\13\E0\00 \A2\0E@\00!\15\15\1F\00P\FA\03\00\E4\8F@\00%\12\12P\00 \0F\02`\00\01 \00\B1\C0\FC\03\00\D2\0F\00\08\D2\15\15`\00!\00\00`\01U\0Br\00\17\17@\00\01\10\00$\15\120\00\10\C8\80\00!\12\12@\00\060\00&\14\14`\00\00\10\00(\12\170\001\D2\17\17@\00\060\00*\19\19`\00)\17\14`\00!\14\14@\00\060\00*\18\18`\00)\14\19`\00!\19\19@\00\060\00*\1B\1B`\00)\19\18`\00!\18\18@\00\060\00*\1C\1C`\00)\18\1B`\00!\1B\1B@\00\060\00*\1D\1D`\00)\1B\1C`\00!\1C\1C@\00\060\00*\1E\1E`\00)\1C\1D`\00!\1D\1D@\00\060\00*\1F\1F`\00)\1D\1E`\00!\1E\1E@\00\060\00*\02\02`\00)\1E\1F`\00!\1F\1F@\00\060\00*\05\05`\00)\1F\02`\00!\02\02@\00\060\00&\06\06`\00\10\01p\00\19\05`\00!\05\05@\00\030\00c\10x\11\11\00\02\F0\02\03p\00\19\06`\00!\06\06@\00\030\00P\0Cr\00\11\10@\06$b\FC\90\00%\13\13\A0\00\01\00\03\11\06\10\00\22\C0\F8\00\031\D2\13\13P\000\00\00\060\03`G\E9\00\00\90\F9A\07\11\83\90\06#Ay\8E\00\0B\10\073\11\0A\8E\A0\07C\02\00p\03 \00\13\E8\F0\06\13\00\F0\06\12\DA\E0\06'@\03@\00+t\03\C0\06\1B\0E\C0\06 \04\0E\C0\033\03\02\8E\E0\03*\02\04\C0\06%\06\0E`\04\01\C0\06*\08\0E\C0\06*\06\06@\00*\19\06\C0\06%\08\08 \00\01\C0\06/\14\0E\C0\06\0B*\0E\08 \05%\14\14@\00\01 \05\1B\10\C0\06%\15\140\00\10d\90\00'\16\10p\00V\04\10x\04\10 \05k\1F\00\81y\16\16\F0\04\16\040\00\00 \05*\04\10 \05*\07\060\00\17\040\00\00\C0\00'\08\10 \05[\01\81y\04\04 \05\08\00\01\07\F0\02\15\F0 \05\18\02 \021\82\02\020\02/\00\00 \05\05%\19\19\D0\02&\8F\08p\05\14\F0 \05$\02\19p\05\00 \051\C2\19\19`\00\03p\05\06`\05\14\F8\B0\02\15\19\A0\05\00\E0\02J\82\0E\0E\190\00&\16\16`\00\16\00\80\05\030\00\14\C2\80\05\050\00*\07\07`\00)\15\16`\00:\16\16\150\00&\04\04`\00\00\A0\03)\16\07`\004\07\07\160\00\11\C80\00)\07\04P\007\04\04\07P\00\0E\D0\09\19\11P\08\06\A0\03\14\F8\80\01$\04\13P\00\00\80\01D\C2\13\13\04P\00\1E\C4\90\03\00\F0\03\01\C0\0A1\16p\00p\03X\89\00\00\A0\01p\03\1B\09p\03\1B\08p\03\11\02\10\02\16\09\A0\02/\08 \C0\07\05%\A2\0Ep\02\15\09p\02/\06\08\C0\07\06%\E2\0E\90\03\060\00?\08\08`\C0\07\0B\06\90\00\10\CC \00(\09\08\F0\02\06\B0\02\1B\F8\B0\02*\FA\03\90\05Q\F0\03\00\D2\8F\80\05\02\C0\02/\80\06\80\05\0B\04\C0\01\04\80\05\050\00*\09\09\D0\02\03\80\05\1B\FA\80\05\05\10\02$\06\09@\03\01P\00!\09\090\00\00@\03\10\CA\00\0C\11\13\00\0C\13\09\E0\0DIAy\01\00\D0\01Sz\00\0F\00[\90\0C\03\00\0D#\90\04 \00\12\D8\A0\0C)p\040\0D\010\005\FF\E1\FF\90\05#`\02 \00\0C\A0\0C\03\90\0C\1B`\A0\05& \02P\00c$v\0E\FF\00[0\02\1B\E2\C0\02\10\C8\C0\01H\0E\0E\A0\FF\90\0C\1B\10`\02\11\02`\02\16\0F`\02\10\02\D0\013\10\02\8E\F0\01\1C\08\90\0C\15\0Fp\02\02\90\0C\18\0F\D0\051$z\05P\001\05\02\8E \001$z\07\10\00\12\07\10\00\05\B0\0C\15\10p\00%\06\07\10\00\1E\E4\90\02W\10x\09\0F``\00\09\80\02\16$\C0\00\01\80\02*\C8\1F\C0\00\020\05+\08\08\80\02\19\08\B0\03(\08\08p\02K\81y\13\020\05*\05\05P\0A$\08\05\D0\07\0F\A0\02\00E\10x\0F\0F\90\06\12\CE \08\01p\00!\00\00\F0\00\0F\10\08\10/\0F\0E\10\08,.\10\FE\10\08\0F\80\02\09#0\01 \00\12\E6p\02\1B p\02(\00\01\A0\04\1F\04@\02\12\15\04@\02\19\06P\01\09@\02\1B\CA0\02\16\C8 \02\16\04\C0\01%\06\06\D0\03\02\C0\01\18\06\00\01\22\82\06\10\01$\00\00\C0\01\18\04p\00\09\10\03\01\D0\05\16\0F\B0\02\09`\01\1B\F8`\01\06\D0\05\01p\00\0F\D0\05\05\04\00\04\07\D0\05.\80\00@\09\0F0\01\0D\15\030\01\16\040\01+\A4\0E\E0\06\02\00\01\19\04\00\01\12\04\80\06\0B\B0\04\1F\04\B0\04\04aGy\00\7F\12'\10\00\10\0B\10\00\FB\00\89\7F\02\13\00\1F \0C\00\00\0E\00\00$\0E`\08\1F\1F`\08\0B\10\CA@\00X\03\02\00\1F@@\00*\03\03@\00)\02\03@\00!\03\03P\00\05@\00X\04\03\00\1F\80@\00*\04\04@\00\1C\03\F0\00\01P\00\05@\00g\05\04\00\1F\00\0D@\00*\05\05@\00)\04\05@\00!\05\05P\00\05@\00a\06\05\00\1F\00\0E@\00ab\00\00$r\02@\011\05\00\8Ep\04(\0B\A2\E0\03,/\08\A0\05T\0B\A2\00\02\060\02\00@\08#\A8\00 \00 p\02\10\02\00\A0\02\14\02@\02q\CA\0F\00\88\A3\00\0A0\00\00\86\16i\E4\03\00Ay\00\A0\01\22Hy\88\18\04\10\066\1D{\00\01\00\12\EC@\14\11P!\0E\02 \00\00\90\13/0\17\90\13\0B&\B0\07\E0\03\0F\80\13\1F\16\03\80\13\010\13\01\1F\00#\80\FF \01\00\90\13\11\03\B0\04\11\F8\00\01@\0Cz\00\03\80\00\01\10\07\000\14VI\00\00 \07\A0\00\09@\00\00\10\03&r\04`\13\01\A0\0B\11\18 \03\01\00\04\1B\E4@\13\10\C6\C0\00%\11\18\C0\00w\E2\0F\04%v\12\18@\10X\04\10x\10\18\90\06G%v\0E\11 \00V\08\81y\06\12 \04\10\22`\06\15\18`\06\01\A0\06*\14\100\00%\05\0E0\00 \E6\10 \0C\17\09 \00\00\E0\0B\06@\0C h\090\07\18\16\10\121\06{\19\90\01\C1\00\90 \00\000\0E\00\08s\19\00 \0B\00\F5\0D $\1E\A0\07Q\19\FE\FF\FF\0F\80\00`\CC\1F\00\05s\0F\8A\02\01\AC\19 \00d\90\025\0E\FF\FF@\04\15\1F\E0\03\10\0F\D0\0Dp\C8/\00$z\13\13`\00\22\FF\02@\01@'r\0F\0F\A0\03\22\0E\00\90\09T'r\12\0F\18@\00\16\C8 \04%\12\0Ap\01\01@\00\13\18p\01V'r\14\0F\11p\01P\01\0Cz\00\18 \00$p`\10\15\02\90\00\15\14@\00%\11\0E\A0\0A\01@\006\0E\0F\10@\00\00\10\02\12\11@\00 \FC\03\10\000\10J\18P\00\11\80\F0\00\16\E2\90\00%\0E\0A0\00\03p\00\12\FAp\00!z\10\A0\00\04\D0\07T\10H\12\12\01\B0\01\10\E2p\007\16\0F\09p\00E\10j\11\11`\00\02\90\02\1A\10\C0\00\11\18p\00\13\16\C0\00'\10XP\00\18\E4\B0\00\04\80\00%\0F\18`\08\00\80\005h\14\140\00\00@\00;J\10\10p\00\15\0F\F0\00\10\CA`\00\070\00\09\90\00\020\04I\10H\0E\0EP\00;j\0F\0FP\00\01\E0\09\22pP`\032\12z\09\F0\09\133\F0\00\17X@\00\09\80\00\02`\00V\07r\12\09\12 \07g\04\07r\14\09\14\10\00H\10h\16\16P\01T$x\0F\12\04P\01\10\E2@\00!\0E\09\90\02\03`\08W$x\10\14\04p\01@\12x\0F\0F\C0\11\22\FF\E2\00\02,\10XP\00(\0E\0EP\00H\12x\10\100\00W\07r\09\09\16`\00T\84y\0F\0F\00 \05u\A2\0E\00\12x\0E\0E0\00\01\E0\18(\09\09P\00E\84y\10\100\00\10\E80\00%\09\090\00\01 \00%\0E\0E \00\10j\10\00%\09\09\10\00!\22\0E\00\18S\9D\98\BB;\FF0\07\84$t\15\FF\00\00|C`\00W\10x\04\04\80\A0\01E\0Cr\00\04\F0\04\84\E2\0F\00!r\0F\06\0F\F3\07P\C8O\00#t\0F\00B\00?\13 \F0\19T!r\10\05\10 \00 \C6\8F \00t\06\01\00@K\15@ \004\07\07\0E \00u\E2\0F\02#t\05\10@\00\B3\E4\0F\08!t\0E\06\7F\00@\CB \00X\00#t\12\07`\00G#t\05\05P\00e\08!r\08\08\09P\00\C4\1F\00#x\0E\0F;\AA\B8?\0E\080\00)\12\12\80\00:t\11\05`\00)\09\08`\00\94x\0E\0F`p\A52\0E\000\00)\14\120\000x\11\10`\00\15\11`\00%\0F\09`\00\10\C4 \00 \14\07 \00&\14\080\00\00P\00\22\11\00\10\00W\08s\0E\00\0E\90\01J!t\09\0F`\00 \14\070\00\01\B4\02\10\C6\10\00 \07\08P\00\22\09\08\10\00T\08s\10\00\11@\00@b\0E\00#\B0\0ECp\A52\07@\00@$x\07\06\0F\01\13\FF\10\05W\08s\09\00\14`\02E$x\05\05 \00\00p\00Cr\02\07\0E\C8\1D\00\90\05X\08s\11\00\080\02:x\12\120\004\05\05\100\00!\E2/p\13\05 \00\11\C6 \00!\12\09P\08\03 \02Dr\02\02\11\10\00\22\E2\1F`\0B\09\C0\13\0A\B0\09\01P\0E\13\0E\10\00\13\E2P\0E\18\0E\F0\07,\04\03\D0\0B/\A0\07P\0E\0A\1C\04P\0E(`\07P\0E\12\04`\04\0FP\0E\06(\04\04P\0EV\10x\09\03 \F0\03F\04$t\18\B0\07\01@\035\05\03@ \00Q\1F\04$z\12\E0\07\01\F0\0A\01 \0E\16\09\C0\0D\01\80\07C\0E\12\00X\90\06\01@\00%\11\03\A0\07\01\D0\07'\14\09 \00F\08\81y\07\A0\07%\A6\00\E0\13\12\05 \00F\04\81y\08\A0\07 \22\07\B0\06\18\0C\B0\06\04\F0\13\22\18\02\10\06G%v\18\11`\00\00\C0\0C\06\D0\07 h#\A0\0E\18\18\E0\106\06{\13\E0\07*b\0E\B0\07\84\E2\1F\00\10x\03\03\80\C0\00\01\80\02\10\13w!\03\00\08l.\00\10x\1B\13\00\08\14\1B\00\08\10\E4P\00\17\15\F0\07j\8F\00$z\15\15\F0\079\16\0F\15\F0\074\0E\16\12\80\00\00\A0\07&r\0F`\07\01\A0\078\13\16\10 \00!z\120\06$\12\02 \00 \0F\16\B0\02\07\C0\07\1A\12@\07\16\14\90\00%\E4\0F\A0\00%\13\0A\E0\1C\18\14\D0\01E$z\10\15\C0\07\02P\00\1A\09\10\08(\12\12\F0\06-\10H\D0\06\15\12\D0\06\01\F0\074\12\16\11\A0\00\1B\E2\E0\07\16\C6\A0\00\22\12\0A \00I\10j\09\09`\005h\0F\0F`\00\11\E2\C0\08\18\14\10\026$t\14\00\06\1F\E4`\07\02\15\09\90\00\1F\C4\00\08\02\15\11\E0\00\00@\00;H\13\13@\00\06\D0\07\01 \08/\0F\0F \08\0A\0B\D0\086\12z\10\00\08\00 \00\1Ah\C0\08A\07r\0E\10\1F\04!\00\00\B0\0ET\07r\0F\10\0F\10\00\010\08)\13\13\00\01)x\09\A0\07\0D\00\098x\0F\0F \00A\07r\13\10\C0\02\02@\08\0Ap\07\1B\E4\10\08\01@\00%\13\13@\00\11\E4\90\07$\09\00P\05*\A8\0E\80\09\0C\00\08\10&p\007\12\10\12\D0\00I\12x\10\13`\00\08\C0\07\00\C0\00\030\0E\04\A0\07\00\B0\05\0A\80\00\01o\19\02`\00\10fP\00%\12\12P\00\10\CC \00\02/\1C\05\90\06Ir\0E\07\0E\E0\07*\07\0E\E0\074\08\08\0F \00p\C6\0F\01#t\07\07\00\07!\14@ \00H\08#t\0FP\07d\08!r\11\06\110\00y\E4\0F\02!t\09\07\F0\068t\06\0F@\00 $x\0F\17\07p\06:t\10\11P\009\05\05\12\F0\07%\09\0E \07\01\B0\07\1B\0F@\08*\10\10\A0\00'\12\05P\00\01@\00\00@\07$\09\00\10\00\11\0Fp\07$\0F\08P\00*\0E\10P\00'\12\12P\00Y\00$x\06\06\B0\00!x\0F\90\07%\0F\00P\08\17\11\90\08I\04!t\08P\08\00\A0\07\14\09\80\01!$\0E0\00\08\80\08@#x\08\05@\00\01\D5\13\12\C8\10\00\00 \00\15\08@\007\0F\00\0F\10\08E$x\05\10\90\00\1B\CEp\08\85\A2\0E\00#r\07\07\09\C0\07\10\1F\C0\07\06\E0\07\01\B0\057\08\00\08\10\021#r\06\E1\09\013\07!\C8/\00\08\11\0E\9D\02\06\E0\07\1B\08\E0\07.\D0\F8\90\13\0F\C0\07\09)0\04\90\13\1D\04\90\13\17\04@\00'x\05\80\07h\1F\00$t\08\FF\F0\02\04\10\07\06p\07\16\11 \07\01\00\07\11\06\00\07\22\08\02P\0EE%v\08\11\10\00\01\00\07\16\05p\15 \A8\00\A0\12\18\08\A0\1B6\06{\12\00\07!\22\0F\F0\06\18@\00\15Es\12\00\12\F0\06\00 \15&\0E\12\F0\06*\0F\01\F0\0E\1A\09@\074\E4\0F\01 \0E\02\00\07.\0F\02\F0\0E\16\13\F0\0E\00\D0\06&t\0E\10\04\01\D0\064\06\13\10P\00\10\C8@\0F\11\09P\00\10\06P\00u\E4/\00'r\07\13P\06\01\10\07%\10\09\B0\06\00@\01!r\080\00#\07\0A\10\00)t\0F \0C\06\B0\05#\FC\03P\06\15\08P\06\00\F0\0C&z\08\B0\05\1B\C8`\05\10\CE\E0\05\0A@\06E\10h\06\06\10\05\1B\E4\E0\06\00 \00?Z\11\11 \0E\03\02p\06\06`\00\12\F0\A0\06\10X\90\04\06p\15'\10H`\00\01\F0\11!\06\08\BC\02\03\80\16'\10\080\00\1B\E4\00\0A\03\E0\03\14\040\01\01\10\06!\07\08\0C\03!\00\00P\00I\12x\06\06\A0\055x\07\070\00\10\CA \00(\08\07 \00\22\84y_\13\04\10\06E\84y\09\08\10\00\10\E2\80\05\10\050\17\08\80\05\01\B0\04$\0E \80\059\09\04\09`\0D\00\C0\00D@K\0F@\A0\04%\0E\090\00\10\C6\C0\04\18\04p\05X\04#t\0E\0E0\00\09p\05\10\E4`\04\11\04p\04$\04\08\00\05%\06\0E@\00\12\C6 \00\00\80\04\22\04\00 \00@#x\06\090\00$\06\08P\00%\05\0EP\00\12\C6 \00\000\00\13\060\00T\08s\04\00\04\F0\00\01 \127\06\00\06\C0\04\00\90\0C\14\04\A0\04 \C8\1F\10\00$\05\06\10\00/\C6/\90\16\01\1C\03\90\16&@\02 \00\0E\C0\17\16\03 \04\01\00\0B%\04\03\80\179\CC\1F\00\90\1Ef\A2\00\00\06{\09\F0\03\10\E2\A0\0A4\06\FF\FFP\00'\E2/\90\02\03@\08&t\050\03&\CA\1F\E0\05\01\10\04 \8E\00\D0&\0A\00\0B4\07\00\07\10\04%$\0E\C0\0A\01\80\03\00\C0\03:z\0F\0F\00\041\06\07\0F\80\05\02\B0\05T'r\06\06\03\90\00\16\C8\C0\03#\06\0A\10\007z\03\08\10\05\010\05\050\04\17\C6@\01!`\F0p\05I\10\0A\03\03\A0\03\1D\08\A0\03\15\03\A0\03\1B\DA`\03\00\10\09)\DA\06\00\04\00 \03\14\07\B0)\1B\CAP\03\10\CA\10\034\03\06\00\00\02\10\A4\00\03\10\03p\10\07\00\03\01\7F\103?\08 \00\070t\04\04\C0\02\22\05@\10\00E!t\08\04\A0\02\02 \07\1B\030\07\18\030\07E$x\03\04\A0\02\1B\CA\F0\06\11$\80\02'\03\08\10\0F\0FP\18\01(\E2\10P\18%\03\02P\18\02\C0\00\04\F0\17C\00\00\CAO\F0\17\17@ \00\11\04\B0\09\06 \00&\05\04\10\18\10\1E\00\04\02\C0\17\01 \00\14\1F\B0\17\03\F0\17Q.\00!r\06\10\03\06 \00%\07\06\D0\17`d\00\00!\A2\07\01\10\03 \00\11/\80\17/\07\80\80\17';M9\00\C0\07\11\00\10\03$\FF\E0\C0\07H\00\00@\0Ap\174\00\00 \10\02\01\00\02\00\FD\16\08p\17\00\FC\01\010\00\03p\17%\00\00p\17\02p\12\11\0D\80\05$b\F2\E0\05\01p\00\05`\17V\19\00\00\C0\09\90\00\04p\1D\11\0D\E0\02\11op\17\05`\02\11\C6\A0\07\15\07\90\00\01@\17\11\0E\80\1D$\02\02\00\04%\10\03\10\00\01\00\08\16\04`\0F \A8\02\E0\16\18\10P\0FY\10x\09\07@\10\1E&\05\07\B0\0F\00\90\1D*\12\09`\00*\0A\05`\00\07\90\17zj\01\00\81y\0A\0A@\17\17\14`\08\0C`\0F\00p#4\0D\0D\80p\00\01\B0\099\14\00\14`\17,\15\14`\0F\19\15`\0F\02\E0+\06`\0F\01`\07\07`\04\11\12\10\17\07`\0F$\12\07\80\00\01`\04\1B\10`\0F)\0F\12\80\048z\0B\10\00\1F\040\00\15\0F0\004\11\12\090\00\02P\08\12\0BP\04\12\F6\D0\0E8z\10\10\B0\04W'r\12\12\05\D0\01\06 \08\19\F2\A0\0F$\11\0Ap\00\16\16\10\0F\00\C0\049:\0B\0B\D0\04\1D8\E0\0E\15\0B\D0\04\11\E2\C0\00\06\D0\0F\18\E2p\08\12\F6`\08\1A\1A\C0\08\06@\00*\FA\03\C0\08\13\F4P\00'\10\16\A0\11:\00\10\18\00\0F6\12z\140\05\00\A0\00\08\10\10\18\C4P\00\02\10\09R\07r\0E\14\0E@\03\11\050\00\1BZ\E0\00\17(`\00\0F\90\16\00\06\B0\00\13\F2P\004\0F\14\0FP\00\01p\0E\16\13\80\16\1B\E2`\0A\1B\E2\00\10\0F0\0F\00H\10X\11\11\80\00T\84y\15\13\00`\05\1B\E2 \01\0F@\0F\00f\84y\0B\13\00\80\80\0C\09\00\19\00\80\01\1C\18`\00%\17\0F`\00\10\E6\D0\004\11\14\11\D0\00\04`\0F\04P\00Wd\0E\00\10(P\00\000\025x\11\11\C0\00\01\10\014\12\14\12@\00\01\B0\06\16\10P\0F\11\E2@\0F\15\11\B0\00\1B\C8\E0\17\01p\00%\19\12\90\00\1Bh\80\0F\22\E8\0E\C0\17\16\80\10\01 \0Bx\C56\94\80~\00B\FC\03\00\C4\1F\10\002\00\00\E2\A0\01B\0Bx\00\0F \00\000\01#/\04\10\00A\00\00\E2\F6P\0BB h\0B\0B\866\11@P\05! \98\10\00\11K\10\00u\E2\0F\00 (\0F\0F \00\01\F0\06\10\0Ba\00\020\04\00 \008\B8\0F\0F0\00Q!r\15\04\15a\00\00 \00\11O@\09\08\00\0CV!r\17\08\17\D0\0FW\01#t\13\15\A0\0AX\08\84y\08\12\C0\01E#t\1B\17 \00\00`\0A\10t\90\114@K\040\175t\1B\1B\10\00\10\E4\D0\07%\14\13\D0\07\01p\007\19\06\190\18E!t\16\1B \00\01\B0\0A(\14\15p\17 $x\CF $\80\00\B0\00@#x\16\17 \00&\16\080\00\05`\17\01`\00%\11\0A\A0\10|\E2\8F\00$x\1B\1B@\00\000\00\13\160\00X\08s\15\00\14\B0\0A8t\17\19\00\01H#t\13\11\80\0BE#t\17\17\E0\00\02 \00\06\00\01\10\E4P\007\16\00\16\A0\0FI!t\0A\17\00\01)t\04\10\01E$x\17\17\A0\00\01\D0\00 \0A\19\E0\00\22\0A\08 \00H#x\04\11\C0\0B\00\90\12\08\D0\00R r\06\06\15\EA7\01\C0\17\00@\00\00\E0\00&\0A\00@\00\08\E0\0B\00\F0\00\15\0F0\02e\02\00 h\06\06P\02\02\C0\02\12\08\A0\02\14\F8\C0\02\15\10\D0\02\01p\005\16\1B\16p\00\14O0\002\00\00\E20\0BH \98\06\06\80\021\08s\0A;4\04@\02K (\16\16p\00\12\10@\00\13\F4`\007\0F\0B\06\D0\00\01`%\16^p\07H \B8\16\16`\00\09`\08\00 \008H\08\08`\00\09\A0\0Cxb\0E\00 h\10\10 \00F r\15\15\D0\00h\8F\00 \D8\08\08`\001\86y\000\11\CA\04\19\10\0C\00\E6\05\00 \A8\10\10\E0\00\03`\19\00P\01\84\22\07\00 r\0A\17\0AP\00u\C8\1F\00 H\0A\0Ap\00\12\C6\F0\19\15\100\00D\0E\00 rp)\02\F0\00H \D8\0A\0A`\00\01\C02\04\00\01z\E4\8F\00 h\13\13\C0\00%\11\11p\00z\C6\0F\01 \A8\13\13@\00%\0A\03@\00\01\C0\08(\02\05P\01T\86y\00\0A\15\E0\00E\E2\05\00  \15\00P\00\10\1F \00$\08\11 \00 \E8\05\10\00'\02\130\00hG\99\00\00@\F6 \12\01=\00\05\D0\099M\09\00\10\009\06{\04\80\0D\00p\0A#0\03 \00\09\A0\07\03P\06\09\80\10\16O\D0\01\00@\01\10\A4\80\0D&\02\04 \09uO\00\05s\03\00\02 \09\15\04\10#\02`\03QO\00$r\05\10\00\10\03P\08 \C8\9F\C0%\1A\050\091\09\03\050\0C\02\10\00\00\E06\15\04@\00\00\00\08\11\04\A0\0A\16\00P\01\01\A0*3\0B\02\8Ep\0E\19\02p(9'r\05\F0\1D\09P\0B\01\F0\17\11\07\90\00\13\05\D0\116$t\0A\A0\05\01p\00*\06\07P\22\1C\07\F0\0D\1A\06\F0\0D)\06\06@\088\08\05\05 \07\060\00\12\F20\00\17\18 \00\01\D0\0B\12\05 (#\00\000\07(\05\05\E0\0DE\12x\03\05 \07\84\CA\1F\00\84y\05\03\00`\03\10\E8 \07F\06\03\00\80\80\0DB\0Bx\00\06@\04\00\C0\1B#\1F\04\10\00\02\10\07W\D6\0F\00 \08\80\04\1B\C8@\04&\CC\0FP\10#\10\00\E0\051\02\02\05A\00\04P\0E\01?+\11?\15\0C\00 \0E\03P\1E\22\0A@\10\008!t\07 \1E\19\040\1D\01\D0\10 \07\02`\05#\07\08@\005x\07\02\80\1D\00\E0(\22s\02\D0\0F\03\D0\00$ r\B0+\11@ \15\01\00\02\13^\00\02\10\C6\D0\00(\05\05\A0\03\09\F0\0C\00 \005\98\05\05\E0\00\00\F0\00#r\05@)\02\D0\07\00`\03\15\05`\03\11\01\00\11/p\FD`\03\08\0B\10\00\04\F0\02\13\13p\02\81\02x\04\00\10:\00\00\F5\05\12\E2\D0'\14\01\D0\01E\C4\0F\00D\80\03\11\C0@\00\00P(\14\07p&\00\C0\17Gt\03\FF\02\C0\02\06\E0.\01\90\1DR\04\08r\13\070\04\02\90!\00p\00\14\80p\00\00\10\07$r\02 \00\14\00p\00)\C0\02p\00\12\02p\00\15\F0p\00\1F\04p\00\02\00\90\06\02P\00\01?\00\00F\16\03p\00\1B\F0p\00\03 \00\14\02p\00\1FPp\00\0C\1F\08p\00\05\01\E0\00\09p\00\01P\01#`;p\00\1F\C4p\00\03/\E0\01p\00\0B\1F\10p\00\17\01\E0\00/\D0;\E0\00\0B\17pp\00\04\D0\12\13\07`\00fGy\00\00\F0\D7@\02\09\00\02\03 \02& <\D0\00\00P\00\15 P\00\11?p\03\01\A0\00\16\00@\02\1B\02@\00\1B`@\00&\E0\00\90\00\0F@\00\01\1B\04@\00\1B\A0@\00\1F\A0@\00\0C\1B\08@\00\1B\E0@\00\19`@\00\1D\06@\00\1B\10@\00&0=@\00\04@\03\17\06 \03\17\10P\00\00P\01(`\EEP\01\00 \14\06P\00fHs\00\00\1A\00\A040\89s\07@*A\0C\0B\00\0E@\04`Py\00\04\80\C2@\00R\C3\03\00\EA\1F\A0\01\13\FFP\00f\C0\0F\00\18y\00\01\00\0F\10\00\D0\0F\01\00-\00\AB\02.\03\00\01\00\22@\00\01\00=%\01\000\00\08\01\00\1F\0B@\00\04\13e)\00\1F\F1@\00\0C\00\DC\04.\02\00\01\00\13XU\00$\C0\00\AD\03\00T\02\02G\06\00\01\00\03\948/\00\E6T\00\00\00\01\00\02\A5!\02\00E\0F\80\00\09\11)\06\00\05\EFC\05\01\00\13\F0@\00\11T\06\00\06\E4\00*\04\00\01\00/[\00@\00\03\12D\95\066\00\DC\00@\00\1F\0A@\00\00\00-'\1F\0B@\00\00\11 \B5\07\0E\C0\00\1B\08\08\00\11\F3\9C2\0E\01\00\13\F8@\00& \00\80\00\17\048\00\02\94\033\00\00\B5\14\01\0C\84\01\13\18\F5\03\00|<\04\01\00\0F\C0\00\01\132@\00\15\06R\00\03\01\00\03\95\00&\80>\80\00j\07\00\00&\80\00\01\00\13\87\94\00*\03\00\01\00\040J\01]\00\0F\80\00\08\13\06l\1D\04hJ\0C\01\00\1B\A8\08\00\17\08W\00\17\05\E8\00\0C\01\00*h@\08\00\088\00\18\06\A0\00\0F\01\00\05\00)\00\B0\00\00\00\08\00\00\00\00\00\00\00\00\00"
@alloc___gpu___pvoid_i64___pvoid = internal constant [32 x i8] c"alloc___gpu___pvoid_i64___pvoid\00"
@ral_recv_input___cpu___pvoid_i64___m3df32 = internal constant [42 x i8] c"ral_recv_input___cpu___pvoid_i64___m3df32\00"

declare ptr @malloc(i64)

declare void @free(ptr)

declare void @disc_ral_call(ptr, ptr, ptr)

define void @main(ptr %0) {
  %2 = alloca %0, align 8
  %3 = alloca ptr, i32 3, align 8
  %4 = getelementptr %0, ptr %2, i32 0, i32 0
  store ptr %0, ptr %4, align 8
  %5 = getelementptr ptr, ptr %3, i32 0
  store ptr %4, ptr %5, align 8
  %6 = getelementptr %0, ptr %2, i32 0, i32 1
  store i64 0, ptr %6, align 4
  %7 = getelementptr ptr, ptr %3, i32 1
  store ptr %6, ptr %7, align 8
  %8 = getelementptr %0, ptr %2, i32 0, i32 2
  %9 = getelementptr ptr, ptr %3, i32 2
  store ptr %8, ptr %9, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_recv_input___cpu___pvoid_i64___m3df32, ptr %3)
  %10 = load { ptr, ptr, i64, [3 x i64], [3 x i64] }, ptr %8, align 8
  %11 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 1
  %12 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 0
  %13 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 2
  %14 = trunc i64 %12 to i32
  %15 = trunc i64 %11 to i32
  %16 = mul i32 %14, %15
  %17 = sext i32 %16 to i64
  %18 = mul i64 %13, %11
  %19 = mul i64 %18, %12
  %20 = getelementptr float, ptr null, i64 %19
  %21 = ptrtoint ptr %20 to i64
  %22 = alloca %.1, align 8
  %23 = alloca ptr, i32 3, align 8
  %24 = getelementptr %.1, ptr %22, i32 0, i32 0
  store ptr %0, ptr %24, align 8
  %25 = getelementptr ptr, ptr %23, i32 0
  store ptr %24, ptr %25, align 8
  %26 = getelementptr %.1, ptr %22, i32 0, i32 1
  store i64 %21, ptr %26, align 4
  %27 = getelementptr ptr, ptr %23, i32 1
  store ptr %26, ptr %27, align 8
  %28 = getelementptr %.1, ptr %22, i32 0, i32 2
  %29 = getelementptr ptr, ptr %23, i32 2
  store ptr %28, ptr %29, align 8
  call void @disc_ral_call(ptr %0, ptr @alloc___gpu___pvoid_i64___pvoid, ptr %23)
  %30 = load ptr, ptr %28, align 8
  %31 = insertvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } undef, ptr %30, 0
  %32 = insertvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %31, ptr %30, 1
  %33 = insertvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %32, i64 0, 2
  %34 = insertvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %33, i64 %13, 3, 2
  %35 = insertvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %34, i64 1, 4, 2
  %36 = mul i64 1, %13
  %37 = insertvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %35, i64 %11, 3, 1
  %38 = insertvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %37, i64 %36, 4, 1
  %39 = mul i64 %36, %11
  %40 = insertvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %38, i64 %12, 3, 0
  %41 = insertvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %40, i64 %39, 4, 0
  %42 = icmp slt i64 %17, 3
  %43 = icmp sge i64 %13, 1024
  %44 = icmp slt i64 %17, 6
  %45 = and i1 %44, %43
  %46 = icmp sge i64 %13, 512
  %47 = icmp sge i64 %17, 6
  %48 = and i1 %47, %46
  %49 = or i1 %42, %45
  %50 = or i1 %49, %48
  br i1 %50, label %51, label %110

51:                                               ; preds = %1
  %52 = alloca ptr, align 8
  %53 = getelementptr ptr, ptr %52, i32 0
  store ptr @main_kernel_blob_gpu.binary, ptr %53, align 8
  %54 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 0
  %55 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 1
  %56 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 2
  %57 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 0
  %58 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 1
  %59 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 2
  %60 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 0
  %61 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 1
  %62 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 2
  %63 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 0
  %64 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 1
  %65 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 2
  %66 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 3, 0
  %67 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 3, 1
  %68 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 3, 2
  %69 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 4, 0
  %70 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 4, 1
  %71 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 4, 2
  %72 = alloca %.5, align 8
  %73 = alloca ptr, i32 3, align 8
  %74 = getelementptr %.5, ptr %72, i32 0, i32 0
  store ptr %55, ptr %74, align 8
  %75 = getelementptr ptr, ptr %73, i32 0
  store ptr %74, ptr %75, align 8
  %76 = getelementptr %.5, ptr %72, i32 0, i32 1
  store i64 %59, ptr %76, align 4
  %77 = getelementptr ptr, ptr %73, i32 1
  store ptr %76, ptr %77, align 8
  %78 = getelementptr %.5, ptr %72, i32 0, i32 2
  store ptr %64, ptr %78, align 8
  %79 = getelementptr ptr, ptr %73, i32 2
  store ptr %78, ptr %79, align 8
  %80 = alloca %.6, align 8
  %81 = alloca ptr, i32 14, align 8
  %82 = getelementptr %.6, ptr %80, i32 0, i32 0
  store ptr %0, ptr %82, align 8
  %83 = getelementptr ptr, ptr %81, i32 0
  store ptr %82, ptr %83, align 8
  %84 = getelementptr %.6, ptr %80, i32 0, i32 1
  store ptr %52, ptr %84, align 8
  %85 = getelementptr ptr, ptr %81, i32 1
  store ptr %84, ptr %85, align 8
  %86 = getelementptr %.6, ptr %80, i32 0, i32 2
  store i64 1, ptr %86, align 4
  %87 = getelementptr ptr, ptr %81, i32 2
  store ptr %86, ptr %87, align 8
  %88 = getelementptr %.6, ptr %80, i32 0, i32 3
  store ptr @main_kernel_main_kStitch_divide__13_1_0___1b1r_kernel_name, ptr %88, align 8
  %89 = getelementptr ptr, ptr %81, i32 3
  store ptr %88, ptr %89, align 8
  %90 = getelementptr %.6, ptr %80, i32 0, i32 4
  store i64 %17, ptr %90, align 4
  %91 = getelementptr ptr, ptr %81, i32 4
  store ptr %90, ptr %91, align 8
  %92 = getelementptr %.6, ptr %80, i32 0, i32 5
  store i64 1, ptr %92, align 4
  %93 = getelementptr ptr, ptr %81, i32 5
  store ptr %92, ptr %93, align 8
  %94 = getelementptr %.6, ptr %80, i32 0, i32 6
  store i64 1, ptr %94, align 4
  %95 = getelementptr ptr, ptr %81, i32 6
  store ptr %94, ptr %95, align 8
  %96 = getelementptr %.6, ptr %80, i32 0, i32 7
  store i64 256, ptr %96, align 4
  %97 = getelementptr ptr, ptr %81, i32 7
  store ptr %96, ptr %97, align 8
  %98 = getelementptr %.6, ptr %80, i32 0, i32 8
  store i64 1, ptr %98, align 4
  %99 = getelementptr ptr, ptr %81, i32 8
  store ptr %98, ptr %99, align 8
  %100 = getelementptr %.6, ptr %80, i32 0, i32 9
  store i64 1, ptr %100, align 4
  %101 = getelementptr ptr, ptr %81, i32 9
  store ptr %100, ptr %101, align 8
  %102 = getelementptr %.6, ptr %80, i32 0, i32 10
  store i32 0, ptr %102, align 4
  %103 = getelementptr ptr, ptr %81, i32 10
  store ptr %102, ptr %103, align 8
  %104 = getelementptr %.6, ptr %80, i32 0, i32 11
  store ptr null, ptr %104, align 8
  %105 = getelementptr ptr, ptr %81, i32 11
  store ptr %104, ptr %105, align 8
  %106 = getelementptr %.6, ptr %80, i32 0, i32 12
  store i32 3, ptr %106, align 4
  %107 = getelementptr ptr, ptr %81, i32 12
  store ptr %106, ptr %107, align 8
  %108 = getelementptr %.6, ptr %80, i32 0, i32 13
  store ptr %73, ptr %108, align 8
  %109 = getelementptr ptr, ptr %81, i32 13
  store ptr %108, ptr %109, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %81)
  br label %178

110:                                              ; preds = %1
  %111 = icmp eq i64 %17, 0
  %112 = sub i64 %17, 1
  %113 = udiv i64 %112, 8
  %114 = add i64 %113, 1
  %115 = select i1 %111, i64 0, i64 %114
  %116 = alloca ptr, align 8
  %117 = getelementptr ptr, ptr %116, i32 0
  store ptr @main_kernel_0_blob_gpu.binary, ptr %117, align 8
  %118 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 0
  %119 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 1
  %120 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 2
  %121 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 0
  %122 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 1
  %123 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 2
  %124 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 0
  %125 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 1
  %126 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 2
  %127 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 0
  %128 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 1
  %129 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 2
  %130 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 3, 0
  %131 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 3, 1
  %132 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 3, 2
  %133 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 4, 0
  %134 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 4, 1
  %135 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 4, 2
  %136 = alloca %.2, align 8
  %137 = alloca ptr, i32 5, align 8
  %138 = getelementptr %.2, ptr %136, i32 0, i32 0
  store ptr %119, ptr %138, align 8
  %139 = getelementptr ptr, ptr %137, i32 0
  store ptr %138, ptr %139, align 8
  %140 = getelementptr %.2, ptr %136, i32 0, i32 1
  store i64 %122, ptr %140, align 4
  %141 = getelementptr ptr, ptr %137, i32 1
  store ptr %140, ptr %141, align 8
  %142 = getelementptr %.2, ptr %136, i32 0, i32 2
  store i64 %123, ptr %142, align 4
  %143 = getelementptr ptr, ptr %137, i32 2
  store ptr %142, ptr %143, align 8
  %144 = getelementptr %.2, ptr %136, i32 0, i32 3
  store i64 %17, ptr %144, align 4
  %145 = getelementptr ptr, ptr %137, i32 3
  store ptr %144, ptr %145, align 8
  %146 = getelementptr %.2, ptr %136, i32 0, i32 4
  store ptr %128, ptr %146, align 8
  %147 = getelementptr ptr, ptr %137, i32 4
  store ptr %146, ptr %147, align 8
  %148 = alloca %.3, align 8
  %149 = alloca ptr, i32 14, align 8
  %150 = getelementptr %.3, ptr %148, i32 0, i32 0
  store ptr %0, ptr %150, align 8
  %151 = getelementptr ptr, ptr %149, i32 0
  store ptr %150, ptr %151, align 8
  %152 = getelementptr %.3, ptr %148, i32 0, i32 1
  store ptr %116, ptr %152, align 8
  %153 = getelementptr ptr, ptr %149, i32 1
  store ptr %152, ptr %153, align 8
  %154 = getelementptr %.3, ptr %148, i32 0, i32 2
  store i64 1, ptr %154, align 4
  %155 = getelementptr ptr, ptr %149, i32 2
  store ptr %154, ptr %155, align 8
  %156 = getelementptr %.3, ptr %148, i32 0, i32 3
  store ptr @main_kernel_0_main_kStitch_divide__13_1_0___1w1r_kernel_name, ptr %156, align 8
  %157 = getelementptr ptr, ptr %149, i32 3
  store ptr %156, ptr %157, align 8
  %158 = getelementptr %.3, ptr %148, i32 0, i32 4
  store i64 %115, ptr %158, align 4
  %159 = getelementptr ptr, ptr %149, i32 4
  store ptr %158, ptr %159, align 8
  %160 = getelementptr %.3, ptr %148, i32 0, i32 5
  store i64 1, ptr %160, align 4
  %161 = getelementptr ptr, ptr %149, i32 5
  store ptr %160, ptr %161, align 8
  %162 = getelementptr %.3, ptr %148, i32 0, i32 6
  store i64 1, ptr %162, align 4
  %163 = getelementptr ptr, ptr %149, i32 6
  store ptr %162, ptr %163, align 8
  %164 = getelementptr %.3, ptr %148, i32 0, i32 7
  store i64 256, ptr %164, align 4
  %165 = getelementptr ptr, ptr %149, i32 7
  store ptr %164, ptr %165, align 8
  %166 = getelementptr %.3, ptr %148, i32 0, i32 8
  store i64 1, ptr %166, align 4
  %167 = getelementptr ptr, ptr %149, i32 8
  store ptr %166, ptr %167, align 8
  %168 = getelementptr %.3, ptr %148, i32 0, i32 9
  store i64 1, ptr %168, align 4
  %169 = getelementptr ptr, ptr %149, i32 9
  store ptr %168, ptr %169, align 8
  %170 = getelementptr %.3, ptr %148, i32 0, i32 10
  store i32 0, ptr %170, align 4
  %171 = getelementptr ptr, ptr %149, i32 10
  store ptr %170, ptr %171, align 8
  %172 = getelementptr %.3, ptr %148, i32 0, i32 11
  store ptr null, ptr %172, align 8
  %173 = getelementptr ptr, ptr %149, i32 11
  store ptr %172, ptr %173, align 8
  %174 = getelementptr %.3, ptr %148, i32 0, i32 12
  store i32 5, ptr %174, align 4
  %175 = getelementptr ptr, ptr %149, i32 12
  store ptr %174, ptr %175, align 8
  %176 = getelementptr %.3, ptr %148, i32 0, i32 13
  store ptr %137, ptr %176, align 8
  %177 = getelementptr ptr, ptr %149, i32 13
  store ptr %176, ptr %177, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %149)
  br label %178

178:                                              ; preds = %51, %110
  %179 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 0
  %180 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 1
  %181 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 2
  %182 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 3, 0
  %183 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 3, 1
  %184 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 3, 2
  %185 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 4, 0
  %186 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 4, 1
  %187 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %41, 4, 2
  %188 = alloca %.4, align 8
  %189 = alloca ptr, i32 11, align 8
  %190 = getelementptr %.4, ptr %188, i32 0, i32 0
  store ptr %0, ptr %190, align 8
  %191 = getelementptr ptr, ptr %189, i32 0
  store ptr %190, ptr %191, align 8
  %192 = getelementptr %.4, ptr %188, i32 0, i32 1
  store i64 0, ptr %192, align 4
  %193 = getelementptr ptr, ptr %189, i32 1
  store ptr %192, ptr %193, align 8
  %194 = getelementptr %.4, ptr %188, i32 0, i32 2
  store ptr %179, ptr %194, align 8
  %195 = getelementptr ptr, ptr %189, i32 2
  store ptr %194, ptr %195, align 8
  %196 = getelementptr %.4, ptr %188, i32 0, i32 3
  store ptr %180, ptr %196, align 8
  %197 = getelementptr ptr, ptr %189, i32 3
  store ptr %196, ptr %197, align 8
  %198 = getelementptr %.4, ptr %188, i32 0, i32 4
  store i64 %181, ptr %198, align 4
  %199 = getelementptr ptr, ptr %189, i32 4
  store ptr %198, ptr %199, align 8
  %200 = getelementptr %.4, ptr %188, i32 0, i32 5
  store i64 %182, ptr %200, align 4
  %201 = getelementptr ptr, ptr %189, i32 5
  store ptr %200, ptr %201, align 8
  %202 = getelementptr %.4, ptr %188, i32 0, i32 6
  store i64 %183, ptr %202, align 4
  %203 = getelementptr ptr, ptr %189, i32 6
  store ptr %202, ptr %203, align 8
  %204 = getelementptr %.4, ptr %188, i32 0, i32 7
  store i64 %184, ptr %204, align 4
  %205 = getelementptr ptr, ptr %189, i32 7
  store ptr %204, ptr %205, align 8
  %206 = getelementptr %.4, ptr %188, i32 0, i32 8
  store i64 %185, ptr %206, align 4
  %207 = getelementptr ptr, ptr %189, i32 8
  store ptr %206, ptr %207, align 8
  %208 = getelementptr %.4, ptr %188, i32 0, i32 9
  store i64 %186, ptr %208, align 4
  %209 = getelementptr ptr, ptr %189, i32 9
  store ptr %208, ptr %209, align 8
  %210 = getelementptr %.4, ptr %188, i32 0, i32 10
  store i64 %187, ptr %210, align 4
  %211 = getelementptr ptr, ptr %189, i32 10
  store ptr %210, ptr %211, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_send_output___cpu___pvoid_i64_m3df32___void, ptr %189)
  ret void
}

host default target triple: x86_64-unknown-linux-gnu
host cpu name: icelake-server
after optimize llvm module:
; ModuleID = 'LLVMDialectModule'
source_filename = "LLVMDialectModule"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%0 = type { ptr, i64, { ptr, ptr, i64, [3 x i64], [3 x i64] } }
%.1 = type { ptr, i64, ptr }
%.5 = type { ptr, i64, ptr }
%.6 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.2 = type { ptr, i64, i64, i64, ptr }
%.3 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.4 = type { ptr, i64, ptr, ptr, i64, i64, i64, i64, i64, i64, i64 }

@main_kernel_main_kStitch_divide__13_1_0___1b1r_kernel_name = internal constant [35 x i8] c"main_kStitch_divide__13_1_0___1b1r\00"
@main_kernel_blob_gpu.binary = internal constant [6648 x i8] c"P\EDU\BA\01\00\10\00\E8\19\00\00\00\00\00\00\02\00\01\01@\00\00\00\A8\19\00\00\00\00\00\00\A2\19\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8:\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\002@:\00\01\00\117\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0D\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F5)_shndx\00.nv.info\00.text.main_kStitch_divide__13_1_0___1b1r2\00\0F,\00\15oshared.\00\15\9Fconstant21\00\1D\1F01\00\12\FD\01debug_frame\00.rel\11\00\1Aa\12\00!nv&\00oactionV\01 \0F\94\00\0F\0Fy\01\A2a__ocg_3\00/\00$0\00\0F\FF\16$__cuda_sm70_shflsync_bfly_p\00$____wg_G\00\0F@_0_$\0D\00/265\00\19_1__281\00\19\122f\00/305\00\19_3__32\91\02\1Fo_param\98\02.\0F\01\00\06\8CU\00\00\00\03\00\0B\00\01\00\11\AA\18\00,\0C\00\01\00\11\D8\18\00,\09\00\01\00\A1\15\01\00\00\22\00\0B\00`,\0F\001\00 \01\07\000\00!\020\00,\0A\00\01\00\11Y\18\00,\04\00\01\00\11\89\18\00,\07\00\01\00f2\00\00\00\12\10\A8\00!\80-\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13P\C0\00\10\04\9B\00R\044\00\00\00E\005\04\D4\0A<\00&8\00H\00\04p\00L\88\80\80(o\00\10\08\15\00x\16\89\80\80(\09\05J\01\004\00\22\18\00\01\00\22p\00\01\00\048\01\13\108\01\90\04/\08\00\08\00\00\00$\0C\00%#\08*\01'\04\12\0C\00\17\11\0C\00\12#0\00\04$\00\17\08$\00\05\0C\00!7\04$\06\90\015\00\00\04\0A\08\00\05x\00\A3\01\18\00\03\19\18\00\04\17\0CH\06U\10\00\00\F0!\10\00\10\01>\00%\F0\11\10\00\01\01\00\F09\F0!\00\03\1B\FF\00\04(X\00 \12\00\00p\12\00\00\B0\12\00\00\00\13\00\00`\13\00\00\90\14\00\00\D0\14\00\00\10\15\00\000!\00\00`!\00\00\80!\00\00\A0!\00\00\C0!\00\00P\22\00\00p\22\00\00\90\08\00\F4\16*\00\00\E0*\00\00`+\00\00\B0+\00\00\F0+\00\000,\00\00\04\1C\08\00@(\00\00@*\00\00\04\1E\E6\00C\00\00K\00\01\00\94\02\02\08\10\0A/\22\00\00\DF\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\E0\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00@\00\00\00\14\FB\01\22\C8\00\01\00\13\02\D1\00*H\00\10\00/\04\00\01\00\FF|@$v\01\FF\D7\04\C1\FF\00\8E\07\00\C4\0F\00\19y\02\00\01\00\F1\00!\00\00\00\22\0E\00\B9z\04\00\00F\00\00\FB\04\90\E2\0F\00Ey\00\00\10\0D!\00 \80\03\10\00\81$t\0C\FF\00\00\80\FF@\00\F3\05\E2\0F\00\10z\00\02\00Z\00\00\FF\E1\FF\07\00\E4\1F\04\0C\10\00\A0pR\F0\03\00\E4\0F\00\10xl\00\F0\07\00\00\FF\E0\FF\07\00\C8\0F\00\12x\03\00\00\FC\FF\FF\FF\C0\8E\07 \002\19y\00\01\00\10%\9D\00\83\0E\00\07r\03\03\FF\00\01\00\000\000r\03\03\A2\001\FF\FC\8E@\00P\0Cr\00\02\03\80\04\D3b\F2\03\00\DA\0F\00G\19\00\00`\0C\A0\00a\EA\0F\00\10r\04 \00\01\90\00\01\C0\00F\01\00\F0\06\C0\001\1Cx\00\01\00?p\F0\F0\D0\00\02@\0Cx\00\04?\062pB\F4 \00Ar\0E\FF\FF\D0\02\C6\8E\07\00\D8\0F\00G\A9\00\00\90\06p\00\05P\00\14\E1\F0\00a\0D\03\00\F4\FF\FF\F0\00b\C4\0F\00\02x\10\04\03\12\0FP\01@$z\0B\00 \01 \0E\02\D0\00\B0\1F\00%v\04\0B\00X\00\00\10\10\00a\CA\0F\00\81y\0F\C3\05\E3\00\19\1E\0C\00\A2\00\00\10x\07\0B\00\01@\01\93\E4\0F\04\10x\09\0B\00\02\10\00z\C6\0F\00%v\06\07@\00%\12\06@\00u\E2\02\00%v\08\09 \00\95\E2\0F\08\10x\0B\0B\00\03P\00v\00\10x\13\0E\00\04\10\00E\81y\11\08@\00fb\09\00%v\0A\A0\00\10\C8\C0\00\11\15\C0\00\12\13P\00U\00\81y\14\0A0\00\10f0\00&\04\15p\00e\1F\04\10x\17\15\C0\00\10\C80\00\16\13\E0\00 b\01\C0\00\16\170\00\88/\00\10x\1D\15\00\020\00\16\16\D0\00zb\03\00%v\1C\1D\D0\00*\1F\15\D0\00E\09\0E\00\08\10\00Z\01\81y\15\1C\D0\00%\1E\1F@\00\01\D0\00\11\19\D0\00\15\09\D0\00%\18\1E0\00\10f`\01' \19p\00\00\80\01\1A\19\D0\00&\17 `\00\1C\0Fp\01Z\00\10x\0B\19\D0\00\1F\1A`\01\07\01\D0\00&\05\19\D0\00\10\1F\00\024\0E\00\0C\10\00j/\00\81y\19\0A@\01%\04\05@\00\01\D0\00\11\1B\D0\00\15\07\D0\00\16\1Cp\01z&\03\01%v\06\1B\D0\00%\1D\1B\D0\00\01@\02\16\07p\01\11\22\D0\00\1A\1D\D0\00H\1D\1B\00\020\00*\08\080\00\17\0A0\00\00\C0\00H\1B\1B\00\030\00*\0B\0A0\00(\04\1B\00\02Q\0Br\00\0F\0F_\03\A1\F4\03\00\E4O\08\0Br\00\0C\10\00\B2\C0\F6\03\00\D6\0F\00\08\A2\0F\0F\D2\07\22\80\05\B0\02\16\0C\E0\00 \A2\0E@\00%\12\12@\00\10\8F@\00!\11\11\10\00\10\F6\A0\03\10\02`\00\01 \00P\C0\F8\03\00\D2P\00!\12\12`\000\00\00\06P\00U\0Br\00\14\14@\00\01\10\00$\12\110\00q\C8\0F\00\08\B2\11\11@\00\060\00&\13\13`\00\00\10\00(\11\140\001\A2\14\14@\00\060\00*\16\16`\00)\14\13`\00!\13\13@\00\060\00*\15\15`\00)\13\16`\00!\16\16@\00\060\00*\18\18`\00)\16\15`\00!\15\15@\00\060\00*\17\17`\00)\15\18`\00!\18\18@\00\060\00*\1A\1A`\00)\18\17`\00!\17\17@\00\060\00*\19\19`\00)\17\1A`\00!\1A\1A@\00\060\00&\1C\1C`\00\10\01p\00\19\19`\00!\19\19@\00\060\00*\07\07`\00)\19\1C`\00!\1C\1C@\00\060\00*\08\08\C0\00)\1C\07`\00!\07\07@\00\060\00*\0B\0B`\00)\07\08`\00!\08\08@\00\030\00f\10x\0E\0E\00\10\90\04\00p\00\19\0B`\00!\0B\0B@\00\030\00Q\0Cr\00\0E\0D\D0\06\14\F8`\00*\0C\0C\00\03)\0B\0C\00\03!\0C\0CP\00\00\00\03\90\E2\0F\00G\C9\00\00\90\F9\9A\0B\11\83\90\06#Ay\89\0B\03\A0\06@$x\04\03\12\005\0E\0A\8E\10\07#p\03 \00\13\E8\F0\06\13\04\F0\06\12\DA\E0\06'@\03@\00St\14\FF\04\000\08\10\E4`\04\1C\0A\C0\06\10\0A\C0\033\14\02\8E\E0\03\1B\0D\C0\06%\06\0A`\04\01\C0\06+\08\0A\C0\06\1A\06@\00\1C\10\C0\06\15\08 \00\01 \05&\0A\0A \05\00p\01\1B\11\C0\06\1C\0F`\05\06\A0\00\01 \05\11\11\C0\00\15\11 \05\16\12\90\04\10f`\06&\04\11p\00\00\C0\06*\13\11 \05*\05\04\F0\05&\06\130\00!/\000\00\09 \05+\06\06 \06\070\00j\0F\01\10x\11\11 \05\1C\09P\05\07\90\00\01 \02\12\0D\8F\09\15\F0 \05\01\10\00\22\C0\F4 \021\82\0D\0D0\02$\00\00 \05\1B\0A \05%\10\10@\00*\8F\08p\05\01 \05$\0D\10p\02\00 \05\22\82\10\FF\09\03p\05\06`\05#\F0\03\10\00$\10\0F0\00\12\C8\A0\05\1A\100\00&\05\05`\00\16\00\80\05\030\00\14\82\80\05\050\00*\06\06`\00)\12\05`\00:\05\05\120\00*\09\090\04)\05\06`\004\06\06\050\00\11\C80\00\11\060\00#\C0\F0P\00!\09\090\00\00\AB\0C\1F\E4\D0\09\01\18\0EP\08\00@\00%\0C\0C\10\04\01\80\01(\09\0C\80\011\B2\0C\0C`\00\00\80\01\1E\C4\90\03\00\F0\03\00_\03Ap\16p\00p\03V\89\00\00\A0\01 \000\02x\0B\CF\02\070\0A\0Fp\03\03\15\0B\C0\07\07`\03\0C\A0\02%\A2\0E`\03\030\00\19\00\80\03\0C\A0\02%\E2\0Ep\03\060\00\07p\03\0F\A0\02\03\06\90\00\10\CC \00(\0A\0A\F0\02\06\00\02\06\B0\02\05\D0\01\09\F0\01\BF\F4\03\00\D2\8F\00\08\82\05\05\0C\E0\01\02\1F\F0\E0\01\03\17\A2\E0\01\020\00*\0A\0A\D0\02(\06\090\00\14\82\F0\01\05\10\02)\09\0A\10\02!\0A\0A0\00\00\10\02\10\CA\00\0C\11\0C\00\0C\13\0A0\0D\14A\B0\0C\04\D0\010z\00\03\A0\01Dpb\F4\030\0D#\B0\04 \00\10\E2\F0\0C0\05\02\1F@\05\00\F0\0C\96\D6\0F\00G)\00\00\80\04@\001$r\07`\00'\03\00\A0\05\16\02@\00\09\B0\0C\10\C6p\0D%\04\07p\0D\13\C8\C0\05\10\03\C0\05\12\F6\C0\05V\B9\00\00 \02`\00\10\02\D0\0D\13ZP\02\0F\E0\02\02F\04\04\00\FD\B0\0C;$t\06\F0\05\16\08P\0A%\C8\1F \023\06\02\8E\10\02*\0D\08\F0\05%\0B\07\90\02\01\F0\05(\0F\07\F0\05\04\00\0D\010\02a\E4\0F\04$z\0F\10\00\12\0F\10\00\05\10\0B\13\06\B0\05E%v\0E\0F\10\00\1B\E4`\02 \E2\0E\00\066\07\00\03`\00E\81y\0F\0E \00\16$\C0\00\1F\11\C0\00\01\09P\05\05\D0\03$\0C\0D\B0\07\01\D0\03\05P\05\12\06\80\00+\0C\08P\05\03p\02\14\F8P\05\12\0DP\02\11\FA`\01\06`\05\01\F0\02U\01\10x\07\07\B0\06q\CE\0F\00\08\C2\0A\0Ap\00Q\00\80\06\00\C80\00$\0A\0F@\00\00\90\02D\B2\0F\0F\0A \00\020\08!\07\040\08*\FA\03\90\04\14\F8\C0\00$\0F\0C\F0\02\00\C0\00\22\C2\0C@\0B\040\08_\D9\00\00\10\FE0\08\07\09`\02\03\90\02\140\A0\04\13\E6p\02\1A\01p\02(\00\010\08\1F\0A@\02\12\1F\0A@\02\02\1A\0E@\02\1B\CA0\02\16\C8 \02\1D\0A\C0\01\1B\F0\C0\01\1B\F6\10\07/\80\05\10\07\01\0E\10\03\16\07\B0\02\09`\01\05 \02)\0D\0C \02\12\0Cp\00\00 \02\0F\F0\05\01!z\00P\01\08\F0\05(\80\00\F0\05\1B\09\F0\05\1B\060\01\11\06\C0\05\15\090\01\16\09\B0\05+\A4\0E \05\02\00\01\19\09\00\014\09\09\0C\00\01\07\D0\04\1F\09\D0\04\04\F1\00\89\7F\07\0C\00\1F \0C\00\00\0E\00\00b\0E\D0\04&`\03\90\04\06p\0B\00p\00\11/p\00\19\07p\00(\07\07p\00r\89\7F\04\07\00\1F@P\00\11d\B0\00*\04\04@\00)\07\04@\009\04\04\07@\00X\09\04\00\1F\80@\00*\09\09@\00\1C\04\F0\00\17\04\F0\01@\19x\04\FF \06@\02\16\01\00 \04\87\89\7F\06\09\00\1F\00\0DP\00*\06\06P\00)\09\06P\007\06\06\09P\00@\0Cr\00\05@\01!pR\D0\03\0DP\12\81\89\7F\0B\06\00\1F\00\0E`\00ft\0E\00$\B4\07\10\03\00\80\0A\10\B6\EF\02\12\80\E0\01\00 \03\12t\00\06#\FF\00\D0\02\14\B2p\0C\12\F8\90\00G\B2\00\06\0BP\043\1C\B8\00p\00Ap\02\00\E4\90\03\10\050\01 p@0\00`\0F\04\19x\05\05\FF\05\10\FF\C5\00\01\C0\094\0B\0B\06@\04\12\E2\E0\05\17\07p\001\0Cx\00\A0\061p@\FA`\06#\88\B3\B0\0C\00\A5\0Cf\E8\0F\00\1D{\00\01\00\89\EC\0F\00GY\00\00@\B0\08\00\9E\04$\80\FF\C0\021\82x\06\F0\13\01\01\00\93\CA\0F\00\84\C9\0A\05\00\04P\00\81\A2\02\00Gy\00\06\C2Q\0E\22\80\0Bp\02C\09\0A\00\07p\02/\A4N\D0\02\01\02\90\07%\C0\FC\E0\01\02\10\05\11\00\10\03c\89\7F\08\09\00\07`\02'\A4\0E\E0\0D\05@\00)\09\08@\00!\08\08P\00\05@\00C\0C\08\00\07`\02a\E2\04\00$r\0B\90\011\08\00\8EP\06$\0B\B2\00\04\11\FC\E0\05\09\F0\01\0A\90\01\01\10\00\00\C0\01\14\0Cp\00\10\DAp\00\04\B0\0D\02p\00c\88\B3\00\FF\0C\00\10\01-\E4\07\80\03\22Hyh\19\01\10\00\1E\E4\80\01T\84y\08\FF\00@\00\22\A2Np\08\07\A0\03!\02x\DE\00\14\80\90\01\00 \15&\80\03`\00B$t\09\FF\B1\01\03\80\02&r\0A\E0\14\10\C6@\00\1B\13\90\04\02\80\05\15\0A\90\04\11\0C\80\05\02\00\14[\8F\04\10x\0E\A0\14\19\10\A0\14J\81y\0D\0C`\0A&\0E\0E@\00\00p\05\16\12\90\14\01\10\0E%\10\10 \00<\E4\0F\08\D0\07G%v\12\12 \00\00\B0\14\15\10 \00\10h\10\00%\13\12\10\00\B4\A2N\00$t\16\FF\9D\98\BB;\F0\02\01\D0\0A\04\80\07\01\10\03a\15\FF\00\00|C \00\11\C6\E0\03!\0A\03`\07\02\D0\03c!r\14\08\0D\00\E4\06`\C8\8F\00#t\0B\9F\112?\16  \01T!r\0D\08\0F \00\D2\C6\0F\01#t\0B\0B\01\00@K\15@ \00E#t\0C\0D0\00\D4\C6\0F\00!t\0F\0B\7F\00@\CB\00\00@\004\11\08\11@\00u\E2\0F\02#t\0C\0C@\00\D4\E4\0F\08#x\0F\14;\AA\B8?\0F\08P\00'\10\11\80\00e\08!r\13\08\13@\00hO\00!t\0E\0C`\00\00@\00e`p\A52\0F\00@\00\17\10\A0\00W\08#t\14\13P\00P\00#x\0E\0Dp\00\12\0Ep\000\04$x\FF\02\15\800\04I!t\12\10`\007t\14\14P\00P\00#x\0D\0Dp\00\13\0E \00T\08s\0F\00\0F\B0\02p\22\0E\00#x\12\11`\00#\12\08\00\018t\0E\14P\00E$x\10\10p\00!\C4\0F0\00\00P\00$\12\00`\00%\0E\13\A0\00`\E4\0F\04\08s\0D\CD\05\02`\00\10\A4`\00(\13\13\80\00T\19x\0E\0B\17\10\05\10\CA\90\004\11\00\120\00\81\E2\0E\00#r\0F\0E\0F\10\07\00\10\00e\1F\00\19x\09\140\00\10\CC0\008\16\00\13\C0\00Cr\0D\0C\0D\1F\03q\00\C8O\00#r\10\91\01#\0D\00 \020r\09\09`\13\13\10P\00\01\E0\0D\18\FCp\09\0Ap\07\00\C0\03&\90\07\C0\03\00 \0C&p\07 \00\04\90\04\08 \0C&\10\040\00\0E \0C+\0A\0B \0C\12\0A \0C\02\90\04XG\E9\00\00\C0\10\041v\0A\FF0\00\03\E0\01\09\10\09\13\C80\03\07 \0C;\02x\12 \04\16\0C\A0\09k\E2\9F\00\10x\0F\10\04\16\11\10\04p\E2\0F\04%v\0C\0C\C0\03$\12\02\90\03\19\13\00\04\0B\10\0C\1C\04@\04\09\C0\11\16\E4 \0C\22\12\02`\07E%v\10\11\10\00\0F@\04\001$z\13@\00\0F@\04\02\10d@\00%\12\13@\00\1F\CCP\04\11\11\0B\B1\0F\03\E0\00L\00$t\17P\04!\0B\0AP\04/\FC\03P\04\03\16\0CP\044\C8\0F\00\00\04$\17@ \04\19\15`\04:!t\0F\E0\03:t\0D\15\10\04\0AP\04E\19x\0C\0C\D0\02\01@\03\09P\04W\04#t\0D\0Dp\00U\08#t\0E\11P\00\03\80\04\08@\04\0Ep\04'\10\0D\90\00X\04#t\0E\0EP\00H$x\0D\0D@\04\0Ep\04 \10\15\90\00\01~\1Ex\E2\0F\04!t\12\0E\E0\00\0D@\04\03p\04&\17@@\00\00\A0\00\02p\03\1A\0F`\04\10\C4`\04\18\16`\04F\04\19x\14\E0\03\1E\E2`\04T\08s\10\00\10p\00\10\A2P\04\11\16p\04#\16\08\A0\01!x\16`\04\22\16\00\D0\05;\08s\12P\04(\0C\0CP\04\10$\90\1A\04\00\01\03P\04\19\16P\04\11\0Cp\13\15\0CP\045\09\09\12\10\00\01P\04'\14\16P\00\00\C0\03/p\FC\C0\0D\09\07\00\04\040\04\19\02\C0\0D\010\12\08\10\04'\E0\01@\00\08\C0\03\01\D0\02\1B\0E\D0\0D\07\F0\03/\E4\9F\C0\0F\00\02\E0\03\1B\0E\C0\0F\15\0Ep\03\07\10\08*\E8\0E\B0\0F\10\22p\00\19\13`\03 \02x\1F\03!|C\05\02\1B\E4\A0\04\13\C4\80\03\06`\04G\00!r\0A`\03\10\CF\F0\06\00\7F\034?\13 \C0\02\1B\12\B0\07\02\10\07\14\14P\025t\13\120\00\01\B0\07*\11\10\E0\02(\13\130\00\09\B0\06\10\E4 \02 \11\0A0\02\22\11\08 \00E!t\0D\13@\00\12\C6 \00\00@\02\22\11\00 \00@#x\0D\120\00$\0D\08P\00%\0A\13P\00\12\C6 \00\000\00\13\0D0\00\00\C0\06\14\11@\0290\0E\00\00\07\00\B0\02Dr\09\10\110\02 \C8\1F\10\00$\0A\0D\10\00/\C6O\A0\0E\01\1C\0B\A0\0E\17\F0 \00?\02x\0D\90\14\02\15\0B\10\0A\02\00\14\15\0D0\05\0A\E0\1B6$t\0D\D0\01\00@\02&t\0F \05\01\90\015\08\08\0B\B0\01PO\01#t\0C\CA\005?\0D  \05\01\10\05\22\0F@\80\03J!t\0D\0C\D0\03\07\E0\04\020\01\15\08P\01\12\C8\10\00\05@\01*\CC\0F0\01\11$0\01\15\0C \01-\CC\1F\A0\07\01`\0C\04\10\0F\03p\0B&\80\01\80\07%!rP\0C@\00\00\CAO\00\0E6\08\00\1F\90\0C2!r\0B\E0\00\05 \00%\0A\0B\D0\0E\01 \00\11\0A \06\06 \00%\0D\0A\A0\0E\01 \00\13\0C \12\00 \00\10\CF \00\15\0C`\0E\00 \00\12\B2\90\08\04@\00c\88\B3\00\04\0D(\89#\0F\C0\0D\04\17\B0\E0\006$t\04\10\0C\0F\C0\0D\02C\04\05\00(0\01!\E2\04\C0\0D(2\09\C0\0D%\05\04\C0\0D\93d\EE\00!r\05\05\04\00\01\00 \CA/p\0F\15\05\A0\0D\11d\F0\008\05\06\00 \00%\07\0A\80\0D\8A\A4\02\00!\B2\07\07\0A\D0\003\FF\07$\80\00/\E4\050\0D 4\05\FF\00@\00!b.\C0\01&0\05\10\05!\84y\00\01\05\C0\05\000\0D(\00\05 \01\08\10\15Q\1F\00$z\0D\00\03\15\02\00\03 \0E\0D\00\03\15\06\90\17%\07\0D@\05 \E4O0 \15\0D\A0\04\0C\A0\14\01\A0\17'\10\07@\00\00\D0\04\18\0D\10\09E%v\12\09 \00\01\D0\08*\11\10\10\0D'\14\0B \00\0A\B0\08\01\10\0D%\15\14\10\00p\E2\8E\00\02x\16\00\90\03\05\D0\03\08\B0\08\00\80\10\10x\FE\00\D0\80\00\00\E2\F2\03\00\E2/\04 x\18\10\00A>\00\00@p\03\03 \004~\00B\80\05C\02\02\00\04\B0\00`\E4\0F\00\08r\181\00\02\01\00P\CE\0F\00 \98\8F\1E#\80K@\00\00\D0\02&\04\0F\F0\03X\00#t\08\0C\10\08T!r\0F\04\11 \00\01\A0\05(\08\08\F0\07V!r\13\04\13 \00W\02!t\11\08\10\04W\00#t\0A\0FP\00Z\08#t\12\13\D0\08)\11\0C\A0\05Dr\15\04\15P\00\10\E2p\04%\0A\0Ap\00\010\00)\0E\0C\B0\05:t\0C\120\09\18\11\80\09\00\10\09\18\0A\A0\04\10$@\0B\04\D0\05\01 \00\1B\12\C0\09'\14\11P\00\00\D0\08\17\0F\10\09@\00\08s\0E\1A\03\060\08:x\12\13@\0D\09\E0\08\01@\00\08\10\09\0E\00\0A*\12\13P\0D%\16\15\E0\08\01P\05(\0F\080\00\00p\06\14\10\80\00xb\0E\00$x\14\14\D0\00\00@\00\00P\00\22\16\000\00\01\00#\13\\\90\02\01p\0D7\13\00\12\A0\06R r\0E\0F\0E\EA/\01\A0\06E \08\0E\0E`\02\10\C60\00\19\15 \09T r\0A\0A\110\00u\E2/\00 \98\0E\0EP\02z\C6\0F\00 \08\0A\0A@\00C\17\00\18\00\81\0D\10b@\005\0C\0C\13@\00jO\00 \98\0A\0A@\00%\0C\0C@\00\84\E2\0F\00 r\14\14\150\00u\C6\1F\00 \98\0C\0C0\00\00 \008\08\14\140\00\06\90&\02\80\0DI \98\14\140\00#r\15\82*!@\00P\035r\11\17\00\01\010\01%\0C\0D0\01\010\006\13\17\0A \00d\04 r\17\17\14\10\00\02\F0\0C\17\070\00P\08\86y\00\0C \01a\04\19\10\0C\00\E6P%\18\0BP\00T\86y\00\0E\13 \00 \E8\01\10\00*\08\15\10\00$\06\17\10\00!\E2\01\00\08(\00\FB`\0A\0Ap\059M)\00\10\00\0E0\04/\02\050\04\03\22\F4\03\90\0AF\00\00\B0\01\80\050\08r\021\00\02\82\03\010\04%\02\02@\01\00\00\19 s\0C?\05\02\D0\01/f\100\17\00\11\02\90\05\15\03\90\05%\06\020\17*\CCO\D0$k\A2\0E\00$t\08`\0A\00\9E\07\06`\0Ac\10x\03\03\00\01\D0\04\18\C8\E0\1B\13\F0\F0\075\05\04\070\04\12\CFp\00\13\\p\00\00\80\08 t\08\CF\003?\08 \80\08\03\C0\04\22\0B@\10\00J!t\0A\08 \04%\08\08P\03\01\C0\08 \0A\05\90\03#\0A\08@\000x\0A\05`\03\01\14\02\01\10\017\05\00\0A\C0\081 r\05q\00\05P\03&(\05\80\01\10\C8P\01(\05\05P\01E r\05\0C0\00!\CA/\00\02\1B\05\00\02/\90\FE\00\02\08\0B\10\00\00\E0\09\00\0B\01\06P\01C\08\00\80*\A0\01T\C4\0F\00Dy`\0CP\C0\03\00\EA\7F\A0\15\15\0A \15\00 \0EGt\0D\FF\02@\00\06p\19\01`\15Q\04\08r\0A\0C\F0\00\00@\150\E4\0F\04`\00\14\F0`\00\01p\06\05 \00\05p\00\15pp\00\17\0Fp\00\01P\00\00\10\16\06\D0\15\10\C4@\006\0B\0C\0A\00 \19\04\F0\0A\12\E4 \00\03\C0\09\00@\00\01\E0\00#p+ \00\16\C6@\15\13\0B\B0\12\00\80\00&\F0\00\80\00\10G\F0\02\16\E9P\01\040\00\11\04\F0\00\11\8F`\0B\17\01p\00\00`\00*\C0+@\01&\A0\00@\01c!r\0A\04\0C\00\01\00\01\C0\07\0A@\01\00\80\01*\00,@\00\17`\90\002!r\0A\10\01\07@\00\1B\04@\00\1B@@\00\17 @\002\02r\07\9C\0E\12\0F@\00\00\E0\00&@\F6\E0\00\03\80\16\05 \00AHs\00\000\03\000\02Q\E8\0F\00\89s\D0\0AA\0C\06\00\0E\C0\02`Py\00\08`\D3@\00Q\C3\03\00\EA\1FP\00#\F0\FFP\00f\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-\11\01\89\02\0E\01\00\22@\00\01\00=V\01\000\00\08\01\00\1F\0B@\00\04\13\96)\00?\98\02\00@\00\0A\00L\06.\02\00\01\00\1105\03\02h3\13\00$\00\13\07\A7\04\00\01\00\03\14(>\00\17\01T\00\00\01\00*\08\05@\00\0F\80\00\05\1A)\7F2\05\01\00\13\E0@\00\11T\06\00\06\E4\00*\04\00\01\00\1F[@\00\0414\06\00\01\00&\C0\00@\00\1F\0B@\00\00!G\01\18\00\0D@\00/\F8\06\C0\00\03\1B\08\08\00#$\01l\0E\0B\01\00\12\D0%\03\03l\03\02\80\00\17\048\00\22\10\00\01\00\13\B5\14\01\0C\84\01*\F0\07h\00\1F\00\C0\00\04\1F\E6@\00\04\13\F8@\00?x\01\00@\00\0A\132@\00\15\06\92\00\02\01\00*\80\09\E04\12\03\8C\00:$\80\00\01\00\13\87\A4\00+\03\00\01\00\03\F09/H\00\80\00\0B\13\06\9F\0D\05(:\0B\01\00*\A8\00\08\00\04\10\01\13\018\00\04(\01\0C\01\00*\10/\08\00\088\00\17\06\A0\00\0F\01\00\05\04\B8\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00"
@ral_send_output___cpu___pvoid_i64_m3df32___void = internal constant [48 x i8] c"ral_send_output___cpu___pvoid_i64_m3df32___void\00"
@ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void = internal constant [101 x i8] c"ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void\00"
@main_kernel_0_main_kStitch_divide__13_1_0___1w1r_kernel_name = internal constant [35 x i8] c"main_kStitch_divide__13_1_0___1w1r\00"
@main_kernel_0_blob_gpu.binary = internal constant [8368 x i8] c"P\EDU\BA\01\00\10\00\A0 \00\00\00\00\00\00\02\00\01\01@\00\00\00` \00\00\00\00\00\00^ \00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(K\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80J\08\00\11G\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F5)_shndx\00.nv.info\00.text.main_kStitch_divide__13_1_0___1w1r2\00\0F,\00\15oshared.\00\15\9Fconstant01\00\12\FD\01debug_frame\00.rel\11\00\1Aa\12\00!nv&\00oaction%\01 \0F\94\00\0F\0FH\01q\1F$$\00\0F\FF\16$__cuda_sm70_shflsync_bfly_p\00$____wg_G\00\0F\00\09\00/261\00\19_1__28\EA\01\1Fo_param\F1\01.\0F\01\00\06\8CU\00\00\00\03\00\0A\00\01\00\11\AA\18\00,\0B\00\01\00\A1\D8\00\00\00\22\00\0A\00@=\0F\001\00@\01\07\000\00z\010\00,\09\00\01\00\11\B2\18\00,\04\00\01\00\11\E2\18\00,\07\00\01\00f2\00\00\00\12\10\90\00!\80>\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\130\C0\00\10\04\9B\00R\04<\00\00\00E\005\04\08\0F<\00&8\00H\00\04p\00L\84\80\80(o\00\10\08\15\00\81\16\85\80\80(\09\05\0C2\00\068\00\22\18\00\01\00\22p\00\01\00\048\01\1308\011\04/\08\06\01\10&\0C\00R#\08\00\03\00\01\00'\04\12\0C\00\17\11\0C\00\12#0\00\04$\00\17\07$\00\05\0C\00!7\044\05q\015\00\00\04\0A\08~\01\A2`\01 \00\03\19 \00\04\17\A2\00u\04\00\18\00\00\F0!\10\00u\03\00\10\00\00\F0\11\10\009\02\00\0C\10\009\01\00\08\10\00\01\01\00\F1_\F0!\00\03\1B\FF\00\04(P\00\C0\12\00\00\00\13\00\00@\13\00\00\80\13\00\00\C0\13\00\00\10+\00\000+\00\00P+\00\00p+\00\00\90+\00\00\00:\00\00p:\00\00\E0:\00\00P;\00\00\C0;\00\00\10<\00\00P<\00\00\90<\00\00\D0<\00\00 =\00\00\04\1C\0C\00\F0+\00\00`6\00\00\C09\00\00\04\1E\04s\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\D8\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00\C8\00\01\001\02\00\00\C8\01*H\00\10\00\0F\01\00\FF\D6@$v\01\FF\8F\05\B1\FF\00\8E\07\00\C4\0F\00\19y\0D\18\00 \00!y\05\A1\0E\00\B9z\04\00\00F\00\00\B3\05\90\E2\0F\00Ey\00\00\10\14!\00 \80\03\10\00c$t\0B\FF\1F\00@\00\10\E2@\00\03:\04\B1%\00\00\00b\0E\00$t\1A\FF\01\00\04 \00\F1\04x\0A\FF\05\00\00\00\0D\16\01\00\00\E4\1F\00\12x\0D\0D@\00\B0\C0\8E\07\00\C6\0F\00$x\0C\03\A9\02 \0A\020\00\D0/\00\0Cz\00\0D\00[\00\00pR\F2p\00g\04$x\0A\0A\04p\00P\0Cr\00\0D\FF\F0\04`R\F4\03\00\E4\0F0\00\81\0C\00\\\00\00pb\F6\10\00@\12r\00\FF\D2\00\A2\FF3\8E\07\00\D6\0F\00G94\04\01\C0\00p\EA\0F\00\10z\02\00`\00B\FF\E0\FF\07\E0\00H\01\00\E0\0C\E0\00R\13\FF\00\00\80\C0\00\93\E4\0F\00\10x\02\02 \000\00\80\C8\0F\00\12x\02\02\80\E0\00!\C0\8E\10\00a\07r\02\02\FF\00@\00\11\00 \001r\0F\02\80\00\12\FC \00\00\B0\00\00\F2\05\E6pb\F0\03\00\DA\0F\00G\09\00\00`\0C\90\00@$x\02\0F\AB\05!\0D\0A\E0\00v\10Ey\02\00\F0\06\90\001\1Cx\00\01\00?p\F0\F0\A0\00\02\B2\0Cx\00\02\80\01\00\00pB\F8 \000r\11\FF\90\00\10\0D \00\96\D8\0F\00G\C9\00\00\90\06p\00\05P\002\E1\F0\03\E0\00a\10\0F\80\FE\FF\FF\E0\00\10\C4`\00%\16\FFp\01p\E4\0F\00$z\09\0C0\01\22\11\02\D0\00\90%v\02\09\00X\00\00\16\10\00p\CA\0F\00\81y\0E\020\00\C5\00\19\1E\0C\00\A2\00\00\10x\05\090\01\84\E4\0F\04\10x\07\09@\10\00\10\C6@\00*\04\05@\00%\15\04@\00u\E2\02\00%v\06\07 \00\86\E2\0F\08\10x\09\09`P\00g\00\10x\17\11\80\10\00E\81y\12\06@\00fb\09\00%v\08\A0\00\10\C8\C0\00\11\1B\C0\003\17\02\8E0\00%\17\080\00\10d0\00&\02\1Bp\00h\1F\04\10x\19\1B\F0\016\81y\14\E0\00 b\01\C0\00\16\190\00y/\00\10x\1D\1B@0\00\16\19\D0\00 b\03\D0\00\16\1D0\00j\0F\09\10x\1B\1B\D0\00F\1D\11\00\01\10\00<\81y\18\D0\00\06\A0\00\01\D0\00\11\1F\D0\00\12\1DP\00F\00\81y\1B\D0\00\11f\D0\00\1A\1F\D0\00*\1D\1F\D0\00\1C\1C\D0\00\1A\1D\D0\00*!\1F\D0\00\1C\1D\D0\00\1A!\D0\00*\1F\1F\D0\00F!\11\80\01\10\00<\81y\1E\D0\00\06\A0\00\01\D0\00\11!\D0\00\15!\D0\00\17\1F\D0\00\00p\00*\02!\D0\00%#!\D0\00\01@\02\17\02\D0\00\01`\02\1A#\D0\009#!@0\00*\05\040\00\17\060\00y\0F\01\10x!!`0\00\16\06\C0\00\10\220\00\18\08\00\01`\00\0Br\00\0E\0E \00\B1\80\F8\03\00\E4O\08\0Br\00\13\10\000\C0\FA\03\80\04A\08\C2\0E\0E\7F\042\00\80\06\10\01\16\13\E0\00 \A2\0E@\00!\15\15\1F\00P\FA\03\00\E4\8F@\00%\12\12P\00 \0F\02`\00\01 \00\B1\C0\FC\03\00\D2\0F\00\08\D2\15\15`\00!\00\00`\01U\0Br\00\17\17@\00\01\10\00$\15\120\00\10\C8\80\00!\12\12@\00\060\00&\14\14`\00\00\10\00(\12\170\001\D2\17\17@\00\060\00*\19\19`\00)\17\14`\00!\14\14@\00\060\00*\18\18`\00)\14\19`\00!\19\19@\00\060\00*\1B\1B`\00)\19\18`\00!\18\18@\00\060\00*\1C\1C`\00)\18\1B`\00!\1B\1B@\00\060\00*\1D\1D`\00)\1B\1C`\00!\1C\1C@\00\060\00*\1E\1E`\00)\1C\1D`\00!\1D\1D@\00\060\00*\1F\1F`\00)\1D\1E`\00!\1E\1E@\00\060\00*\02\02`\00)\1E\1F`\00!\1F\1F@\00\060\00*\05\05`\00)\1F\02`\00!\02\02@\00\060\00&\06\06`\00\10\01p\00\19\05`\00!\05\05@\00\030\00c\10x\11\11\00\02\F0\02\03p\00\19\06`\00!\06\06@\00\030\00P\0Cr\00\11\10@\06$b\FC\90\00%\13\13\A0\00\01\00\03\11\06\10\00\22\C0\F8\00\031\D2\13\13P\000\00\00\060\03`G\E9\00\00\90\F9A\07\11\83\90\06#Ay\8E\00\0B\10\073\11\0A\8E\A0\07C\02\00p\03 \00\13\E8\F0\06\13\00\F0\06\12\DA\E0\06'@\03@\00+t\03\C0\06\1B\0E\C0\06 \04\0E\C0\033\03\02\8E\E0\03*\02\04\C0\06%\06\0E`\04\01\C0\06*\08\0E\C0\06*\06\06@\00*\19\06\C0\06%\08\08 \00\01\C0\06/\14\0E\C0\06\0B*\0E\08 \05%\14\14@\00\01 \05\1B\10\C0\06%\15\140\00\10d\90\00'\16\10p\00V\04\10x\04\10 \05k\1F\00\81y\16\16\F0\04\16\040\00\00 \05*\04\10 \05*\07\060\00\17\040\00\00\C0\00'\08\10 \05[\01\81y\04\04 \05\08\00\01\07\F0\02\15\F0 \05\18\02 \021\82\02\020\02/\00\00 \05\05%\19\19\D0\02&\8F\08p\05\14\F0 \05$\02\19p\05\00 \051\C2\19\19`\00\03p\05\06`\05\14\F8\B0\02\15\19\A0\05\00\E0\02J\82\0E\0E\190\00&\16\16`\00\16\00\80\05\030\00\14\C2\80\05\050\00*\07\07`\00)\15\16`\00:\16\16\150\00&\04\04`\00\00\A0\03)\16\07`\004\07\07\160\00\11\C80\00)\07\04P\007\04\04\07P\00\0E\D0\09\19\11P\08\06\A0\03\14\F8\80\01$\04\13P\00\00\80\01D\C2\13\13\04P\00\1E\C4\90\03\00\F0\03\01\C0\0A1\16p\00p\03X\89\00\00\A0\01p\03\1B\09p\03\1B\08p\03\11\02\10\02\16\09\A0\02/\08 \C0\07\05%\A2\0Ep\02\15\09p\02/\06\08\C0\07\06%\E2\0E\90\03\060\00?\08\08`\C0\07\0B\06\90\00\10\CC \00(\09\08\F0\02\06\B0\02\1B\F8\B0\02*\FA\03\90\05Q\F0\03\00\D2\8F\80\05\02\C0\02/\80\06\80\05\0B\04\C0\01\04\80\05\050\00*\09\09\D0\02\03\80\05\1B\FA\80\05\05\10\02$\06\09@\03\01P\00!\09\090\00\00@\03\10\CA\00\0C\11\13\00\0C\13\09\E0\0DIAy\01\00\D0\01Sz\00\0F\00[\90\0C\03\00\0D#\90\04 \00\12\D8\A0\0C)p\040\0D\010\005\FF\E1\FF\90\05#`\02 \00\0C\A0\0C\03\90\0C\1B`\A0\05& \02P\00c$v\0E\FF\00[0\02\1B\E2\C0\02\10\C8\C0\01H\0E\0E\A0\FF\90\0C\1B\10`\02\11\02`\02\16\0F`\02\10\02\D0\013\10\02\8E\F0\01\1C\08\90\0C\15\0Fp\02\02\90\0C\18\0F\D0\051$z\05P\001\05\02\8E \001$z\07\10\00\12\07\10\00\05\B0\0C\15\10p\00%\06\07\10\00\1E\E4\90\02W\10x\09\0F``\00\09\80\02\16$\C0\00\01\80\02*\C8\1F\C0\00\020\05+\08\08\80\02\19\08\B0\03(\08\08p\02K\81y\13\020\05*\05\05P\0A$\08\05\D0\07\0F\A0\02\00E\10x\0F\0F\90\06\12\CE \08\01p\00!\00\00\F0\00\0F\10\08\10/\0F\0E\10\08,.\10\FE\10\08\0F\80\02\09#0\01 \00\12\E6p\02\1B p\02(\00\01\A0\04\1F\04@\02\12\15\04@\02\19\06P\01\09@\02\1B\CA0\02\16\C8 \02\16\04\C0\01%\06\06\D0\03\02\C0\01\18\06\00\01\22\82\06\10\01$\00\00\C0\01\18\04p\00\09\10\03\01\D0\05\16\0F\B0\02\09`\01\1B\F8`\01\06\D0\05\01p\00\0F\D0\05\05\04\00\04\07\D0\05.\80\00@\09\0F0\01\0D\15\030\01\16\040\01+\A4\0E\E0\06\02\00\01\19\04\00\01\12\04\80\06\0B\B0\04\1F\04\B0\04\04aGy\00\7F\12'\10\00\10\0B\10\00\FB\00\89\7F\02\13\00\1F \0C\00\00\0E\00\00$\0E`\08\1F\1F`\08\0B\10\CA@\00X\03\02\00\1F@@\00*\03\03@\00)\02\03@\00!\03\03P\00\05@\00X\04\03\00\1F\80@\00*\04\04@\00\1C\03\F0\00\01P\00\05@\00g\05\04\00\1F\00\0D@\00*\05\05@\00)\04\05@\00!\05\05P\00\05@\00a\06\05\00\1F\00\0E@\00ab\00\00$r\02@\011\05\00\8Ep\04(\0B\A2\E0\03,/\08\A0\05T\0B\A2\00\02\060\02\00@\08#\A8\00 \00 p\02\10\02\00\A0\02\14\02@\02q\CA\0F\00\88\A3\00\0A0\00\00\86\16i\E4\03\00Ay\00\A0\01\22Hy\88\18\04\10\066\1D{\00\01\00\12\EC@\14\11P!\0E\02 \00\00\90\13/0\17\90\13\0B&\B0\07\E0\03\0F\80\13\1F\16\03\80\13\010\13\01\1F\00#\80\FF \01\00\90\13\11\03\B0\04\11\F8\00\01@\0Cz\00\03\80\00\01\10\07\000\14VI\00\00 \07\A0\00\09@\00\00\10\03&r\04`\13\01\A0\0B\11\18 \03\01\00\04\1B\E4@\13\10\C6\C0\00%\11\18\C0\00w\E2\0F\04%v\12\18@\10X\04\10x\10\18\90\06G%v\0E\11 \00V\08\81y\06\12 \04\10\22`\06\15\18`\06\01\A0\06*\14\100\00%\05\0E0\00 \E6\10 \0C\17\09 \00\00\E0\0B\06@\0C h\090\07\18\16\10\121\06{\19\90\01\C1\00\90 \00\000\0E\00\08s\19\00 \0B\00\F5\0D $\1E\A0\07Q\19\FE\FF\FF\0F\80\00`\CC\1F\00\05s\0F\8A\02\01\AC\19 \00d\90\025\0E\FF\FF@\04\15\1F\E0\03\10\0F\D0\0Dp\C8/\00$z\13\13`\00\22\FF\02@\01@'r\0F\0F\A0\03\22\0E\00\90\09T'r\12\0F\18@\00\16\C8 \04%\12\0Ap\01\01@\00\13\18p\01V'r\14\0F\11p\01P\01\0Cz\00\18 \00$p`\10\15\02\90\00\15\14@\00%\11\0E\A0\0A\01@\006\0E\0F\10@\00\00\10\02\12\11@\00 \FC\03\10\000\10J\18P\00\11\80\F0\00\16\E2\90\00%\0E\0A0\00\03p\00\12\FAp\00!z\10\A0\00\04\D0\07T\10H\12\12\01\B0\01\10\E2p\007\16\0F\09p\00E\10j\11\11`\00\02\90\02\1A\10\C0\00\11\18p\00\13\16\C0\00'\10XP\00\18\E4\B0\00\04\80\00%\0F\18`\08\00\80\005h\14\140\00\00@\00;J\10\10p\00\15\0F\F0\00\10\CA`\00\070\00\09\90\00\020\04I\10H\0E\0EP\00;j\0F\0FP\00\01\E0\09\22pP`\032\12z\09\F0\09\133\F0\00\17X@\00\09\80\00\02`\00V\07r\12\09\12 \07g\04\07r\14\09\14\10\00H\10h\16\16P\01T$x\0F\12\04P\01\10\E2@\00!\0E\09\90\02\03`\08W$x\10\14\04p\01@\12x\0F\0F\C0\11\22\FF\E2\00\02,\10XP\00(\0E\0EP\00H\12x\10\100\00W\07r\09\09\16`\00T\84y\0F\0F\00 \05u\A2\0E\00\12x\0E\0E0\00\01\E0\18(\09\09P\00E\84y\10\100\00\10\E80\00%\09\090\00\01 \00%\0E\0E \00\10j\10\00%\09\09\10\00!\22\0E\00\18S\9D\98\BB;\FF0\07\84$t\15\FF\00\00|C`\00W\10x\04\04\80\A0\01E\0Cr\00\04\F0\04\84\E2\0F\00!r\0F\06\0F\F3\07P\C8O\00#t\0F\00B\00?\13 \F0\19T!r\10\05\10 \00 \C6\8F \00t\06\01\00@K\15@ \004\07\07\0E \00u\E2\0F\02#t\05\10@\00\B3\E4\0F\08!t\0E\06\7F\00@\CB \00X\00#t\12\07`\00G#t\05\05P\00e\08!r\08\08\09P\00\C4\1F\00#x\0E\0F;\AA\B8?\0E\080\00)\12\12\80\00:t\11\05`\00)\09\08`\00\94x\0E\0F`p\A52\0E\000\00)\14\120\000x\11\10`\00\15\11`\00%\0F\09`\00\10\C4 \00 \14\07 \00&\14\080\00\00P\00\22\11\00\10\00W\08s\0E\00\0E\90\01J!t\09\0F`\00 \14\070\00\01\B4\02\10\C6\10\00 \07\08P\00\22\09\08\10\00T\08s\10\00\11@\00@b\0E\00#\B0\0ECp\A52\07@\00@$x\07\06\0F\01\13\FF\10\05W\08s\09\00\14`\02E$x\05\05 \00\00p\00Cr\02\07\0E\C8\1D\00\90\05X\08s\11\00\080\02:x\12\120\004\05\05\100\00!\E2/p\13\05 \00\11\C6 \00!\12\09P\08\03 \02Dr\02\02\11\10\00\22\E2\1F`\0B\09\C0\13\0A\B0\09\01P\0E\13\0E\10\00\13\E2P\0E\18\0E\F0\07,\04\03\D0\0B/\A0\07P\0E\0A\1C\04P\0E(`\07P\0E\12\04`\04\0FP\0E\06(\04\04P\0EV\10x\09\03 \F0\03F\04$t\18\B0\07\01@\035\05\03@ \00Q\1F\04$z\12\E0\07\01\F0\0A\01 \0E\16\09\C0\0D\01\80\07C\0E\12\00X\90\06\01@\00%\11\03\A0\07\01\D0\07'\14\09 \00F\08\81y\07\A0\07%\A6\00\E0\13\12\05 \00F\04\81y\08\A0\07 \22\07\B0\06\18\0C\B0\06\04\F0\13\22\18\02\10\06G%v\18\11`\00\00\C0\0C\06\D0\07 h#\A0\0E\18\18\E0\106\06{\13\E0\07*b\0E\B0\07\84\E2\1F\00\10x\03\03\80\C0\00\01\80\02\10\13w!\03\00\08l.\00\10x\1B\13\00\08\14\1B\00\08\10\E4P\00\17\15\F0\07j\8F\00$z\15\15\F0\079\16\0F\15\F0\074\0E\16\12\80\00\00\A0\07&r\0F`\07\01\A0\078\13\16\10 \00!z\120\06$\12\02 \00 \0F\16\B0\02\07\C0\07\1A\12@\07\16\14\90\00%\E4\0F\A0\00%\13\0A\E0\1C\18\14\D0\01E$z\10\15\C0\07\02P\00\1A\09\10\08(\12\12\F0\06-\10H\D0\06\15\12\D0\06\01\F0\074\12\16\11\A0\00\1B\E2\E0\07\16\C6\A0\00\22\12\0A \00I\10j\09\09`\005h\0F\0F`\00\11\E2\C0\08\18\14\10\026$t\14\00\06\1F\E4`\07\02\15\09\90\00\1F\C4\00\08\02\15\11\E0\00\00@\00;H\13\13@\00\06\D0\07\01 \08/\0F\0F \08\0A\0B\D0\086\12z\10\00\08\00 \00\1Ah\C0\08A\07r\0E\10\1F\04!\00\00\B0\0ET\07r\0F\10\0F\10\00\010\08)\13\13\00\01)x\09\A0\07\0D\00\098x\0F\0F \00A\07r\13\10\C0\02\02@\08\0Ap\07\1B\E4\10\08\01@\00%\13\13@\00\11\E4\90\07$\09\00P\05*\A8\0E\80\09\0C\00\08\10&p\007\12\10\12\D0\00I\12x\10\13`\00\08\C0\07\00\C0\00\030\0E\04\A0\07\00\B0\05\0A\80\00\01o\19\02`\00\10fP\00%\12\12P\00\10\CC \00\02/\1C\05\90\06Ir\0E\07\0E\E0\07*\07\0E\E0\074\08\08\0F \00p\C6\0F\01#t\07\07\00\07!\14@ \00H\08#t\0FP\07d\08!r\11\06\110\00y\E4\0F\02!t\09\07\F0\068t\06\0F@\00 $x\0F\17\07p\06:t\10\11P\009\05\05\12\F0\07%\09\0E \07\01\B0\07\1B\0F@\08*\10\10\A0\00'\12\05P\00\01@\00\00@\07$\09\00\10\00\11\0Fp\07$\0F\08P\00*\0E\10P\00'\12\12P\00Y\00$x\06\06\B0\00!x\0F\90\07%\0F\00P\08\17\11\90\08I\04!t\08P\08\00\A0\07\14\09\80\01!$\0E0\00\08\80\08@#x\08\05@\00\01\D5\13\12\C8\10\00\00 \00\15\08@\007\0F\00\0F\10\08E$x\05\10\90\00\1B\CEp\08\85\A2\0E\00#r\07\07\09\C0\07\10\1F\C0\07\06\E0\07\01\B0\057\08\00\08\10\021#r\06\E1\09\013\07!\C8/\00\08\11\0E\9D\02\06\E0\07\1B\08\E0\07.\D0\F8\90\13\0F\C0\07\09)0\04\90\13\1D\04\90\13\17\04@\00'x\05\80\07h\1F\00$t\08\FF\F0\02\04\10\07\06p\07\16\11 \07\01\00\07\11\06\00\07\22\08\02P\0EE%v\08\11\10\00\01\00\07\16\05p\15 \A8\00\A0\12\18\08\A0\1B6\06{\12\00\07!\22\0F\F0\06\18@\00\15Es\12\00\12\F0\06\00 \15&\0E\12\F0\06*\0F\01\F0\0E\1A\09@\074\E4\0F\01 \0E\02\00\07.\0F\02\F0\0E\16\13\F0\0E\00\D0\06&t\0E\10\04\01\D0\064\06\13\10P\00\10\C8@\0F\11\09P\00\10\06P\00u\E4/\00'r\07\13P\06\01\10\07%\10\09\B0\06\00@\01!r\080\00#\07\0A\10\00)t\0F \0C\06\B0\05#\FC\03P\06\15\08P\06\00\F0\0C&z\08\B0\05\1B\C8`\05\10\CE\E0\05\0A@\06E\10h\06\06\10\05\1B\E4\E0\06\00 \00?Z\11\11 \0E\03\02p\06\06`\00\12\F0\A0\06\10X\90\04\06p\15'\10H`\00\01\F0\11!\06\08\BC\02\03\80\16'\10\080\00\1B\E4\00\0A\03\E0\03\14\040\01\01\10\06!\07\08\0C\03!\00\00P\00I\12x\06\06\A0\055x\07\070\00\10\CA \00(\08\07 \00\22\84y_\13\04\10\06E\84y\09\08\10\00\10\E2\80\05\10\050\17\08\80\05\01\B0\04$\0E \80\059\09\04\09`\0D\00\C0\00D@K\0F@\A0\04%\0E\090\00\10\C6\C0\04\18\04p\05X\04#t\0E\0E0\00\09p\05\10\E4`\04\11\04p\04$\04\08\00\05%\06\0E@\00\12\C6 \00\00\80\04\22\04\00 \00@#x\06\090\00$\06\08P\00%\05\0EP\00\12\C6 \00\000\00\13\060\00T\08s\04\00\04\F0\00\01 \127\06\00\06\C0\04\00\90\0C\14\04\A0\04 \C8\1F\10\00$\05\06\10\00/\C6/\90\16\01\1C\03\90\16&@\02 \00\0E\C0\17\16\03 \04\01\00\0B%\04\03\80\179\CC\1F\00\90\1Ef\A2\00\00\06{\09\F0\03\10\E2\A0\0A4\06\FF\FFP\00'\E2/\90\02\03@\08&t\050\03&\CA\1F\E0\05\01\10\04 \8E\00\D0&\0A\00\0B4\07\00\07\10\04%$\0E\C0\0A\01\80\03\00\C0\03:z\0F\0F\00\041\06\07\0F\80\05\02\B0\05T'r\06\06\03\90\00\16\C8\C0\03#\06\0A\10\007z\03\08\10\05\010\05\050\04\17\C6@\01!`\F0p\05I\10\0A\03\03\A0\03\1D\08\A0\03\15\03\A0\03\1B\DA`\03\00\10\09)\DA\06\00\04\00 \03\14\07\B0)\1B\CAP\03\10\CA\10\034\03\06\00\00\02\10\A4\00\03\10\03p\10\07\00\03\01\7F\103?\08 \00\070t\04\04\C0\02\22\05@\10\00E!t\08\04\A0\02\02 \07\1B\030\07\18\030\07E$x\03\04\A0\02\1B\CA\F0\06\11$\80\02'\03\08\10\0F\0FP\18\01(\E2\10P\18%\03\02P\18\02\C0\00\04\F0\17C\00\00\CAO\F0\17\17@ \00\11\04\B0\09\06 \00&\05\04\10\18\10\1E\00\04\02\C0\17\01 \00\14\1F\B0\17\03\F0\17Q.\00!r\06\10\03\06 \00%\07\06\D0\17`d\00\00!\A2\07\01\10\03 \00\11/\80\17/\07\80\80\17';M9\00\C0\07\11\00\10\03$\FF\E0\C0\07H\00\00@\0Ap\174\00\00 \10\02\01\00\02\00\FD\16\08p\17\00\FC\01\010\00\03p\17%\00\00p\17\02p\12\11\0D\80\05$b\F2\E0\05\01p\00\05`\17V\19\00\00\C0\09\90\00\04p\1D\11\0D\E0\02\11op\17\05`\02\11\C6\A0\07\15\07\90\00\01@\17\11\0E\80\1D$\02\02\00\04%\10\03\10\00\01\00\08\16\04`\0F \A8\02\E0\16\18\10P\0FY\10x\09\07@\10\1E&\05\07\B0\0F\00\90\1D*\12\09`\00*\0A\05`\00\07\90\17zj\01\00\81y\0A\0A@\17\17\14`\08\0C`\0F\00p#4\0D\0D\80p\00\01\B0\099\14\00\14`\17,\15\14`\0F\19\15`\0F\02\E0+\06`\0F\01`\07\07`\04\11\12\10\17\07`\0F$\12\07\80\00\01`\04\1B\10`\0F)\0F\12\80\048z\0B\10\00\1F\040\00\15\0F0\004\11\12\090\00\02P\08\12\0BP\04\12\F6\D0\0E8z\10\10\B0\04W'r\12\12\05\D0\01\06 \08\19\F2\A0\0F$\11\0Ap\00\16\16\10\0F\00\C0\049:\0B\0B\D0\04\1D8\E0\0E\15\0B\D0\04\11\E2\C0\00\06\D0\0F\18\E2p\08\12\F6`\08\1A\1A\C0\08\06@\00*\FA\03\C0\08\13\F4P\00'\10\16\A0\11:\00\10\18\00\0F6\12z\140\05\00\A0\00\08\10\10\18\C4P\00\02\10\09R\07r\0E\14\0E@\03\11\050\00\1BZ\E0\00\17(`\00\0F\90\16\00\06\B0\00\13\F2P\004\0F\14\0FP\00\01p\0E\16\13\80\16\1B\E2`\0A\1B\E2\00\10\0F0\0F\00H\10X\11\11\80\00T\84y\15\13\00`\05\1B\E2 \01\0F@\0F\00f\84y\0B\13\00\80\80\0C\09\00\19\00\80\01\1C\18`\00%\17\0F`\00\10\E6\D0\004\11\14\11\D0\00\04`\0F\04P\00Wd\0E\00\10(P\00\000\025x\11\11\C0\00\01\10\014\12\14\12@\00\01\B0\06\16\10P\0F\11\E2@\0F\15\11\B0\00\1B\C8\E0\17\01p\00%\19\12\90\00\1Bh\80\0F\22\E8\0E\C0\17\16\80\10\01 \0Bx\C56\94\80~\00B\FC\03\00\C4\1F\10\002\00\00\E2\A0\01B\0Bx\00\0F \00\000\01#/\04\10\00A\00\00\E2\F6P\0BB h\0B\0B\866\11@P\05! \98\10\00\11K\10\00u\E2\0F\00 (\0F\0F \00\01\F0\06\10\0Ba\00\020\04\00 \008\B8\0F\0F0\00Q!r\15\04\15a\00\00 \00\11O@\09\08\00\0CV!r\17\08\17\D0\0FW\01#t\13\15\A0\0AX\08\84y\08\12\C0\01E#t\1B\17 \00\00`\0A\10t\90\114@K\040\175t\1B\1B\10\00\10\E4\D0\07%\14\13\D0\07\01p\007\19\06\190\18E!t\16\1B \00\01\B0\0A(\14\15p\17 $x\CF $\80\00\B0\00@#x\16\17 \00&\16\080\00\05`\17\01`\00%\11\0A\A0\10|\E2\8F\00$x\1B\1B@\00\000\00\13\160\00X\08s\15\00\14\B0\0A8t\17\19\00\01H#t\13\11\80\0BE#t\17\17\E0\00\02 \00\06\00\01\10\E4P\007\16\00\16\A0\0FI!t\0A\17\00\01)t\04\10\01E$x\17\17\A0\00\01\D0\00 \0A\19\E0\00\22\0A\08 \00H#x\04\11\C0\0B\00\90\12\08\D0\00R r\06\06\15\EA7\01\C0\17\00@\00\00\E0\00&\0A\00@\00\08\E0\0B\00\F0\00\15\0F0\02e\02\00 h\06\06P\02\02\C0\02\12\08\A0\02\14\F8\C0\02\15\10\D0\02\01p\005\16\1B\16p\00\14O0\002\00\00\E20\0BH \98\06\06\80\021\08s\0A;4\04@\02K (\16\16p\00\12\10@\00\13\F4`\007\0F\0B\06\D0\00\01`%\16^p\07H \B8\16\16`\00\09`\08\00 \008H\08\08`\00\09\A0\0Cxb\0E\00 h\10\10 \00F r\15\15\D0\00h\8F\00 \D8\08\08`\001\86y\000\11\CA\04\19\10\0C\00\E6\05\00 \A8\10\10\E0\00\03`\19\00P\01\84\22\07\00 r\0A\17\0AP\00u\C8\1F\00 H\0A\0Ap\00\12\C6\F0\19\15\100\00D\0E\00 rp)\02\F0\00H \D8\0A\0A`\00\01\C02\04\00\01z\E4\8F\00 h\13\13\C0\00%\11\11p\00z\C6\0F\01 \A8\13\13@\00%\0A\03@\00\01\C0\08(\02\05P\01T\86y\00\0A\15\E0\00E\E2\05\00  \15\00P\00\10\1F \00$\08\11 \00 \E8\05\10\00'\02\130\00hG\99\00\00@\F6 \12\01=\00\05\D0\099M\09\00\10\009\06{\04\80\0D\00p\0A#0\03 \00\09\A0\07\03P\06\09\80\10\16O\D0\01\00@\01\10\A4\80\0D&\02\04 \09uO\00\05s\03\00\02 \09\15\04\10#\02`\03QO\00$r\05\10\00\10\03P\08 \C8\9F\C0%\1A\050\091\09\03\050\0C\02\10\00\00\E06\15\04@\00\00\00\08\11\04\A0\0A\16\00P\01\01\A0*3\0B\02\8Ep\0E\19\02p(9'r\05\F0\1D\09P\0B\01\F0\17\11\07\90\00\13\05\D0\116$t\0A\A0\05\01p\00*\06\07P\22\1C\07\F0\0D\1A\06\F0\0D)\06\06@\088\08\05\05 \07\060\00\12\F20\00\17\18 \00\01\D0\0B\12\05 (#\00\000\07(\05\05\E0\0DE\12x\03\05 \07\84\CA\1F\00\84y\05\03\00`\03\10\E8 \07F\06\03\00\80\80\0DB\0Bx\00\06@\04\00\C0\1B#\1F\04\10\00\02\10\07W\D6\0F\00 \08\80\04\1B\C8@\04&\CC\0FP\10#\10\00\E0\051\02\02\05A\00\04P\0E\01?+\11?\15\0C\00 \0E\03P\1E\22\0A@\10\008!t\07 \1E\19\040\1D\01\D0\10 \07\02`\05#\07\08@\005x\07\02\80\1D\00\E0(\22s\02\D0\0F\03\D0\00$ r\B0+\11@ \15\01\00\02\13^\00\02\10\C6\D0\00(\05\05\A0\03\09\F0\0C\00 \005\98\05\05\E0\00\00\F0\00#r\05@)\02\D0\07\00`\03\15\05`\03\11\01\00\11/p\FD`\03\08\0B\10\00\04\F0\02\13\13p\02\81\02x\04\00\10:\00\00\F5\05\12\E2\D0'\14\01\D0\01E\C4\0F\00D\80\03\11\C0@\00\00P(\14\07p&\00\C0\17Gt\03\FF\02\C0\02\06\E0.\01\90\1DR\04\08r\13\070\04\02\90!\00p\00\14\80p\00\00\10\07$r\02 \00\14\00p\00)\C0\02p\00\12\02p\00\15\F0p\00\1F\04p\00\02\00\90\06\02P\00\01?\00\00F\16\03p\00\1B\F0p\00\03 \00\14\02p\00\1FPp\00\0C\1F\08p\00\05\01\E0\00\09p\00\01P\01#`;p\00\1F\C4p\00\03/\E0\01p\00\0B\1F\10p\00\17\01\E0\00/\D0;\E0\00\0B\17pp\00\04\D0\12\13\07`\00fGy\00\00\F0\D7@\02\09\00\02\03 \02& <\D0\00\00P\00\15 P\00\11?p\03\01\A0\00\16\00@\02\1B\02@\00\1B`@\00&\E0\00\90\00\0F@\00\01\1B\04@\00\1B\A0@\00\1F\A0@\00\0C\1B\08@\00\1B\E0@\00\19`@\00\1D\06@\00\1B\10@\00&0=@\00\04@\03\17\06 \03\17\10P\00\00P\01(`\EEP\01\00 \14\06P\00fHs\00\00\1A\00\A040\89s\07@*A\0C\0B\00\0E@\04`Py\00\04\80\C2@\00R\C3\03\00\EA\1F\A0\01\13\FFP\00f\C0\0F\00\18y\00\01\00\0F\10\00\D0\0F\01\00-\00\AB\02.\03\00\01\00\22@\00\01\00=%\01\000\00\08\01\00\1F\0B@\00\04\13e)\00\1F\F1@\00\0C\00\DC\04.\02\00\01\00\13XU\00$\C0\00\AD\03\00T\02\02G\06\00\01\00\03\948/\00\E6T\00\00\00\01\00\02\A5!\02\00E\0F\80\00\09\11)\06\00\05\EFC\05\01\00\13\F0@\00\11T\06\00\06\E4\00*\04\00\01\00/[\00@\00\03\12D\95\066\00\DC\00@\00\1F\0A@\00\00\00-'\1F\0B@\00\00\11 \B5\07\0E\C0\00\1B\08\08\00\11\F3\9C2\0E\01\00\13\F8@\00& \00\80\00\17\048\00\02\94\033\00\00\B5\14\01\0C\84\01\13\18\F5\03\00|<\04\01\00\0F\C0\00\01\132@\00\15\06R\00\03\01\00\03\95\00&\80>\80\00j\07\00\00&\80\00\01\00\13\87\94\00*\03\00\01\00\040J\01]\00\0F\80\00\08\13\06l\1D\04hJ\0C\01\00\1B\A8\08\00\17\08W\00\17\05\E8\00\0C\01\00*h@\08\00\088\00\18\06\A0\00\0F\01\00\05\00)\00\B0\00\00\00\08\00\00\00\00\00\00\00\00\00"
@alloc___gpu___pvoid_i64___pvoid = internal constant [32 x i8] c"alloc___gpu___pvoid_i64___pvoid\00"
@ral_recv_input___cpu___pvoid_i64___m3df32 = internal constant [42 x i8] c"ral_recv_input___cpu___pvoid_i64___m3df32\00"

define void @disc_ral_call(ptr nocapture readonly %0, ptr %1, ptr %2) local_unnamed_addr {
entry:
  %3 = load ptr, ptr %0, align 8
  %4 = getelementptr ptr, ptr %0, i64 1
  %5 = load ptr, ptr %4, align 8
  %6 = load ptr, ptr %2, align 8
  store ptr %3, ptr %6, align 8
  tail call void %5(ptr %3, ptr %1, ptr nonnull %2)
  ret void
}

define void @main(ptr %0) local_unnamed_addr {
  %2 = alloca %0, align 8
  %3 = alloca [3 x ptr], align 8
  store ptr %2, ptr %3, align 8
  %4 = getelementptr inbounds %0, ptr %2, i64 0, i32 1
  store i64 0, ptr %4, align 8
  %5 = getelementptr inbounds ptr, ptr %3, i64 1
  store ptr %4, ptr %5, align 8
  %6 = getelementptr inbounds %0, ptr %2, i64 0, i32 2
  %7 = getelementptr inbounds ptr, ptr %3, i64 2
  store ptr %6, ptr %7, align 8
  %8 = load ptr, ptr %0, align 8
  %9 = getelementptr ptr, ptr %0, i64 1
  %10 = load ptr, ptr %9, align 8
  store ptr %8, ptr %2, align 8
  call void %10(ptr %8, ptr nonnull @ral_recv_input___cpu___pvoid_i64___m3df32, ptr nonnull %3)
  %.fca.1.gep = getelementptr inbounds %0, ptr %2, i64 0, i32 2, i32 1
  %.fca.1.load = load ptr, ptr %.fca.1.gep, align 8
  %.fca.3.0.gep = getelementptr inbounds %0, ptr %2, i64 0, i32 2, i32 3
  %.fca.3.0.load = load i64, ptr %.fca.3.0.gep, align 8
  %.fca.3.1.gep = getelementptr inbounds %0, ptr %2, i64 0, i32 2, i32 3, i64 1
  %.fca.3.1.load = load i64, ptr %.fca.3.1.gep, align 8
  %.fca.3.2.gep = getelementptr inbounds %0, ptr %2, i64 0, i32 2, i32 3, i64 2
  %.fca.3.2.load = load i64, ptr %.fca.3.2.gep, align 8
  %11 = shl i64 %.fca.3.0.load, 32
  %sext = mul i64 %11, %.fca.3.1.load
  %12 = ashr exact i64 %sext, 32
  %13 = mul i64 %.fca.3.2.load, %.fca.3.1.load
  %14 = shl i64 %.fca.3.0.load, 2
  %.idx = mul i64 %14, %13
  %15 = alloca %.1, align 8
  %16 = alloca [3 x ptr], align 8
  store ptr %15, ptr %16, align 8
  %17 = getelementptr inbounds %.1, ptr %15, i64 0, i32 1
  store i64 %.idx, ptr %17, align 8
  %18 = getelementptr inbounds ptr, ptr %16, i64 1
  store ptr %17, ptr %18, align 8
  %19 = getelementptr inbounds %.1, ptr %15, i64 0, i32 2
  %20 = getelementptr inbounds ptr, ptr %16, i64 2
  store ptr %19, ptr %20, align 8
  %21 = load ptr, ptr %0, align 8
  %22 = load ptr, ptr %9, align 8
  store ptr %21, ptr %15, align 8
  call void %22(ptr %21, ptr nonnull @alloc___gpu___pvoid_i64___pvoid, ptr nonnull %16)
  %23 = load ptr, ptr %19, align 8
  %24 = icmp slt i64 %sext, 12884901888
  %25 = icmp sgt i64 %.fca.3.2.load, 1023
  %26 = icmp slt i64 %sext, 25769803776
  %27 = and i1 %26, %25
  %28 = icmp sgt i64 %.fca.3.2.load, 511
  %29 = icmp sgt i64 %sext, 21474836480
  %30 = and i1 %29, %28
  %31 = or i1 %24, %27
  %32 = or i1 %30, %31
  br i1 %32, label %33, label %71

33:                                               ; preds = %1
  %34 = alloca ptr, align 8
  store ptr @main_kernel_blob_gpu.binary, ptr %34, align 8
  %35 = alloca %.5, align 8
  %36 = alloca [3 x ptr], align 8
  store ptr %.fca.1.load, ptr %35, align 8
  store ptr %35, ptr %36, align 8
  %37 = getelementptr inbounds %.5, ptr %35, i64 0, i32 1
  store i64 %.fca.3.2.load, ptr %37, align 8
  %38 = getelementptr inbounds ptr, ptr %36, i64 1
  store ptr %37, ptr %38, align 8
  %39 = getelementptr inbounds %.5, ptr %35, i64 0, i32 2
  store ptr %23, ptr %39, align 8
  %40 = getelementptr inbounds ptr, ptr %36, i64 2
  store ptr %39, ptr %40, align 8
  %41 = alloca %.6, align 8
  %42 = alloca [14 x ptr], align 8
  store ptr %41, ptr %42, align 8
  %43 = getelementptr inbounds %.6, ptr %41, i64 0, i32 1
  store ptr %34, ptr %43, align 8
  %44 = getelementptr inbounds ptr, ptr %42, i64 1
  store ptr %43, ptr %44, align 8
  %45 = getelementptr inbounds %.6, ptr %41, i64 0, i32 2
  store i64 1, ptr %45, align 8
  %46 = getelementptr inbounds ptr, ptr %42, i64 2
  store ptr %45, ptr %46, align 8
  %47 = getelementptr inbounds %.6, ptr %41, i64 0, i32 3
  store ptr @main_kernel_main_kStitch_divide__13_1_0___1b1r_kernel_name, ptr %47, align 8
  %48 = getelementptr inbounds ptr, ptr %42, i64 3
  store ptr %47, ptr %48, align 8
  %49 = getelementptr inbounds %.6, ptr %41, i64 0, i32 4
  store i64 %12, ptr %49, align 8
  %50 = getelementptr inbounds ptr, ptr %42, i64 4
  store ptr %49, ptr %50, align 8
  %51 = getelementptr inbounds %.6, ptr %41, i64 0, i32 5
  store i64 1, ptr %51, align 8
  %52 = getelementptr inbounds ptr, ptr %42, i64 5
  store ptr %51, ptr %52, align 8
  %53 = getelementptr inbounds %.6, ptr %41, i64 0, i32 6
  store i64 1, ptr %53, align 8
  %54 = getelementptr inbounds ptr, ptr %42, i64 6
  store ptr %53, ptr %54, align 8
  %55 = getelementptr inbounds %.6, ptr %41, i64 0, i32 7
  store i64 256, ptr %55, align 8
  %56 = getelementptr inbounds ptr, ptr %42, i64 7
  store ptr %55, ptr %56, align 8
  %57 = getelementptr inbounds %.6, ptr %41, i64 0, i32 8
  store i64 1, ptr %57, align 8
  %58 = getelementptr inbounds ptr, ptr %42, i64 8
  store ptr %57, ptr %58, align 8
  %59 = getelementptr inbounds %.6, ptr %41, i64 0, i32 9
  store i64 1, ptr %59, align 8
  %60 = getelementptr inbounds ptr, ptr %42, i64 9
  store ptr %59, ptr %60, align 8
  %61 = getelementptr inbounds %.6, ptr %41, i64 0, i32 10
  store i32 0, ptr %61, align 8
  %62 = getelementptr inbounds ptr, ptr %42, i64 10
  store ptr %61, ptr %62, align 8
  %63 = getelementptr inbounds %.6, ptr %41, i64 0, i32 11
  store ptr null, ptr %63, align 8
  %64 = getelementptr inbounds ptr, ptr %42, i64 11
  store ptr %63, ptr %64, align 8
  %65 = getelementptr inbounds %.6, ptr %41, i64 0, i32 12
  store i32 3, ptr %65, align 8
  %66 = getelementptr inbounds ptr, ptr %42, i64 12
  store ptr %65, ptr %66, align 8
  %67 = getelementptr inbounds %.6, ptr %41, i64 0, i32 13
  store ptr %36, ptr %67, align 8
  %68 = getelementptr inbounds ptr, ptr %42, i64 13
  store ptr %67, ptr %68, align 8
  %69 = load ptr, ptr %0, align 8
  %70 = load ptr, ptr %9, align 8
  store ptr %69, ptr %41, align 8
  call void %70(ptr %69, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %42)
  br label %116

71:                                               ; preds = %1
  %72 = add nsw i64 %12, -1
  %73 = lshr i64 %72, 3
  %74 = add nuw nsw i64 %73, 1
  %75 = alloca ptr, align 8
  store ptr @main_kernel_0_blob_gpu.binary, ptr %75, align 8
  %76 = alloca %.2, align 8
  %77 = alloca [5 x ptr], align 8
  store ptr %.fca.1.load, ptr %76, align 8
  store ptr %76, ptr %77, align 8
  %78 = getelementptr inbounds %.2, ptr %76, i64 0, i32 1
  store i64 %.fca.3.1.load, ptr %78, align 8
  %79 = getelementptr inbounds ptr, ptr %77, i64 1
  store ptr %78, ptr %79, align 8
  %80 = getelementptr inbounds %.2, ptr %76, i64 0, i32 2
  store i64 %.fca.3.2.load, ptr %80, align 8
  %81 = getelementptr inbounds ptr, ptr %77, i64 2
  store ptr %80, ptr %81, align 8
  %82 = getelementptr inbounds %.2, ptr %76, i64 0, i32 3
  store i64 %12, ptr %82, align 8
  %83 = getelementptr inbounds ptr, ptr %77, i64 3
  store ptr %82, ptr %83, align 8
  %84 = getelementptr inbounds %.2, ptr %76, i64 0, i32 4
  store ptr %23, ptr %84, align 8
  %85 = getelementptr inbounds ptr, ptr %77, i64 4
  store ptr %84, ptr %85, align 8
  %86 = alloca %.3, align 8
  %87 = alloca [14 x ptr], align 8
  store ptr %86, ptr %87, align 8
  %88 = getelementptr inbounds %.3, ptr %86, i64 0, i32 1
  store ptr %75, ptr %88, align 8
  %89 = getelementptr inbounds ptr, ptr %87, i64 1
  store ptr %88, ptr %89, align 8
  %90 = getelementptr inbounds %.3, ptr %86, i64 0, i32 2
  store i64 1, ptr %90, align 8
  %91 = getelementptr inbounds ptr, ptr %87, i64 2
  store ptr %90, ptr %91, align 8
  %92 = getelementptr inbounds %.3, ptr %86, i64 0, i32 3
  store ptr @main_kernel_0_main_kStitch_divide__13_1_0___1w1r_kernel_name, ptr %92, align 8
  %93 = getelementptr inbounds ptr, ptr %87, i64 3
  store ptr %92, ptr %93, align 8
  %94 = getelementptr inbounds %.3, ptr %86, i64 0, i32 4
  store i64 %74, ptr %94, align 8
  %95 = getelementptr inbounds ptr, ptr %87, i64 4
  store ptr %94, ptr %95, align 8
  %96 = getelementptr inbounds %.3, ptr %86, i64 0, i32 5
  store i64 1, ptr %96, align 8
  %97 = getelementptr inbounds ptr, ptr %87, i64 5
  store ptr %96, ptr %97, align 8
  %98 = getelementptr inbounds %.3, ptr %86, i64 0, i32 6
  store i64 1, ptr %98, align 8
  %99 = getelementptr inbounds ptr, ptr %87, i64 6
  store ptr %98, ptr %99, align 8
  %100 = getelementptr inbounds %.3, ptr %86, i64 0, i32 7
  store i64 256, ptr %100, align 8
  %101 = getelementptr inbounds ptr, ptr %87, i64 7
  store ptr %100, ptr %101, align 8
  %102 = getelementptr inbounds %.3, ptr %86, i64 0, i32 8
  store i64 1, ptr %102, align 8
  %103 = getelementptr inbounds ptr, ptr %87, i64 8
  store ptr %102, ptr %103, align 8
  %104 = getelementptr inbounds %.3, ptr %86, i64 0, i32 9
  store i64 1, ptr %104, align 8
  %105 = getelementptr inbounds ptr, ptr %87, i64 9
  store ptr %104, ptr %105, align 8
  %106 = getelementptr inbounds %.3, ptr %86, i64 0, i32 10
  store i32 0, ptr %106, align 8
  %107 = getelementptr inbounds ptr, ptr %87, i64 10
  store ptr %106, ptr %107, align 8
  %108 = getelementptr inbounds %.3, ptr %86, i64 0, i32 11
  store ptr null, ptr %108, align 8
  %109 = getelementptr inbounds ptr, ptr %87, i64 11
  store ptr %108, ptr %109, align 8
  %110 = getelementptr inbounds %.3, ptr %86, i64 0, i32 12
  store i32 5, ptr %110, align 8
  %111 = getelementptr inbounds ptr, ptr %87, i64 12
  store ptr %110, ptr %111, align 8
  %112 = getelementptr inbounds %.3, ptr %86, i64 0, i32 13
  store ptr %77, ptr %112, align 8
  %113 = getelementptr inbounds ptr, ptr %87, i64 13
  store ptr %112, ptr %113, align 8
  %114 = load ptr, ptr %0, align 8
  %115 = load ptr, ptr %9, align 8
  store ptr %114, ptr %86, align 8
  call void %115(ptr %114, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %87)
  br label %116

116:                                              ; preds = %33, %71
  %117 = alloca %.4, align 8
  %118 = alloca [11 x ptr], align 8
  store ptr %117, ptr %118, align 8
  %119 = getelementptr inbounds %.4, ptr %117, i64 0, i32 1
  store i64 0, ptr %119, align 8
  %120 = getelementptr inbounds ptr, ptr %118, i64 1
  store ptr %119, ptr %120, align 8
  %121 = getelementptr inbounds %.4, ptr %117, i64 0, i32 2
  store ptr %23, ptr %121, align 8
  %122 = getelementptr inbounds ptr, ptr %118, i64 2
  store ptr %121, ptr %122, align 8
  %123 = getelementptr inbounds %.4, ptr %117, i64 0, i32 3
  store ptr %23, ptr %123, align 8
  %124 = getelementptr inbounds ptr, ptr %118, i64 3
  store ptr %123, ptr %124, align 8
  %125 = getelementptr inbounds %.4, ptr %117, i64 0, i32 4
  store i64 0, ptr %125, align 8
  %126 = getelementptr inbounds ptr, ptr %118, i64 4
  store ptr %125, ptr %126, align 8
  %127 = getelementptr inbounds %.4, ptr %117, i64 0, i32 5
  store i64 %.fca.3.0.load, ptr %127, align 8
  %128 = getelementptr inbounds ptr, ptr %118, i64 5
  store ptr %127, ptr %128, align 8
  %129 = getelementptr inbounds %.4, ptr %117, i64 0, i32 6
  store i64 %.fca.3.1.load, ptr %129, align 8
  %130 = getelementptr inbounds ptr, ptr %118, i64 6
  store ptr %129, ptr %130, align 8
  %131 = getelementptr inbounds %.4, ptr %117, i64 0, i32 7
  store i64 %.fca.3.2.load, ptr %131, align 8
  %132 = getelementptr inbounds ptr, ptr %118, i64 7
  store ptr %131, ptr %132, align 8
  %133 = getelementptr inbounds %.4, ptr %117, i64 0, i32 8
  store i64 %13, ptr %133, align 8
  %134 = getelementptr inbounds ptr, ptr %118, i64 8
  store ptr %133, ptr %134, align 8
  %135 = getelementptr inbounds %.4, ptr %117, i64 0, i32 9
  store i64 %.fca.3.2.load, ptr %135, align 8
  %136 = getelementptr inbounds ptr, ptr %118, i64 9
  store ptr %135, ptr %136, align 8
  %137 = getelementptr inbounds %.4, ptr %117, i64 0, i32 10
  store i64 1, ptr %137, align 8
  %138 = getelementptr inbounds ptr, ptr %118, i64 10
  store ptr %137, ptr %138, align 8
  %139 = load ptr, ptr %0, align 8
  %140 = load ptr, ptr %9, align 8
  store ptr %139, ptr %117, align 8
  call void %140(ptr %139, ptr nonnull @ral_send_output___cpu___pvoid_i64_m3df32___void, ptr nonnull %118)
  ret void
}

object file to shared library command: gcc --shared -o /root/.cache/bazel/_bazel_root/4cf787dd368afbf575ff6140b2d24691/execroot/org_tensorflow/_tmp/e877672218cbaf0733fb74fb1644a598FullyDynamicShape3DF32_0.so /root/.cache/bazel/_bazel_root/4cf787dd368afbf575ff6140b2d24691/execroot/org_tensorflow/_tmp/e877672218cbaf0733fb74fb1644a598FullyDynamicShape3DF32_0.so.o
save shared lib file to : /root/.cache/bazel/_bazel_root/4cf787dd368afbf575ff6140b2d24691/execroot/org_tensorflow/_tmp/e877672218cbaf0733fb74fb1644a598FullyDynamicShape3DF32_0.so

============ END ============

2022-08-24 06:21:51.037946: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-24 06:21:51.038944: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:925] --- MLIR Execution uses: 0.966 ms
2022-08-24 06:21:51.039024: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:183] out buffer = 0x7f6ef2a1ac00
2022-08-24 06:21:51.039029: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:184] out shape:
2022-08-24 06:21:51.039032: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:186]   dim #0: 13
2022-08-24 06:21:51.039034: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:186]   dim #1: 21
2022-08-24 06:21:51.039036: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:186]   dim #2: 100
2022-08-24 06:21:51.093711: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:264] Executed: tensorflow/compiler/mlir/tf-mlir-translate -mlir-to-graphdef /root/.cache/bazel/_bazel_root/4cf787dd368afbf575ff6140b2d24691/execroot/org_tensorflow/_tmp/e877672218cbaf0733fb74fb1644a598/tempfile-330bb2f99ddd-844a7b9d-16839-5e6f6b239176e -o /root/.cache/bazel/_bazel_root/4cf787dd368afbf575ff6140b2d24691/execroot/org_tensorflow/_tmp/e877672218cbaf0733fb74fb1644a598FullyDynamicShape3DF32_0.pbtxt 
2022-08-24 06:21:51.093721: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:265] tensorflow/compiler/mlir/tf-mlir-translate: 0
2022-08-24 06:21:51.093723: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:266] -- stdout:

============ END ============

2022-08-24 06:21:51.093725: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:268] -- stderr:

============ END ============

2022-08-24 06:21:51.093733: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:384] graphdef_path: /root/.cache/bazel/_bazel_root/4cf787dd368afbf575ff6140b2d24691/execroot/org_tensorflow/_tmp/e877672218cbaf0733fb74fb1644a598FullyDynamicShape3DF32_0.pbtxt
2022-08-24 06:21:51.094896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-08-24 06:21:51.096562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-24 06:21:51.205001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-24 06:21:51.205662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-24 06:21:51.519049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-24 06:21:51.519823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-24 06:21:51.520469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-24 06:21:51.521065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20834 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:00:07.0, compute capability: 8.6
2022-08-24 06:21:51.539342: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:460] --- TF Execution uses: 4.192 ms
2022-08-24 06:21:51.539362: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:467] 	output shape #0: [13,21,100]
2022-08-24 06:21:51.539686: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:460] --- TF Execution uses: 0.318 ms
2022-08-24 06:21:51.539689: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:467] 	output shape #0: [13,21,100]
2022-08-24 06:21:51.539911: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:460] --- TF Execution uses: 0.218 ms
2022-08-24 06:21:51.539915: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:467] 	output shape #0: [13,21,100]
2022-08-24 06:21:51.540141: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:460] --- TF Execution uses: 0.222 ms
2022-08-24 06:21:51.540144: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:467] 	output shape #0: [13,21,100]
2022-08-24 06:21:51.540363: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:460] --- TF Execution uses: 0.214 ms
2022-08-24 06:21:51.540366: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:467] 	output shape #0: [13,21,100]
2022-08-24 06:21:51.540372: I tensorflow/compiler/mlir/disc/tests/mlir_test.cc:477] processing output 0
[       OK ] TFSoftmaxOpTest.FullyDynamicShape3DF32 (1558 ms)
[----------] 1 test from TFSoftmaxOpTest (1558 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (1558 ms total)
[  PASSED  ] 1 test.
