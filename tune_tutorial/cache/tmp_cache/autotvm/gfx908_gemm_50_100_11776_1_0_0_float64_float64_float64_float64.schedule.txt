@main = primfn(data_A_1: handle, data_B_1: handle, T_matmul_TN_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {T_matmul_TN: Buffer(T_matmul_TN_2: Pointer(float64), float64, [50, 100], []),
             data_A: Buffer(data_A_2: Pointer(float64), float64, [11776, 50], []),
             data_B: Buffer(data_B_2: Pointer(float64), float64, [11776, 100], [])}
  buffer_map = {data_A_1: data_A, data_B_1: data_B, T_matmul_TN_1: T_matmul_TN} {
  allocate(T_matmul_TN.rf: Pointer(global float64), float64, [640000]), storage_scope = global {
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 128;
    allocate(data_A.shared: Pointer(shared float64), float64, [184]), storage_scope = shared;
    allocate(T_matmul_TN.rf.local: Pointer(local float64), float64, [1]), storage_scope = local;
    attr [IterVar(blockIdx.y: int32, (nullptr), "ThreadIndex", "blockIdx.y")] "thread_extent" = 25;
    attr [IterVar(blockIdx.z: int32, (nullptr), "ThreadIndex", "blockIdx.z")] "thread_extent" = 2 {
      for (ax0.outer: int32, 0, 2) {
        attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 64;
        for (ax1: int32, 0, 2) {
          if @tir.likely((((ax0.outer*64) + threadIdx.x) < 92), dtype=bool) {
            if @tir.likely(((((blockIdx.x*92) + (ax0.outer*64)) + threadIdx.x) < 11776), dtype=bool) {
              data_A.shared[(((ax0.outer*128) + (threadIdx.x*2)) + ax1)] = (float64*)data_A_2[(((((blockIdx.x*4600) + (ax0.outer*3200)) + (threadIdx.x*50)) + (blockIdx.y*2)) + ax1)]
            }
          }
        }
      }
      attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 2;
      attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 64 {
        T_matmul_TN.rf.local[0] = 0f64
        for (k.inner: int32, 0, 92) {
          if @tir.likely((((blockIdx.z*64) + threadIdx.x_1) < 100), dtype=bool) {
            T_matmul_TN.rf.local[0] = ((float64*)T_matmul_TN.rf.local[0] + ((float64*)data_A.shared[((k.inner*2) + threadIdx.y)]*(float64*)data_B_2[((((blockIdx.x*9200) + (k.inner*100)) + (blockIdx.z*64)) + threadIdx.x_1)]))
          }
        }
        if @tir.likely((((blockIdx.z*64) + threadIdx.x_1) < 100), dtype=bool) {
          T_matmul_TN.rf[(((((blockIdx.x*5000) + (blockIdx.y*200)) + (threadIdx.y*100)) + (blockIdx.z*64)) + threadIdx.x_1)] = (float64*)T_matmul_TN.rf.local[0]
        }
      }
    }
    attr [IterVar(blockIdx.y_1: int32, (nullptr), "ThreadIndex", "blockIdx.y")] "thread_extent" = 50;
    allocate(T_matmul_TN.repl.rf: Pointer(local float64), float64, [1]), storage_scope = local;
    allocate(reduce_temp0: Pointer(local float64), float64, [1]), storage_scope = local;
    attr [IterVar(blockIdx.x_1: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 7;
    attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 16;
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 16 {
      T_matmul_TN.repl.rf[0] = 0f64
      for (k.outer.v.inner: int32, 0, 8) {
        if @tir.likely((((blockIdx.x_1*16) + threadIdx.y_1) < 100), dtype=bool) {
          T_matmul_TN.repl.rf[0] = ((float64*)T_matmul_TN.repl.rf[0] + (float64*)T_matmul_TN.rf[(((((threadIdx.x_2*40000) + (k.outer.v.inner*5000)) + (blockIdx.y_1*100)) + (blockIdx.x_1*16)) + threadIdx.y_1)])
        }
      }
      attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
      @tir.tvm_thread_allreduce(1u32, (float64*)T_matmul_TN.repl.rf[0], True, reduce_temp0, threadIdx.x_2, dtype=handle)
      T_matmul_TN_2[(((blockIdx.y_1*100) + (blockIdx.x_1*16)) + threadIdx.y_1)] = (float64*)reduce_temp0[0]
    }
  }
}


